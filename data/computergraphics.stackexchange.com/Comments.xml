<?xml version="1.0" encoding="utf-8"?>
<comments>
  <row Id="1" PostId="4" Score="2" Text="You could post the relevant shader code here in case of bitrot, no?" CreationDate="2015-08-04T18:17:21.623" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2" PostId="3" Score="6" Text="Really, it depends on many factors. NVIDIA provides hardware acceleration for vector graphics. Have you seen it? https://developer.nvidia.com/nv-path-rendering-videos" CreationDate="2015-08-04T18:18:45.080" UserId="36" ContentLicense="CC BY-SA 3.0" />
  <row Id="3" PostId="4" Score="1" Text="we should have some prettier code markup for shader code, i'll post on meta if someone hasn't beaten me to it!" CreationDate="2015-08-04T18:21:29.027" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="4" PostId="5" Score="0" Text="A crude solution would be to just paint one contrasting color onto another through the stencil, save that buffer out, and count the number of pixels that were altered." CreationDate="2015-08-04T18:28:55.233" UserId="36" ContentLicense="CC BY-SA 3.0" />
  <row Id="5" PostId="5" Score="0" Text="Hmm, the specification says *occulsion queries* count the number of fragments that pass the *depth test*, but off the top of my head I'm not sure how that interacts with the stencil test right now." CreationDate="2015-08-04T18:33:09.697" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="6" PostId="6" Score="9" Text="I'd want more than &quot;A recommendation of a solid book&quot; as this question should be given a direct answer by someone who knows about this. We should try to put information on the site instead of pointing to it off-site." CreationDate="2015-08-04T18:34:42.330" UserId="27" ContentLicense="CC BY-SA 3.0" />
  <row Id="7" PostId="6" Score="0" Text="@robobenklein Question edited, thought better be careful ;)" CreationDate="2015-08-04T18:37:46.930" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="11" PostId="6" Score="0" Text="@ChristianRau Removing &quot;Thanks&quot; should discussed in a meta, this is different on every stack exchange site..." CreationDate="2015-08-04T18:41:36.920" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="12" PostId="6" Score="1" Text="@poor No, it isn't really. That's something that nowhere ever changes. And as long as we don't have special rules, we employ the general SE ones anyway. But even then, I can hardly imagine anyone for voting this to be allowed here. I've never seen this being good practice on any other SE site." CreationDate="2015-08-04T18:43:15.267" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="18" PostId="10" Score="1" Text="See also http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf" CreationDate="2015-08-04T19:00:48.043" UserId="34" ContentLicense="CC BY-SA 3.0" />
  <row Id="19" PostId="10" Score="0" Text="yeah that's a very cool technique, that's the second technique i mentioned, and i link to that same pdf above as well." CreationDate="2015-08-04T19:02:10.423" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="20" PostId="10" Score="0" Text="Woops, missed it, sorry." CreationDate="2015-08-04T19:03:04.390" UserId="34" ContentLicense="CC BY-SA 3.0" />
  <row Id="24" PostId="10" Score="1" Text="An adaptive solution for the edge anti-aliasing of distance fields can be found here: http://www.essentialmath.com/blog/?p=151." CreationDate="2015-08-04T19:35:15.710" UserId="1" ContentLicense="CC BY-SA 3.0" />
  <row Id="28" PostId="19" Score="2" Text="I'm voting to close this question as off-topic because it doesn't pertain to computer graphics programming specifically." CreationDate="2015-08-04T19:43:58.817" UserId="71" ContentLicense="CC BY-SA 3.0" />
  <row Id="29" PostId="19" Score="0" Text="VTC for more visibility; should be discussed. (Although I would love to know the answer either way)." CreationDate="2015-08-04T19:44:27.143" UserId="71" ContentLicense="CC BY-SA 3.0" />
  <row Id="30" PostId="19" Score="3" Text="@Qix [Discussion on Meta.](http://meta.computergraphics.stackexchange.com/q/23/16)" CreationDate="2015-08-04T19:46:55.293" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="32" PostId="21" Score="0" Text="It's one character, but `cleaver` -&gt; `clever`. I can't edit it D: Great answer by the way, this makes the most sense so far." CreationDate="2015-08-04T19:54:51.953" UserId="71" ContentLicense="CC BY-SA 3.0" />
  <row Id="35" PostId="25" Score="0" Text="Additionally I also sometimes have a command that will dump a specified fbo out to an image when I press a key. It's the low-tech way." CreationDate="2015-08-04T20:33:29.797" UserId="125" ContentLicense="CC BY-SA 3.0" />
  <row Id="36" PostId="27" Score="0" Text="This article is much more &quot;gamer focused&quot; but it might give you some insights... http://www.pcgamer.com/what-directx-12-means-for-gamers-and-developers/" CreationDate="2015-08-04T22:18:49.450" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="37" PostId="5" Score="0" Text="@ChristianRau It seems that only the fragments that pass the depth tests will be counted, but stencil, discard and alpha tests are ignored." CreationDate="2015-08-04T22:54:39.427" UserId="64" ContentLicense="CC BY-SA 3.0" />
  <row Id="38" PostId="31" Score="1" Text="Just curious, what is your reason for wanting jpg, is it the smaller file size? In case it helps your specific situation, do you know about dxt compression or distance field textures?" CreationDate="2015-08-05T03:20:34.207" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="39" PostId="15" Score="0" Text="At SIGGRAPH 2014, in advances in real-time rendering, there was talk of subdivision used in call of duty. I don't remember the specifics but there is probably some good info there for you!" CreationDate="2015-08-05T03:21:46.453" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="40" PostId="40" Score="1" Text="Looking at your other answer, it looks like your math background is better than I was working with, but I hope that it still goes into sufficient detail to be helpful. I wanted it to be useful for people of any background coming into this." CreationDate="2015-08-05T04:34:09.823" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="41" PostId="40" Score="1" Text="If you are talking to me, not at all.  Your answer and Bert's are amazingly enlightening.  Thank you so much!  Gotta digest the info a bit now (:" CreationDate="2015-08-05T05:03:31.397" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="42" PostId="38" Score="3" Text="Really interesting question, and though I suspect most of the nitty-gritty details are secret sauce, this might be a good jumping off point for anyone who wants to do the research and write up a summary: http://blog.imgtec.com/powervr/a-look-at-the-powervr-graphics-architecture-tile-based-rendering" CreationDate="2015-08-05T06:05:53.240" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="43" PostId="11" Score="5" Text="I don't know for certain, but I would definitely suspect that glTexSubImage on a texture that has been rendered in the last frame or two will stall the pipeline, since PC drivers often try to buffer up a frame or two, and are not likely to want to make copies of entire textures because of a tiny update. So I would expect double- or triple-buffering the textures (or pixel buffer objects) to be required for maximum performance." CreationDate="2015-08-05T06:14:42.460" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="44" PostId="5" Score="2" Text="@ChristianRau and Maurice, the original [ARB_occlusion_query](https://www.opengl.org/registry/specs/ARB/occlusion_query.txt) spec explicitly says it counts samples passing both depth *and* stencil tests, though. See also [this StackOverflow question](https://stackoverflow.com/questions/26410637/occlusion-queries-with-stencil-test-only)." CreationDate="2015-08-05T06:19:36.760" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="45" PostId="15" Score="4" Text="This sounds like a question best answered by a survey paper on subdivision surfaces, and indeed, searching Google for &quot;subdivison surfaces survey&quot; brings up a number of relevant publications. For example, &quot;Algorithms for direct evaluation [Sta98, ZK02], editing [BKZ01, BMBZ02, BMZB02, BLZ00], texturing [PB00], and conversion&#xA;to other popular representations [Pet00] have been devised and hardware support for rendering of subdivision surfaces has been proposed [BAD+01, BKS00, PS96]&quot; —[Boier-Martin et al., 2005](http://mrl.nyu.edu/~dzorin/papers/boiermartin2005sbt.pdf)." CreationDate="2015-08-05T07:09:06.853" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="46" PostId="15" Score="1" Text="&quot;We also examine the reason for the low adoption of new schemes with theoretical advantages, [and] explain why Catmull–Clark surfaces have become a de facto standard in geometric modelling&quot; —[Cashman, 2011](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2011.02083.x/abstract)." CreationDate="2015-08-05T07:12:13.487" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="48" PostId="31" Score="1" Text="It's unclear to me what your question is. Do you want to know whether it's good to compress using JPEG? Do you want to know what kinds of images compress well with JPEG? Or, are you already using JPEG and you'd like to know how to author your images in order to minimize the artifacts caused by JPEG?" CreationDate="2015-08-05T07:31:18.370" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="49" PostId="28" Score="1" Text="As an extra note OpenGL generally only works on one thread so a graphics intensive app could max out one core. Something like Vulkan allows multiple threads to dispatch commands to a queue which means many graphics calls can be made from multiple threads." CreationDate="2015-08-05T07:46:37.030" UserId="214" ContentLicense="CC BY-SA 3.0" />
  <row Id="50" PostId="51" Score="0" Text="Maybe triplanar projection, [as seen in GPU Gems 3](http://http.developer.nvidia.com/GPUGems3/gpugems3_ch01.html)? The question is whether the noise would look blurry or otherwise undesirable when it's blending between different projections." CreationDate="2015-08-05T08:37:33.923" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="53" PostId="52" Score="1" Text="It's also easy to generate an SDF from a voxel model." CreationDate="2015-08-05T09:10:38.670" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="54" PostId="31" Score="0" Text="@AlanWolfe I have encountered a few occasions that I was only able to use JPEG (mostly in web apps), and that's why I needed it to be in JPEG. Thanks, but I was not familiar with dxt compression nor distance field textures, from what I have seen in [wikipedia](https://en.wikipedia.org/wiki/S3_Texture_Compression), dxt algorithms are different from the ones used in JPEG, do you mean they can be used to create JPEGs?" CreationDate="2015-08-05T10:23:18.237" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="55" PostId="31" Score="0" Text="@Moshoka thanks, it's more related to your last question: how to minimize the &quot;visual impact&quot; brought by artifacts in JPEG images?" CreationDate="2015-08-05T10:24:46.870" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="56" PostId="48" Score="0" Text="Thank you very much for your detailed explanation Nathan +1. I understand the compression algorithms have their limitations for JPEG, but I just wanted to find out if there's a right balance of the amount of compression together with other options (when saving it) that can make the artifacts less noticeable. I edited my question with samples." CreationDate="2015-08-05T10:51:02.013" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="57" PostId="57" Score="0" Text="Are you looking for a more accurate way to sample from a distribution, without montecarlo test i mean? If you don't care much about the computational approach (i.e. you're looking for an accurate approach rather than the computational effort) i could have a solution, but it could be optimized of course." CreationDate="2015-08-05T11:33:16.990" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="58" PostId="57" Score="3" Text="Do you know the analytical function or can you only sample it? Do you know its analytical derivative?" CreationDate="2015-08-05T12:04:10.887" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="60" PostId="5" Score="0" Text="@NathanReed Sounds like you're about to write an answer." CreationDate="2015-08-05T14:00:55.260" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="61" PostId="57" Score="0" Text="@JulienGuertault Does my edit clarify?" CreationDate="2015-08-05T14:07:56.533" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="62" PostId="57" Score="0" Text="@Lukkio I'd like accuracy first, then optimisation can come later once the approach is working." CreationDate="2015-08-05T14:08:44.487" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="63" PostId="29" Score="0" Text="Although all cubic splines are, in a sense, equivalent, it's probably conceptually easier to use Catmull-Rom splines. e.g.  https://www.cs.cmu.edu/~462/projects/assn2/assn2/catmullRom.pdf" CreationDate="2015-08-05T14:51:27.773" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="64" PostId="63" Score="0" Text="So, if `dFdx(p) = p(x1) - p(x)`, then `x1` can be either `(x+1)` or `(x-1)`, depending on the position of pixel `x` in the quad. Either way, `x1` has to be in the same warp/wavefront as `x`. Am I correct?" CreationDate="2015-08-05T15:44:31.643" UserId="88" ContentLicense="CC BY-SA 3.0" />
  <row Id="65" PostId="63" Score="3" Text="@ApoorvaJ Essentially the same value for `dFdx` is computed for each of the 2 neighbouring pixels in the 2x2 grid. And this value is just computed using the difference between the two neighbor values, if that is `p(x+1)-p(x)` or `p(x)-p(x-1)` just depends on your notion of what `x` is exactly here. The result is the same however. So yeah, you're correct." CreationDate="2015-08-05T15:46:43.313" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="66" PostId="63" Score="0" Text="@ChristianRau That answers my question. Thanks." CreationDate="2015-08-05T15:47:59.020" UserId="88" ContentLicense="CC BY-SA 3.0" />
  <row Id="67" PostId="29" Score="0" Text="Do you think the tau parameter helps or hinders in this case?  I might be wrong but i feel like catmull rom is too &quot;user defined&quot; (and must be tuned), whereas the hermite spline attempts to just use information from the data that's there.  It seems like cubic hermite is closer to a &quot;ground truth&quot;, which i guess would be something like an ideal sinc filter.  What do you think though?" CreationDate="2015-08-05T15:57:32.830" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="68" PostId="53" Score="1" Text="So my question is how do we do this on modern GPUs? I don't think there's any way to query the scanout, and it seems to me you can't really submit per-scanline draw calls. Even if you could -- what guarantees do you have that your draws are going to get there before the scanout?" CreationDate="2015-08-05T16:10:53.227" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="70" PostId="68" Score="0" Text="This doesn't sound like Computer Graphics as our scope intended it to be, this site is specifically oriented towards 3d, rendering, simulation, and the process of viewing them. This sounds like it belongs on graphic design." CreationDate="2015-08-05T17:15:14.783" UserId="27" ContentLicense="CC BY-SA 3.0" />
  <row Id="71" PostId="68" Score="0" Text="@robobenklein this would most likely get rejected from graphics design, and its borderline. Besides gamma is about viewing your art" CreationDate="2015-08-05T17:41:02.177" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="72" PostId="72" Score="0" Text="Do we have latex yet?" CreationDate="2015-08-05T17:53:28.320" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="73" PostId="67" Score="1" Text="To come up with this question, you must have had some advantages of hexagonal arrangement of pixels. Can you add those to the question?" CreationDate="2015-08-05T17:54:03.760" UserId="180" ContentLicense="CC BY-SA 3.0" />
  <row Id="74" PostId="72" Score="2" Text="No, but [hopefully we will in the future](http://meta.computergraphics.stackexchange.com/q/18/6). Feel free to add you answer as an example for its necessity, though." CreationDate="2015-08-05T18:18:01.940" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="75" PostId="68" Score="5" Text="I'd say this post is on topic. It seems to relate heavily to GPU Gems 3: Chapter 24. The Importance of Being Linear&#xA;http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html" CreationDate="2015-08-05T18:30:29.643" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="76" PostId="68" Score="0" Text="Ah actually i lied... This question would be welcome to GD.SE because a similar is there. IF i can get to it first, if im away its most likely gets modded to oblivion." CreationDate="2015-08-05T18:32:39.813" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="77" PostId="68" Score="0" Text="@AlanWolfe i used to own that book, somebody at work stole it from my hand reference." CreationDate="2015-08-05T18:37:02.960" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="78" PostId="74" Score="2" Text="You dont need to store each color plane in each pixel even, or have the planes in same configuration.  BUT you loose separable filtering" CreationDate="2015-08-05T19:07:42.360" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="79" PostId="73" Score="1" Text="No latex support yet." CreationDate="2015-08-05T19:13:27.257" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="80" PostId="74" Score="0" Text="Good points! Separable filtering is pretty big.  I wonder if you could do 3 axis filtering for hexagons?" CreationDate="2015-08-05T19:16:29.310" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="81" PostId="68" Score="5" Text="Whether something is on topic elsewhere is not relevant to this decision. We just need to know whether it's on topic *here*." CreationDate="2015-08-05T19:20:25.787" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="82" PostId="68" Score="1" Text="I'm deliberately posting borderline questions to measure where the border is. Please raise as many uncertainties as possible on Meta." CreationDate="2015-08-05T19:21:00.567" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="83" PostId="72" Score="0" Text="If you want to show how your answer *would* look if we had MathJax, to help make the case, you can use [mathurl.com](http://mathurl.com)." CreationDate="2015-08-05T19:36:50.790" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="84" PostId="68" Score="3" Text="Being welcome elsewhere is *never* a proper reason alone to declare something off-topic *here*." CreationDate="2015-08-05T19:43:17.330" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="85" PostId="67" Score="0" Text="@nitishch it was only the one advantage I had in mind. Does my edit get it across better?" CreationDate="2015-08-05T19:57:54.847" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="86" PostId="68" Score="2" Text="@robobenklein nowhere in this stack's scope states this stack is for 3D graphics specifically." CreationDate="2015-08-05T20:03:42.837" UserId="71" ContentLicense="CC BY-SA 3.0" />
  <row Id="87" PostId="68" Score="1" Text="Such discussions are important to have - that's what this private beta is all about. So I've raised it on [meta](http://meta.computergraphics.stackexchange.com/questions/35/are-questions-about-gamma-on-topic)" CreationDate="2015-08-05T20:06:19.210" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="88" PostId="53" Score="1" Text="@Mokosha Correct, there's no way to query the scanout directly AFAIK. At best, you can figure out when vsync is (via some OS signal) and estimate where the scanout is by timing relative to that (knowing details of the video mode). For rendering, you can experiment to find out how long it usually takes between glFlush and when the rendering is done, and make some guesses based on that. Ultimately, you have to build in some slack in your timing in case of error (e.g. stay 2-3 ms ahead of scanout), and accept that there will be probably be occasional artifacts." CreationDate="2015-08-05T20:39:58.377" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="90" PostId="75" Score="0" Text="I remember that answer last time round :) (I was githubphagocyte back then). Interesting to see it expanded into a blog post." CreationDate="2015-08-05T21:01:28.423" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="91" PostId="62" Score="3" Text="I'd never heard of Fibonacci grids before; that's pretty cool!" CreationDate="2015-08-05T21:09:35.380" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="92" PostId="73" Score="0" Text="I was looking for something that could be used with an implicit surface even if it doesn't have a parameterisation. Is it always possible to parameterise an implicit surface if the derivative is known?" CreationDate="2015-08-05T21:10:49.550" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="93" PostId="73" Score="0" Text="Any questions that would benefit from MathJax for formulae can be added to [this meta answer](http://meta.computergraphics.stackexchange.com/a/34/231) to increase our chances of getting MathJax. (This one has already been added.)" CreationDate="2015-08-05T21:12:23.440" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="94" PostId="73" Score="0" Text="Remember that what you need is the distribution function derived from the curvature, you said you can derive everything (by the way what kind of surface have you got? i.e. the equation). Anyway... what do you mean with &quot;derivative known&quot;? do you know an explicit formula of the derivative? or it is implicit too? (i.e. described by means of differential equation)?" CreationDate="2015-08-05T21:27:45.793" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="95" PostId="73" Score="1" Text="By the way... if the curve/surface is algebric (i mean expressed by polynomial or rational staff) there are computational methods based on bspline/nurbs that explain how to perform parametrization of such curves. I had a glance here http://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1827&amp;context=cstech, further method (even advanced) could be found in one of my favourite book on Nurbs (The NURBS book by Tiller)." CreationDate="2015-08-05T21:37:28.987" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="96" PostId="4" Score="0" Text="Is that a specific shader language not available in the list of languages covered by our syntax highlighting?" CreationDate="2015-08-05T22:24:19.397" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="97" PostId="4" Score="0" Text="I'm not sure.  It's just GLSL (from webgl to be exact!).  I just did 4 spaces before each line of code, not sure if there's a better way to mark it up..." CreationDate="2015-08-05T22:37:41.687" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="98" PostId="53" Score="0" Text="The effect of increased latency is due to vsync, which causes the front and backbuffer swaps to synchronize with the vblank of the monitor. Double buffering itself doesn't cause this issue by itself and it is useful to minimize flickering because a pixel can only change once in the front buffer." CreationDate="2015-08-06T00:29:21.403" UserId="64" ContentLicense="CC BY-SA 3.0" />
  <row Id="99" PostId="57" Score="0" Text="@trichoplax: yes. Now thinking about how it can be used." CreationDate="2015-08-06T05:01:17.340" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="100" PostId="80" Score="0" Text="Wider gamut monitors will also start to be commonplace in near future. For example I do not calibrate to sRGB on my art worstation, but the profile to profile converter makes images still look the same as on my dev machine." CreationDate="2015-08-06T06:41:08.183" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="101" PostId="81" Score="1" Text="It is indeed a complicated paper (still gives me nightmares from time to time!) but I tried to simplify the whole thing a bit. Let me know if you think the below answer should be adjusted in a way or another :)" CreationDate="2015-08-06T08:29:37.570" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="102" PostId="29" Score="0" Text="I don't see how Catmull-Rom is &quot;user defined&quot;. Once you have a sequence of 4 contiguous points, P[i-1], P[i], P[i+1], P[i+2] (4x4 for 2D case) the curve segment is defined between P[i] and P[i+1] and is C1 continuous with the neighbouring segments. &#xA;&#xA;A sinc filter is fine for audio but not video. See Mitchell &amp; Netravali: https://www.cs.utexas.edu/~fussell/courses/cs384g-fall2013/lectures/mitchell/Mitchell.pdf   &#xA;IIRC Catmull-Rom is a special case of the family of filters they propose, but I think that filter is an approximating curve so, unlike C-R, might not go through the original points." CreationDate="2015-08-06T09:24:46.650" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="103" PostId="83" Score="0" Text="Do you mean you have both the pre and post gamma images and you want to find the gamma applied?" CreationDate="2015-08-06T13:55:01.850" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="104" PostId="83" Score="0" Text="@cifz Yes, the original image is from [trichoplax's profile](http://i.stack.imgur.com/iukw5.png?s=328&amp;g=1)." CreationDate="2015-08-06T13:56:52.307" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="105" PostId="29" Score="0" Text="That's how he hermite spline works except that the catmull rom spline has an additional parameter tau (tension) that is user defined.  Also, sinc does apply to video, DSP is DSP :P" CreationDate="2015-08-06T14:04:06.173" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="106" PostId="84" Score="3" Text="Though, I think part of his problem as posed in the question is also that he might not know if it is gamma corrected in any way at all or if the colors were not otherwise (linearly or whatever) modified in contrast to mere gamma correction. But ok, in this case you just take a bigger sample size and try to see if it can be approximated sufficiently well with a gamma transformation." CreationDate="2015-08-06T14:07:57.013" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="107" PostId="67" Score="0" Text="The DSP stack exchange (signal processing) probably would have a more formal answer for you on this BTW." CreationDate="2015-08-06T14:16:31.293" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="108" PostId="84" Score="0" Text="Exactly @ChristianRau, ideally it is to determine the difference even when other color transformations were applied. Thanks cifz, so if you sample several of each image's pixels and the resulting G is approximately 1, then we can conclude that no gamma correction was made?" CreationDate="2015-08-06T14:27:07.320" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="109" PostId="29" Score="0" Text="I must admit, I've never seen a tension parameter associated with Catmull Rom splines before, but then I've really only learned of them via Foley &amp; van Dam (et al) or, say, Watt &amp; Watt which, AFAICR, make no mention of such.&#xA;&#xA;Actually, having said that, given there are four constraints - i.e. the curve has to pass through 2 points and have two defined tangents** at those points and it's a cubic - I'm at a bit of a loss as to how there's any more degrees of freedom to support a tension parameter ....&#xA;&#xA;**Unless you mean the tangents can be scaled?" CreationDate="2015-08-06T14:29:47.557" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="110" PostId="29" Score="0" Text="The tension parameter is shown in that first paper you linked.  And yeah I just mean, the fact that there's a tunable parameter means it must be tuned :p. There is a value of tau you can use such that you end up with a regular cubic hermite spline, and I think it might be &quot;1&quot; but not 100% sure on that." CreationDate="2015-08-06T14:34:42.063" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="111" PostId="84" Score="0" Text="If also the other transformations are unknown, then to my limited knowledge I don't know how and if you can find the gamma. Intuitively I'd say you can't" CreationDate="2015-08-06T14:35:32.827" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="112" PostId="84" Score="1" Text="As @ChristianRau correctly said, you can try and fit the combination of transformations into a gamma function, but that will not tell you what gamma was applied on top of the other unknown transform, but rather an gamma that once applied to the source will give you roughly your destination" CreationDate="2015-08-06T14:45:00.293" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="113" PostId="86" Score="3" Text="There is a authoritatively best filter, its a infinitely wide sinc filter. Its just not possible to use it. untill that time lanczos windowed sinc is a good alternative to michell" CreationDate="2015-08-06T15:38:39.483" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="114" PostId="69" Score="1" Text="This is a really good question that has practical implications as realtime interactive stereo rendering is becoming more prevalent with VR" CreationDate="2015-08-06T16:33:09.960" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="115" PostId="82" Score="0" Text="Thanks! I'm still wondering a couple of things. (1) Even with the diffusion profile approximation, the BSSRDF model still requires integrating over a radius of nearby points on the surface to gather incoming light, correct? How is that accomplished in, say, a path tracer? Do you have to build some data structure so you can sample points on the surface nearby a given point? And (2) Why one positive and one negative light? Is the goal for them to cancel each other in some way?" CreationDate="2015-08-06T18:21:57.887" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="116" PostId="83" Score="1" Text="I don't know for certain that the [CC BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0/) licence applies to profile images, but I operate under the assumption that anything that I use as an avatar is automatically licensed that way, and in any case I'm very happy for the image to be reused :)" CreationDate="2015-08-06T18:26:47.613" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="117" PostId="83" Score="1" Text="[Meta Stack Exchange](http://meta.stackexchange.com/questions/176253/profile-images-with-copyright/176256#176256) suggests that profile images are also CC BY-SA 3.0 so as long as you give credit you should be OK using anyone's avatar (provided they complied with the requirement to not post works they don't have the right to...)." CreationDate="2015-08-06T18:42:05.193" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="119" PostId="82" Score="0" Text="1) Indeed, what they propose in the paper with their monte carlo ray tracer is a stochastic sampling with a specific density based on distance and extinction coefficient.  I guess you can dart-throw to find samples and use an appropriate acceptance probability based on the extinction coeff. and distance. (1/2)" CreationDate="2015-08-06T19:37:57.230" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="120" PostId="85" Score="1" Text="These two old articles from *The Inner Product* talk about filters for mipmap generation, which might be relevant to you: [Link1](http://number-none.com/product/Mipmapping,%20Part%201/index.html), [link2](http://number-none.com/product/Mipmapping,%20Part%202/index.html)." CreationDate="2015-08-06T19:38:03.663" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="121" PostId="82" Score="0" Text="I know that Jensen published a hierarchical integration approach in 2002 that unfortunately I just read once and a while ago, so I just remember few bits. The core concept was to decouple the sampling from the diffusion approximation and cluster distant samples. IIRC they used an octree as hierarchical structure. &#xA;&#xA;I never got myself into an offline implementation so I am not of so much help on other details on this I am  afraid.  (2/2)" CreationDate="2015-08-06T19:38:05.870" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="122" PostId="82" Score="0" Text="2) It is this way to satisfy some boundary conditions, you want to let the fluence become zero at a certain extrapolated boundary that has a specific distance from the medium. This distance is computed based on scattering coefficient and scattering albedo." CreationDate="2015-08-06T19:38:19.410" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="123" PostId="86" Score="0" Text="For reference, there's also the [Nvidia image tools](https://code.google.com/p/nvidia-texture-tools/)." CreationDate="2015-08-06T19:48:01.210" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="124" PostId="86" Score="0" Text="If you are using a cubic-esque or lanczos filter, do those guys work equally well for scaling up as they do for scaling down?" CreationDate="2015-08-06T20:14:41.767" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="125" PostId="91" Score="1" Text="A good answer is predicated on knowing how the displays will be connected to the available GPUs (getting 9+ display outputs on a single computer isn't trivial) and the software responsible for your render. Of course it's possible to combine the processing power of multiple GPUs to solve your problem, but the devil is in the details of how the outputs are connected and how many computers are involved." CreationDate="2015-08-06T22:58:52.010" UserId="259" ContentLicense="CC BY-SA 3.0" />
  <row Id="126" PostId="90" Score="0" Text="&quot;Laser light is a single frequency. White light however is light made up of the sum of different wavelengths of light.&quot;&#xA;(https://www.physicsforums.com/threads/not-hw-why-are-light-waves-in-the-form-of-the-sine-wave-instead-of-some-other-wave.347805/). One thing you would have to account for is the spectral nature of light." CreationDate="2015-08-07T04:17:34.387" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="127" PostId="82" Score="0" Text="@NathanReed Let me know if this has clarified something, otherwise I can try and expand the thoughts in this comment in the answer itself" CreationDate="2015-08-07T05:38:10.067" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="128" PostId="95" Score="3" Text="You could checkout logarithmic depth buffers. There's an article on [gamasutra](http://www.gamasutra.com/blogs/BranoKemen/20090812/85207/Logarithmic_Depth_Buffer.php)" CreationDate="2015-08-07T08:12:14.850" UserId="214" ContentLicense="CC BY-SA 3.0" />
  <row Id="129" PostId="29" Score="0" Text="Re that first paper: That serves me right for pasting in the first link I found that seemed suitable :-)    Again, I think I've only ever seen the tangent at a segment end point set to the (assuming I've done the matrix maths correctly) 1/2 the difference of the two surrounding neighbours." CreationDate="2015-08-07T08:25:01.567" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="132" PostId="97" Score="3" Text="Hm yeah, I'm aware that I can get compiler errors. I was hoping for better runtime debugging. I've also used WebGL inspector in the past, but I believe it only shows you state changes, but you can't look into a shader invocation. I guess this could have been clearer in the question." CreationDate="2015-08-07T09:42:23.417" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="133" PostId="96" Score="0" Text="Have you looked into gDEBugger? Quoting the site: &quot;gDEBugger is an advanced OpenGL and OpenCL Debugger, Profiler and Memory Analyzer. gDEBugger does what no other tool can - lets you trace application activity on top of the OpenGL and OpenCL APIs and see what is happening within the system implementation.&quot; Granted, no VS style debugging/stepping through code, but it might give you some insight in what your shader does (or should do). Crytec released a similar tool for Direct shader &quot;debugging&quot; called RenderDoc (free, but strictly for HLSL shaders, so maybe not relevant for you)." CreationDate="2015-08-07T11:06:15.930" UserId="194" ContentLicense="CC BY-SA 3.0" />
  <row Id="134" PostId="96" Score="0" Text="@Bert Hm yeah, I guess gDEBugger is the OpenGL equivalent to WebGL-Inspector? I've used the latter. It's immensely useful, but it's definitely more debugging OpenGL calls and state changes than shader execution." CreationDate="2015-08-07T11:47:56.203" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="135" PostId="96" Score="1" Text="I've never done any WebGL programming and hence I'm not familiar with WebGL- Inspector. With gDEBugger you can at least inspect the entire state of your shader pipeline including texture memory, vertex data, etc. Still, no actual stepping through code afaik." CreationDate="2015-08-07T11:57:21.997" UserId="194" ContentLicense="CC BY-SA 3.0" />
  <row Id="136" PostId="96" Score="0" Text="gDEBugger is extremely old and not supported since a while. If you are looking from frame and GPU state analysis you this is other question is strongly related: http://computergraphics.stackexchange.com/questions/23/how-can-i-debug-what-is-being-rendered-to-a-frame-buffer-object-in-opengl/25#25" CreationDate="2015-08-07T12:00:52.567" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="137" PostId="100" Score="4" Text="This question may be a bit controversial, because several other SE sites don't want questions about best practices. This is intentional to see how this community stands regarding such questions." CreationDate="2015-08-07T12:39:15.597" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="138" PostId="100" Score="2" Text="Hmm, looks good to me. I'd say we are to a large degree a bit &quot;broader&quot;/&quot;more general&quot; in our questions than, say, StackOverflow." CreationDate="2015-08-07T12:40:15.333" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="139" PostId="95" Score="1" Text="When you say &quot;co-planar&quot; do you mean &quot;nearly&quot;  or &quot;exactly&quot; co-planar and if the latter, are those surfaces ever identical surfaces/triangles? The rendering hardware should be deterministic (assuming you aren't submitting in random order) for the last case and not have fighting.  &#xA;If it's a case of non-identical surfaces but exactly co-planar, could you update the model to split surfaces into overlapping and non-overlapping portions?" CreationDate="2015-08-07T12:55:57.560" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="140" PostId="29" Score="0" Text="Ah OK! Anyways yeah, catmull rom / hermite is really freaking awesome I totally agree. Really neat way to interpolate between data points and have C1 continuity at the edges. It really simplifies things (:" CreationDate="2015-08-07T13:36:17.957" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="141" PostId="100" Score="2" Text="StackOverflow went from being a 'ask us' to a 'dont ask us unless you **have to** please' board." CreationDate="2015-08-07T14:24:01.983" UserId="296" ContentLicense="CC BY-SA 3.0" />
  <row Id="142" PostId="52" Score="0" Text="There's an article on GPU Gems 3 about the construction of signed distance fields for arbitrary meshes using the GPU, which is freely available here: http://http.developer.nvidia.com/GPUGems3/gpugems3_ch34.html" CreationDate="2015-08-07T14:32:46.747" UserId="281" ContentLicense="CC BY-SA 3.0" />
  <row Id="143" PostId="100" Score="0" Text="If it's meant to determine on-topicness, then how about an associated Meta question?" CreationDate="2015-08-07T15:43:26.963" UserId="21" ContentLicense="CC BY-SA 3.0" />
  <row Id="144" PostId="100" Score="0" Text="@S.L.Barth I figured I could still do that once anyone objects, but sure, why not." CreationDate="2015-08-07T15:51:43.950" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="145" PostId="100" Score="2" Text="[Discussion on meta.](http://meta.computergraphics.stackexchange.com/q/53/16)" CreationDate="2015-08-07T16:06:09.930" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="146" PostId="1" Score="4" Text="Please don't call these optical tricks &quot;holograms&quot;. Refer to the &quot;[Things often confused with holograms](https://en.wikipedia.org/wiki/Holography#Things_often_confused_with_holograms)&quot; for an overview. In this case, you're talking about a classic [Pepper's ghost](https://en.wikipedia.org/wiki/Pepper%27s_ghost) illusion. No 3D at all, apart from having 4 different perspectives. **update:** I've proposed an edit to adjust the question accordingly." CreationDate="2015-08-07T16:13:30.893" UserId="30" ContentLicense="CC BY-SA 3.0" />
  <row Id="147" PostId="86" Score="0" Text="I've not tried lanczos so I can't speak to that. We chose catmull-rom for upscaling, which is a cubic, and it worked well." CreationDate="2015-08-07T17:13:17.077" UserId="125" ContentLicense="CC BY-SA 3.0" />
  <row Id="148" PostId="103" Score="1" Text="I'm a bit confused about your premise ... how is a lowpass filter qualitatively any different than downsampling? I mean, I get that the algorithms are different and all but they both gather samples from neighboring pixels and suppress high frequencies. The big difference is the resolution of the result image, otherwise the two operations are isomorphic. Seems like applying both is redundant." CreationDate="2015-08-07T17:20:12.627" UserId="125" ContentLicense="CC BY-SA 3.0" />
  <row Id="149" PostId="103" Score="0" Text="Well here's what confuses me.  I know that you can't just downsample an image without getting aliasing.  Doing bicubic interpolation of pixels when making an image larger works really well and looks nice.  Doing the same when making an image smaller SEEMS to work decently, but I wasn't sure if the result is likely to have much aliasing as a result.  I was wondering if technically, you'd need to do some kind of low pass filter on the image before the doing bicubic sampling, or if the bicubic sampling was good enough in practice?  I could see it being a low pass filter of sorts on its own maybe." CreationDate="2015-08-07T17:24:12.550" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="150" PostId="103" Score="1" Text="That Mitchell-Netravali paper I mentioned in the other question addresses this idea specifically - he generalized cubics and then found the parameters that alias the least. That doesn't mean they don't alias at all, but perhaps it would direct you towards which cubic to use to minimize aliasing." CreationDate="2015-08-07T18:30:49.443" UserId="125" ContentLicense="CC BY-SA 3.0" />
  <row Id="151" PostId="82" Score="0" Text="Thanks, if you wouldn't mind putting the info into the answer as well, that would be great!" CreationDate="2015-08-07T18:37:29.810" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="152" PostId="95" Score="1" Text="@SimonF, by &quot;co-planar&quot; I mean &quot;exactly co-planar&quot;.  Soapy's solution only works in the &quot;nearly co-planar&quot; case." CreationDate="2015-08-07T18:38:11.437" UserId="158" ContentLicense="CC BY-SA 3.0" />
  <row Id="153" PostId="95" Score="0" Text="Could you give an example of your surfaces? The only thing I can think of off the top of my head is duplicate triangles as @SimonF mentioned." CreationDate="2015-08-07T20:28:32.447" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="154" PostId="95" Score="0" Text="@RichieSams the most common case that I can think of is decals, where you don't need exactly duplicate triangles." CreationDate="2015-08-07T23:15:29.387" UserId="259" ContentLicense="CC BY-SA 3.0" />
  <row Id="155" PostId="25" Score="0" Text="It's also probably a good idea to make sure you have the right depth test setup, for the rendering going into your visualisation FBO." CreationDate="2015-08-07T23:18:31.293" UserId="259" ContentLicense="CC BY-SA 3.0" />
  <row Id="156" PostId="91" Score="1" Text="I found a blog post where someone was able to do it: http://www.rchoetzlein.com/website/multi-monitor-rendering-in-opengl/  @rys is right though. In order to get a solid answer we would need to know more details of your setup." CreationDate="2015-08-08T00:39:22.360" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="157" PostId="106" Score="2" Text="GL_ARB_shading_language_include is not a core feature. In fact, only NVIDIA supports it. (http://delphigl.de/glcapsviewer/listreports2.php?listreportsbyextension=GL_ARB_shading_language_include)" CreationDate="2015-08-08T01:08:40.533" UserId="311" ContentLicense="CC BY-SA 3.0" />
  <row Id="158" PostId="82" Score="0" Text="@NathanReed Done :) I tried not to dwell on too much to avoid an even longer answer, but let me know if you think something needs to be added or clarified further!" CreationDate="2015-08-08T09:00:20.137" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="159" PostId="82" Score="0" Text="Just to add a new reference, there's a new model per the dipole approximation presented this year at siggraph: http://people.compute.dtu.dk/jerf/code/dirsss/" CreationDate="2015-08-08T09:09:35.210" UserId="281" ContentLicense="CC BY-SA 3.0" />
  <row Id="162" PostId="110" Score="0" Text="From my understanding, I think this does not solve the problem of sharing struct definitions." CreationDate="2015-08-08T17:35:24.210" UserId="311" ContentLicense="CC BY-SA 3.0" />
  <row Id="163" PostId="110" Score="0" Text="@NicolasLouisGuillemot: yes you are right, only instructions code is shared this way, not declarations." CreationDate="2015-08-08T17:39:53.133" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="164" PostId="107" Score="5" Text="Re-rendering the same geometry with the same transforms does reliably generate the same depth values every time. (I.e. it's not a *might*, it's a *will*). That's why multi-pass forward lighting works, for instance." CreationDate="2015-08-08T17:51:51.780" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="166" PostId="118" Score="0" Text="Yes but how do you handle change of basis to dimensions? If i have is a R^3 -&gt; R^4 or R^5 mapping? Anyway yes this will make it work atleast halfway." CreationDate="2015-08-08T20:58:31.147" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="167" PostId="118" Score="1" Text="I've not had to approach this problem before, but I'm skeptical that this answer will work since color spaces aren't always linear." CreationDate="2015-08-08T22:39:28.103" UserId="125" ContentLicense="CC BY-SA 3.0" />
  <row Id="168" PostId="100" Score="0" Text="I wrote [this](http://stackoverflow.com/a/20605122/1888983) a while back. Beyond `#include`, I find injecting `#define`s is most useful. One shader, many permutations." CreationDate="2015-08-09T00:36:10.310" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="169" PostId="118" Score="0" Text="Dot product surely will come into play" CreationDate="2015-08-09T02:22:05.913" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="170" PostId="111" Score="0" Text="Yep, I think this was my conclusion too. Do you know offhand if this is the case with DX12 or (the unreleased) Vulkan? It has pretty large applications for deferred rendering as you've mentioned and none of the existing alternatives right now seem satisfactory." CreationDate="2015-08-09T03:37:53.700" UserId="87" ContentLicense="CC BY-SA 3.0" />
  <row Id="171" PostId="111" Score="0" Text="@jeremyong Sorry, I don't think there are any changes to blending operations in DX12. Not sure about Vulkan, but I'd be surprised; the blending hardware hasn't changed. FWIW, in games I've worked on we did a variant of bullet #3 for deferred decals, and preprocessed the geometry to separate it into non-overlapping groups." CreationDate="2015-08-09T03:58:19.847" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="172" PostId="118" Score="0" Text="@jorgeRodriguez you can treat the space linear for max values" CreationDate="2015-08-09T04:50:36.953" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="173" PostId="111" Score="0" Text="Gotcha thanks for the recommendation. Deferred decals is precisely what I'm implementing" CreationDate="2015-08-09T05:30:12.440" UserId="87" ContentLicense="CC BY-SA 3.0" />
  <row Id="176" PostId="124" Score="1" Text="Thanks! It looks like I'll need two depth buffers for this. I.e. one to store and filter out the last depths, and one to do the depth testing for the current render. Correct me if I'm wrong but I assume I'll need two depth textures for the FBO which I swap between each peeling pass." CreationDate="2015-08-09T09:07:19.353" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="177" PostId="124" Score="1" Text="@jozxyqk Correct, two depth buffers are necessary for this procedure." CreationDate="2015-08-09T09:09:45.863" UserId="170" ContentLicense="CC BY-SA 3.0" />
  <row Id="178" PostId="24" Score="1" Text="For more information, see [Pepper's Ghost](https://en.wikipedia.org/wiki/Pepper's_ghost)" CreationDate="2015-08-09T09:12:03.990" UserId="158" ContentLicense="CC BY-SA 3.0" />
  <row Id="179" PostId="133" Score="0" Text="Yes something like this." CreationDate="2015-08-09T11:40:33.210" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="180" PostId="122" Score="2" Text="Yes, using an appropriate color space is what you want." CreationDate="2015-08-09T11:55:11.810" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="182" PostId="140" Score="0" Text="I've actually been thinking about this. Ive also toyed on asking what other approaches modeling pipes than matrix and quat hierarchies one could use." CreationDate="2015-08-09T13:17:36.497" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="183" PostId="120" Score="1" Text="Have you heard of anyone using the depth buffer value as a sanity check?" CreationDate="2015-08-09T14:16:34.443" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="187" PostId="120" Score="0" Text="@AlanWolfe No, and I think that is because the motion vector texture is usually 2-component: you'd need a change-in-Z component to know what the depth buffer value should be, and that's not nicely bounded by the screen size. I suspect you could get better rejection strategies than that by adding more per-pixel information." CreationDate="2015-08-09T16:48:45.367" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="188" PostId="120" Score="0" Text="Ah OK. What kind of information do you think would be helpful to add. Shading parameter type stuff to be able to tell if it's the same material?" CreationDate="2015-08-09T17:26:05.140" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="189" PostId="120" Score="1" Text="@AlanWolfe I don't have a lot of concrete ideas. I'm not an expert on when temporal reprojection with neighborhood clamping breaks down and produces artifacts and what information would be useful in those situations. Perhaps transparents (no motion vector information) combined with high-frequency lighting are producing artifacts, and you need some obscurance information. Perhaps geometric aliasing is your problem and you need some other information." CreationDate="2015-08-09T18:11:28.410" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="190" PostId="145" Score="1" Text="Should this be several questions." CreationDate="2015-08-09T18:33:42.517" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="191" PostId="145" Score="0" Text="@joojaa Potentially. The answers to these questions are interrelated, though. I'm looking for an answer of the form &quot;well, a photon can do X, Y, or Z when it interacts with media; X is described by the phase function, Y is described by the Beer-Lambert law, …&quot;" CreationDate="2015-08-09T18:37:55.153" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="192" PostId="107" Score="0" Text="@NathanReed Corrected. Thank you for the clarification" CreationDate="2015-08-09T18:44:49.937" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="193" PostId="130" Score="1" Text="I haven't looked in too much detail at the code, but the image seems to be alright. The Fresnel effect shows up as a red ring. With the roughness so high (0.9), it makes sense that the rest of the image is mostly yellow (ie. mostly diffuse). If you lower the roughness, you may get a red specular highlight" CreationDate="2015-08-09T18:49:36.253" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="194" PostId="131" Score="1" Text="Here are two source code resources for [Tiled](https://software.intel.com/en-us/articles/deferred-rendering-for-current-and-future-rendering-pipelines) and [Clustered](https://software.intel.com/en-us/articles/forward-clustered-shading)" CreationDate="2015-08-09T18:52:48.500" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="195" PostId="130" Score="0" Text="@RichieSams I added new images for different roughness values but can't see red shiny specular highlight yet." CreationDate="2015-08-09T20:24:29.623" UserId="83" ContentLicense="CC BY-SA 3.0" />
  <row Id="196" PostId="15" Score="0" Text="These comments sound like the perfect material for someone to write a summarising answer." CreationDate="2015-08-09T21:22:15.260" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="198" PostId="41" Score="1" Text="Could you add a brief note to explain why the two different 5 by 5 kernels have slightly different numbers (one summing to 273, the other summing to 256)? It seems like a potential confusion for someone new to this." CreationDate="2015-08-09T22:14:56.317" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="199" PostId="41" Score="0" Text="Similarly, could you explain why the kernel is flipped in your second diagram? I don't think it's relevant to the explanation, but the fact that it's an apparent extra step may hinder understanding to someone who doesn't know that it isn't necessary." CreationDate="2015-08-09T22:16:34.077" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="200" PostId="62" Score="0" Text="That's a fascinating paper. It looks like you can adjust the parameters to give nearer to a square grid or a hexagonal grid, which would allow different approaches to noise generation." CreationDate="2015-08-09T22:43:53.933" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="201" PostId="151" Score="1" Text="Does &quot;trackball&quot; mean a camera that orbits around an object, like in 3D modeling apps? If so, I think it's usually done by just keeping track of the 2D mouse coordinates and mapping x=yaw, y=pitch for the camera rotation." CreationDate="2015-08-09T23:15:07.823" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="202" PostId="150" Score="0" Text="Say that we don't care very much about efficiency. How would we go about doing coverage calculations for boolean operations on shapes? Is that possible in general, or only for specific shapes?" CreationDate="2015-08-09T23:18:37.830" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="203" PostId="154" Score="1" Text="Are you intentionally restricting the question to bounding volume hierarchies, or are you open to other forms of spatial partitioning?" CreationDate="2015-08-09T23:19:31.850" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="204" PostId="154" Score="1" Text="@JohnCalsbeek I've edited to clarify - thanks for pointing out my inadvertent restriction." CreationDate="2015-08-09T23:32:37.030" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="205" PostId="153" Score="1" Text="This is super interesting! So is the Disney paper you linked." CreationDate="2015-08-09T23:41:26.250" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="206" PostId="151" Score="1" Text="@NathanReed the other option is [axis-angle based](http://codereview.stackexchange.com/q/51205/32061), You project 2 mouse points onto a (virtual) sphere and then find the rotation from one to the other." CreationDate="2015-08-09T23:42:13.803" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="207" PostId="46" Score="2" Text="I am selecting this answer since it gives orders of magnitude, which is the closest so far to what I asked, even though the mentioned source doesn't give much explanation." CreationDate="2015-08-09T23:42:35.030" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="208" PostId="118" Score="0" Text="@JorgeRodriguez would it work if you first translate to a linear colour space? For more than 3 primary colours there will in general be more than one combination that approximates a given colour, but finding one of those combinations should be possible with this approach, provided that the chosen primary colours form a basis." CreationDate="2015-08-10T01:02:36.503" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="209" PostId="15" Score="0" Text="@Rahul Would you mind turning your comments and any other information you might have into an answer?" CreationDate="2015-08-10T01:04:58.047" UserId="70" ContentLicense="CC BY-SA 3.0" />
  <row Id="210" PostId="130" Score="1" Text="Your 2nd and 3rd images do appear to have less red in general (in the yellow diffuse area) than your original image. This isn't very apparent because adding a little red to a yellow area leaves it a similar colour (orange-yellow rather than yellow). Do you see any more detail of the red distribution if you reduce the yellow significantly? Omitting the yellow altogether may help in identifying what is going wrong." CreationDate="2015-08-10T01:26:42.160" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="212" PostId="15" Score="0" Text="Sorry, I'm at SIGGRAPH and have absolutely no free time (except to write this comment). Someone else should feel free to write an answer using the links in my comments." CreationDate="2015-08-10T04:17:38.400" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="213" PostId="21" Score="5" Text="If you have an expensive per-pixel shader, the stencil buffer can also be used to batch similar threads, reducing divergence and increasing occupancy as in [this OIT paper, sec 2.6](http://goanna.cs.rmit.edu.au/~pknowles/rbs-preprint.pdf) (disclaimer: I'm an author)." CreationDate="2015-08-10T05:45:33.600" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="214" PostId="162" Score="0" Text="Ha, there you go, taking images from others is more efficient than drawing them yourself." CreationDate="2015-08-10T08:05:00.250" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="215" PostId="15" Score="2" Text="Apologies to @NoviceInDisguise for also not having time, but WRT to Catmull-Clark, perhaps one of the reasons for it still being very much in use was DeRose et al's extensions to it to include, e.g. sharpness factors in the tessellation to allow creases etc. http://www.cs.rutgers.edu/~decarlo/readings/derose98.pdf  &#xA;IIRC those extensions weren't initially free to use (but some commercial tools licensed it from Pixar) however, unless I'm mistaken, it now seems to be free e.g. http://graphics.pixar.com/opensubdiv/docs/subdivision_surfaces.html#arbitrary-topology" CreationDate="2015-08-10T08:12:12.877" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="216" PostId="130" Score="0" Text="@trichoplax i reduced the yellow but again no way to see red specular. I just see red ring(fresnel) effect. Doesnt matter what i set value for roughness I cant see specular effect that focused a point." CreationDate="2015-08-10T08:32:09.737" UserId="83" ContentLicense="CC BY-SA 3.0" />
  <row Id="217" PostId="162" Score="0" Text="@joojaa Yes, faster if you remember where they were, but without that gratifying feeling of doing it yourself :P. Also I have this stupid [sub-pixel rendering bug](http://superuser.com/q/930036/264276) in chrome so the text is all colourful." CreationDate="2015-08-10T08:44:38.337" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="218" PostId="162" Score="1" Text="Well that subpixel rendering is something that has not been asked. Yet." CreationDate="2015-08-10T08:59:04.347" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="219" PostId="162" Score="1" Text="[done](http://computergraphics.stackexchange.com/q/165/198)." CreationDate="2015-08-10T09:41:23.373" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="220" PostId="165" Score="0" Text="I believe that some screens have a different layout of the primary colours. Have you viewed your results on different types of screen?" CreationDate="2015-08-10T09:58:22.040" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="221" PostId="165" Score="0" Text="@trichoplax no, but I'm confident both my monitors are RGB. Also here I'm more interested in how subpixel antialiasing techniques are meant to work than a fix for my issue." CreationDate="2015-08-10T10:02:22.253" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="222" PostId="165" Score="0" Text="I didn't mean different primary colours, I just meant that the red, green and blue are arranged in different geometric patterns, so your algorithm would need to know which pattern is being used in order to give good results." CreationDate="2015-08-10T10:06:17.063" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="223" PostId="165" Score="0" Text="[This image](https://en.wikipedia.org/wiki/Subpixel_rendering#/media/File:Pixel_geometry_01_Pengo.jpg) shows how varied the arrangement of subpixels can be." CreationDate="2015-08-10T10:06:48.327" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="224" PostId="165" Score="0" Text="Incidentally, I like the question and I'm interested to see the answers, but I can't upvote until midnight..." CreationDate="2015-08-10T10:09:22.740" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="225" PostId="165" Score="1" Text="@trichoplax yes, sorry I should have clarified, both monitors have pixels split into thirds in R-G-B order from left to right as in [this photo](http://i.stack.imgur.com/SZl3W.png)." CreationDate="2015-08-10T10:09:51.153" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="226" PostId="165" Score="1" Text="To a certain extent, italic text will have less noticeable colour fringes since the sloping lines don't allow the same colour to be present for more than a few consecutive pixels vertically." CreationDate="2015-08-10T10:14:26.257" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="227" PostId="165" Score="0" Text="Hopefully someone with knowledge of this can confirm, but my guess is that the first image has been antialiased at the pixel level first, and then that antialiased image has been antialiased again at the subpixel level. You can see areas where the original antialiasing aligns with whole pixels and has no colour fringes, despite clearly having variations in brightness due to the initial antialiasing, and areas where it does not align resulting in flat colour fringes, rather than the graduated colour fringes in the last image. It appears to be a scaled rasterised font, rather than a vector font." CreationDate="2015-08-10T10:30:46.243" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="228" PostId="107" Score="1" Text="To get that functionality you need to use the invariant qualifier in glsl: https://www.opengl.org/wiki/Type_Qualifier_%28GLSL%29#Invariance_qualifiers" CreationDate="2015-08-10T11:11:30.830" UserId="135" ContentLicense="CC BY-SA 3.0" />
  <row Id="229" PostId="107" Score="0" Text="Note that identical shader expressions (and inputs, obviously) when evaluating vertex positions may not be enough to get identical results as some optimisations may depend on the rest of the shader. &#xA;&#xA;GLSL has the &quot;invariant&quot; keyword to declare shader outputs that must be evaluated identically in different shaders." CreationDate="2015-08-10T11:13:07.507" UserId="344" ContentLicense="CC BY-SA 3.0" />
  <row Id="231" PostId="130" Score="2" Text="First normalise the Normal vector before using it and second the viewDirection is the outgoing vector from the Position to the camera: uCameraPosition - Position." CreationDate="2015-08-10T11:35:59.267" UserId="290" ContentLicense="CC BY-SA 3.0" />
  <row Id="232" PostId="167" Score="0" Text="I would expect problems with pixel geometry, gamma or colour space conversion to show up as colour distortion at arbitrary points in the image, rather than the regular cycle seen in the question's image. The fact that it cycles horizontally between exaggerated colour antialiasing and pure greyscale antialiasing hints that the first application of antialiasing was performed at a different scale." CreationDate="2015-08-10T12:01:41.727" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="233" PostId="167" Score="0" Text="I don't have the full explanation, as the distortion does not seem to be aligned between the different rows of text, but it does seem the problem is related to sub-pixel rendering of already rasterised text rather than vector text." CreationDate="2015-08-10T12:16:59.983" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="234" PostId="167" Score="0" Text="@trichoplax What I try to say is that I doubt there an issue with Anti-Grain's sub-pixel rendering. Instead I would guess the input gets mangled earlier than it enters the rasterizer. Or later, but not in the rasterizer itself." CreationDate="2015-08-10T12:39:48.860" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="235" PostId="167" Score="0" Text="Yes I think the sub-pixel rendering is being applied correctly, but when applied to pre-rasterised text it is not possible to give a good result. I don't think that the renderer is broken, I just think it is being fed the wrong kind of text." CreationDate="2015-08-10T12:46:31.897" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="236" PostId="169" Score="0" Text="You won't find anything relevant just looking at OpenGL, maybe a JavaScript library can do that" CreationDate="2015-08-10T13:18:39.680" UserId="217" ContentLicense="CC BY-SA 3.0" />
  <row Id="238" PostId="169" Score="0" Text="I dont think you can, that's why Microsoft asks the user." CreationDate="2015-08-10T13:50:34.907" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="239" PostId="169" Score="0" Text="I wonder if that means there is never a way to determine this, or just that some monitors do not provide that information." CreationDate="2015-08-10T13:58:40.427" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="240" PostId="151" Score="0" Text="@NathanReed yes that is what I meant by trackball, I thought it was a common name in the CG community." CreationDate="2015-08-10T14:11:54.123" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="242" PostId="169" Score="3" Text="well, i think you can get the model and make of your monitor maybe there is some repository that lists this? If there is not what stops anybody of us form building it?" CreationDate="2015-08-10T14:15:14.630" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="243" PostId="151" Score="0" Text="@ratchetfreak yes my approach considers an axis-angle based rotation. My doubt is that if it is needed to map the 2D mouse coords to world-coord or not. I know I can use the (x,y) to compute the `z` value of a sphere of radius `r`, however I'm not sure if that sphere lives in world-space or image-space and what the implications are. Perhaps I'm overthinking the problem." CreationDate="2015-08-10T14:17:15.137" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="245" PostId="24" Score="1" Text="Its an interesting comment about Pepper's ghost. Notice that Pepper's ghost was originally a reflection of a 3D object so the reflection itself was 3D. In the case of these popular device based projections, the projections are only 2D so the effect is only a 2D reflection suspended in space. The most popular Youtube videos meant for doing this don't even have 4 different perspectives, just 1 repeated 4 times." CreationDate="2015-08-10T15:19:01.417" UserId="20" ContentLicense="CC BY-SA 3.0" />
  <row Id="246" PostId="164" Score="1" Text="Depth of field may help too." CreationDate="2015-08-10T15:31:29.733" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="247" PostId="175" Score="3" Text="I would take http://www.littlecms.com/ for a spin" CreationDate="2015-08-10T15:44:00.240" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="248" PostId="175" Score="0" Text="@joojaa This is great! They actually say: &quot;lcms is a CMM engine; it implements fast transforms between ICC profiles&quot;. And by their trial of the professional solution, they provide a lot of options (&quot;input for RGB, Gray, and CMYK spaces&quot;, &quot;intents&quot;, &quot;destination color space&quot;, etc.)... I never imagined that it could be so vast." CreationDate="2015-08-10T16:07:35.907" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="249" PostId="175" Score="0" Text="The [international color consortium](http://www.color.org/index.xalter) defines 4 transformation intents for the colors that can not fit the gamut, read the tutorial." CreationDate="2015-08-10T16:08:51.743" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="250" PostId="175" Score="0" Text="@joojaa actually, in their own program they have a small description associated to all of the 7 types of &quot;rendering intents&quot; we can choose from when converting." CreationDate="2015-08-10T16:13:02.087" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="251" PostId="118" Score="0" Text="I expanded my answer to be explicit about how to deal with bases of differing dimensions. Is there any way to use LaTeX for math here? This will probably be a very common feature request." CreationDate="2015-08-10T16:18:55.310" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="252" PostId="118" Score="0" Text="And regarding linearity: afaik most if not all color spaces are linear or have a 1:1 mapping that makes them linear. The CIE XYZ and xyY color spaces (which are supposed to represent all colors visible to the human eye) fit that definition, and are probably what you'd want to use as the &quot;conversion&quot; color space rather than RGB." CreationDate="2015-08-10T16:26:33.650" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="253" PostId="175" Score="0" Text="hey allow for more intents and different white point/ blackpoint combinations. (adobe thus has 3*4 intents)" CreationDate="2015-08-10T16:33:24.357" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="255" PostId="118" Score="2" Text="MathJax request is in works you should add your post as a reference to this [meta post](http://meta.computergraphics.stackexchange.com/a/34/38)" CreationDate="2015-08-10T16:36:56.713" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="256" PostId="164" Score="2" Text="As an addendum: Sebastien Lagarde did a [series of  blog posts](https://seblagarde.wordpress.com/2012/12/10/observe-rainy-world/) that documented how they implemented dynamic rain in the game &quot;Remember Me&quot;" CreationDate="2015-08-10T16:39:34.113" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="257" PostId="175" Score="0" Text="@joojaa I am impressed with the quantity of destination spaces they provide, for the conversions I tested there are shocking differences when ProPhotoRGB is used, [it's very hard to tell the differences when different intents are applied though], EDIT: in some situations intents are also very noticeable." CreationDate="2015-08-10T16:44:22.730" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="260" PostId="170" Score="0" Text="Just noting that I've definitely used extensions via GLES 2.0 on android (and in native code) so you shouldn't have any issues on that part of your solution." CreationDate="2015-08-10T18:37:46.093" UserId="329" ContentLicense="CC BY-SA 3.0" />
  <row Id="261" PostId="21" Score="0" Text="@jozxyqk, that's some outside the box thinking right there! Very nice." CreationDate="2015-08-10T19:05:39.823" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="262" PostId="176" Score="0" Text="I'm not sure I'm understanding you. Slightly transparent == translucent. So you're already using translucency. Could you clarify what you're trying to do? Fully transparent background? (Aka, clear/invisible) Or translucent background? (Aka, you can see some stuff behind it, but covered with the background color)" CreationDate="2015-08-10T19:44:53.450" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="263" PostId="130" Score="0" Text="Thanks all of you(RichieSams, trichoplax and xpicox ) for the answers. I lower the roughness, change the color of material and reversed the ViewDirection then i start to see shiny red specular :). Actually i was using right ViewDirection but i could not see the specular because of the color and roughness value. All answers help to fix my problem." CreationDate="2015-08-10T20:05:33.543" UserId="83" ContentLicense="CC BY-SA 3.0" />
  <row Id="264" PostId="176" Score="0" Text="@RichieSams Currently I use a simple black background with an applied alpha transparency. So the scene can be seen, but is still sharp, which distracts the readability of the text. Thus I would like to change the simple alpha transparency to translucency. A real-world example maybe is the difference between looking through colored respectively iced glass." CreationDate="2015-08-10T20:07:21.307" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="265" PostId="176" Score="0" Text="@Nero In graphics, &quot;translucent&quot; doesn't usually imply &quot;frosted&quot; or &quot;iced&quot;, so I suggest making what your looking for clearer in your question, since I was also quite confused by it." CreationDate="2015-08-10T20:22:07.777" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="266" PostId="181" Score="0" Text="I reposted this question [from SO where it went unanswered](https://stackoverflow.com/questions/30687682/how-to-render-an-object-that-recieves-shadows-but-does-not-cast-them-in-a-varian). I don't know if it's a good thing to do but it seemed like a good idea since I think it's a good fit here." CreationDate="2015-08-10T20:41:02.740" UserId="349" ContentLicense="CC BY-SA 3.0" />
  <row Id="267" PostId="166" Score="2" Text="This GPU Gems article rasterizes quadratic curves by identifying parts of the hull that are curved and analytically computing the coverage in the pixel shader, might be worth a look: https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch25.html" CreationDate="2015-08-10T20:41:47.357" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="268" PostId="183" Score="1" Text="While certainly interesting, I don't think we want to be flooded with plain &quot;How do I achieve effect X?&quot; type questions later on. So: What have you tried?" CreationDate="2015-08-10T22:00:55.653" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="269" PostId="166" Score="2" Text="@yuriks Yeah, Loop &amp; Blinn, totally forgot about it. But isn't it patented?" CreationDate="2015-08-10T22:12:03.490" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="270" PostId="177" Score="0" Text="I think you would also be interested in techniques to simulate Rough Refraction such as those described in http://www-sop.inria.fr/reves/Basilic/2011/DBSHR11/RoughRefr_I3D_Final.pdf. Look at the screenshots and see how it could benefit ice cube rendering !" CreationDate="2015-08-10T22:21:21.550" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="271" PostId="177" Score="1" Text="@wil While those results for rough surfaces are very impressive (especially for a real-time algorithm), ice tends to be very smooth on the surface, and rough inside - almost the opposite effect." CreationDate="2015-08-10T22:31:17.223" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="273" PostId="183" Score="3" Text="I think the question is fine up to the title, but the body is kind of vague. There should be more context like if it's meant for real-time of offline use, level of detail and scale required, etc.  The current answer for example assumes the scale to be able to give a more specific answer taking into account possible tradeoffs, which I think is an important part of a good answerable question." CreationDate="2015-08-11T00:06:56.847" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="274" PostId="170" Score="0" Text="Are we considering very specific programming questions like these? This is a very API-specific programming question and doesn't even involve anything related to rendering techniques or usage of an API to implement them." CreationDate="2015-08-11T00:46:25.163" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="275" PostId="130" Score="0" Text="If the various different suggestions came together into a solution, you could write them up and post it as a self-answer. That's generally encouraged. Then you can include the working final image too..." CreationDate="2015-08-11T01:28:24.907" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="276" PostId="118" Score="0" Text="@trichoplax yuriks and joojaa seem to be confident in this answer so I'll defer to them." CreationDate="2015-08-11T02:47:12.823" UserId="125" ContentLicense="CC BY-SA 3.0" />
  <row Id="277" PostId="177" Score="3" Text="Maybe questionslike this would require pictures" CreationDate="2015-08-11T04:44:58.767" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="278" PostId="193" Score="1" Text="I know about the Notification Center and Control Center. But I doubt that these blurs are calculated in real time as the active app is frozen while one of them is shown (or at least it does not update the UI anymore). But the links look promising. Thanks!" CreationDate="2015-08-11T06:40:26.967" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="279" PostId="183" Score="1" Text="@yuriks Yes, the question could use more detail as well, but I think the &quot;What have you tried?&quot; part is important. We should set ourselves similar quality standards as other SE sites like Stack Overflow and expect question authors to show *some* effort on their part. Otherwise, we'll just become a site where people go to have others do their work for them." CreationDate="2015-08-11T08:22:19.557" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="280" PostId="170" Score="0" Text="@yuriks Yes, I think we should definitely except if they are very specific programming questions related to computer graphics. In fact, I think that specific programming questions are what we are lacking the most so far (probably because these are harder to come up with if you don't encounter them right then and there)." CreationDate="2015-08-11T08:30:34.893" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="281" PostId="183" Score="1" Text="[Relevant meta discussion.](http://meta.computergraphics.stackexchange.com/a/69/16)" CreationDate="2015-08-11T08:57:32.987" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="282" PostId="177" Score="2" Text="Do you want to render stills or animation? If the cube is to be animated are you looking for real-time effects? And yes, if this wasn't the private beta, an image of your current results would be nice." CreationDate="2015-08-11T09:00:35.547" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="283" PostId="187" Score="1" Text="&quot;Normally, this kind of data will undergo a lot of processing after being loaded from a file until it is, for instance, displayed on the screen.&quot; like an image gets transformed to a pixel array before getting pushed on screen *regardless* of what the original format was. (hardware compression notwithstanding)" CreationDate="2015-08-11T09:04:51.697" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="284" PostId="199" Score="1" Text="It would be great if you could contain some information about this method within the answer to make it more self-contained." CreationDate="2015-08-11T09:39:32.047" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="285" PostId="196" Score="2" Text="This seems close to being a link-only answer. Could you include an explanation so that this can be understood even without following the link?" CreationDate="2015-08-11T11:21:46.560" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="286" PostId="184" Score="0" Text="Nice explanation and great links... Your video should be visible in this answer :)" CreationDate="2015-08-11T14:04:11.550" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="287" PostId="193" Score="1" Text="@Nero I suspect that's mostly for power reasons. When you swipe down from the Springboard to get to Spotlight, you get a variable-width blur depending on how far you've pulled down, and that's certainly real-time." CreationDate="2015-08-11T14:30:37.093" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="288" PostId="51" Score="0" Text="I added another possible method to my post. It may be worth clarifying your question with more detailed requirements. i.e. why does a 2D slice of 3D noise not satisfy your visual requirements? What are your performance requirements?" CreationDate="2015-08-11T15:02:26.827" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="289" PostId="201" Score="0" Text="It's not your question, but you see commonly on TV that 4:3 images are extended to widescreen by using a blurred and stretched copy in the background. That may work for you." CreationDate="2015-08-11T15:50:17.723" UserId="125" ContentLicense="CC BY-SA 3.0" />
  <row Id="290" PostId="196" Score="0" Text="Apologies, you are correct.  I should have given screenshots at least, and it turns out i even linked to the wrong link!" CreationDate="2015-08-11T15:55:09.820" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="291" PostId="121" Score="0" Text="I'm not sure if this should go as an answer or not, but the errors in your image are caused by rendering without depth testing on *all* primitives. You should render the scene in 2 passes: First render normally all solid geometry. Afterwards, disable depth *writes* (not GL_DEPTH_TEST) and render translucent geometry in roughly back-to-front order. This will ensure that transparent geometry will not be drawn in front of solid geometry that's in front of it." CreationDate="2015-08-11T17:45:01.473" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="292" PostId="196" Score="1" Text="Thanks for the images, but it would also be interesting to see an explanation of how this works and why it improves the appearance. A good answer should provide understanding without the need to leave the site - then the links are there for further reading in more detail." CreationDate="2015-08-11T20:43:49.680" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="293" PostId="196" Score="0" Text="I mentioned you apply bump mapping to lighting and refraction calculations. Would you expect my answer to explain bump mapping?" CreationDate="2015-08-11T20:48:29.437" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="294" PostId="196" Score="0" Text="Explain whatever you feel would be helpful. If you are presenting something which answers the question, then you can explain why it helps." CreationDate="2015-08-11T21:41:51.340" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="295" PostId="209" Score="1" Text="A couple questions... are you doing the drawing logic on the CPU or GPU?  Also, are you looking for integer based algorithms or floating point?" CreationDate="2015-08-11T23:54:29.043" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="296" PostId="209" Score="5" Text="@AlanWolfe, integer algorithms on the CPU -- the same environment that Bresenham's algorithm was designed for." CreationDate="2015-08-11T23:56:30.247" UserId="158" ContentLicense="CC BY-SA 3.0" />
  <row Id="297" PostId="209" Score="3" Text="https://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm is the classic one, though the wikipedia page is pretty half-baked and I don't have access to the paper. This feels like a lazy question though, since it's pretty easy to find this by doing some basic googling." CreationDate="2015-08-12T00:33:52.797" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="298" PostId="209" Score="0" Text="@yuriks, mind turning that into a full-fledged answer?" CreationDate="2015-08-12T00:34:21.103" UserId="158" ContentLicense="CC BY-SA 3.0" />
  <row Id="299" PostId="209" Score="0" Text="I fat-fingered the enter key, so I just edited my comment. :)" CreationDate="2015-08-12T00:35:01.630" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="300" PostId="196" Score="1" Text="I had an awesome chance to see ice cubes close up today during dinner, and they seemed to actually be pretty smooth and without bumps. I think the vital part of this is giving them that &quot;wet&quot; look." CreationDate="2015-08-12T00:36:31.713" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="301" PostId="168" Score="1" Text="I guess the division by 300 is just a parameter for the sensitivity of the rotation given the displacement of the mouse?" CreationDate="2015-08-12T00:43:43.130" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="302" PostId="210" Score="2" Text="Are you using path tracing or physically-accurate ray tracing? If so, then the ambient occlusion is already a built-in consequence of the algorithm and does not need to be specifically modelled. Intuitively, to me it seems correct that the shadow is faint: your spheres are mirrors and so diffuse rays shot from the ground near the sphere will tend to be reflected away and towards the skybox instead of back towards the ground as would happen with diffuse spheres." CreationDate="2015-08-12T00:48:52.457" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="303" PostId="211" Score="0" Text="Wouldn't albedo values of 1 produce an image that never converges? Afaik, albedo should always be &lt; 1 for convergence to happen. An albedo of 1 would specifically not cause a contact shadow because rays would be perfectly reflected until they reach a light source such as the sky." CreationDate="2015-08-12T00:52:09.333" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="304" PostId="168" Score="0" Text="Correct. It's what happened to work well with my resolution at the time." CreationDate="2015-08-12T01:02:50.050" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="305" PostId="196" Score="0" Text="Interesting! I wonder what would make it look wet?" CreationDate="2015-08-12T01:35:33.710" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="306" PostId="213" Score="1" Text="It can be more accurate too.  The farther you get away from zero, the less resolution there is between integers, when using floating point numbers." CreationDate="2015-08-12T01:39:21.023" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="307" PostId="203" Score="2" Text="I remembered this paper I read some time ago. The authors compare spectral and RGB-rendered results with different illuminants. Unfortunately the comparison is done on a color chart, so I'm not sure how much the differences affect real life scenes. http://cg.cs.uni-bonn.de/en/publications/paper-details/lyssi-2009-btfspectral/" CreationDate="2015-08-12T02:12:42.263" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="308" PostId="214" Score="2" Text="Your ideas sound a bit like cone tracing and importance sampling, you might give them a read (:" CreationDate="2015-08-12T02:14:08.993" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="309" PostId="151" Score="2" Text="On your edit: Yes. You would need to transform your (x, y, z) values to world space using the View matrix." CreationDate="2015-08-12T02:19:09.637" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="310" PostId="215" Score="1" Text="Do you get a similarly smooth image from only the second light? i.e. it's definitely an artifact of multiple lights, not just the geometry of the second light?" CreationDate="2015-08-12T03:07:56.277" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="311" PostId="214" Score="2" Text="This sounds a bit like this thesis: http://www.graphics.cornell.edu/pubs/2004/Don04.pdf It's effectively importance sampling with an adaptive probability density function. My instinct is that it could work but you'd need to take care to avoid missing and therefore ignoring small features (small distant lights, say)." CreationDate="2015-08-12T03:27:44.193" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="312" PostId="214" Score="2" Text="Here's yet another paper on adaptive importance sampling: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2520" CreationDate="2015-08-12T03:31:37.200" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="313" PostId="212" Score="0" Text="What do you mean by &quot;only compute the transform once&quot;? Meaning you don't have to compute the inverse transform, or you only ever need to transform an object once? Computing the inverse transform is cheap compared to a geometry intersection, and you definitely would need to transform an object multiple times (into the space of each different ray)." CreationDate="2015-08-12T03:35:20.343" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="314" PostId="215" Score="0" Text="Yes, using any kind of single light gives a smooth image. The noise only comes back when there are multiple lights to choose from. (And I suppose that the 3rd example image is somewhat misleading, but when there are 2 lights, the image looks almost as bad as the first one which uses the naive algorithm if I use only 1024 spp.)" CreationDate="2015-08-12T03:36:00.983" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="315" PostId="215" Score="0" Text="To me, that suggests a bug. But it's rather difficult to debug code just by reading it. I'd try disabling pieces of code—for example, in this scene the importance weight of the BRDF sample should be extremely small since light sampling is far more likely to hit the light, so you should be able to set brdf_pdf to 0 and only use light sampling and see if the noise goes away." CreationDate="2015-08-12T03:39:30.193" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="316" PostId="209" Score="2" Text="Just thinking out loud, I figure it should be easy to adapt Bresenham for drawing multi-pixel-thick lines. Then you can do antialiasing by calculating the distance of each pixel center from the mathematical ideal line, and applying some falloff function." CreationDate="2015-08-12T03:57:05.190" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="317" PostId="209" Score="1" Text="Regarding Bresenham being adapted to do AA, this page shows that with some simple code.  From the description of the xaolin wu algorithm it may be similar.  http://members.chello.at/easyfilter/bresenham.html" CreationDate="2015-08-12T04:06:42.050" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="318" PostId="212" Score="3" Text="I think the assumption is that all primitives would be transformed to a flat world-space position during scene preparation. (Which does dramatically increase memory usage.) Then rays would be directly intersected against the primitives without a preceding transformation between spaces, since they're now both in world-space." CreationDate="2015-08-12T04:19:11.740" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="319" PostId="213" Score="2" Text="@AlanWolfe, wouldn't the same resolution issues show up when computing the inverse transformation and applying it to the ray, though?" CreationDate="2015-08-12T04:27:13.073" UserId="158" ContentLicense="CC BY-SA 3.0" />
  <row Id="321" PostId="215" Score="0" Text="@JohnCalsbeek Looks like you gave me a good lead! This is how it looks like if I ignore BRDF sampling: i.imgur.com/4c7bRCo.png I'll try to further track down the issue later when I have more time. It seems like the issue is that the impact of the BRDF is overestimated, causing it to overwhelm the light contribution in some cases." CreationDate="2015-08-12T05:54:57.620" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="322" PostId="121" Score="0" Text="@yuriks In this case it's probably a poor example on my part, but everything is meant to be transparent. I wanted something to show how wrong transparency might look when done badly. Also an example where sorting geometry would be amazingly difficult (for example here the floor is one giant polygon and covers the entire depth range)." CreationDate="2015-08-12T06:00:44.490" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="323" PostId="214" Score="1" Text="It's not quite the same thing, but the focus on areas of high gradient reminds me of [gradient-domain path tracing](https://mediatech.aalto.fi/publications/graphics/GPT/). However, before attacking a complex technique like that, I'd start with more basic things like stratified sampling and importance sampling to get the variance down." CreationDate="2015-08-12T06:24:04.383" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="324" PostId="214" Score="1" Text="Here's [a reference on stratified and low-discrepancy sampling](http://cg.informatik.uni-freiburg.de/course_notes/graphics2_04_sampling.pdf) btw." CreationDate="2015-08-12T06:30:08.633" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="325" PostId="211" Score="0" Text="In global illumination, there would be no contact shadow, yes. However, the question is about ambient occlusion, which only performs one bounce. Additionally, as long as the scene is not closed (i.e. the sky is visible), the scene would converge if global illumination was used even with albedo of 1." CreationDate="2015-08-12T08:20:01.360" UserId="79" ContentLicense="CC BY-SA 3.0" />
  <row Id="326" PostId="215" Score="2" Text="You should actually add the significant parts of your code into the question instead of linking them elsewhere." CreationDate="2015-08-12T09:26:27.740" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="327" PostId="216" Score="1" Text="This seems to be more of a comment than an answer. Maybe you could elaborate on why fluorescent materials depend on an ultraviolet channel and provide some references?" CreationDate="2015-08-12T09:35:18.963" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="328" PostId="208" Score="3" Text="Ideally, answers should be self-contained and depend vitally on external links. Having links is nice for supplementary material, but an answer shouldn't just consist of a keyword. If you could include some details on what an A-buffer is and how it works that would greatly improve your answer." CreationDate="2015-08-12T09:38:58.897" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="329" PostId="218" Score="1" Text="Have you tried a closed surface with control points, i.e. nurbs or bspline? or free form deformation stuff? both of these are described by using point, but moving a point you deforms the surface described. (In the mean while i try to gather more info on the problem). I was even thinking to convex hull, but i'm not sure of the result, since a liquid deformation could be not convex at all." CreationDate="2015-08-12T10:08:43.980" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="330" PostId="51" Score="0" Text="@JohnCalsbeek Honestly, I don't have any hard requirements at hand. This was just a question I was curious about which I thought I'd give a go for the private beta. Of course, cutting a 2D slice out of 3D noise will be sufficient for many applications, but I'm sure it will have some performance impact and anisotropies (which may or may not be noticeable). &quot;Cutting the sphere out of 3D noise is your best option, because...&quot; is definitely a valid answer." CreationDate="2015-08-12T11:44:37.037" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="331" PostId="217" Score="2" Text="I'm not sure that's the correct takeaway. I would first double-check all the computations in the BRDF sampling branch. I think ShapeSphere::areaPdf looks suspect. It should depend on the distance squared and both the surface normal and the light normal." CreationDate="2015-08-12T13:37:15.447" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="332" PostId="203" Score="0" Text="Beer's law (absorption of color through a transparent object over distance) is hard to model with rgb." CreationDate="2015-08-12T13:50:13.877" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="333" PostId="219" Score="1" Text="Could you perhaps expand on your answer so it's self-contained. Reading it without the link and its contents, the answer isn't all that clear." CreationDate="2015-08-12T15:36:25.090" UserId="4" ContentLicense="CC BY-SA 3.0" />
  <row Id="334" PostId="219" Score="0" Text="Actually i think the &quot;idea&quot; (not the implementation) is quite clear. @Cristian said he implemented the simulation on a set of points, basically he should consider the set of points as they belong to a deformable surface (i.e. a continuous surface described by a set of discrete points), then when the points position are updated the surface surface shape too is updated. The effectiveness depends on what he wants specifically to achieve, but i've assumed he wanted to use a kind of &quot;continuous&quot; instead of a discrete point set." CreationDate="2015-08-12T15:48:24.733" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="335" PostId="219" Score="0" Text="Also... i don't know why i can't use formulas here... but of course they would be useful for a more detailed explanation. However physically speaking good reference is any book of computational fluidodynamics, for rendering game physics of course, simplified methods could be found in literature too. For &quot;code&quot; itself depends on the environment he's using he can even find a complete implementation on the CUDA toolkit, i'm not sure about openCL, even the directX SDK presents some example of fluid simulation (espacially the last version, but probably it simulates using tons of points too)." CreationDate="2015-08-12T15:53:13.983" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="336" PostId="219" Score="3" Text="My understanding of the question is that the author has the simulation part covered, but is only asking about visualization. I think it'll prove to be quite difficult to create sets of spline patches that satisfactorily represent the &quot;shape&quot; of the water, and there is little detail on this answer about how you'd do that." CreationDate="2015-08-12T16:39:23.130" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="337" PostId="151" Score="0" Text="Actually, after reading your answer again and thinking about it. If I have the current position of the camera I can transform that into spherical coords and then add to the angles the delta produced by the mouse movement, then go back to cartesian coords and update my view matrix, would that make sense?" CreationDate="2015-08-12T16:47:01.913" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="338" PostId="150" Score="0" Text="@JohnCalsbeek ok im starting to build the analytic answer, its going to take a while" CreationDate="2015-08-12T17:50:18.053" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="339" PostId="225" Score="0" Text="Some others: photon mapping, sphere tracing, cone tracing" CreationDate="2015-08-12T20:11:02.493" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="340" PostId="210" Score="1" Text="do you know about srgb correction? If not that could be a factor.  http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html" CreationDate="2015-08-12T21:57:00.563" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="341" PostId="62" Score="0" Text="I had some not-bad results interpolating near the edges of the tile (edge-wrapped), but it depends on what effect you're trying to achieve and the exact noise parameters. Works great for somewhat blurry noise, not so good with spikey/fine-grained ones." CreationDate="2015-08-13T03:17:25.727" UserDisplayName="user379" ContentLicense="CC BY-SA 3.0" />
  <row Id="345" PostId="231" Score="2" Text="Do you happen to have links to any more info about tiled raytracing?  I'm guessing there are multiple ways that tiles can help, and probably different ways that people have exploited those benefits.  Trying to get a toe hold on some info to look a bit deeper at existing techniques." CreationDate="2015-08-13T03:39:39.467" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="346" PostId="231" Score="0" Text="@AlanWolfe, nothing really useful.  The 2x2 tiling was a paper I read several years ago and can't find now, while the larger tiles were just a passing mention on a webpage." CreationDate="2015-08-13T05:20:40.270" UserId="158" ContentLicense="CC BY-SA 3.0" />
  <row Id="347" PostId="216" Score="1" Text="I mentioned this in my post just not using the word flourescent. Anyway this can be accomplised at shader level." CreationDate="2015-08-13T05:35:47.267" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="348" PostId="159" Score="3" Text="Subsurface scattering is for sure going to be important in such a close-up. You might look at Jorge Jimenez's work for some inspiration; see his talks at [SIGGRAPH 2012](http://advances.realtimerendering.com/s2012/index.html) and [GDC 2013](http://www.iryoku.com/stare-into-the-future). His work is for real-time but I'm sure some of the ideas can be adapted." CreationDate="2015-08-13T06:13:57.473" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="349" PostId="233" Score="3" Text="Short answer: &quot;light probes&quot; usually means a compact spherical harmonic representation of the environment. They're blurry both because they're very compact (only a few tens of bytes storage) and because they're prefiltered for use as diffuse lighting. I'll expand into a longer answer when I have a chance. :)" CreationDate="2015-08-13T06:18:42.793" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="350" PostId="226" Score="0" Text="related: http://computergraphics.stackexchange.com/questions/161/what-is-ray-marching-is-sphere-tracing-the-same-thing" CreationDate="2015-08-13T07:36:10.913" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="352" PostId="176" Score="0" Text="@yuriks `translucent: (of a substance) allowing light, but not detailed shapes, to pass through`. Translucent generally has the meaning used in the question, so this seems clear." CreationDate="2015-08-13T10:58:40.523" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="354" PostId="181" Score="0" Text="There is discussion and guidance about SE cross posting [on Meta Stack Exchange](http://meta.stackexchange.com/questions/64068/is-cross-posting-a-question-on-multiple-stack-exchange-sites-permitted-if-the-qu). Despite it saying not to cross post, I think it is a good thing that you have posted here and the question clearly fits here. The meta post mentions migrating rather than cross posting, and alternatively deleting the original post (since it is too late to migrate in this case, now that you have posted)." CreationDate="2015-08-13T11:21:38.833" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="355" PostId="233" Score="0" Text="Is there any context for your question? The term 'light probe' is ambigious and used differently in different scenarios, algorithms and engines." CreationDate="2015-08-13T12:07:22.630" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="356" PostId="233" Score="0" Text="Specifically, if you have been to shadertoy.com you'll see two sets of cube maps available for use.  One set is environment maps, the other set looks the same but blurry that are labeled light probes.  Just curious about that and light probes in general." CreationDate="2015-08-13T14:49:46.413" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="357" PostId="234" Score="0" Text="Just want to note that you can do actual integration of a ray path analytically as well, you don't have to use Ray marching if it's undesirable." CreationDate="2015-08-13T15:29:46.127" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="358" PostId="234" Score="0" Text="@AlanWolfe thats what you do in the uniform case, however if the medium participates with geometry then you need to do something more nifty. Anyway i didnt claim this is all methods." CreationDate="2015-08-13T15:53:14.523" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="359" PostId="234" Score="0" Text="For sure, just adding to your answer.  When you say uniform case not sure what you mean exactly but for the case of fog, it doesn't have to be uniform density, just some density function that you can integrate. Is that what you meant by uniform case?" CreationDate="2015-08-13T16:05:33.680" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="360" PostId="219" Score="0" Text="Many of us would like to see the ability to use MathJax for formulas, and plenty of other SE sites already have it. All we need to do is demonstrate a need for it. Any questions that would benefit from MathJax, just add a link to them to [this meta answer](http://meta.computergraphics.stackexchange.com/a/34/231)." CreationDate="2015-08-13T17:23:20.537" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="361" PostId="219" Score="0" Text="The additional information in your comments would make good additions to the answer." CreationDate="2015-08-13T17:25:43.667" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="363" PostId="241" Score="1" Text="Perhaps because CMYK has 4 components? RGB is a triangle, where each corner represents one of the components. So, it would makes sense that CMYK would be a 4 sided shape, since there are 4 components (Cyan, Magenta, Yellow, Black)" CreationDate="2015-08-13T19:54:53.377" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="366" PostId="241" Score="0" Text="@RichieSams yes, thats what i used to tgink. but black is absense of luminance the luminance is on a axis thats perpendicular on your screen. So black should be away from your viewing direction." CreationDate="2015-08-13T20:37:18.053" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="367" PostId="238" Score="0" Text="I've edited to add syntax highlighting - feel free to change the language if there's a more suitable one." CreationDate="2015-08-13T20:51:25.383" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="368" PostId="241" Score="0" Text="Ok, i think i know why... i need to check this from the media department. But ill leave this question for a while so others have time to answer the question. Its definitely not the K component." CreationDate="2015-08-13T21:06:05.027" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="369" PostId="242" Score="0" Text="Actually i think its clipped that way for the same reason that the cromaticity graphs outer edge abruptly changes direction. The space model is not entirely trivial but yes your right the space shape is complex. And its volume cant be drawn because my fogra measurements in the icc profile dont work all the way.+1" CreationDate="2015-08-13T21:23:10.087" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="370" PostId="241" Score="0" Text="Yea, that makes sense, now that I think about it. Black isn't a color. It's the absence of color." CreationDate="2015-08-13T21:23:33.710" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="371" PostId="243" Score="0" Text="iir is likely &quot;infinite impulse response&quot; which is a type of filtering (the other being finite impulse response). Not sure what rle could stand for. It makes me think of &quot;run length encoding&quot; but I don't think that's what it stands for." CreationDate="2015-08-14T04:03:51.747" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="372" PostId="243" Score="0" Text="I guess I was right about rle but don't understand how it applies.  http://docs.gimp.org/en/plug-in-gauss.html" CreationDate="2015-08-14T04:08:03.363" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="373" PostId="244" Score="0" Text="Great explanation! Just to confirm about the part about using trilinear filtering on the mips... Mips are preferred over volume textures because they use less resolution, and higher Blur levels need less resolution?" CreationDate="2015-08-14T05:37:33.373" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="374" PostId="244" Score="1" Text="I'd say that both these &quot;kinds&quot; of light probes are the same. Light probes are a measure of the radiance (usually pre-convoluted in some way) received at a point in the scene. Cubemaps and SH are just different ways to store/compute a light probe, making different storage/perf/quality trade-offs. (EDIT: Just to make it clear, I agree with the answer, I just think it's counter-productive to think of them as separate things.)" CreationDate="2015-08-14T05:39:21.823" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="375" PostId="216" Score="0" Text="@joojaa: Sorry.. missed that. I'd delete my post if there was an obvious button to do so. Though, having said that, I would say that you'd still need extra channels elsewhere (and not just shaders) to handle it, e.g. on-the-fly generation of environment maps." CreationDate="2015-08-14T08:50:14.530" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="376" PostId="216" Score="2" Text="Delete or don't delete it, same for me. I would rather see you  expand it., there's nothing wrong with supporting evidence and things said differently as long a you contribute with better clarity or new info." CreationDate="2015-08-14T09:19:49.443" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="377" PostId="240" Score="1" Text="Well, there are *quite* a bit more and *quite* a bit more advanced algorithms for mesh parameterization than what you linked to, so I wouldn't really subscribe to the idea that it *&quot;won't be pretty&quot;*, let alone such a short answer that barely touches the tip of the iceberg." CreationDate="2015-08-14T11:02:31.647" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="378" PostId="244" Score="0" Text="@AlanWolfe There is a use and that is to have maps for diffuse ligtning taken from a real environment (called gray ball/white ball), not just specular. Heres a disucssion of both [Mirror/Gray Ball Shaders](http://docs.autodesk.com/MENTALRAY/2015/ENU/mental-ray-help/files/shaders/production/prod_mirrorball.html)" CreationDate="2015-08-14T11:04:40.630" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="379" PostId="240" Score="0" Text="Could you elaborate/link any of those algorithms? I will be happy to add more to the answer." CreationDate="2015-08-14T11:40:49.107" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="380" PostId="240" Score="0" Text="Not at the moment really." CreationDate="2015-08-14T13:37:17.943" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="381" PostId="244" Score="1" Text="@AlanWolfe Correct&amp;mdash;you need less resolution on the more blurred levels. Also, cubemap volume textures aren't a thing that exists in hardware. :)" CreationDate="2015-08-14T18:12:25.503" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="382" PostId="243" Score="0" Text="&quot;RLE Gaussian Blur is best used on computer-generated images or those with large areas of constant intensity.&quot; Makes me think it's some kind of optimization of the blur process, that's somehow data-dependent so it works better on certain kinds of images? Is there any visible difference between the results of the two?" CreationDate="2015-08-14T21:32:57.017" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="383" PostId="243" Score="1" Text="This question seems very poorly formulated... You could have taken the time to do a quick search to at least get the definitions for the acronyms, since the previous commenters don't seem to know what they mean exactly." CreationDate="2015-08-15T04:18:03.440" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="384" PostId="252" Score="4" Text="Side by side images *really* make a difference for getting an intuitive feel for the explanation. I'm going to bear this in mind for my own answers." CreationDate="2015-08-15T09:49:46.923" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="385" PostId="240" Score="2" Text="@RichieSams: A very popular algorithm that quickly gained popularity is the least sqares conformal maps algorithm: https://www.cs.jhu.edu/~misha/Fall09/Levy02.pdf" CreationDate="2015-08-15T10:53:50.973" UserId="29" ContentLicense="CC BY-SA 3.0" />
  <row Id="387" PostId="258" Score="0" Text="Does this mean that applying a Gaussian blur 4 times would give twice the size, as required?" CreationDate="2015-08-15T20:08:25.447" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="388" PostId="258" Score="0" Text="Yes, absolutely!" CreationDate="2015-08-15T20:12:56.647" UserId="79" ContentLicense="CC BY-SA 3.0" />
  <row Id="389" PostId="257" Score="0" Text="Is the directional blurring you are referring to motion blur?" CreationDate="2015-08-15T20:14:08.747" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="390" PostId="257" Score="0" Text="An example before and after image would help confirm what type of blur is being asked about." CreationDate="2015-08-15T20:14:44.000" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="391" PostId="258" Score="0" Text="That covers the question entirely then - would it be worth mentioning how to get twice the size in the question so we can tidy away the comments?" CreationDate="2015-08-15T20:19:31.133" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="392" PostId="260" Score="5" Text="Rendering to a smaller texture and upsampling is a good way to do it. But if for some reason you really need to write to every 16th pixel of the large texture, using a compute shader with one invocation for every 16th pixel plus image load/store to scatter the writes into the render target could be a good option." CreationDate="2015-08-16T00:28:39.233" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="393" PostId="258" Score="0" Text="If it takes 4 to double the size, what size would 3  represent?" CreationDate="2015-08-16T01:01:10.063" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="394" PostId="258" Score="0" Text="I found the answer is N*sqrt(3). It turns out that the total Blur amount of multiple blurs is equal to the square root of the sum of the sizes squared. Wikipedia says that: https://en.wikipedia.org/wiki/Gaussian_blur" CreationDate="2015-08-16T01:08:55.603" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="395" PostId="15" Score="0" Text="Since [the wikipedia page](https://en.wikipedia.org/wiki/Subdivision_surface) lists a number of different improvements over the intervening time (almost 4 decades), and branching into several different types (approximating/interpolating, quads/triangles), I suspect this question may be too broad to be a good fit for this site." CreationDate="2015-08-16T11:58:54.183" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="396" PostId="15" Score="1" Text="I've [raised this on meta](http://meta.computergraphics.stackexchange.com/questions/82/is-this-question-too-broad) to see what people think." CreationDate="2015-08-16T12:06:31.653" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="398" PostId="266" Score="1" Text="My guess would be that if the UV coords are calculated in VS, the texture unit can start to prefetch them while the PS is starting. If they're calculated in the PS, the texture unit has to wait first." CreationDate="2015-08-16T14:41:33.030" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="399" PostId="266" Score="2" Text="Fwiw this is called a &quot;dependant texture read&quot;, in case it helps your search." CreationDate="2015-08-16T14:51:05.583" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="400" PostId="263" Score="0" Text="BTW just to help future folk. Bicubic interpolation will give higher quality results than bilinear when taking samples, at a higher computational cost." CreationDate="2015-08-16T15:21:07.823" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="401" PostId="271" Score="1" Text="I know there is Instancing in OpenGL, that's why I also posted an answer here. But maybe there is also some other way to achieve the same result." CreationDate="2015-08-16T16:28:04.357" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="402" PostId="266" Score="0" Text="Do you have some measurements showing the perf difference? I actually wouldn't expect there to be much difference at all; texture fetch latency should swamp a few ALU ops. BTW, dependent texture reads are where there are two (or more) texture reads, with the coordinates for the second dependent on the output of the first. Those are slower because of the strict ordering required between the two texture reads." CreationDate="2015-08-16T17:58:36.403" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="403" PostId="271" Score="1" Text="Seems you were right... :)" CreationDate="2015-08-16T21:17:37.130" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="405" PostId="266" Score="0" Text="Well, any operation done in the fragment shader will be more expensive then in the vertex shader. Each triangle takes 3 invocations of a vertex shader, but it might take orders of magnitude more invocations of the fragment shader, depending on its screen size." CreationDate="2015-08-17T01:13:54.647" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="406" PostId="51" Score="0" Text="You might check out this shadertoy which does noise on a sphere:&#xA;https://www.shadertoy.com/view/4sfGzS" CreationDate="2015-08-17T14:58:17.797" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="407" PostId="278" Score="1" Text="Have you tried the normal distribution (Gaussian function)? It seems that it would help here since it is used to figure out where things will be on average with certain characteristics of probabilities.  Dust settling randomly but less often where there's more airflow and more often in crevices seems right in its wheelhouse." CreationDate="2015-08-17T17:23:19.647" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="408" PostId="277" Score="3" Text="Are you sure they were talking about multiple gaussian blurs? Doing several Box blurs is a common way to approximate a gaussian blur." CreationDate="2015-08-17T17:26:18.647" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="409" PostId="277" Score="0" Text="Interesting info.  I believe so, yes, but could be mistaken!" CreationDate="2015-08-17T17:32:44.280" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="410" PostId="278" Score="0" Text="@AlanWolfe thanks for the suggestion - I've added in some more images based on that." CreationDate="2015-08-17T18:22:25.437" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="411" PostId="278" Score="0" Text="exponential looks better to me than linear or the normal distribution based one, but i don't have any non opinion answers to back anything up about correctness :P" CreationDate="2015-08-17T18:38:04.013" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="412" PostId="277" Score="1" Text="It may be simpler just to sample neighboring pixels, its also much more intuitive as a physical model of diffusion, see [12 steps to Navier-Stokes, step 7](http://nbviewer.ipython.org/github/barbagroup/CFDPython/blob/master/lessons/09_Step_7.ipynb)" CreationDate="2015-08-17T18:41:06.873" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="413" PostId="278" Score="0" Text="How about a cellular automata of some kind? Diffusion step and then erode diffuse then erode..." CreationDate="2015-08-17T18:51:53.140" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="414" PostId="280" Score="1" Text="possible duplicate of [Why is this conditional in my fragment shader so slow?](http://computergraphics.stackexchange.com/questions/259/why-is-this-conditional-in-my-fragment-shader-so-slow)" CreationDate="2015-08-18T08:46:09.777" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="415" PostId="280" Score="0" Text="As the answer explains on my question, the fragments get grouped into &quot;warps&quot; or &quot;wavefronts&quot; and if all fragments in such a group use the same branch, only that branch is executed." CreationDate="2015-08-18T08:47:44.803" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="416" PostId="280" Score="0" Text="But what about shaders different from fragment?" CreationDate="2015-08-18T08:58:20.317" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="417" PostId="280" Score="2" Text="I believe [vertices get assembled into warps or wavefronts just the same](https://fgiesen.wordpress.com/2011/10/09/a-trip-through-the-graphics-pipeline-2011-part-13/)." CreationDate="2015-08-18T09:01:05.100" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="418" PostId="280" Score="0" Text="The result being that if all threads follow the same path there will have less &quot;divergence&quot; and the shader will run faster. E.g. I don't think putting `if (true) { ... }` around the entire program would measurably alter the performance (assuming it's not optimized it out, which it would be)." CreationDate="2015-08-18T09:02:36.593" UserId="198" ContentLicense="CC BY-SA 3.0" />
  <row Id="419" PostId="280" Score="0" Text="@jozxyqk I think the OP is more interested in wrapping the entire program in `if (false) { ... }` but yes. :)" CreationDate="2015-08-18T09:04:51.867" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="420" PostId="266" Score="0" Text="@NathanReed I don't think you have to limit &quot;dependent texture reads&quot; to just those that come from a previous texture access. I'd probably also include any coordinates computed in the frag shader, as opposed to those that can be determined merely from the linear (well, hyperbolic with perspective) interpolation of vertex attributes." CreationDate="2015-08-18T10:15:51.753" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="422" PostId="280" Score="1" Text="I suspect this is not a duplicate, but it needs to be edited to make it clear what is being asked before that can be determined. Some example code or an explanation of the two options being compared would help a lot." CreationDate="2015-08-18T13:29:55.827" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="423" PostId="280" Score="0" Text="`Is condition anyway will be executed every time?` Are you asking whether the condition is calculated for each pixel/vertex/fragment, or whether the condition is recalculated each time the shader is applied?" CreationDate="2015-08-18T13:31:58.820" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="424" PostId="280" Score="0" Text="Even though there is an accepted answer, the question will still benefit from being edited so that it is clear to future readers." CreationDate="2015-08-18T13:34:47.717" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="430" PostId="275" Score="0" Text="By the way: I experienced that on an iPad 3. So maybe this is actually hardware specific." CreationDate="2015-08-18T17:18:20.950" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="436" PostId="283" Score="0" Text="I think you might get a good answer from the signal processing or math site." CreationDate="2015-08-19T01:01:03.970" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="437" PostId="283" Score="1" Text="I'm hoping that asking on computergraphics.SE will lead to answers that don't just give me signal processing theory or mathematical proofs, but the perspective of people who work with and research computer graphics. There may be something I haven't thought of that makes the question irrelevant, or it may only matter in certain circumstances, and if so I want the computer graphics angle on that." CreationDate="2015-08-19T10:02:58.427" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="438" PostId="283" Score="0" Text="I've no idea how you would efficiently achieve random access to the final constructed data, nor how to extend it to 3D, but could you use something based on aperiodic tiling, e.g. https://en.wikipedia.org/wiki/Penrose_tiling ? i.e. have a random value at the centre of each tile?" CreationDate="2015-08-19T11:47:19.700" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="439" PostId="283" Score="0" Text="(caught by the edit period timeout) ...this was presuming you are concerned with more global alignment of the grid points and not just local effects." CreationDate="2015-08-19T11:53:46.887" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="440" PostId="283" Score="0" Text="@SimonF I'm happy to use any kind of aligned grid (just a square grid would be fine) if a random offset can be applied to each vertex using a probability distribution that results in no grid aligned average frequency variations. I'd be interested to hear about another grid type if that makes the frequency isotropic - or any approaches that haven't occurred to me. My suspicion is that an aperiodic tiling, although globally isotropic, will be locally biased for any practical length scales." CreationDate="2015-08-19T12:08:53.813" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="441" PostId="283" Score="1" Text="@trichoplax Another thought that occurred to me is that the displacements you're suggesting sound like the schemes used to approximate minimum distance Poisson disc distributions using a jittered grid, e.g as used for antialiasing.  I believe some care is needed when choosing how to generate those jittered offsets. I tried quick search in my papers collection and one that sprang up is &quot;Filtered Jitter&quot;, by V. Klassen, (http://onlinelibrary.wiley.com/doi/10.1111/1467-8659.00459/abstract). It's from 2000 so there may be better approaches, but it's surely worth a try." CreationDate="2015-08-19T13:05:35.270" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="443" PostId="284" Score="1" Text="What do you mean by &quot;texture baking&quot;? I'm not familiar with this use of the term." CreationDate="2015-08-19T14:23:54.310" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="444" PostId="283" Score="2" Text="Here's an interesting paper: http://www.cs.utah.edu/~aek/research/noise.pdf (useful keywords: &quot;Fourier spectrum&quot;)" CreationDate="2015-08-19T14:39:14.103" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="445" PostId="284" Score="0" Text="@Calsbeek, its turning 3d calculation on the surface back to a texture in 2d for re-using. Pixar has a paper or technical report where they coin the name. Id search for you but its a bit painfull to do these things on the phone while in transit." CreationDate="2015-08-19T14:39:17.913" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="446" PostId="166" Score="0" Text="See also [Massively-Parallel Vector Graphics](http://w3.impa.br/~diego/projects/GanEtAl14/), published in SIGGRAPH Asia 2014." CreationDate="2015-08-20T01:24:20.347" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="447" PostId="166" Score="0" Text="Between the options you've listed in your question and the Loop and Blinn paper, I think you've pretty much exhausted all the possibilities." CreationDate="2015-08-20T05:41:34.620" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="448" PostId="57" Score="1" Text="You might want to take a look into [finite-element methods](https://en.wikipedia.org/wiki/Finite_element_method), which also use triangulation (or more generally: simplices) and often face the problem of needing a higher sampling density in selected regions. They are bound to have developed algorithms for this." CreationDate="2015-08-20T06:56:31.913" UserId="394" ContentLicense="CC BY-SA 3.0" />
  <row Id="449" PostId="293" Score="0" Text="You were like a few seconds faster than I and even have information about Spir-V, which I did not even know." CreationDate="2015-08-20T11:07:42.403" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="450" PostId="166" Score="0" Text="You can tessellate a line, like described [here](http://www.gamedev.net/page/resources/_/technical/directx-and-xna/d3d11-tessellation-in-depth-r3059). Or you can triangulate in a compute shader." CreationDate="2015-08-20T13:28:20.537" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="451" PostId="297" Score="2" Text="You're comparing a cube with a more complex mesh however. Why not replicate the same scenario? The Susan model is easy to get." CreationDate="2015-08-20T15:15:03.147" UserId="4" ContentLicense="CC BY-SA 3.0" />
  <row Id="452" PostId="297" Score="0" Text="It's not so easy in a shadertoy implementation! (:" CreationDate="2015-08-20T15:16:15.037" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="453" PostId="297" Score="2" Text="You cube looks correct to me: Get more transparent as it approaches the edges. If you can do full blown Suzanne the a sphere should at least give a better approximation of the look in the other picture." CreationDate="2015-08-20T15:51:06.460" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="454" PostId="297" Score="0" Text="ok fair enough.  i'll try it with better geometry, but i think you are right.  I'll accept it if you make an aswer!" CreationDate="2015-08-20T16:37:42.147" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="455" PostId="299" Score="1" Text="GLSL functions cannot return a value? Hum? From where did you get this idea?" CreationDate="2015-08-20T17:41:34.143" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="456" PostId="280" Score="0" Text="If you add some example code to your question, we can more easily tell you if it is a compile time constant or not (:" CreationDate="2015-08-20T17:59:09.763" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="457" PostId="299" Score="0" Text="I always used to the value-return calling convention as described here https://www.opengl.org/wiki/Core_Language_%28GLSL%29#Functions. If it is possible to define the return type and use &quot;return variable;&quot; syntax I will change it. EDIT: nvm, you're right. The OpenGL 4.5 specification says it should be possible (but I don't know when it was introduced)" CreationDate="2015-08-20T18:14:37.627" UserId="64" ContentLicense="CC BY-SA 3.0" />
  <row Id="458" PostId="299" Score="1" Text="Return values have always been supported, AFAIK. If you look at the [1.2 specification, section 6.1](https://www.opengl.org/registry/doc/GLSLangSpec.Full.1.20.8.pdf), you can see the function declaration syntax: `returnType functionName (type0 arg0, type1 arg1, ..., typen argn);`. Plus, a lot of builtin functions return a value... Perhaps the existence of `in/out` parameters has tricked you into thinking it didn't support return values, but both are orthogonal concepts. Apart from that, your answer is pretty good, btw ;)" CreationDate="2015-08-20T18:26:30.610" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="459" PostId="299" Score="1" Text="Yeah, I probably just looked at the wiki instead of the specification itself. Never occurred to me that both methods are possible, although indeed the buildin functions do use it." CreationDate="2015-08-20T18:42:21.180" UserId="64" ContentLicense="CC BY-SA 3.0" />
  <row Id="460" PostId="300" Score="0" Text="This question would benefit from a few (artificial) sample pictures" CreationDate="2015-08-20T18:59:56.403" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="462" PostId="304" Score="0" Text="Indeed, the idea was to use complex 3D models, but surround them with rough brushes for portaling. I had brifely considered using voxelization, but dismissed it due to thinking the cost of traversing it would be prohibitive, but I didn't even think of using a tree structure! (Which looks obvious in restrospect...)" CreationDate="2015-08-20T21:26:14.777" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="463" PostId="303" Score="0" Text="Now that you mentioned culling this makes a lot of sense. I just checked and there's a culling mask property in light objects to control which meshes (in a layers) are affected by it." CreationDate="2015-08-20T21:39:29.170" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="465" PostId="281" Score="4" Text="This is true, but it is *not* the only thing you need to consider for performance. GPUs still statically schedule resources per shader, so this may still resources as though you were executing both branches, which may hurt occupancy." CreationDate="2015-08-21T06:16:53.847" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="467" PostId="306" Score="0" Text="As a matter of fact Adam already posted source code on shadertoy. Here's the link: https://www.shadertoy.com/view/ltXSDB" CreationDate="2015-08-21T13:02:37.213" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="468" PostId="306" Score="0" Text="You had me excited.  He did post the bezier stuff on shadertoy but not the texture distance field stuff!" CreationDate="2015-08-21T14:33:14.070" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="469" PostId="313" Score="1" Text="Thanks for answering. What does BSDF stand for?" CreationDate="2015-08-21T15:41:28.387" UserId="433" ContentLicense="CC BY-SA 3.0" />
  <row Id="470" PostId="298" Score="6" Text="The new explicit graphics APIs aren't really designed for consumption by the average graphics programmer, they're more for the people doing infrastructure work in engines, or for those who *really* need the performance. They give you a much bigger gun and point it right at your foot." CreationDate="2015-08-21T16:07:06.500" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="471" PostId="313" Score="3" Text="BSDF = Biderectional Scattering Distribution Function" CreationDate="2015-08-21T16:41:42.903" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="472" PostId="315" Score="4" Text="There can definitely be differences between IHV implementations. It could be due to a driver bug, or different interpretations of ambiguities in the standard, or even possibly differences in how the compilers treat floating-point arithmetic, etc. Would need some more detailed debugging to understand what's going on." CreationDate="2015-08-21T20:20:52.653" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="474" PostId="306" Score="0" Text="@AlanWolfe I think he has done only for procedurally set bezier curves. I'm not sure the effort required to integrate this into a ttf render lib. When I have some time I'll take a look at it." CreationDate="2015-08-21T22:07:14.407" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="475" PostId="306" Score="0" Text="it looks he has some magic sauce on the side of actually storing and retrieving the distances from a texture.  Without a texture in play, the shadertoy examples are missing that part of the equation." CreationDate="2015-08-21T22:11:30.443" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="476" PostId="315" Score="1" Text="@NathanReed ,what kind of information could be helpful?" CreationDate="2015-08-22T00:20:14.553" UserId="437" ContentLicense="CC BY-SA 3.0" />
  <row Id="477" PostId="315" Score="7" Text="Start by debugging it like any other graphics/shader problem. Isolate each pass and see in which pass the error is being introduced, then isolate where in that shader is something going wrong." CreationDate="2015-08-22T00:47:23.243" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="478" PostId="320" Score="0" Text="I dont know but it certainly does make sense not to gamma correct." CreationDate="2015-08-22T12:27:02.963" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="479" PostId="319" Score="1" Text="This is actually a pretty good method. But is it less expensive?" CreationDate="2015-08-22T12:28:39.017" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="480" PostId="311" Score="0" Text="It would be great if you had reference footage for the effect you want. Say, something like this? https://www.youtube.com/watch?v=XH-groCeKbE" CreationDate="2015-08-22T15:20:44.137" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="481" PostId="311" Score="0" Text="@JohnCalsbeek yes that would make it easier to get across what I want. In the video you linked to the individual birds are discernable (just). I'm looking to render a flock a little more distant so that individuals are not visible, but the variations in density are still consistent and realistic." CreationDate="2015-08-22T15:24:42.263" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="482" PostId="26" Score="2" Text="I believe Simplex noise is only patented for 3D and above." CreationDate="2015-08-22T15:49:00.753" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="483" PostId="319" Score="0" Text="It's as expensive as you want your simulation to be. If it's *too* expensive, use fewer samples." CreationDate="2015-08-22T16:02:29.190" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="484" PostId="320" Score="0" Text="I'm not posting this as an answer since I'm not confident in it, but human vision's perception of brightness is *not* linear. In fact, sRGB does a quite good job of compensating for that and giving the most precision in the areas that matter. So you might find that gamma correcting before compressing luma may actually yield worse results." CreationDate="2015-08-22T19:55:49.240" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="489" PostId="324" Score="0" Text="If you are worried about the square root, you can probably square both sides of the equation." CreationDate="2015-08-24T00:26:41.520" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="490" PostId="324" Score="0" Text="@AlanWolfe I can precompute the square root - it's only ever root 2. My main reason for changing the side length is the significant reduction in area. I just wonder if it broke anything..." CreationDate="2015-08-24T00:35:46.710" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="491" PostId="320" Score="0" Text="AFAIK, video standards assume R'G'B', ie. a non-linear colour space, when applying the 3x3 colour transforms to/from YCbCr.  In an application such as video where one wants to maximise quality per bit, it doesn't make sense to use linear.&#xA; I think sections 27 and 29 of Charles Poyton's Color FAQ express it more clearly: http://poynton.com/notes/colour_and_gamma/ColorFAQ.html#RTFToC27" CreationDate="2015-08-24T07:54:04.493" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="492" PostId="320" Score="0" Text="&quot;Video demystified&quot; also says: _&quot;YCbCr is the color space originally defined by BT.601, and now used for all digital component video formats. .... The technically correct notation is&#xA;Y'Cb'Cr' since all three components are derived from R'G'B'.&quot;_" CreationDate="2015-08-24T08:07:05.627" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="493" PostId="247" Score="0" Text="Subdivision surfaces are used a lot more than constructive solid geometry. They still involve triangles (or alternatively splines)." CreationDate="2015-08-24T18:06:28.447" UserDisplayName="user458" ContentLicense="CC BY-SA 3.0" />
  <row Id="494" PostId="326" Score="1" Text="Isn't the point of stereoscopic rendering to render the occluded parts of both views for each eye?" CreationDate="2015-08-24T20:55:31.253" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="495" PostId="326" Score="0" Text="IMO, occlusion isn't really the point of it.&#xA;&#xA;The point is the illusion of depth that happens when giving different levels of parallax to each eye based on an object's distance." CreationDate="2015-08-24T20:58:08.737" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="496" PostId="325" Score="0" Text="This is going to be a zero net comment, but... just wanted to mention, if you are unsure of performance, you could profile and see.  But, of course, there might be different characteristics on different hardware that you might not have access to, and you may not be aware of the ways it might be faster or slower.  Like, texture reads are really cheap til you are texture read bound :P" CreationDate="2015-08-24T23:36:28.950" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="497" PostId="328" Score="0" Text="I've watched that presentation before, it is very good. I did not know about what you mention in the first paragraph though, so thanks for that info!" CreationDate="2015-08-25T01:39:07.027" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="498" PostId="318" Score="0" Text="I hadn't heard of bilateral up sampling. Do you have any links by chance? It's different than bicubic or bilinear sampling right?" CreationDate="2015-08-25T01:42:47.320" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="499" PostId="329" Score="0" Text="The question lacks some basic explanation about what is being asked. What exactly are you trying to do? From your video, what seems to be happening is that when the rotation crosses a certain point, the rotation instantly snaps back/forward by 180 degrees. (That is, the colors aren't being swapped, you're seeing the back of the cube instead.)" CreationDate="2015-08-25T04:39:10.677" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="500" PostId="329" Score="0" Text="I assume that the `m_setPan` path isn't used here? Also, why does your rotation code modify position? (And I don't see it setting a rotation anywhere.)" CreationDate="2015-08-25T04:49:56.480" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="501" PostId="329" Score="0" Text="You assume correct. What I'm doing is to orbit the camera over a sphere centered at the origin, therefore given the displacement in (x,y) of the mouse, I compute the spherical coords of the position of the camera on that sphere, once I have placed the camera I update the view-matrix (done inside `setPosition()`)" CreationDate="2015-08-25T04:53:57.070" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="502" PostId="329" Score="1" Text="Just as a note, this is the kind of thing that can be most easily debugged by printing the `m_position`, `theta` and `phi` values to the screen on every update and observing their values as you drag the cube around. This should make it easy to observe any singularities or abnormalities that could be causing the problem." CreationDate="2015-08-25T05:06:13.067" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="503" PostId="330" Score="2" Text="This is usually called &quot;stereo reprojection&quot;, since one way of implementing it is to take the matrix that transforms from the new perspective to the old perspective." CreationDate="2015-08-25T05:15:09.187" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="504" PostId="330" Score="0" Text="@yuriks that makes sense." CreationDate="2015-08-25T05:16:25.527" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="505" PostId="333" Score="0" Text="You could render every pass to a texture and read back the buffers to obtain the output." CreationDate="2015-08-25T06:08:38.767" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="507" PostId="331" Score="1" Text="or use atan2 which is there to deal with all quadrants correctly" CreationDate="2015-08-25T07:53:23.183" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="508" PostId="302" Score="1" Text="FWIW, some monitors come with a built-in motorised calibration unit. It's amusing to watch the &quot;arm&quot; rise out of the bezel so it can sample the screen output during the calibration process." CreationDate="2015-08-25T09:20:40.457" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="509" PostId="331" Score="2" Text="@ratchetfreak Yes, but in general, solution that avoid trigonometry or angles tend to be more robust and elegant." CreationDate="2015-08-25T10:06:14.517" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="510" PostId="335" Score="0" Text="This is a bit too much stuff for one question. I can essentially boil down my introductory lecture on the subject in a few hours. Bit going into detail of use of De casteljanu and the de boor's algorithm would take me too much time." CreationDate="2015-08-25T13:42:10.423" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="511" PostId="335" Score="1" Text="So i would like to see questions 3, 5 and possibly 6 split off as separate questions to make answering and understanding more meal sized." CreationDate="2015-08-25T13:51:16.213" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="512" PostId="335" Score="0" Text="@joojaa Sure, I can split the question, just a sec..." CreationDate="2015-08-25T14:03:19.830" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="513" PostId="335" Score="1" Text="So the question [3](http://computergraphics.stackexchange.com/questions/338), [5](http://computergraphics.stackexchange.com/questions/339/nurbs-curve-drawing) and [6](http://computergraphics.stackexchange.com/questions/340/splitting-of-nurbs-curves) were split off to separate questions." CreationDate="2015-08-25T14:11:09.837" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="514" PostId="340" Score="0" Text="I don't know if it will do the same, but De Boor's algorithm is the equivalent of De Casteljeau.  Interestingly, I know you can use De Boor's algorithm to split a NURBS or b-spline into a piecewise Bezier curve." CreationDate="2015-08-25T14:44:00.587" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="515" PostId="338" Score="0" Text="What do you mean exactly by offset curves?" CreationDate="2015-08-25T14:44:27.810" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="516" PostId="338" Score="0" Text="An &quot;offset&quot; curve has constant distance to the given curve, aka [parallel curve](http://mathworld.wolfram.com/ParallelCurves.html)." CreationDate="2015-08-25T14:47:56.580" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="525" PostId="342" Score="2" Text="I don't have a citation, but if I recall correctly Monster's University was the first Pixar movie to move towards a near-full radiosity solution instead of doing lots of artist controlled &quot;False Radiosity,&quot; so it would certainly seem like it's still used." CreationDate="2015-08-25T18:38:31.753" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="528" PostId="341" Score="0" Text="My understanding is that if your not using the peculiarities of nonuniformness/rationality  then they are using  possibly the same berentein basis formulations. In fact i seem to remember somebody telling me that in fact B-splines started that way. Its only later that we have realized they are the same. Some examples: https://reference.wolfram.com/language/ref/BSplineBasis.html" CreationDate="2015-08-25T19:33:50.240" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="529" PostId="225" Score="4" Text="I vote to close this question, because it is too broad. There are simply too many variants, especially when including *possibly others* like the ones [Alan Wolfe named in his comment](http://computergraphics.stackexchange.com/questions/225/ray-based-rendering-terms#comment339_225). More specific/narrow questions that compare a few techniques e.g. to achieve a specific goal could very well be suited for this format." CreationDate="2015-08-25T20:29:11.020" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="530" PostId="341" Score="0" Text="Right. A bezier curve is a special case of a bspline and a bspline is a special case of a NURBS." CreationDate="2015-08-25T21:47:18.070" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="531" PostId="344" Score="0" Text="The notation for path tracing suggests that it can't handle paths like `ES*L` but of course it can if they are area lights (not punctual lights). Plus, I think that statement in your reference [2] is just plain wrong. Path tracing doesn't ignore caustics; it's just not very efficient at them (photon mapping, Metropolis, VCM etc. are better)." CreationDate="2015-08-25T22:20:52.263" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="532" PostId="344" Score="0" Text="Thanks Ecir for the explanation (specially the regex... I wonder if they ever considered E{2} for both eyes ;). When I mentioned &quot;ray tracing&quot; I was kind of quoting the tutorial of Cornell University, they didn't mention any specific technique, that's why I was doubting if radiosity was a type or partly belonged to ray tracing. So if you were to create a diffuse reflection, would you choose path-tracing over radiosity? Why (which one would be more efficient)?" CreationDate="2015-08-26T01:17:20.417" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="533" PostId="225" Score="1" Text="I would split this question to a few more questions.. We already have a pretty good answer for [ray-marching](http://computergraphics.stackexchange.com/questions/161/what-is-ray-marching-is-sphere-tracing-the-same-thing)." CreationDate="2015-08-26T10:27:57.190" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="534" PostId="343" Score="0" Text="regarding (*), Bezier curves have the same problem.  The issue there is that the curves (in both cases) are defined as x = f(t), y = f(t). However, you can also define a univariate / explicit / 1 dimensional curve (again, in both cases) as y = f(x), using x in place of t.  In the case of rational curves, instead of being able to represent conic sections, you can represent sine and cosine (and more of course).  NURBS / b-splines aren't special in that regard." CreationDate="2015-08-26T14:04:49.080" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="536" PostId="343" Score="0" Text="I think your statement about length is wrong (only works with linear functions?), and not sure how length calculations is supposed to fit in your explanation (good info you gave, just sayin'!)" CreationDate="2015-08-26T14:16:36.920" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="538" PostId="343" Score="0" Text="@AlanWolfe deleted anyway" CreationDate="2015-08-26T14:21:08.100" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="539" PostId="287" Score="1" Text="Just for future people's curiosity, you might want to make &quot;another question&quot; be a link to that question." CreationDate="2015-08-26T15:22:03.767" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="540" PostId="287" Score="1" Text="@porglezomp that's a good point - done." CreationDate="2015-08-26T15:26:13.020" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="541" PostId="342" Score="2" Text="Games use fake fill lights all the time. One good example I know of is Tomb Raider (2013). There was an awesome presentation about it at GDC 2013. http://www.gdcvault.com/play/1017934/Casting-a-New-Light-on" CreationDate="2015-08-26T15:41:45.873" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="542" PostId="351" Score="4" Text="Also, I've gotten the impression that &quot;albedo&quot; in astronomy is averaged over the whole visible spectrum (and sometimes a wider spectrum, including UV and infrared), while diffuse/specular coefficients are RGB or ideally spectral quantities." CreationDate="2015-08-26T21:00:52.863" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="544" PostId="344" Score="1" Text="@NathanReed I asked about it at [ompf2](http://ompf2.com/viewtopic.php?f=6&amp;t=2057) and ingenious says: &quot;The only type of light paths that a forward path tracer cannot sample is E(D|G)*S+L, where L is a light source whose definition involves a delta distribution, either in the directional emission or the positional. Examples are point lights and directional lights. Such paths can be described using Veach's extended notation for luminaires and sensors, see section 8.3.2 in his thesis.&quot;" CreationDate="2015-08-26T21:10:36.907" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="545" PostId="344" Score="0" Text="@Armfoot I would definitely go with path tracing. Lots of research, books, code to learn from. I don't know which would be faster, though, too many variables (acceleration structure, shading system, ...). Radiosity apparently simulates the heat propagation after splitting the scene into many tiny triangles ([FEM](https://en.wikipedia.org/wiki/Finite_element_method)), I never tried it and the only product to used it I know of was Autodesk Lightscape. Last but not least, are you really sure you will ever need only diffuse reflections?" CreationDate="2015-08-26T21:19:59.523" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="546" PostId="343" Score="0" Text="Most awesome! Thanks a lot, very good explanation!" CreationDate="2015-08-26T21:28:37.290" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="547" PostId="343" Score="0" Text="Typo perhaps? &quot;Instead the underlying surface has a customizable parameter range. The parameter is stored in something called a knot, and each knot can have a arbitrary value that is bigger than the next.&quot; -&gt; &quot;Instead the underlying **curve** has a customizable parameter range. The parameter is stored in something called a knot, and each knot can have a arbitrary value that is bigger than the **previous**.&quot; Btw., could you please clarify what you mean by &quot;UV range&quot;? &quot;UV&quot; implies 2D..?" CreationDate="2015-08-26T21:31:50.683" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="548" PostId="344" Score="1" Text="@Armfoot The notation doesn't use E{2} for the same reason that it doesn't use L{n} for multiple lights. This describes a single path, or a single sample. The way that we normally formalise Monte Carlo rendering is to take the Kajiya rendering equation, and then turn it into a random variable, the expected value of which is the solution to the equation. You can then calculate the value of a pixel by taking lots of samples and estimating the mean. Light paths more or less correspond to Feynman diagrams." CreationDate="2015-08-26T21:34:20.893" UserId="159" ContentLicense="CC BY-SA 3.0" />
  <row Id="549" PostId="352" Score="0" Text="I'm sorry I'm very new to NURBS: what do you mean by &quot;maximum multiplicity&quot;? I mean, when I do it in the former way, do I end up with multiple overlapping control points?" CreationDate="2015-08-26T22:03:14.283" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="550" PostId="351" Score="3" Text="Interesting. In that case, it doesn't make sense to call something Albedo Map does it?" CreationDate="2015-08-26T22:31:51.383" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="551" PostId="354" Score="0" Text="This seems like several interesting questions in one. At least (4), (5) and (6) sound like independent questions, and maybe (2) and (3) could be a single question. I'd upvote any of those as separate questions, but all in one it seems too broad." CreationDate="2015-08-26T22:31:57.127" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="552" PostId="351" Score="0" Text="@PhilLira that does seem like an unhelpful use of the term... Hopefully it won't catch on and cause more confusion..." CreationDate="2015-08-26T22:34:28.340" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="553" PostId="354" Score="0" Text="Ok! Will do separates and then delete this. thx!" CreationDate="2015-08-26T22:34:30.550" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="554" PostId="354" Score="0" Text="Actually, I just edited this and kept link to others." CreationDate="2015-08-26T22:54:09.513" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="555" PostId="352" Score="0" Text="Let me try to explain in the answer." CreationDate="2015-08-27T01:20:13.860" UserId="159" ContentLicense="CC BY-SA 3.0" />
  <row Id="556" PostId="353" Score="0" Text="Hey mjp how are you. I remember you from gamedev.net. I asked a question about actual usages of curves in games and I remember you gave some good info.  Howdy!" CreationDate="2015-08-27T02:17:52.843" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="557" PostId="166" Score="0" Text="You should also look into signed distance fields and signed distance textures." CreationDate="2015-08-27T02:19:42.313" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="558" PostId="352" Score="1" Text="Pseudonym err no not a good knot vector to demonstrate this. I See thet i might need to expand the other post. Altough @EcirHana it might be a good idea to ask what a multilicity is." CreationDate="2015-08-27T09:03:53.187" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="559" PostId="344" Score="0" Text="Thanks again Ecir, I get your point: if we can have a lot more with one method, it's kind of pointless to pursue another if we don't know exactly how better it will perform." CreationDate="2015-08-27T10:14:45.790" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="560" PostId="344" Score="0" Text="I was trying to make a little joke when I mentioned E{2} @Pseudonym but thanks for further explaining why it's a single path and the reason behind it :)" CreationDate="2015-08-27T10:22:20.827" UserId="157" ContentLicense="CC BY-SA 3.0" />
  <row Id="561" PostId="343" Score="0" Text="@EcirHana Done do you need the multiplicity explanation here" CreationDate="2015-08-27T11:05:22.887" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="562" PostId="343" Score="0" Text="@joojaa Thanks! Yes, I've just posted another [question](http://computergraphics.stackexchange.com/questions/358/)" CreationDate="2015-08-27T11:20:51.860" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="563" PostId="359" Score="0" Text="You might want to add meta on wether or not this is a relevant qestion?" CreationDate="2015-08-27T12:05:32.300" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="564" PostId="359" Score="0" Text="@joojaa no one has complained about it yet, but feel free to raise it on meta if you like." CreationDate="2015-08-27T12:14:03.480" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="565" PostId="360" Score="0" Text="When you say &quot;knots lie on top of each other&quot;, you mean that their values don't change in the knot vector? So in your example [0, 0, 0, 0], [1, 1 ,1], [2, 2, 2] all &quot;lie on top of each other&quot;?" CreationDate="2015-08-27T12:33:48.817" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="566" PostId="360" Score="0" Text="@EcirHana yes that is what i mean a more normal parametrisation would be [0, 0, 0, 0, 0.5, 1, 1.5, 2, 2, 2, 2] Ill add a animation" CreationDate="2015-08-27T12:45:26.903" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="567" PostId="361" Score="0" Text="The cross sectional area visible dimishes as your incidence angle grows." CreationDate="2015-08-27T13:01:45.767" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="568" PostId="352" Score="0" Text="You're probably right about that @joojaa." CreationDate="2015-08-27T13:03:38.617" UserId="159" ContentLicense="CC BY-SA 3.0" />
  <row Id="569" PostId="361" Score="1" Text="@joojaa I follow that bit, but the bit in bold seems to be talking about tilting the surface away from its initial normal vector, which would only make sense for the specific case that the incident light is perpendicular to the surface, or I'm missing something." CreationDate="2015-08-27T13:10:07.617" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="570" PostId="360" Score="0" Text="Just to check: so to split a NURBS curve the main task is to insert knots so that the overall shape remains unchanged, correct?" CreationDate="2015-08-27T13:28:23.723" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="571" PostId="360" Score="0" Text="Yes, once you have enough knots you can delete whats on the other side." CreationDate="2015-08-27T13:31:22.490" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="572" PostId="360" Score="0" Text="There is a different strategy also you can insert one knot and delete the points that nolonger affect the knot you get a slightly different parametrisation though. I prefer this method for most of my own modeling. But nobody else seems to have heard of this." CreationDate="2015-08-27T13:34:43.487" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="573" PostId="360" Score="0" Text="Out of curiosity, what kind of modeling software do you use? Or you mean in your own code?" CreationDate="2015-08-27T13:54:31.220" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="574" PostId="360" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/27460/discussion-between-joojaa-and-ecir-hana)." CreationDate="2015-08-27T13:56:28.700" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="575" PostId="166" Score="0" Text="Something to keep an eye on: https://twitter.com/sheredom/status/636572086211903488" CreationDate="2015-08-27T16:20:06.833" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="577" PostId="356" Score="1" Text="What do you mean by vertical vs horizontal layout of triangles?" CreationDate="2015-08-28T04:04:39.290" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="578" PostId="338" Score="0" Text="I wonder what property of NURBS make them able to do this.  I would think it would be that they are rational, but then rational bezier curves would also have this property." CreationDate="2015-08-28T04:07:40.140" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="579" PostId="315" Score="0" Text="This might be a vsync issue if you're using a variable timestep per-frame." CreationDate="2015-08-28T04:09:40.887" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="580" PostId="323" Score="2" Text="This might be a better question for [Physics.SE](https://physics.stackexchange.com/) or [Astronomy.SE](https://astronomy.stackexchange.com/). I know a point mass does produce lensing effects (see e.g. [this](http://spiro.fisica.unipd.it/~antonell/schwarzschild/)) but no idea if a galaxy can be well-approximated by a point mass for something like this." CreationDate="2015-08-28T05:40:39.697" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="582" PostId="333" Score="1" Text="As an improvement to depth peeling, you might look into [Multi-Layer Depth Peeling Via Fragment Sort](http://research-srv.microsoft.com/pubs/70307/tr-2006-81.pdf) which allows you to extract multiple layers in each pass, reducing the total number of passes needed." CreationDate="2015-08-28T06:10:02.113" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="584" PostId="366" Score="3" Text="Besides a source of advancements its also a great way to learn and an even better place to find examples. I know myself have learned shaders over the past year mostly through Shadertoy. I've found it to be such an open source community it's awesome how everyone shares their techniques." CreationDate="2015-08-28T15:32:31.113" UserId="376" ContentLicense="CC BY-SA 3.0" />
  <row Id="585" PostId="366" Score="4" Text="Good point! I also have to say, that as a professional game programmer, when I see a demo scene person make something I didn't even think was possible, it makes me want to learn about it and try to bring those techniques into the games I'm working on." CreationDate="2015-08-28T15:46:22.373" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="586" PostId="359" Score="0" Text="It really does seem like a &quot;conversation&quot; and not a question that can be answered.  I know on other stack exchange sites that they prefer questions that can be answered, but not sure what the policy here is." CreationDate="2015-08-28T16:17:22.667" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="587" PostId="365" Score="1" Text="Thanks! That was really helpful. Regading the constants/uniform cache. Are they any tips besides precision (mediump, lowp) I could use to improve uniforms cache hit ratio? Does the order in which I declare uniforms make any difference (as for packing more tightly)?" CreationDate="2015-08-28T18:32:24.263" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="588" PostId="359" Score="0" Text="@AlanWolfe we're in the progress of deciding collectively what our policy will be, so go ahead and mention anything you find relevant on Meta. That way we can have clear guidelines before opening up to a wider community in public beta. I do like to ask questions on the borderline to try and kick start that discussion about policy..." CreationDate="2015-08-28T18:33:44.007" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="589" PostId="365" Score="2" Text="@PhilLira Packing can make a difference, yeah. The compiler will insert padding to prevent vectors from being split across 16-byte boundaries, so try to avoid that. I don't think mediump/lowp actually does anything on uniforms, at least on desktop GPUs (maybe it does on mobile). I wouldn't worry too much about uniform cache hit ratio though. That's extremely rarely, if ever, a bottleneck." CreationDate="2015-08-28T20:23:42.223" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="590" PostId="367" Score="2" Text="GPUs work in a different way. (Some architectures) don't have the concept of a global &quot;program stack&quot;, so recursive function calls are not possible in those. OpenCL probably adopts the lowest common denominator, thus disallowing it completely to remain portable across GPUs. Newer CUDA hardware seems to have introduced support for recursion at some point: http://stackoverflow.com/q/3644809/1198654" CreationDate="2015-08-29T02:04:27.370" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="591" PostId="368" Score="7" Text="I'm reluctant to share this secret sauce, but I've had pretty good luck having a fixed maximum bounce count and having a stack of a fixed size (and a loop with a fixed number of iterations) to handle this. Also (and this is the real secret sauce imo!) I have my materials be either reflective or refractive but never both, which makes it so rays don't split when they bounce. The end result of all this is recursive type raytraced rendering, but through fixed size iteration, not recursion." CreationDate="2015-08-29T03:03:43.217" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="592" PostId="366" Score="0" Text="...and on the other hand, Wolfenstein 3D used ray marching (in 2D)." CreationDate="2015-08-29T11:05:01.710" UserId="159" ContentLicense="CC BY-SA 3.0" />
  <row Id="593" PostId="366" Score="0" Text="Oh right totally! John Carmack did some amazing things with ray casting" CreationDate="2015-08-29T14:25:16.227" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="594" PostId="370" Score="1" Text="Have you seen this? Not a whole lot of info but some.  http://www.gamedev.net/topic/573051-lighting-in-object-space/" CreationDate="2015-08-29T18:53:47.497" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="595" PostId="372" Score="1" Text="Can you confirm whether the code shown (with blue removed) also causes the artefacts? If you have narrowed down the code then showing the image from the narrowed down code will help exclude any irrelevant details and highlight the problem." CreationDate="2015-08-30T17:23:05.893" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="596" PostId="369" Score="0" Text="Welcome to Compiter Graphics.SE,  glad to see a familiar face im sure you will be a great addition to our small community. Sorry I cant answer your question though." CreationDate="2015-08-30T18:51:35.957" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="601" PostId="203" Score="0" Text="@trichoplax Sorry for the noise!" CreationDate="2015-08-30T20:41:41.770" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="602" PostId="203" Score="0" Text="@luserdroog thanks for the interest :) Even though this question is only about materials, we could do with new questions related to colour spaces..." CreationDate="2015-08-30T20:43:36.530" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="603" PostId="374" Score="0" Text="Are you looking for a general method that you can apply to an arbitrary Bezier surface, or a way of preparing a fast method for a specific surface? Will your surface shape be fixed before runtime?" CreationDate="2015-08-30T22:05:22.257" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="606" PostId="209" Score="0" Text="@Mark-Both the answers suggested by Alan Wolfe and yuriks are correct" CreationDate="2015-08-31T02:27:59.957" UserDisplayName="user489" ContentLicense="CC BY-SA 3.0" />
  <row Id="608" PostId="374" Score="1" Text="Note that you can raymarch bezier surfaces a lot easier than raytracing it. You can also raytrace or raymarch univariate surfaces a lot easier than other kinds!  http://blog.demofox.org/2015/07/28/rectangular-bezier-patches/" CreationDate="2015-08-31T04:16:10.380" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="609" PostId="372" Score="0" Text="Yes it does, I figured I'd take a screenshot with colour in there otherwise it looks even uglier :)" CreationDate="2015-08-31T04:54:39.483" UserId="193" ContentLicense="CC BY-SA 3.0" />
  <row Id="610" PostId="209" Score="2" Text="I can't mark a comment as correct, though." CreationDate="2015-08-31T04:59:46.233" UserId="158" ContentLicense="CC BY-SA 3.0" />
  <row Id="611" PostId="379" Score="0" Text="Technically light from the sun is white." CreationDate="2015-08-31T08:09:59.047" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="612" PostId="379" Score="0" Text="@ratchetfreak: True; there's something to say about temperature, but the topic is unrelated to the original question." CreationDate="2015-08-31T09:17:15.757" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="613" PostId="372" Score="0" Text="I can understand that, but since it's the ugliness that you're asking for help with, you might have a better chance of someone seeing the problem if you include the screenshot that matches the code too." CreationDate="2015-08-31T11:04:49.763" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="614" PostId="372" Score="1" Text="Fair enough. I've added an extra screenshot." CreationDate="2015-08-31T13:35:00.863" UserId="193" ContentLicense="CC BY-SA 3.0" />
  <row Id="615" PostId="376" Score="0" Text="think about it this way: a deep crease will have shadows in it." CreationDate="2015-08-31T14:21:11.977" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="616" PostId="372" Score="0" Text="Have you tried playing around a bit more with the bias?" CreationDate="2015-08-31T19:36:47.713" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="617" PostId="372" Score="0" Text="This looks like shadow acne https://msdn.microsoft.com/en-us/library/windows/desktop/ee416324(v=vs.85).aspx - this is what the bias factor is supposed to alleviate, as cifz suggests try adjusting the value." CreationDate="2015-08-31T19:49:59.960" UserId="495" ContentLicense="CC BY-SA 3.0" />
  <row Id="618" PostId="376" Score="1" Text="The key thing to understand here is that we are trying to calculate occlusion of the ambient light, not occlusion from view." CreationDate="2015-08-31T20:15:57.220" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="619" PostId="368" Score="0" Text="Like tail recursion?" CreationDate="2015-08-31T20:34:32.567" UserId="240" ContentLicense="CC BY-SA 3.0" />
  <row Id="621" PostId="325" Score="0" Text="Desktop or mobile? Uniforms can be surprisingly costly on some mobile GPUs." CreationDate="2015-08-31T21:50:42.060" UserId="506" ContentLicense="CC BY-SA 3.0" />
  <row Id="622" PostId="306" Score="0" Text="Bit late to the party but this older thread from reddit has a ton of info on various methods of improving the sharpness of SDF based rendering: https://www.reddit.com/r/gamedev/comments/2879jd/just_found_out_about_signed_distance_field_text/" CreationDate="2015-08-31T21:56:04.753" UserId="102" ContentLicense="CC BY-SA 3.0" />
  <row Id="623" PostId="388" Score="2" Text="You are probably thinking of occlusion of one point from another. If you think of bodies or light sources with non-zero spatial extent, then the object can be partially occluded (like how the sun may be more or less occluded during a solar eclipse)." CreationDate="2015-08-31T23:26:19.390" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="628" PostId="390" Score="0" Text="&quot;the surface is lit based on how many of those directions are not occluded by objects in the scene. &quot; I unterstand this. But the question is from where comes the light? Where is the origin?&#xA;&#xA;Because if I have two balls, and the ambient lighting has it's origin between those balls, then there won't be nothing occluded, because there is nothing between objects and light source. But if the origin is behind one of the balls, then one ball is occluded by the other. -&gt; http://i.imgur.com/IRDvCzF.png&#xA;So the position of the source is important to determine what will be occluded. But Amb.L. has no pos." CreationDate="2015-09-01T00:28:55.947" UserId="480" ContentLicense="CC BY-SA 3.0" />
  <row Id="629" PostId="392" Score="5" Text="Great answer. You might want to add that one way to think about affine transforms is that they keep parallel lines parallel. Hence, scaling, rotation, translation, shear and combinations, count as affine. Perspective projection is an example of a non-affine transformation." CreationDate="2015-09-01T06:08:21.133" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="630" PostId="393" Score="0" Text="I did post an answer, but i don't think looking up Wiktionary/wikipedia for you is a good use of this sites resources. Anyway you may wish to refine your question or we can wait for community opinion. Setting boundaries for what is and what is not in scope, is still valuable for the community." CreationDate="2015-09-01T07:51:17.370" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="631" PostId="392" Score="2" Text="You could add some pictures. If you wont I will :P Also might be good to mention order in matrix and row/column orientation is arbitrary. And that rotations in 3d are not comutative." CreationDate="2015-09-01T07:54:03.350" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="632" PostId="390" Score="2" Text="@Joey With one point light source every point on a surface will be either lit or not. With two point light sources a point on a surface may be lit by zero, one or two lights, giving three different light levels. With many point lights, there is gradual variation in the lighting. Ambient occlusion pretends that there are an infinite number of point light sources in the distance in all directions. This is not physically realistic, but it gives an approximation to the lighting in  a real scene, where light is reflected from the objects in the scene so that they are all lit by second hand light." CreationDate="2015-09-01T09:05:56.367" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="633" PostId="393" Score="0" Text="@joojaa I've added a [meta post](http://meta.computergraphics.stackexchange.com/questions/133/should-we-allow-word-definition-questions) so people can discuss whether word definitions should be on topic." CreationDate="2015-09-01T09:24:47.337" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="634" PostId="393" Score="1" Text="I'm voting to close this question as off-topic because its something you should be able to google in 60 seconds." CreationDate="2015-09-01T11:30:01.913" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="635" PostId="396" Score="0" Text="Oops. the shared models of the red, metal and green balls don't react to ambient light but they do react to my ambient occlusion light. Hope you get the point nonetheless." CreationDate="2015-09-01T11:40:49.123" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="636" PostId="393" Score="0" Text="Thought I marked answer as accepted I'm not 100% satisfied (but that's maybe because I asked wrongly). What is *temporal* in some algorithm? The answers says *temporal* means *depends on time*. But which time? How can I measure something, say 10 seconds ago? And *stochastic*. Of course I googled it before asking and far more than 60 second. But how is it differ from *random*?" CreationDate="2015-09-01T12:19:49.823" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="637" PostId="393" Score="0" Text="The answers answer to what you asked. The time is one :) How to get info from previous frames is a different question (you can reconstruct infos, store them in a buffer etc. etc.). Sthocastic and random are basically synonyms, often you use sthocastic to describe a process and random to describe a variable." CreationDate="2015-09-01T13:13:59.727" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="638" PostId="390" Score="2" Text="It might help a bit for understanding to specify that ambient occlusion is usually simulating light coming from the &quot;sky,&quot; which would clear up questions about what the source of the light is." CreationDate="2015-09-01T13:23:28.933" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="639" PostId="390" Score="2" Text="@porglezomp that's a useful way of thinking about it to gain understanding, but it can also be used in a closed room with no sky, where the ambient occlusion is the occlusion of the light reflected multiple times from the walls." CreationDate="2015-09-01T13:44:08.270" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="640" PostId="390" Score="0" Text="@trichoplax Yeah, that's why I put it in quotes." CreationDate="2015-09-01T13:48:59.960" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="641" PostId="397" Score="0" Text="I feel like temporal coherence could be used to denoise video as well." CreationDate="2015-09-01T16:17:03.930" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="642" PostId="297" Score="1" Text="I can't separate the refraction from the attenuation. Can you render the cube with IOR=1.0 please?" CreationDate="2015-09-01T16:19:11.247" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="643" PostId="297" Score="0" Text="added imallett, also linked to the shadertoy" CreationDate="2015-09-01T16:26:15.773" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="644" PostId="393" Score="0" Text="I think you should ask the question that you want to know. Like i sad its very clear what the words mean but the implementation of those things is a different story." CreationDate="2015-09-01T16:52:55.770" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="648" PostId="386" Score="0" Text="I updated the second item as the edit wasn't what I meant. Otherwise, it looks great!" CreationDate="2015-09-01T23:15:18.120" UserId="511" ContentLicense="CC BY-SA 3.0" />
  <row Id="649" PostId="297" Score="1" Text="@AlanWolfe Your IOR=1 render looks exactly as I would expect it, and I skimmed the shadertoy impl and it looks good." CreationDate="2015-09-02T01:26:01.410" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="651" PostId="382" Score="0" Text="On modern hardware, I'm unaware of hardware-based Phong modes. Also, incidentally, for programmable shaders, which by now are ubiquitous, people almost always use microfacet-based BRDFs." CreationDate="2015-09-02T02:14:37.727" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="652" PostId="359" Score="0" Text="At SIGGRAPH this year, there was a demoscener who showed an old demo. They did texture mapping with two instructions per pixel, by using self-rewriting code. Not exactly a discovery, but pretty neat." CreationDate="2015-09-02T02:21:52.700" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="653" PostId="382" Score="0" Text="My point is that the speed increase of Gourard over Phong isn't enough to justify the loss of quality -- modern computers can (or at least should) be able to do both in realtime." CreationDate="2015-09-02T02:32:31.847" UserId="158" ContentLicense="CC BY-SA 3.0" />
  <row Id="654" PostId="382" Score="0" Text="The way you phrased it, it sounded like there is fixed-function Phong functionality, while I don't believe there is. Separately, you say you should use Phong &quot;if quality is important&quot;, but Phong is actually a poor quality BRDF model. By some measures, worse even than Blinn-Phong, which is what hardware Gouraud shading uses to shade vertices." CreationDate="2015-09-02T02:37:40.240" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="655" PostId="392" Score="2" Text="@joojaa I made pictures! [postscript sources](https://groups.google.com/d/topic/comp.lang.postscript/m2QqV4QFFaM/discussion)" CreationDate="2015-09-02T02:45:00.567" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="656" PostId="403" Score="1" Text="I aim to leave regional spelling differences as the author intended, and only standardise in tags (where it matters). However, for my own posts I am interested in the accepted local spelling, so I'll be using matte from now on - thank you :)" CreationDate="2015-09-02T06:17:13.163" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="657" PostId="405" Score="2" Text="I personally see this question more on PostScript than it is on computer graphics." CreationDate="2015-09-02T07:45:08.153" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="658" PostId="387" Score="2" Text="I answered a similar question http://gamedev.stackexchange.com/questions/23/what-is-ambient-occlusion/66638#66638" CreationDate="2015-09-02T09:10:06.683" UserId="8" ContentLicense="CC BY-SA 3.0" />
  <row Id="661" PostId="406" Score="0" Text="I dont really see the point of generating the encapsulation with ps2eps cant you just type the eps header in your template. It makes the instruction cleaner (and user needs less dependenies). Instead of converting each file separately do `mogrify -format png *.eps`" CreationDate="2015-09-02T09:43:01.093" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="663" PostId="386" Score="0" Text="Hi Christophe, did you mean &quot;equilateral&quot; triangles rather than &quot;isosceles&quot;?&#xA;&#xA;Rather than &quot;Hilbert&quot; I'd have said &quot;Morton&quot; order as the addressing is *much* easier in hardware." CreationDate="2015-09-02T11:04:42.923" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="664" PostId="407" Score="0" Text="When you say &quot;more colorful&quot; do you mean specifically as if looking through tinted glass?" CreationDate="2015-09-02T11:48:29.010" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="665" PostId="407" Score="0" Text="Yes, that's what i mean" CreationDate="2015-09-02T11:50:53.190" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="666" PostId="407" Score="0" Text="Do you also want to take the tint colour as a separate input, or do you just want an arbitrary tint colour that happens to be cheap to implement?" CreationDate="2015-09-02T11:51:10.013" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="667" PostId="407" Score="0" Text="Sorry, I think I don't understand your question. Could you explain that more?" CreationDate="2015-09-02T11:53:58.373" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="668" PostId="408" Score="0" Text="What do you mean by &quot;composite&quot;? Is there some mathematical operation  that lets me to composite two colours?" CreationDate="2015-09-02T12:00:33.587" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="669" PostId="408" Score="0" Text="That's what the example equations in his post do." CreationDate="2015-09-02T12:51:13.703" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="670" PostId="386" Score="0" Text="@Christophe thanks! This is really helpfull. So, for the border pixels, doesn't texture cache matter? That's was kind of what I was wondering. So, If I have a triangle that intersects tiles (x, y) and (x+1, y) and GPU just rasterized tile (x, y). Assuming tile (x+1, y) will be next, even if a different Execution Units processes it,  won't I benefit from texture cache when sampling texels for this triangle?" CreationDate="2015-09-02T12:57:25.303" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="671" PostId="386" Score="0" Text="Also, I got curious about the Hilbert pattern. I always assumed this was true for block compressed textures. Is this true for all textures? &#xA;&#xA;PS: I also didn't follow the last paragraph." CreationDate="2015-09-02T13:00:59.193" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="672" PostId="356" Score="0" Text="@Mokosha sorry, this somehow got unnoticed to me. I just saw it now. This is more a theoretical than pratical question and I don't even know if this makes sense now. Anyway, what I meant was, say a triangle intersect tiles (x, y) and (x+1, y) and that the these two tiles get processed one after another. Would that be a better for texture cache than If I had a triangle intersecting (x, y) and (x, y+1)? (Because of the border pixels and the layout of triangles not being in the same direction the tiles processing is)" CreationDate="2015-09-02T13:06:14.887" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="673" PostId="408" Score="2" Text="I don't think this is what he wants. If you wear red-tinted glasses, you see red wavelengths at basically full strength, and very little from other wavelengths. That's not a great match for alpha compositing." CreationDate="2015-09-02T14:58:30.927" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="674" PostId="407" Score="0" Text="If [Kostas Anagnostou's answer](http://computergraphics.stackexchange.com/a/410/231) is what you want then ignore my comments. I was just asking whether you wanted that or a hardcoded colour." CreationDate="2015-09-02T15:32:20.770" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="675" PostId="411" Score="0" Text="Pretty much any 3d animation software should be able to do this, so in alphabetic order 3ds Max, Blender, Maya..." CreationDate="2015-09-02T15:33:33.073" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="676" PostId="405" Score="1" Text="@cifz right this would fit [GD.SE](http://graphicdesign.stackexchange.com/) better. But thats not a reason why it can not be here." CreationDate="2015-09-02T19:12:16.817" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="677" PostId="410" Score="0" Text="Also you might want to tint something other than 100% pure one channel color." CreationDate="2015-09-02T19:14:16.117" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="679" PostId="411" Score="0" Text="@trichoplax, the models are things like an upper arm, or a hand." CreationDate="2015-09-02T20:35:11.567" UserId="540" ContentLicense="CC BY-SA 3.0" />
  <row Id="682" PostId="406" Score="0" Text="Cool. I didn't know about `mogrify`. For the `ps2eps` part, it automatically calculates (and adds) the bounding-box info. Omitting that part, `convert` will produce full-page-sized images" CreationDate="2015-09-02T20:48:28.243" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="683" PostId="405" Score="0" Text="@citz I think I see what you mean. What if I take it out of the question, and open it up for tikz and asymptote and whatnot." CreationDate="2015-09-02T21:20:08.617" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="684" PostId="412" Score="5" Text="[Vote for MathJax formatting on this site.](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support)" CreationDate="2015-09-02T22:28:52.713" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="685" PostId="412" Score="2" Text="I've added this question to that meta post as an example of further need for MathJax" CreationDate="2015-09-02T23:14:53.637" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="686" PostId="414" Score="1" Text="`Here is what a cube looks like (...) using my own path tracer.`  &#xA;Do you happen to have open-sourced it by any chance ?" CreationDate="2015-09-03T02:53:30.507" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="687" PostId="414" Score="2" Text="No, not yet. I was planning on finishing and releasing this particular variant along with a blog post about glass rendering, but it's been on my backlog for a while." CreationDate="2015-09-03T04:27:27.400" UserId="207" ContentLicense="CC BY-SA 3.0" />
  <row Id="689" PostId="410" Score="3" Text="I went ahead and added some example images to the post - hope you don't mind!" CreationDate="2015-09-03T05:29:52.243" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="691" PostId="407" Score="0" Text="For more realistic results, you might find this answer interesting: http://photo.stackexchange.com/a/49267/23075" CreationDate="2015-09-03T09:06:49.237" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="692" PostId="349" Score="0" Text="I depends on the amount of calculations.GPU is really fast about math operations.So it is a matter of try and benchmarking." CreationDate="2015-09-03T09:30:42.943" UserId="213" ContentLicense="CC BY-SA 3.0" />
  <row Id="693" PostId="405" Score="0" Text="I am answering here to show that this is a bit opinionated as a question, and answers definitely will be. Maybe this entire thread should live in our Meta, like this does on [GD.SE](http://meta.graphicdesign.stackexchange.com/questions/790/how-to-embed-screen-capture-videos-as-animated-gifs-in-answers). Any thoughts, @cifz" CreationDate="2015-09-03T10:15:48.103" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="697" PostId="410" Score="0" Text="That is exactly what I meant. Thanks for effort and examples :)" CreationDate="2015-09-03T16:17:26.723" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="699" PostId="326" Score="1" Text="It is not blue you are thinking of; it is cyan. There are two options: Red/Cyan, and Magenta/Green. In both cases, all three human cone types (color channels) are covered, not just two. I think Magenta/Green is universally recognized as being superior but I could be misremembering; I prefer it anyway." CreationDate="2015-09-03T16:26:25.673" UserId="504" ContentLicense="CC BY-SA 3.0" />
  <row Id="700" PostId="369" Score="0" Text="Sorry, but I'm a bit lost as to what you are aiming to do. Are you trying to write an &quot;arbitrary polygon&quot; filling routine that also, say, clips to the sides of the viewing rectangle?" CreationDate="2015-09-03T16:32:18.670" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="703" PostId="424" Score="0" Text="Will your subpixel-rendered images still look okay if viewed on screens in a portrait orientation?" CreationDate="2015-09-03T18:28:39.267" UserId="551" ContentLicense="CC BY-SA 3.0" />
  <row Id="704" PostId="428" Score="3" Text="Playing devil's advocate, if possible to do color subpixel rendering (i'm a skeptic!), it seems like it could be useful in VR situations, where they struggle to have sufficient resolution.  Perhaps on (non retina?) phones or tables as well?" CreationDate="2015-09-03T18:30:18.223" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="705" PostId="428" Score="1" Text="@AlanWolfe yes but its also 3 times as expensive to render" CreationDate="2015-09-03T18:36:03.860" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="706" PostId="428" Score="0" Text="yeah for sure, that's true. There are some situations where that isn't a problem though.  For instance, I know of a couple algorithms where you don't need to shoot rays for every pixel every frame.  It seems like this could also have play in rasterized graphics btw.  Again, i am not convinced color subpixel rendering is a real thing, but ya know, IF! :P" CreationDate="2015-09-03T18:39:14.930" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="707" PostId="427" Score="0" Text="With that explanation it totally makes sense, thank you! In case you remember, how was your experience with cone tracing in comparison to normal ray tracing? Of course it's an approximation, but does it achieve a considerable speedup at acceptable quality?" CreationDate="2015-09-03T19:07:57.057" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="708" PostId="428" Score="1" Text="@AlanWolfe I don't understand your doubts. All images are already rendered with sub pixels, but the colours are misaligned by between a third and a half of a pixel. I can't see how correcting that misalignment would fail to produce a higher quality image. Do you have any specific concerns (which might make a good question...)?" CreationDate="2015-09-03T19:10:56.147" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="709" PostId="428" Score="0" Text="@trichoplax Quality would riase at the cost of portability and simplicity. Not a good trade if the quality increase is minuscule. Its more important to know the color profile of the device than this." CreationDate="2015-09-03T20:02:04.447" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="711" PostId="215" Score="0" Text="Try randomly picking a light source *per sample* instead of per pixel." CreationDate="2015-09-03T21:01:10.693" UserId="553" ContentLicense="CC BY-SA 3.0" />
  <row Id="714" PostId="428" Score="0" Text="@joojaa I agree that the effort is unlikely to be worth it. I'm just asserting that it is possible. My final section is meant as advice to not work on this unless there is a purely theoretical interest." CreationDate="2015-09-03T21:57:48.737" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="715" PostId="405" Score="1" Text="[Meta question concerning this question.](http://meta.computergraphics.stackexchange.com/questions/147/simple-2d-illustrations-question-main-or-meta)" CreationDate="2015-09-03T22:09:35.070" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="716" PostId="428" Score="4" Text="@joojaa There's no reason it would be 3x as expensive to render. You wouldn't need to shoot 3x the number of rays; you'd just apply 3 different weights when accumulating the rays into the framebuffer. Effectively, you're using a different antialiasing kernel for each color channel." CreationDate="2015-09-03T23:02:28.283" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="717" PostId="429" Score="0" Text="Nathan you are the one that wrote that about depth precision? Wow cool... i read that thing and it helped a lot! small world :P" CreationDate="2015-09-03T23:03:24.317" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="718" PostId="405" Score="0" Text="I wish we could use latex figures like we can latex math markup :P" CreationDate="2015-09-03T23:04:05.170" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="720" PostId="1431" Score="0" Text="By displaying the images magnified, you entirely negate the benefit of using subpixels, so the comparison images aren't representative of the actual results. You should try posting the original-size images if possible." CreationDate="2015-09-04T05:48:17.307" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="721" PostId="1431" Score="1" Text="@yuriks There's another smaller resolution that I can find (http://journals.cambridge.org/fulltext_content/SIP/SIP1/S2048770312000030_fig11p.jpeg) but it's not immediately apparent to me that that version is not minified or magnified. I do think it's educational to see the magnified version, since it lets you see the fringing and doesn't actually get rid of the sharpness relative to the regular downsampling." CreationDate="2015-09-04T05:57:12.960" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="722" PostId="1431" Score="1" Text="@yuriks, problem with these kinds of images are that not all monitors have same subpixel sequence. And on mobile devices the orientation needs to change when the user turns the device." CreationDate="2015-09-04T06:21:43.853" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="723" PostId="405" Score="0" Text="@AlanWolfe that sounds like a Meta Question tagged `[feature-request]` to me, if you want to post one..." CreationDate="2015-09-04T08:49:10.267" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="724" PostId="1436" Score="5" Text="RIP maps also probably aren't used because they don't help on the, rather common, diagonal case.&#xA;&#xA;FWIW, if you can find the code for the Microsoft Refrast, the anistropic filter implementation in that is probably a good reference for how today's HW does it." CreationDate="2015-09-04T09:43:19.890" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="725" PostId="427" Score="0" Text="Oh, gosh, it was a long time ago.  Actually, I only implemented the cone tracing. Whether I actually tried turning off the radius part I simply can't recall but, if I get time, I'll try to remember the pros and cons of going down the cone-tracing route." CreationDate="2015-09-04T09:55:20.113" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="726" PostId="1436" Score="1" Text="&quot;This can be verified by noting that texture usage requirements don't increase when using AF, rather, only bandwidth does.&quot; Killer argument. Good answer!" CreationDate="2015-09-04T10:22:07.030" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="727" PostId="432" Score="1" Text="Illumination does not actually fall of the surface area towards the light is just smaller" CreationDate="2015-09-04T17:11:50.277" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="728" PostId="430" Score="0" Text="Tervetuloa! Yes your point is valid, id say that the system or hardware has to do this as only the system can in future realisticallt be aware of the orientation of screen and the organisation of colors on screen. Preferably the screen itself would do this." CreationDate="2015-09-04T17:17:42.007" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="729" PostId="1434" Score="2" Text="I generally agree with your argumentation. But I think what the author means is, that the approach needs to perform its calculations for all existing patches, not only the visible ones. One could argue that path-tracing in contrast computes radiance only for visible patches/samples. While rays may still go everywhere, there might be parts of the scene that never receive any view-rays/paths; therefore there are no computations at all. Comparing with local GI the &quot;problem of viewpoint independence&quot; its even more apparent. Though, I still agree with you that this should be rephrased." CreationDate="2015-09-04T17:25:00.083" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="733" PostId="1432" Score="3" Text="The spec for [`GL_EXT_texture_filter_anisotropic`](https://www.opengl.org/registry/specs/EXT/texture_filter_anisotropic.txt) is very detailed. Maybe it might help you better understand the process." CreationDate="2015-09-04T20:39:03.017" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="734" PostId="1434" Score="1" Text="I agree with Wumpf, the view-independent technique does not take visibility into account which results in extra calculations because it needs to compute lighting for *entire scene* no matter where the camera is looking. Furthermore, you cannot reduce resolution of your computation in areas that are far. I think @Wumpf should paraphrase his comment as an answer." CreationDate="2015-09-04T23:02:46.410" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="735" PostId="1438" Score="2" Text="SIGGRAPH 2014 advances in real time rendering has a really interesting talk on call of duty's subdivision surfaces.  You should check it out.  Instead of having a high poly mesh that is made lower poly, they defined shapes analytically and added more triangles as needed" CreationDate="2015-09-05T03:09:18.943" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="736" PostId="1438" Score="0" Text="We can talk about the state of the art in LOD algorithms and data structures here, but if the question is about how modern games do it specifically, you might have more luck asking in gamedev.se: http://gamedev.stackexchange.com" CreationDate="2015-09-05T06:02:37.420" UserId="159" ContentLicense="CC BY-SA 3.0" />
  <row Id="737" PostId="1434" Score="0" Text="@ap_ Done. Feel free to edit :)" CreationDate="2015-09-05T12:13:20.047" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="740" PostId="283" Score="0" Text="@JohnCalsbeek I think that paper contains the makings of a great answer if you or anyone else wants to post it." CreationDate="2015-09-05T15:24:12.010" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="741" PostId="432" Score="0" Text="You're right I think, &quot;fall-off&quot; to me is anything that makes the surface area smaller from the lights perspective, so distance from and rotation away from the light have the same effect to me, but my definition of &quot;fall-off&quot; is probably not mathematically correct :P" CreationDate="2015-09-05T15:27:52.980" UserId="554" ContentLicense="CC BY-SA 3.0" />
  <row Id="742" PostId="1438" Score="3" Text="I doubt that. Game dev is very graphics light. It's mostly unity and java questions with some path finding and fixed frame rate questions thrown in :p" CreationDate="2015-09-05T16:52:26.470" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="743" PostId="432" Score="0" Text="well yes but that would be hard for a layman to understand. lots of things can fall off." CreationDate="2015-09-05T17:38:40.670" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="744" PostId="283" Score="0" Text="@trichoplax One of the reasons that I haven't tried to write it up yet is that I don't see the anisotropy that you are talking about in any of the frequency domain images in that paper, so I feel like I'm missing something." CreationDate="2015-09-05T17:52:22.860" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="745" PostId="283" Score="0" Text="@JohnCalsbeek The first image shatters my intuition that Perlin noise would be heavily anisotropic, and shows that the problems are due to poor implementation and not really related to being grid based. I still don't have an understanding of why my initial intuition was so wrong." CreationDate="2015-09-05T18:27:23.203" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="746" PostId="283" Score="0" Text="@trichoplax I can see directional artifacts in image 1b in that paper, even though the Fourier transform doesn't seem to show it. Unless that's what the extremely thin lines at right angles are?" CreationDate="2015-09-05T21:45:58.577" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="747" PostId="1438" Score="1" Text="@Alan, Activision used a lot of state of the arts algorithm to create a real 3D strategic game not sn isometric Sprite base one which I am ok with that, but they did a great job in COD however it is still a bit sluggish and lazy even at the early levels with small number of assets (I am talking about their mobile game on an Iphone 5s). I think you need to learn OpenGLES expert features and underlying layers to succeed writing such a game." CreationDate="2015-09-06T06:58:27.263" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="755" PostId="1446" Score="0" Text="Down voter, please consider you vote again, if it is still a -1, Consider leaving a comment for me.Thanks" CreationDate="2015-09-06T13:00:25.020" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="756" PostId="1438" Score="0" Text="I'm talking about their console version of call of duty in case that clears it up." CreationDate="2015-09-06T13:53:12.393" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="758" PostId="405" Score="0" Text="[Another Meta question about this question.](http://meta.computergraphics.stackexchange.com/questions/150/2d-illustrations-question-help-crafting-the-question)" CreationDate="2015-09-06T15:58:21.757" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="759" PostId="1446" Score="0" Text="I didnt downvote but part of me wonders what is the sate of the art here? So i cant up vote either." CreationDate="2015-09-06T15:59:03.380" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="761" PostId="1446" Score="1" Text="I've downvoted because it was hard to read and IMHO it's not relevant to LODs in games. After reconsidering I decided to cancel my donwvote by upwoting and simply add my own answer." CreationDate="2015-09-06T18:02:06.800" UserId="93" ContentLicense="CC BY-SA 3.0" />
  <row Id="762" PostId="1446" Score="0" Text="State of the art suppose to mean a very outstanding job but different methods result differently on various cases, for example Call of duty has Layer management and mipmaping, however Dear haunting(DH 2014) uses background with parallax and a mipmaping which has pre rendered generalized textures. Subway surfer is completely a diffrent story, and I say state of the art to all of them, even though Subway Surfer discretely draws buildings and other urban objects or Call of duty is a bit sluggish while zooming or panning. I think they are all best in their case." CreationDate="2015-09-06T18:04:05.847" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="764" PostId="1451" Score="1" Text="game engines usually have a set of pre defined methods and you can not get in to the core to actually changing the LOD algorithm. am I right? I was talking about the case you are writing a game yourself with OpenGL or SpriteKit framework, I dont know if one is able to customize LOD's algorithm in Unity or Unreal, is it possible?" CreationDate="2015-09-06T18:19:19.567" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="765" PostId="1450" Score="1" Text="I'm fairly certain that these values are hardware/version specific, but there are minimum values that an implementation must support. You can query them with [`glGet`](https://www.opengl.org/sdk/docs/man/html/glGet.xhtml)." CreationDate="2015-09-06T19:12:20.497" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="766" PostId="1452" Score="0" Text="Also as glampert hinted, you want to find the minimum that must be supported, because that is the amount you can actually rely on, on all hardware." CreationDate="2015-09-06T20:43:18.473" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="767" PostId="1452" Score="0" Text="Well, OpenGL3 enforces 48 as I have mentioned. There might be a higher minimum in practice of course. Or did I misunderstand you?" CreationDate="2015-09-06T20:45:16.870" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="769" PostId="1450" Score="2" Text="Note that the textures you've described are 64 MB each, so you may get limited by available VRAM before you hit API limits on the number of textures. 8 textures = 512 MB, so should be fairly safe, but many older cards or mobile cards only have 1 or 2 GB of VRAM, so you won't be able to go much more than 8 of these textures and still have VRAM left over for anything else." CreationDate="2015-09-07T00:01:07.253" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="771" PostId="1453" Score="0" Text="thanks, they even have binary gltf (`.bgltf`) versions available! +1" CreationDate="2015-09-07T08:48:35.527" UserId="361" ContentLicense="CC BY-SA 3.0" />
  <row Id="772" PostId="1450" Score="1" Text="What do you mean by 256*256*256? what is the third number?" CreationDate="2015-09-07T09:44:37.250" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="773" PostId="1440" Score="1" Text="Ok, seems the main problem was that I read &quot;viewpoint independent&quot; as &quot;BRDF is isotropic&quot;, not &quot;lighting is calculated, whether you look at the surface or not&quot;. I'll wait another day for another answer and then probably accept this, thanks :)" CreationDate="2015-09-07T10:20:12.323" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="774" PostId="1455" Score="0" Text="I feel this is a very broad question. What exactly do you want to learn that you can't when using one of said low-level graphic APIs?" CreationDate="2015-09-07T11:56:37.447" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="775" PostId="1455" Score="0" Text="@DavidKuri, drawing a 2D geometric primitive (circle or line) in 3D is easy (just need to convert/translate 3D coordinates to 2D coordinates or vice versa). suppose I want to draw a sphere without OpenGL. Where to start from? Just give me a guideline to study." CreationDate="2015-09-07T11:59:45.210" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="776" PostId="1455" Score="0" Text="You mean software rendering?" CreationDate="2015-09-07T12:02:32.213" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="777" PostId="1455" Score="0" Text="@ratchetfreak, yes. I need to draw a sphere/ellipsoid/or whatever it is on the screen. Rasterization in 3D." CreationDate="2015-09-07T12:03:56.820" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="778" PostId="1455" Score="0" Text="@ratchetfreak, I need to demonstrate 3D algorithms like Z-buffer algorithm. So, first, I need to know how to draw a 3D object in 3D." CreationDate="2015-09-07T12:05:46.417" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="780" PostId="1455" Score="0" Text="@trichoplax, I need to learn both." CreationDate="2015-09-07T12:12:21.513" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="782" PostId="1455" Score="0" Text="@trichoplax, line drawing. I will learn shadows and shading later." CreationDate="2015-09-07T12:16:41.050" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="784" PostId="1454" Score="1" Text="It does extend to 3D. For example,  you could consider that, when texturing (a portion of) a triangle, you are, in effect, evaluating a surface cut through the 3D texture.  That shape doesn't have to be isotropic.&#xA;&#xA;Alternatively, just like a 2D anisotropic filtering may approximate with an elliptical footprint, the 3D version could use an ellipsoid." CreationDate="2015-09-07T15:09:05.573" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="785" PostId="1454" Score="0" Text="@SimonF Hmmm, you're right! That is actually I think perhaps a better generalization than the one I gave, and it seems better behaved (as in, it's more obvious what to do)." CreationDate="2015-09-07T16:51:22.520" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="786" PostId="386" Score="0" Text="PVRTC encodes texture blocks in a [morton order](https://en.wikipedia.org/wiki/PVRTC#Data_structure)" CreationDate="2015-09-07T19:09:25.310" UserId="135" ContentLicense="CC BY-SA 3.0" />
  <row Id="789" PostId="369" Score="0" Text="@SimonF Sorry I didn't notice your comment till now. I'm trying to implement a fully-generalized clipping routine for complex self-intersecting shapes AND complex self-intersecting clipping-regions and parameterized with a winding-number rule. But if I could visualize the data structure in concrete terms, that usually works to get me moving." CreationDate="2015-09-07T23:21:32.447" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="790" PostId="412" Score="0" Text="@trichoplax, you used mathurl for pasting images? I'm reading from Android app and i can't check." CreationDate="2015-09-08T01:56:03.440" UserId="316" ContentLicense="CC BY-SA 3.0" />
  <row Id="792" PostId="1458" Score="1" Text="I dont really understand why this shopping list question is within scope while asking what graphics api questions are available fo ubuntu is not." CreationDate="2015-09-08T04:18:42.570" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="794" PostId="420" Score="0" Text="I tend to think it does.He can learn from the code inside how the trackball works." CreationDate="2015-09-08T07:59:31.830" UserId="213" ContentLicense="CC BY-SA 3.0" />
  <row Id="796" PostId="369" Score="0" Text="*If* you have solved the fill problem for an &quot;unclipped&quot; arbitrary polygon, then you 'only' need to do CSG with an intersection (i.e. AND) operator to then clip it against another arb. poly.  Does that make sense?  In screen/scanline space the CSG is relatively easy. OTOH if you need a vector model, it seems to me that  a scanline model could be extended so that you construct sets of trapezia  (http://mathworld.wolfram.com/Trapezium.html - British  definition (i.e. correct def)) for each that describe the interiors. It should be relatively easy to intersect those to generate the final model." CreationDate="2015-09-08T09:26:11.867" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="798" PostId="386" Score="0" Text="@yuumei No. Although early MBX PVRTC files *were* in morton(ish) order, for later systems (e.g. SGX and Rogue), the data is in raster(ish) order and the driver/GPU loads and rearranges them into whatever is the preferred order for that particular GPU." CreationDate="2015-09-08T12:23:01.563" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="800" PostId="1459" Score="0" Text="`3D computer graphics and geometric modeling` includes a ton of things, ranging from lighting algorithms over mesh processing algorithms to the 'creative' task of creating a 3-dimensional virtual object, complete with textures and possibly animations. Except for being in the same space, these things don't have too much in common. Could you go into detail on what exactly you want to learn?" CreationDate="2015-09-08T12:36:58.927" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="802" PostId="1459" Score="0" Text="@DavidKuri,  http://computergraphics.stackexchange.com/questions/1455/how-to-get-started-with-drawing-3d-primitives-without-using-opengl-or-directx" CreationDate="2015-09-08T13:49:42.797" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="803" PostId="1459" Score="0" Text="The way I read your other questions, you want to learn everything from the ground up. So after software rasterization (in which you are essentially replicating the job of the dedicated GPU), plain renderings APIs would be next." CreationDate="2015-09-08T13:54:27.313" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="804" PostId="1459" Score="0" Text="@DavidKuri, I didn't get you." CreationDate="2015-09-08T13:56:01.723" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="805" PostId="1459" Score="0" Text="You seem to be interested in learning, more so than creating an application. You will probably not learn how, for example, shadow techniques work if you start using a game engine, because they are already there and ready to use. Start with a low-level rendering API and implement a shadowing technique yourself to learn it." CreationDate="2015-09-08T13:58:50.823" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="806" PostId="412" Score="0" Text="@psicomante yes I used mathurl.com and included the links inline - are they readable on Android? It's the next best thing until we get MathJax activated." CreationDate="2015-09-08T14:05:50.627" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="807" PostId="412" Score="0" Text="@psicomante press edit under the question if you want to see the markup (you need to add `.png` to the end of the mathurl.com link to make it show as an image here). Any questions just @mention me in [chat]." CreationDate="2015-09-08T14:07:56.430" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="808" PostId="1460" Score="5" Text="Nitpicking: Blender has also an [integrated Game Engine](https://www.blender.org/manual/game_engine/index.html)." CreationDate="2015-09-08T14:09:02.743" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="810" PostId="412" Score="0" Text="@trichoplax yep, they are almost perfectly readable on Android SE app. It's a good tool until MathJax activated" CreationDate="2015-09-08T15:38:13.600" UserId="316" ContentLicense="CC BY-SA 3.0" />
  <row Id="811" PostId="413" Score="0" Text="I agree---my answer is likely not valid any more because I haven't used diagrams in any answers so far. But I expect that to happen soon. So if it's OK for this answer to hang around for a few days, I am sure I will be able to find an answer." CreationDate="2015-09-08T19:04:01.790" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="813" PostId="1455" Score="0" Text="Since this is more than one question, it may help to ask them separately rather than trying to fit them all into one post, which is making this too broad" CreationDate="2015-09-08T20:23:05.353" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="814" PostId="1450" Score="0" Text="@Iman it's a 3d texture rather than a 2d texture - if you want more info you could ask a separate question if you like." CreationDate="2015-09-08T20:44:07.357" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="817" PostId="1444" Score="1" Text="Is there disagreement between this answer and John Calsbeek's answer? Does the implementation match both descriptions? If not then it would be useful to have a reference for one or other (or both if they are two different techniques both in use)." CreationDate="2015-09-08T20:54:34.700" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="818" PostId="1442" Score="0" Text="Is there disagreement between this answer and Nathan Reed's answer? Does the implementation match both descriptions? If not then it would be useful to have a reference for one or other (or both if they are two different techniques both in use)." CreationDate="2015-09-08T20:55:10.693" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="819" PostId="1442" Score="1" Text="@trichoplax I think Nathan's assertion that &quot;generating good quality mips for a non-power-of-two texture is a little trickier&quot; makes our answers disagree at least slightly. That alone probably merits more elaboration." CreationDate="2015-09-08T21:09:24.700" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="822" PostId="315" Score="0" Text="Are you using any vendor-dependent GLSL functions like noise*(which as far as I know, most vendors don't implement anyway)?" CreationDate="2015-09-08T22:42:38.267" UserId="1578" ContentLicense="CC BY-SA 3.0" />
  <row Id="824" PostId="1461" Score="0" Text="See http://stackoverflow.com/questions/21593786/in-gouraud-shading-what-is-the-t-junction-issure-and-how-to-demonstrate-it-with." CreationDate="2015-09-09T01:35:03.463" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="825" PostId="1462" Score="0" Text="See also http://stackoverflow.com/questions/23530807/glsl-tessellated-environment-gaps-between-patches." CreationDate="2015-09-09T01:48:28.167" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="827" PostId="315" Score="0" Text="@Sam no. I am only using texture(). That shouldn't cause a problem, right?" CreationDate="2015-09-09T10:00:19.927" UserId="437" ContentLicense="CC BY-SA 3.0" />
  <row Id="828" PostId="1450" Score="0" Text="I never heard such a thing, I thought textures are 2D (u,v) raster map that draped on 3d objects, however I take a look at tutorials and I become familiar with the concept. Thanks for your tip Terry" CreationDate="2015-09-09T13:19:38.813" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="832" PostId="1467" Score="1" Text="Original question: http://math.stackexchange.com/questions/1428101/an-introduction-to-lane-riesenfeld-algorithms" CreationDate="2015-09-09T15:59:06.333" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="833" PostId="1442" Score="1" Text="I think the problem here is that we're confusing the logical position of texels with their &quot;physical&quot; layout in memory. 1) Pixels are discrete items, i.e. you always need an integer dimension, and so down scaling an odd dimension means that we have to either round up or round down. Since we have to round up once we get to an Nx1 or 1xN texture, it makes sense to always round up.&#xA;2) When laid out in physical addresses, it is not uncommon to pad the texture out to some &quot;convenient&quot;  &quot;stride&quot; size. This may be done for 2 reasons: a) It may make HW cheaper &amp; b) if a P.of.2, Morton order is easy." CreationDate="2015-09-09T16:23:49.693" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="834" PostId="1468" Score="3" Text="As hinted by @BenediktBitterli &quot;Physically Based Rendering&quot; isn't really a yes or a no. In rendering, we always have to balance realism with computational cost. Some renderers will have just a few 'Physically Based' features, for example, Microfacet BRDFs and HDR render targets. Whereas others may have many, for example, full BSDFs, full spectrum render target, light tracing, area lights, etc." CreationDate="2015-09-09T18:19:46.420" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="835" PostId="1436" Score="0" Text="The &quot;High-Performance Software Rasterization on GPUs&quot; link only mentions anisotropic filtering in passing once, and gives no mention of any details. So I'm going to edit it out of the answer because I don't think it's relevant in a helpful way." CreationDate="2015-09-09T18:30:27.923" UserId="327" ContentLicense="CC BY-SA 3.0" />
  <row Id="836" PostId="1464" Score="2" Text="When you say &quot;vertex (X,Y) values are nearly always represented by fixed-point numbers&quot;, I'm guessing you mean the screen-space vertex coordinates in the rasterizer, right? Not the original model-space vertices." CreationDate="2015-09-09T20:17:47.533" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="838" PostId="1456" Score="0" Text="Scratchapixel is definitely the best place to learn about that stuff (2D &amp; 3D).  They explain rasterisation and also ray-tracing and everything there is to know around 3D techniques (texturing, how to store polygonal objects in memory, etc.). Really cool website and it's free content." CreationDate="2015-09-09T21:49:30.003" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="839" PostId="315" Score="0" Text="@nilspin I don't think so. They're both running on the same OpenGL version, right?" CreationDate="2015-09-09T23:25:30.043" UserId="1578" ContentLicense="CC BY-SA 3.0" />
  <row Id="840" PostId="315" Score="0" Text="@Sam yes they are." CreationDate="2015-09-10T04:36:55.543" UserId="437" ContentLicense="CC BY-SA 3.0" />
  <row Id="841" PostId="1464" Score="2" Text="@NathanReed Yes. Just the screen-space X&amp;Y (and, perhaps on some GPUs the Z).  I'll edit it to make that clearer." CreationDate="2015-09-10T05:41:56.587" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="842" PostId="1460" Score="1" Text="@Wumpf there is also nothing that states maya can not be used for interactive game like elements. It can, this is how mayas motion capture works, its just not very conductive as a game engine given the software price. Anyway the terms are decidedly diffuse." CreationDate="2015-09-10T06:20:41.840" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="843" PostId="26" Score="1" Text="@trichoplax IANAPL but, as all the claims in the link provided by Benedikt , either explicitly mention either 3 dimensions (i,j,k  or x y z) or a hypercube, it seems you are correct." CreationDate="2015-09-10T08:40:51.893" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="844" PostId="26" Score="0" Text="@SimonF I wasn't as diligent as you - I was basing my opinion on  [this statement on Wikipedia](https://en.wikipedia.org/wiki/Simplex_noise#Legal_status)." CreationDate="2015-09-10T08:44:55.913" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="845" PostId="1460" Score="0" Text="I edited the answer to include your remarks. Thanks." CreationDate="2015-09-10T09:26:04.657" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="846" PostId="1470" Score="1" Text="I don't see yet why the dirac deltas are a problem. While it is impossible to compute that with a computer using sampling (hence the `if`s), the mathematics are clearly defined, right? Looking forward for somebody who can clarify that. Besides, in nature there are no real dirac deltas / infinity values since there are no perfect mirrors; but I guess that is another topic." CreationDate="2015-09-10T10:08:48.700" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="848" PostId="1470" Score="0" Text="As long as BSDF and radiance can be dirac deltas at the same time than the rendering equation(as it is) is not mathematically well defined. Even if only BSDF would be allowed to be Dirac delta and we would formally treat BSDF as distribution than radiance needs to be smooth function in order to be mathematically 100% correct. But radiance under no way can be smooth function e.g.  sharp shadows form discontinuities in radiance." CreationDate="2015-09-10T10:25:34.243" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="849" PostId="1470" Score="0" Text="Yes in reality you cannot have perfect mirrors, point and directional light sources or pin hole cameras. But we write programs where these things are and we need a theory which underpins them." CreationDate="2015-09-10T10:31:31.473" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="850" PostId="1471" Score="0" Text="&quot;I would like to point out that correlation is &quot;always&quot; bad. If you can afford to make brand new sample than do it.&quot; Could you elaborate? To me this sounds like any kind of heuristic for sample distribution is bad, which is probably not what you wanted to say." CreationDate="2015-09-10T11:37:55.023" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="851" PostId="1471" Score="0" Text="I edited the answer, I hope that cleared a thing or two." CreationDate="2015-09-10T12:28:23.113" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="853" PostId="1472" Score="6" Text="I think you need anisotropic texture sampling" CreationDate="2015-09-10T14:01:59.483" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="854" PostId="1472" Score="1" Text="Please attach your shader completely, it's variable definition I mean, may be you need to define a High precision or mid precision variable instead of lowp" CreationDate="2015-09-10T14:29:46.250" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="855" PostId="309" Score="0" Text="Would you like to specify which texture compression format you prefer? I am guessing but your answer will likely involve a compute-mode texture compression routine." CreationDate="2015-09-10T17:13:32.990" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="856" PostId="1472" Score="3" Text="It migh be an issue with the texture filtering you're using. Which filter is it? Point, bilininear, trilinear? Also, make sure you did compute the correct mipmaps for the texture." CreationDate="2015-09-10T18:22:26.520" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="857" PostId="1470" Score="1" Text="@tom The rigorous mathematics that underlies delta distributions is [measure theory](https://en.wikipedia.org/wiki/Measure_%28mathematics%29). See the [definition of the Dirac delta as a measure](https://en.wikipedia.org/wiki/Dirac_delta_function#As_a_measure). I don't know off the top of my head of a work specifically treating the rendering equation in the context of measure theory, but pretty sure all this stuff is well-founded at the level of mathematical physics." CreationDate="2015-09-10T18:28:11.070" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="864" PostId="1473" Score="1" Text="I don't think that there's any meaningful way to make that kind of judgement call at the moment, aside from checking who makes the GPU. Ultimately there's more factors than just &quot;can the hardware execute commands from multiple queues simultaneously&quot;, and D3D12 abstracts away those details. In fact D3D12 doesn't even distinguish between hardware that might execute queues concurrently and those that might do it sequentially, the docs just say that their abstraction *allows* for concurrent execution." CreationDate="2015-09-10T23:46:30.187" UserId="207" ContentLicense="CC BY-SA 3.0" />
  <row Id="865" PostId="1473" Score="1" Text="good question ! i also feel it would be special to gain perf to exec compute and shading concurrently. maybe gains can happens thanks to the same facts that makes hyperthreading somehow faster. interleaving operations when some units are busy for the other queue. like shaders clogging the texture units, which are not used by the compute stage, which itself clogs the FPU or DPU." CreationDate="2015-09-11T01:26:37.987" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="866" PostId="1470" Score="0" Text="I don't understand why the BSDF can have a dirac. BSDF can represents 100% reflectivity using a value of 1, not infinite ? you don't need to make so that the integral is 1 over the hemisphere, you need to make so that it is less than one, strictly. or there is something i don't get. more than 1, for some angle, would mean light from other directions than the perfect reflection also contributes to this particular angle. which would not be a mirror, but some kind of lens !" CreationDate="2015-09-11T01:48:58.433" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="867" PostId="424" Score="0" Text="as an engineer I would never agree to implement such a... broken idea. EXCEPT if I'm sure the display is totally fixed. like an appli for iPhone 5S. Using this technique generates broken images from screens using reversed patterns, or different arrangements." CreationDate="2015-09-11T02:00:10.627" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="868" PostId="1436" Score="0" Text="@SimonF also we can add that the additional bandwidth requirement is pretty scary." CreationDate="2015-09-11T02:06:37.223" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="869" PostId="1455" Score="0" Text="your second image will be much harder to generate than the first." CreationDate="2015-09-11T02:08:03.693" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="871" PostId="1472" Score="0" Text="@AlanWolfe You were right! I will add some proper answer - hope you don' t mind :)  Lman I am using floats everywhere, but suggestion above solved the problem anyway. Glampert As I sad I think I don't compute mipmaps at all (I think because it might be done by default somewhere, but I don' t know about it :) I used &quot;LinearWrap&quot; sampler state if that's what you mean" CreationDate="2015-09-11T06:11:48.617" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="872" PostId="1470" Score="1" Text="(Disclaimer: I Am Not A Rendering Person.) At any surface point $x$, the role of the BSDF is to act as a linear operator mapping the incident light $L_i$ to the exitant light $L_o$. Now there is no problem with $L_i$ and $L_o$ both being distributions, i.e. linear functions $D(\mathbb S^2)\to\mathbb R$, because addition and scalar multiplication of distributions is well-defined so they form a vector space. When $L_i$ and $L_o$ are functions we can represent the BSDF $\rho$ as a distribution, but if they're not we can still speak of linear transformations." CreationDate="2015-09-11T06:24:04.620" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="875" PostId="1473" Score="0" Text="Hm too bad. Maybe then &quot;aside from checking who makes the GPU, no&quot;  counts already as answer if there is not more to it. After reading all those AMD marketing stuff I'm glad to hear that I'm not alone with my confusion." CreationDate="2015-09-11T07:52:22.560" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="876" PostId="336" Score="1" Text="_&quot;I never really understood FFT, but I saw it being used for JPEG&quot;_.  I'm not sure exactly what you meant by this, but FWIW, JPEG doesn't use FFT. Instead it uses a different transform, the Discrete Cosine Transform (DCT).  The DCT has some advantages over the FFT in that, although they both repeat ad infinitum, the DCT reflects at each repetition which thus implies better continuity." CreationDate="2015-09-11T08:33:51.733" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="877" PostId="336" Score="0" Text="@SimonF You see, I even thought DCT is a special case of FFT. Thanks for clarification!" CreationDate="2015-09-11T09:19:07.820" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="878" PostId="1455" Score="1" Text="Stack Exchange works best when you ask about very specific problems you might encounter in your day to day work/studies. It doesn't work as well for book-length studies. If an answer cannot comfortably fit in the space of a post, it is probably too soon for a Q&amp;A site like this. That is why we close these questions as *too broad.*" CreationDate="2015-09-11T12:10:52.413" UserId="53" ContentLicense="CC BY-SA 3.0" />
  <row Id="881" PostId="1470" Score="0" Text="@Rahul Good idea to think about BSDF as linear operator taking $L_i$ to $L_o$, but I still wonder if it is somehow advantageous to define radiance as distribution, because it seams to me that measure is sufficient, in which case you can too think about BSDF as linear mapping from one measure $L_i$ to another $L_o$ and in addition it can be represented it as integral of some measure valued function over measure $L_i$. And decomposition of that measure valued function into abs. continuous and singular part with the respect to the solid angle gives you diffusive and specular part of BSDF." CreationDate="2015-09-11T14:22:31.350" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="882" PostId="1476" Score="1" Text="Glad to help! Anisotropic filtering is more expensive than bilinear. If that becomes a problem for you, you might try a  higher resolution texture, or distance field textures since it looks to be just 2 colors.  http://blog.demofox.org/2014/06/30/distance-field-textures/" CreationDate="2015-09-11T14:33:35.453" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="883" PostId="1478" Score="1" Text="It's probably feasible, but will have to be done on a game-to-game basis. More recent consoles, like the PS3/XB360 also use shaders, so assuming you can reverse engineer the assets, you could modify the shaders to apply additional effects. But having the protected disc complicates things, so you would probably also need a jailbroken device to run the modified software as if it was homebrew." CreationDate="2015-09-11T17:46:16.693" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="885" PostId="1479" Score="1" Text="Try the book [Space-Filling Curves - An Introduction with Applications in Scientific Computing](http://www.space-filling-curves.org/)." CreationDate="2015-09-11T19:14:30.400" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="886" PostId="1479" Score="0" Text="See also section 2.1.1.2 of Samet's *Foundations of Multidimensional and Metric Data Structures*." CreationDate="2015-09-12T01:02:50.127" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="887" PostId="342" Score="1" Text="@porglezomp Mostly marketing speak, but http://www.theverge.com/2013/6/21/4446606/how-pixar-changed-the-way-light-works-for-monsters-university" CreationDate="2015-09-13T07:13:11.773" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="888" PostId="1484" Score="0" Text="What kind of properties are you after for that surface?" CreationDate="2015-09-13T12:35:40.917" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="889" PostId="1486" Score="3" Text="What, [no MathJax](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support) ? :-(" CreationDate="2015-09-13T14:24:25.790" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="891" PostId="1485" Score="0" Text="It seems that you think &quot;project this pyramid into a 2D star-shaped object&quot; is a defined operation. It is not, until you do so." CreationDate="2015-09-13T16:47:28.220" UserId="504" ContentLicense="CC BY-SA 3.0" />
  <row Id="892" PostId="1488" Score="4" Text="I don't think questions asking for tool recommendations are on topic here. Computer Graphics SE is a Q&amp;A site for computer graphics researchers and programmers." CreationDate="2015-09-13T18:05:31.800" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="893" PostId="1486" Score="0" Text="Yep, looks like it's not working yet :(. Would you mind just using plain code formatting for time being, so all those symbols won't distract from the formulas?" CreationDate="2015-09-13T18:09:00.987" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="894" PostId="413" Score="0" Text="Done! My answer should be valid now." CreationDate="2015-09-13T18:24:50.363" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="895" PostId="413" Score="0" Text="Excellent. Deleting my earlier comments." CreationDate="2015-09-13T19:44:01.363" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="896" PostId="1489" Score="0" Text="Please do not spread the idea of using the .obj format. Please choose a more capable format to promote, whether exporting manually or using the .blend directly, which will probably be best for a beginner's purposes." CreationDate="2015-09-13T22:52:40.237" UserId="504" ContentLicense="CC BY-SA 3.0" />
  <row Id="897" PostId="1489" Score="0" Text="Although I do not agree with your comment, this is not the right place for that debate. I have edited my answer to remove the OBJ reference." CreationDate="2015-09-13T23:40:54.133" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="898" PostId="1485" Score="2" Text="In order to UV map in that way, you must think of (5) as four different vertices that happen to have the same XYZ coordinates." CreationDate="2015-09-13T23:43:35.430" UserId="1634" ContentLicense="CC BY-SA 3.0" />
  <row Id="899" PostId="1489" Score="0" Text="I'd love to hear your thoughts. Give us a link if you ever post them." CreationDate="2015-09-14T01:54:02.063" UserId="504" ContentLicense="CC BY-SA 3.0" />
  <row Id="900" PostId="1473" Score="1" Text="You know just to lift a bit of weight into the importance (actually UNimportance) of this matter. The PS4 SDK has a bug that doesnt allow emitting to any other queue than queue 0. I think if it was so crucial it would have been fixed faster." CreationDate="2015-09-14T02:03:50.897" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="901" PostId="1489" Score="0" Text="Post a question and watch everyone fire shots :-)" CreationDate="2015-09-14T05:45:09.800" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="902" PostId="1490" Score="0" Text="_&quot;I have also another doubt: are scan converting and rastering the same thing ?&quot;_   &lt;sarcasm&gt;That might depend on who's paying the patent lawyer you meet &lt;/sarcasm&gt;.  I, however, would tend to say that scan converting is probably a subset of the rasterisation process, i.e. Scan conversion being the process that determines which pixels are inside a (or all) each primitive(s). Whether you should also include the shading/texturing in the &quot;scan conversion&quot; is a bit uncertain. I tend to think of those as a 'separate' step." CreationDate="2015-09-14T08:00:20.543" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="903" PostId="1488" Score="3" Text="[Relevant meta post.](http://meta.computergraphics.stackexchange.com/a/143/16)" CreationDate="2015-09-14T09:20:30.070" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="904" PostId="1490" Score="0" Text="See https://en.wikipedia.org/wiki/Bézier_curve#Computer_graphics." CreationDate="2015-09-14T11:03:25.847" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="905" PostId="1486" Score="0" Text="I've added this question to the [list of examples on meta](http://meta.computergraphics.stackexchange.com/questions/18/should-we-have-mathjax-support) to add to the case for adding MathJax to our site." CreationDate="2015-09-14T13:37:44.347" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="906" PostId="1486" Score="0" Text="&quot;which works for _every quadrilateral_.&quot; Unless I've messed up my &quot;back of the envelope sketch&quot;, I think you need to amend that to &quot;every _convex_ quadrilateral&quot;." CreationDate="2015-09-14T13:45:54.443" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="908" PostId="1477" Score="0" Text="From these details, as I understand it, JPEG expects the input RGB values to be encoded in a way that the display will apply a power function upon display. In order to recreate those specific RGB values, they should *not* be corrected prior to encoding." CreationDate="2015-09-14T23:06:07.037" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="909" PostId="320" Score="0" Text="The gamma exponent is stored in JPEG exif data. most software totally ignore it. but you can assume than after decoding a jpeg its already in gamma space so there is no conversion to do before sending the rgb value on the display buffer." CreationDate="2015-09-15T02:23:33.447" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="910" PostId="185" Score="1" Text="There has been enough psychology of perception studies that told that we cannot tell what image looks more real. using eyeballing would be a terrible measurement method." CreationDate="2015-09-15T02:26:55.233" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="911" PostId="1471" Score="0" Text="it feels indeed contradictory, but I would not say stratified sampling reduces the error, it reduces the noise only." CreationDate="2015-09-15T02:30:16.700" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="912" PostId="1477" Score="1" Text="The trouble with stating it like that is that it's a bit ambiguous. We should probably state that, if your &quot;RGB&quot; data is, in fact, R'G'B' (and let's assume sRGB falls into that category) then you shouldn't modify the values before applying the R'G'B'=&gt;YCbCr matrix.  If, however, the data has, say, been computed with a renderer (so possibly linear), been processed using downscaling (which should be done in linear space) or, say, captured (and cleaned up) with a CCD (which I think is linear), then it has to be remapped prior to JPEG compression." CreationDate="2015-09-15T08:24:24.187" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="913" PostId="1493" Score="3" Text="Assuming a perspective projection, AFAICS the 'boundary' formed by the view-points horizon will be a (truncated) cone and thus most of the projection will be a conic section: https://en.wikipedia.org/wiki/Conic_section.  An ellipse is thus a possibility, but not the only one." CreationDate="2015-09-15T12:27:54.890" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="914" PostId="1493" Score="0" Text="Pardon my naivety, isn't an ellipse a conic section? Could a projected sphere ever result in a parabola or hyperbola?" CreationDate="2015-09-15T12:31:23.007" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="915" PostId="1493" Score="0" Text="If you look at the wikipedia diagram, https://en.wikipedia.org/wiki/Conic_section#/media/File:Conic_Sections.svg, and consider the plane onto which you are projecting, you can get anything from an ellipse/circle, through to unbounded parabolas or hyperbolas (and I guess if the plane passes through the eye, even degenerate cases)" CreationDate="2015-09-15T12:35:38.403" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="916" PostId="1493" Score="0" Text="Apologies! I omitted a key element of my question, that I was only concerned with *perspective projection*. I'm very rusty in this field and its terminology after many years away from it, yet I remain interested. By the way a [tag:perspective] would be a worthwhile addition to the site for questions such as this." CreationDate="2015-09-15T14:55:59.870" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="917" PostId="1493" Score="1" Text="In that case I will promote my comments to an answer..." CreationDate="2015-09-15T15:06:46.273" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="918" PostId="1498" Score="1" Text="I'm unable to imagine how the result could be a parabola or hyperbola despite the absolute logic of your argument. Some words clarifying what kind of layout would lead to these would be great. The best I can get my brain around is &quot;something to do with infinities somehow&quot; ..." CreationDate="2015-09-15T15:27:42.237" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="919" PostId="1497" Score="2" Text="Thanks for your answer. Please see my addenda about perspective projection. Apologies for this oversight in my original wording." CreationDate="2015-09-15T15:28:33.860" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="921" PostId="1498" Score="3" Text="Maybe something equivalent might help. Imagine you are holding a torch (flashlight for those in North America), which makes a conic beam, and you are in in a dark empty (infinite) warehouse.  Shining the torch at the floor you see an ellipse. Now _gradually_ tilt the axis of the torch back towards the horizontal. The ellipse will get longer and longer until the point when the topmost 'edge' of the beam itself is horizontal, i.e. parallel to the floor. Now the projection is a parabola and it stretches on forever. Tilting it further will form a hyperbola." CreationDate="2015-09-15T15:42:19.340" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="923" PostId="1497" Score="0" Text="Circles are special kind of ellipsis," CreationDate="2015-09-15T16:41:54.680" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="924" PostId="1497" Score="2" Text="Yes I tried to cover that in my original question. Points and line segments are other degenerate ellipses too I believe." CreationDate="2015-09-15T17:27:34.050" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="925" PostId="1501" Score="1" Text="Very prettily illustrative! What do you think about tackling the parabola and hyperbola cases?" CreationDate="2015-09-15T20:47:34.230" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="927" PostId="1501" Score="2" Text="@hippietrail Unfortunately, vector art programs don't have parabola and hyperbola tools the way they have ellipse tools, so it would be a bit harder... :)" CreationDate="2015-09-15T22:00:33.020" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="928" PostId="1498" Score="1" Text="@hippietrail: It's perhaps worth noting that, with a view plane in front of the camera, the only way you can end up with a parabola or a hyperbola is if at least part of the sphere is *between* the focal point and the view plane." CreationDate="2015-09-15T22:43:58.657" UserId="525" ContentLicense="CC BY-SA 3.0" />
  <row Id="929" PostId="1497" Score="3" Text="@hippietrail: The Earth is actually an excellent example also for perspective projections. If you take an ordinary photograph outdoors, pointing the camera towards the horizon, then (assuming that your lens has no distortion, and that the Earth is approximately a perfect sphere) the image of the Earth in the picture will be (a section of) a very broad hyperbola." CreationDate="2015-09-15T22:53:04.833" UserId="525" ContentLicense="CC BY-SA 3.0" />
  <row Id="930" PostId="1493" Score="1" Text="you need to add a constraint. fisheye is also a perspective projection, and you won't get ellipses. the constraint you need is linearity." CreationDate="2015-09-16T01:16:17.283" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="931" PostId="1506" Score="2" Text="awesome explanation. however a bit fast on 2 points, a bit more details would be loved : 1. how do you jump from dot products to matrix products ? 2. between line 2 and 3 of last quoted section, what happens (n is moved from left to right a bit magically to me)" CreationDate="2015-09-16T01:24:32.717" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="932" PostId="1506" Score="4" Text="1. (a^T)b is the same as dot(a, b) if a and b are column matrices of the same dimension. Try out the math for yourself!&#xA;2. (AB)^T = (B^T)(A^T), and (A^T)^T = A&#xA;For more matrix identities, check out [The Matrix Cookbook](http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)" CreationDate="2015-09-16T01:30:29.597" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="933" PostId="1493" Score="0" Text="@v.oddou: Thanks for your help with the terminology. Would that result in something like &quot;projected into linear perspective&quot; or something else?" CreationDate="2015-09-16T05:18:15.043" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="934" PostId="1497" Score="1" Text="@IlmariKaronen: Wow that makes it super clear and is worthy of an answer of its own! Would there be a version of this that would result in a parabola?" CreationDate="2015-09-16T05:20:12.070" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="935" PostId="1506" Score="3" Text="@v.oddou Yep, Mokosha is right. Dot product can be expressed as multiplying a 1×n matrix (row vector) with a n​×1 matrix (column vector); the result is a 1×1 matrix whose single component is the dot product. The transpose of a column vector is a row vector, so we can write a·b as a^T b. For the second question, transposing a product of matrices is equivalent to transposing the individual factors and reversing their order." CreationDate="2015-09-16T05:42:33.543" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="936" PostId="1506" Score="0" Text="perfect, its all clear without issue now. thanks both." CreationDate="2015-09-16T06:09:21.337" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="937" PostId="1493" Score="1" Text="I would rather say something like &quot;where the projection is a linear application&quot;. There might be some shortcut term for this, like &quot;linear epimorphism&quot; or something, but I long forgot that." CreationDate="2015-09-16T06:14:06.193" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="938" PostId="338" Score="0" Text="Where did you see this claim? It is rather surprising to me (but I don't immediately see that it's impossible)." CreationDate="2015-09-16T06:19:04.597" UserId="1657" ContentLicense="CC BY-SA 3.0" />
  <row Id="939" PostId="1493" Score="0" Text="@v.oddou: I've tweaked the wording of the question based on your advice." CreationDate="2015-09-16T06:21:29.973" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="940" PostId="1509" Score="0" Text="surely you mean illustrator and inkscape (instead of illustrator and illustrator)" CreationDate="2015-09-16T08:10:30.780" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="942" PostId="1509" Score="0" Text="I sure do! Tnx!" CreationDate="2015-09-16T08:30:45.907" UserId="1659" ContentLicense="CC BY-SA 3.0" />
  <row Id="943" PostId="1506" Score="0" Text="@NathanReed (Gosh this takes me back to the early PowerVR days where we modelled most things with planes). It might also be worth mentioning that, for optimisation purposes, if you have a matrix _Mr_ that only contain rotations, (i.e. is orthogonal) then Inverse(_Mr_) = Transpose(_Mr_), and so Trans(Inverse(_Mr_)=_Mr_.   You can also take shortcuts with the translation part and if you know the scaling is uniform. FWIW in the SGL PowerVR graphics library, we used to keep booleans to track whether a transformation matrix had these properties to save costs with the normal transformations." CreationDate="2015-09-16T08:32:11.153" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="944" PostId="1500" Score="1" Text="What might be really nice is if your animation could change the shading for the various outcomes: Say white for ellipse, green (for the 'one frame' of parabola), and red for hyperbola. :-)" CreationDate="2015-09-16T08:42:32.223" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="945" PostId="1500" Score="2" Text="@SimonF i thought about this, i was planning something like nathan reed. But i was in a bit of hurry, i was lucky to get this render done. Initially i was a bit sceptical whether hyperbola could exist at all, but yes now it seems obvious." CreationDate="2015-09-16T09:08:58.803" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="946" PostId="1501" Score="0" Text="@NathanReed sure but they do have general graphing tools, (if not you can get one from me) graph a generic parabola and scale/rotate to fit." CreationDate="2015-09-16T09:11:46.980" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="947" PostId="1497" Score="1" Text="@hippietrail I add some explanation at the end of my answer, hope it could answer new aspects of edited question. and thanks for your complement." CreationDate="2015-09-16T10:41:15.820" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="955" PostId="1510" Score="0" Text="Yeah, complex surfaces leave something to be desired if the textures need to be appraised, as you can not distinguish easily what is texture and what is not. In this case Suzanne is also badly oriented as its predominantly showing the flat angle towards the camera." CreationDate="2015-09-17T09:34:35.957" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="956" PostId="1510" Score="0" Text="@joojaa You're right, the main reason why Suzanne loses here is probably because of the angle. I'll mention that in the answer, or better, render another pic with better orientation." CreationDate="2015-09-17T10:31:02.377" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="957" PostId="1510" Score="0" Text="Looks better, still complex shapes are harder to appraise now i dont know if certain features are because of the normal map or are they there because of Suzannes shape. PS: maybe the literal answer to the question should be &quot;because they are trying to solve same problem&quot;" CreationDate="2015-09-17T11:35:36.283" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="959" PostId="1508" Score="1" Text="What is $G^n$ ? Didn't you mean $C^n$ i.e. $n$-times continuously differentiable? Actually I would be interested if there is subdivision algorithm which gives higher smoothness than Catmul-Clark. Catmul-Clark gives you $C^1$ at extraordinary vertices and $C^2$ everywhere else. People making 3d models for living are actually quite concerned about minimizing number of those extraordinary vertices in their meshes." CreationDate="2015-09-17T21:21:30.903" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="960" PostId="1515" Score="1" Text="How about rendering a series of comparison swatches and try to fit a conversion curve?" CreationDate="2015-09-18T06:36:53.597" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="961" PostId="1514" Score="1" Text="Why does the incident ray is split after reflection/refraction? If the light is a particle does that means that that this particle recursively split? And if the light is a wave does that means that is splits by frequency (but in this case why it splits after second/third/etc hit)?" CreationDate="2015-09-18T06:59:34.400" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="962" PostId="1514" Score="5" Text="The particle does not split. Rather, the images show the potential paths it could take." CreationDate="2015-09-18T07:03:06.197" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="963" PostId="1514" Score="2" Text="Many particles will hit the (nearly) same spot from the (nearly) same angle. For every particle going out there is (usually) a particle that went in. That means that averaged out the *beam* of particles from a certain angle on a certain spot gets split up in several (a lot) reflections." CreationDate="2015-09-18T09:09:51.863" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="964" PostId="1497" Score="0" Text="I've made some edits which hopefully don't change the intended meaning of your answer - feel free to roll back the changes if you wish." CreationDate="2015-09-18T09:33:41.090" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="965" PostId="1514" Score="1" Text="Great answer shedding light on most of my questions. Why is the specular part of non-metals colorless and not affected by the albedo? How and where does [polarization](http://filmicgames.com/archives/547) come into play?" CreationDate="2015-09-18T10:58:27.123" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="966" PostId="1488" Score="1" Text="A quick search over at [softwarerecs.stackexchange.com](http://softwarerecs.stackexchange.com/) should help you." CreationDate="2015-09-18T11:01:02.690" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="967" PostId="1514" Score="1" Text="_&quot;A material's observed color is the light that is not absorbed.&quot;_ At this point it might be worth referencing the [Are there common materials that aren't represented well by RGB?](https://computergraphics.stackexchange.com/questions/203/are-there-common-materials-that-arent-represented-well-by-rgb) discussion, as fluorescent materials spring to mind." CreationDate="2015-09-18T11:01:51.843" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="968" PostId="1497" Score="0" Text="I changed quote blocks to headings since they weren't quoting anything. Hopefully this is clearer now - I've opened a [meta discussion](http://meta.computergraphics.stackexchange.com/questions/162/should-use-of-quote-blocks-be-restricted-to-quotes) about whether to edit to convert to headings like this." CreationDate="2015-09-18T11:16:52.733" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="969" PostId="1514" Score="0" Text="@DavidKuri I don't know the specific physical reason to *why* metals absorb all diffracted light and non-metals highly attenuate reflected light (IE. black diffuse for metals, and very small, monochrome specular for non-metals). Rather, it's just an observed phenomenon in nature. If anyone has a link or explanation, I would love to see it." CreationDate="2015-09-18T14:55:06.097" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="970" PostId="1514" Score="1" Text="@DavidKuri For polarization, you have to look into the other way of representing light, ie. waves. Polarization filters, for example in cameras, use long polymer strands  to block out certain wave orientations. A similar, but much less pronounced, process happens when the light wave interacts with all materials. This is one of the ways that light 'loses' energy." CreationDate="2015-09-18T15:01:41.967" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="971" PostId="1517" Score="2" Text="I wonder if it will be helpful to plot the various methods on a single plot. For example - I plotted two of your links here: http://i.imgur.com/YJjIMOQ.png" CreationDate="2015-09-18T17:22:46.927" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="972" PostId="1518" Score="0" Text="&quot;you may be able to use a 16-bit RGBA texture and a&quot; AND A WHAT??? :)" CreationDate="2015-09-19T01:12:24.700" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="973" PostId="1518" Score="2" Text="Haha, this is what happens when you leave the tab open to finish later. Edited." CreationDate="2015-09-19T01:13:15.507" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="974" PostId="1498" Score="0" Text="@IlmariKaronen: What would &quot;focal point&quot; mean in this context? The point the eye is focussing on? The vanishing point? (I taught myself 3D perspective rotation and projection as a 12 or 13 year old and never gained fluency in the math and terminology.)" CreationDate="2015-09-19T04:47:42.520" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="975" PostId="1519" Score="3" Text="Thank you! I only knew the simplifications. These extra details are awesome" CreationDate="2015-09-21T06:07:42.403" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="976" PostId="1498" Score="0" Text="@hippietrail Focal point, in this context, would be the apex of the cone. Effectively the &quot;pinhole&quot; of the perspective, pinhole camera model. (PS Does the name imply meeting &quot;a strange lady. She made me nervous..&quot;?)" CreationDate="2015-09-21T07:49:41.237" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="977" PostId="1521" Score="0" Text="owell it is possible to do this analytically for polygons. Turn each poly into polar coordinates. You can then either analytically render these or sample like any other polygon, except they are now curved. Samplig may be faster though." CreationDate="2015-09-21T19:25:14.143" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="979" PostId="1519" Score="1" Text="This is a fascinating answer. Could you clarify/link the acronym SSS please?" CreationDate="2015-09-21T20:28:47.780" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="982" PostId="1523" Score="0" Text="What's &quot;normal compression&quot; - things like JPEG and PNG? Are you asking about the differences between those and hardware-supported formats like DXT and ASTC?" CreationDate="2015-09-21T21:14:09.627" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="984" PostId="1519" Score="0" Text="@trichoplax Thanks! SSS == sub-surface scattering." CreationDate="2015-09-21T22:06:34.003" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="985" PostId="1523" Score="6" Text="(At last, a subject I know a bit about!)&#xA;&#xA;What makes it different to PNG/JPEG is random access.  Given you want to access Texel(X.Y) you can quickly determine the small footprint of data needed to produce that texel. JPG or PNG might require decompression of up to all of the data!   Sections 1 and 2 of the [Wikipedia article](https://en.wikipedia.org/wiki/Texture_compression) are a good summary." CreationDate="2015-09-21T22:36:22.550" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="987" PostId="1521" Score="0" Text="Thanks for your comment @joojaa, but I cann't get your point. I intend to get the viewing field for each vertex (why do you mention &quot;polygons&quot;?). Is this what you mean: make each vertex as coordinate origin; then express all other vertices in polar coordiates (theta, phi, r) according to this origin; then do some analytical analysis using these coordinates or do sampling?  Could you please make it more clear?" CreationDate="2015-09-22T01:45:42.093" UserId="1692" ContentLicense="CC BY-SA 3.0" />
  <row Id="988" PostId="1521" Score="1" Text="Possibly relevant reading: [Interactive Horizon Mapping](http://research.microsoft.com/en-us/um/people/cohen/bs.pdf). The visibility maps you describe are similar to what are called horizon maps in the literature, which store the elevation angle of the &quot;horizon&quot; for a predefined set of directions around each point on a surface. That paper is mainly about using them at runtime, though, with not much detail on generating them." CreationDate="2015-09-22T06:18:14.137" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="990" PostId="1519" Score="0" Text="Thanks :) If you clarify it in the question, it will survive deletion of the comments (which are not guaranteed to be long lived). I've edited in a link and hover text which hopefully leaves your intended presentation intact." CreationDate="2015-09-22T09:48:26.563" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="991" PostId="1525" Score="0" Text="Instead of software to test against yest against known real world measurements and fluid dynamics bechmarks. Otherwise your error is tainted. I saw the same question posted elsewhere on the stackexhange network btw" CreationDate="2015-09-22T13:11:06.700" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="993" PostId="1525" Score="1" Text="I think that testing against real world measurement is good for testing if you have the physics right. If you only want to debug you program, than testing against others code is better idea. Plus in computer simulation you can measure anything without affecting the experiment. For example measuring fluid speed at any point is just impossible in real world experiment, but trivial in computer simulation." CreationDate="2015-09-22T13:32:19.240" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="994" PostId="1519" Score="2" Text="While i do appreciate the pedantry of this answer. Sub surface scattering is considered a mm scale effect while its true that at molecular ranges everything passes the surface to some degree. But the base constraint is that we are generally counting mm scale effects and trying to abstract lower levels as statistical models. Hence micrometer is equal to immediately as most pixels see much greater area than this. Same applies to color which does not meaningfully exist in physics the same way as our eyes and brain precieve it" CreationDate="2015-09-22T13:38:11.347" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="995" PostId="1525" Score="0" Text="Yes but you also inherit problems of their solvers. I admit i did do this a few times developing a multibody simulator and checking against results form MSC Adams but in hindsight that wasn't really stellarly useful" CreationDate="2015-09-22T14:30:26.947" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="997" PostId="1526" Score="3" Text="Would be interesting to see some results of your detailed recipe if there are any you can share." CreationDate="2015-09-22T14:52:22.670" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="998" PostId="1525" Score="0" Text="Checking against real world experiment was any better? I doubt it, but I might be wrong. The situation with multibody physics is quite different to fluid physics. Even something as simple as billiard has chaotic behavior. Moreover rigid body dynamics with contacts is not even well posed mathematical problem, do you know Painlevé paradox? So doing numerical simulation of multibody physics is doomed to fail in general. Some references: https://plus.maths.org/content/chaos-billiard-table https://en.wikipedia.org/wiki/Painlev%C3%A9_paradox" CreationDate="2015-09-22T15:12:51.850" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="999" PostId="1525" Score="0" Text="The situation with fluids is quite different. You know that numerical solution converges to weak solution of Navier-Stokes eq. So every solver, if it is consistent, has to converge to the same answer. But you have to do it for small Reynolds numbers, so no turbulence occur." CreationDate="2015-09-22T15:16:46.017" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="1000" PostId="1525" Score="1" Text="Yes i am aware of how multi body dynamics work, i kind of teach it (and briefly researched it for a year or two). But no checking against known analytical solutions was easier. But a real fluid is similarly chaotic as a multi body dynamic. So one should be able to check against laminar flow situations etc. Friction is a bitch though." CreationDate="2015-09-22T15:18:55.793" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1001" PostId="1523" Score="0" Text="As SimonF wrote. This is an extremely broad question, and the answer depends on which type you're interested in. Did you look at the specification for e.g. DXT?" CreationDate="2015-09-22T16:08:01.373" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1002" PostId="1507" Score="0" Text="Also, [LuxRender](http://www.luxrender.net/wiki/images/7/79/Luxball.png)." CreationDate="2015-09-22T16:12:16.720" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1003" PostId="1524" Score="0" Text="Maybe you can recompute the terms of the N-S equation with numerical differentiation and check how they cancel out." CreationDate="2015-09-23T06:19:43.570" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1004" PostId="1527" Score="0" Text="Thanks @joojaa. I think I understand your point now. So, basically, your method is to project the whole polygon sets to a unit sphere that is located at a observer point. Then, the spherical field that is covered by the projected polygons is not visible, right?&#xA;&#xA;This is an analytic method that gives the precise result. I think when apply this to all vertices, at each vertex I need to project to whole polygon set. So, this maybe computational heavy." CreationDate="2015-09-23T06:23:54.490" UserId="1692" ContentLicense="CC BY-SA 3.0" />
  <row Id="1005" PostId="1527" Score="0" Text="Depends on how many polygons you have. You can do all the tricks of scanline rendering. And backface cull etc. In fact you can do this with normal hardware all you need to do is duplicate the triangles on the wrapping seams. So all things that apply for normal perspective projection apply here. This may be faster than raytracing extremely many rays though." CreationDate="2015-09-23T06:28:53.833" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1006" PostId="1491" Score="0" Text="For cubic curves, the scanline approach can also be looked at as a 1D problem, only involving the Y equation, which is a cubic. You can find the two extrema analytically (it's a quadratic equation) to achieve reliable separation of the roots. Full analytical computation of the roots is also possible, but incremental resolution (Newton or other) is less expensive." CreationDate="2015-09-23T06:33:21.050" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1007" PostId="1527" Score="0" Text="Strictly from a CS perspective both raytracing and polygon projecting has a O(N) complexity its just that N is dramatically smaller for the polygon projection method. So your raytracer shoots 1-8 rays at the cost of one analytic polygon." CreationDate="2015-09-23T06:37:35.083" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1009" PostId="1529" Score="1" Text="There are many scanning techniques. Becuase they have a camera, and a accurate position sensor, that is basically a video you could use video tracking techniques to produce point clouds and build polygonmodels out of that.  (for some ideas see https://www.ssontech.com/learning.html great resource even if you dont use their apps)" CreationDate="2015-09-23T08:43:29.903" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1012" PostId="1526" Score="0" Text="Not at the moment, sorry :/" CreationDate="2015-09-23T13:40:05.343" UserId="1699" ContentLicense="CC BY-SA 3.0" />
  <row Id="1013" PostId="1531" Score="0" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/167/how-should-we-respond-to-homework-questions) about homework questions." CreationDate="2015-09-23T17:23:14.007" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1014" PostId="1531" Score="4" Text="Could you refine your question? You have a nice introduction, but it's difficult to know what you're really asking. How to use GL_POINTS? How to use points to draw a 2D shape? etc.." CreationDate="2015-09-23T18:34:42.750" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1017" PostId="1533" Score="0" Text="Bit of nitpicking its not impossible to do exactly. Its just not possible to do exactly for all possible cases. There are lots of cases where this can in fact be done exactly its just a bit hard to generalize this." CreationDate="2015-09-23T23:40:56.507" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1018" PostId="1533" Score="0" Text="@joojaa IMO possible cases are exceptions rather than the rule. Can you link to a reference ?" CreationDate="2015-09-23T23:50:12.467" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1024" PostId="1533" Score="0" Text="Yes but thats enough to refute impossible. Not in front of a desktop right now bit consider this: Since rational splines can do exact circular arcs that means you can do exact offsets of said arc. Since linear splines are possible and their offsets contains lines and circular arcs that too is possible. A big set of realworld uses there." CreationDate="2015-09-23T23:55:40.947" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1029" PostId="1533" Score="0" Text="So if you say impossible to do exactly in the general case then its ok." CreationDate="2015-09-24T00:04:09.457" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1032" PostId="1533" Score="0" Text="Yes; I meant impossible in the general case (at least for Bézier). If you actually read the answers I was linking to when I said this, you'll find it says exactly that." CreationDate="2015-09-24T02:16:05.580" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1034" PostId="1533" Score="1" Text="Point being, what you say should be self contained. You should update your post so that others who dont have the time dont make wrong conclusions. Remember discussions under posts are not permanent. What you dont say on the otherhand you can leave to the link." CreationDate="2015-09-24T06:58:26.377" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1035" PostId="1533" Score="0" Text="@joojaa: are there any other cases than circles that can be exploited ?" CreationDate="2015-09-24T08:21:08.310" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1036" PostId="1514" Score="1" Text="@RichieSams : the explanation is held in Maxwell 4 equations of electromagnetism. Because of conduction there can be no wave intensity at the surface of a metal. This is the reason why grids protects micro-wave doors." CreationDate="2015-09-25T03:02:51.627" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1039" PostId="1539" Score="0" Text="&quot;Calculations in affine coordinates often require divisions&quot;: I don't see why. In fact you compute exactly the same expressions." CreationDate="2015-09-25T07:07:58.123" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1040" PostId="1495" Score="0" Text="This is an image-processing related question, you are not really on the right site. (Maybe Signal Processing.)" CreationDate="2015-09-25T07:14:34.163" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1041" PostId="1539" Score="0" Text="@Yves: I'm responding to the more general &quot;use in computer graphics&quot; topic, not the specific &quot;computing matrix transformations&quot; question." CreationDate="2015-09-25T07:54:39.000" UserDisplayName="user1713" ContentLicense="CC BY-SA 3.0" />
  <row Id="1042" PostId="1539" Score="0" Text="@Hurkyl: so do I. When rendering a scene, you compute exactly the same expressions, with the same amount of divisions (the difference lies in dummy terms with a 0 factor)." CreationDate="2015-09-25T08:02:52.633" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1043" PostId="1539" Score="0" Text="@Yves: Hrm. I'm used to doing calculations where the conversion back to affine can be deferred to some extent; I'll cede to your expertise if you say that doesn't come up often." CreationDate="2015-09-25T08:05:49.240" UserDisplayName="user1713" ContentLicense="CC BY-SA 3.0" />
  <row Id="1044" PostId="1540" Score="1" Text="In principle, data types can be implemented that don't actually store those entries even though they act like they do." CreationDate="2015-09-25T08:06:53.710" UserDisplayName="user1713" ContentLicense="CC BY-SA 3.0" />
  <row Id="1045" PostId="1540" Score="1" Text="@Hurkyl Obviously. This is rarely done, as general-purpose matrix toolboxes are on hand." CreationDate="2015-09-25T08:09:54.507" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1046" PostId="1538" Score="3" Text="I think this question is too broad. In the end your formula will boil down to a sum of *some terms* that depend on *some input* (e.g. normals, light data etc.). The procedure is basically rewriting the calculation code as formulas. There is no special rendering magic there. If you can get any more specific, please add details to the question." CreationDate="2015-09-25T08:51:57.443" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1049" PostId="1537" Score="0" Text="What kind of examples are you looking for? Translation matrices and anything related to perspective projections should be easy enough to look up?" CreationDate="2015-09-25T15:07:33.867" UserId="4" ContentLicense="CC BY-SA 3.0" />
  <row Id="1050" PostId="1537" Score="0" Text="@Bart, Analogy needed." CreationDate="2015-09-25T15:20:27.290" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="1051" PostId="1537" Score="2" Text="I'm sorry @anonymous, but that doesn't really tell me anything. You're going to have to use more words to explain what exactly you are looking for." CreationDate="2015-09-25T15:21:27.963" UserId="4" ContentLicense="CC BY-SA 3.0" />
  <row Id="1055" PostId="1543" Score="0" Text="This will certainly help. Are you aware of any simpler hextocolor conversions, much narrower? Was thinking more like 8 or 16 different color names" CreationDate="2015-09-26T14:23:44.540" UserId="1723" ContentLicense="CC BY-SA 3.0" />
  <row Id="1056" PostId="1543" Score="0" Text="This would work with any number of colors." CreationDate="2015-09-26T14:33:20.363" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1057" PostId="1543" Score="0" Text="No, no, I understand that. But I want to limit the results to an extremely limited palette consisting of red, blue, green, white, orange, purple, brown and grey, for example" CreationDate="2015-09-26T16:11:13.527" UserId="1723" ContentLicense="CC BY-SA 3.0" />
  <row Id="1058" PostId="1543" Score="0" Text="Web or CSS colors have a very high number of possible matches" CreationDate="2015-09-26T16:11:49.663" UserId="1723" ContentLicense="CC BY-SA 3.0" />
  <row Id="1059" PostId="1543" Score="0" Text="Yes, whats the problem? Instead of taking the color names form web colors supply your own" CreationDate="2015-09-26T16:52:15.577" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1060" PostId="1543" Score="0" Text="The problem? Brain not working properly, I think.   Makes perfect sense,  thanks!" CreationDate="2015-09-26T16:55:00.963" UserId="1723" ContentLicense="CC BY-SA 3.0" />
  <row Id="1061" PostId="1542" Score="2" Text="Not a full answer, but [color surveys like this one from xkcd](http://blog.xkcd.com/2010/05/03/color-survey-results/) may give some approximation if you're prepared to do a fair bit of preprocessing to exclude meaningless results and narrow down to the number of colors you require." CreationDate="2015-09-26T21:19:58.773" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1062" PostId="1536" Score="1" Text="There's a very similar question on Gamedev.SE: [What does the graphics card do with the fourth element of a vector as the final position?](https://gamedev.stackexchange.com/questions/17987/what-does-the-graphics-card-do-with-the-fourth-element-of-a-vector-as-the-final)" CreationDate="2015-09-26T22:32:25.247" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1064" PostId="1535" Score="0" Text="You write &quot;arbitrary kernel with circular symmetry&quot;: Doesn't that mean that you actually only need the convolution with the (Hemispheric) Zonal Harmonics part? If your symmetry axis is different you can still use it by adding rotations before and after the Zonal convolution. How to perform rotations is described in the paper. Integration with the Zonal part (m=0) should be comparatively easy. However, as with Spherical Harmonics, it won't be analytically solvable for arbitrary functions. Simple things like cosine lobes should work fine (haven't tried yet though)." CreationDate="2015-09-27T12:46:29.590" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="1065" PostId="1546" Score="3" Text="The [Graphics Programming Black Book](http://www.jagregory.com/abrash-black-book/) is certainly a classic worth reading. A lot of oldschool black-magic in it ;)" CreationDate="2015-09-27T19:08:12.660" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1066" PostId="1545" Score="0" Text="excellent answer. also you can add that there are some papers mentioning that in some situations you can do compression yourself and decode it with client code in a pixel shader, rather than depend on dedicated hardware. i know of no real world use of that, that may be only worth for research, but it exists." CreationDate="2015-09-28T03:25:52.790" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1067" PostId="1535" Score="0" Text="@Wumpf You're right, that's pretty much what it boils down to. For SH, I'd just scale &quot;each band of f by the corresponding m=0 term from [kernel function] h&quot; (quoting Sloan's Stupid SH Tricks). Question is, can I do the same for HSH?" CreationDate="2015-09-28T07:42:25.580" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1069" PostId="1541" Score="0" Text="&quot;Sony's PS4 can perform massive matrix multiplications.&quot; You mean the Cell processor of the PS3, right? The PS4 has a rather ordinary x86 processor." CreationDate="2015-09-28T20:40:51.503" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="1070" PostId="1545" Score="1" Text="@Nathan-Reed re transform-based compression, actually, Microsofts's Talisman project used a compression scheme called TREC which (as one of the modes) used DCT but unlike JPEG, allowed random access to blocks (I suspect there must have been a table containing addresses).  This'd then allow variable length data for various blocks, but the indirection is unpleasant for HW - a reason VQ TC fell out of fashion. FWIW I experimented with about a dozen TC ideas B4 PVRTC; some were fixed-rate, transform-based, but &quot;missing&quot; coeffs still use bits. BTC-like fixed coeff locations implies &quot;free&quot; info." CreationDate="2015-09-29T06:08:22.323" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1073" PostId="1550" Score="1" Text="It might be that he does not need to draw a solid object! Or just draw in a 2d projection screen." CreationDate="2015-09-29T16:34:04.137" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1074" PostId="1552" Score="0" Text="Welcome to Computer Graphics StackExchange. Your question is very broad and also primarily opinion based. That's why I flagged it for closure. For more information about [How to ask?](http://computergraphics.stackexchange.com/help/how-to-ask), visit our HelpCenter." CreationDate="2015-09-30T07:16:13.547" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1075" PostId="1552" Score="0" Text="Do you mean 2D or 3D graphics ? They are completely different worlds." CreationDate="2015-09-30T08:08:00.657" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1076" PostId="1553" Score="0" Text="IMO, advising Direct3D to a beginner is pure cruelty." CreationDate="2015-09-30T08:09:53.530" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1077" PostId="1546" Score="0" Text="Yes, I second the book by Michael Abrash. It is a GREAT read. There are a lot more trick in the sleeve to what is written in this book but the philosophy behind it is important (even to this day !)" CreationDate="2015-09-30T10:06:23.587" UserId="105" ContentLicense="CC BY-SA 3.0" />
  <row Id="1079" PostId="1556" Score="4" Text="Not really, it can also mean just that it is redder than what  your monitor can make." CreationDate="2015-09-30T11:42:36.957" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1080" PostId="1545" Score="2" Text="@Nathan-Reed. From what I have seen, all HW decoders can be implemented with pure logic path (bit decode, some lookup, some math in the data path) but no loop / register necessity. Are you aware of any scheme that add cycle latency to the texture lookup ? (I have for fun implemented a VHDL ETC1 decoder) I was under the impression that each texture unit (TU) had decoders embedded." CreationDate="2015-09-30T14:06:51.400" UserId="105" ContentLicense="CC BY-SA 3.0" />
  <row Id="1081" PostId="1553" Score="0" Text="@YvesDaoust, not necessarily true. For a programmer with familiarity with OOP languages, D3D will probably feel much more natural than the procedural ways of OpenGL. At least that's how I felt when I started with D3D9 and C++." CreationDate="2015-09-30T18:54:23.403" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1082" PostId="1553" Score="0" Text="@glampert: it seems that you never were a beginner." CreationDate="2015-09-30T18:59:29.393" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1083" PostId="1553" Score="0" Text="@YvesDaoust, haha, in graphics yes, not in programming, true. My reading is that the OP has some knowledge of programming, but in any case, this is not a point worth discussing, I was just raising another perspective ;)" CreationDate="2015-09-30T19:05:56.603" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1084" PostId="1557" Score="0" Text="i heard once that we do get the same noise than cameras because noise is actually physical and not only electrical. (i.e there are not so many photons after all). But the brain erases it, using temporal antialiasing I reckon. (i.e we see with lots of motion blur at night)." CreationDate="2015-10-01T01:27:05.613" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1085" PostId="1556" Score="1" Text="I don't know about rgb but about energies, it is very possible to get &gt;1 reflectance thanks to fluorescence. Some materials can gather energy from other wavelengths." CreationDate="2015-10-01T01:36:30.790" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1086" PostId="1555" Score="0" Text="I'm not too familiar with NVENC, but it looks as if the `dstPitch` for the Y array is being supplied by the NVENC API, so it might conceivably differ from `srcPitch`, thus requiring the 2D copy instead of a straight memory copy. Not sure why the Y channel would require this where Uand V don't, though." CreationDate="2015-10-01T04:45:16.193" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1087" PostId="1561" Score="0" Text="What a coincidence that we would post at the same time. You save me from having to post an image which is hard to draw when im on mobile." CreationDate="2015-10-01T05:39:51.630" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1088" PostId="1561" Score="4" Text="@joojaa “You wrote all that on mobile? …You’re braver than I thought.”" CreationDate="2015-10-01T05:51:24.907" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="1089" PostId="1557" Score="0" Text="I don't quite get the idea. If you render an image in low light and simulate a Purkinje effect, it won't look realistic as the human eye will add its own effect, won't it ?" CreationDate="2015-10-01T06:47:31.977" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1090" PostId="1557" Score="1" Text="@YvesDaoust Since the image is shown on a LDR monitor under unknown lighting conditions, probably not. Simply put, the image you see on the screen will be brighter so it's easier to perceive. If we were using a HDR monitor and could reproduce the luminance values of a nighttime scene exactly (and have an otherwise dark room), you're right." CreationDate="2015-10-01T06:50:51.667" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1091" PostId="1561" Score="1" Text="Yes I did. I was thinking of drawing a graph with Mathematica on my mobile over ssh, but that would have been a bit too much... Anyway the spellchecker of my mobile sucks so if you see a lot of misspellings feel free to fix." CreationDate="2015-10-01T07:00:43.170" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1092" PostId="1557" Score="1" Text="There's nothing wrong with what your striving for, but im afraid that this seems a bit too broad to me as there are so many effects that we need to consider. I could not write this in SE format, because it would indeed be wrong. However if you adjust your scope a bit like &quot;Can you suggest **some** of the effects that i would need to consider&quot; than it would be easier to begin." CreationDate="2015-10-01T07:10:35.407" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1093" PostId="1557" Score="0" Text="@joojaa I changed the questions as you suggested, thanks." CreationDate="2015-10-01T08:36:03.313" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1094" PostId="1552" Score="1" Text="Turtle graphics! turtles all the way down" CreationDate="2015-10-01T12:42:44.113" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1095" PostId="1562" Score="0" Text="Need more context. Are you trying to calculate analytic normals for a parametric surface? An implicit surface? Or do you want to calculate the normals from a generic triangle mesh? Or something else?" CreationDate="2015-10-02T01:23:41.690" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1096" PostId="1562" Score="0" Text="Thanks, I added more detail. To answer your question I need to calculate normals from a generic triangle mesh. Though to be clear that mesh is different depending on the inputs. My shape is a 3D arrow, as an example here is a screenshot of it 2 different forms (i.e. radial, and linear). The class changes the width, depth, length, arc, and radius of the mesh as requested. http://cl.ly/image/3O0P3X3N3d1d You can see the odd lighting I am getting with my poor attempts at solving this." CreationDate="2015-10-02T01:42:12.217" UserId="1774" ContentLicense="CC BY-SA 3.0" />
  <row Id="1097" PostId="1562" Score="3" Text="The short version is: calculate each vertex normal as the normalized sum of the normals of all the triangles that touch it. However, this will make everything smooth, which may not be what you want for this shape. I'll try to expand into a full answer later." CreationDate="2015-10-02T01:58:22.140" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1098" PostId="1562" Score="0" Text="Smooth is what I am going for!" CreationDate="2015-10-02T02:03:42.117" UserId="1774" ContentLicense="CC BY-SA 3.0" />
  <row Id="1099" PostId="1562" Score="0" Text="I implemented my understanding of your short answer and the result is better, but still a bit off. http://cl.ly/image/2T3k2R1Z0V25/o I am going for the smooth look of the cube in that screenshot. On the right is all the shape points along with the surface normals as my code as calculated them." CreationDate="2015-10-02T02:42:44.147" UserId="1774" ContentLicense="CC BY-SA 3.0" />
  <row Id="1100" PostId="1562" Score="4" Text="In most cases, if you calculate the vertex positions analytically, you can also calculate the normals analytically. For a parametric surface, the normals are the cross product of the two gradient vectors. Calculating the average of triangle normals is just an approximation, and often results in visually much poorer quality. I would post an answer, but I already posted a detailed example on SO (http://stackoverflow.com/questions/27233820/providing-normals-with-triangle-strips-and-fans-for-opengl-gouraud-shading/27244693#27244693), and I'm not sure if we want replicated content here." CreationDate="2015-10-02T07:03:06.370" UserId="331" ContentLicense="CC BY-SA 3.0" />
  <row Id="1101" PostId="1564" Score="0" Text="Related: [Do I need to rebind uniforms or attributes when changing shader programs?](http://computergraphics.stackexchange.com/q/305/127)" CreationDate="2015-10-02T08:36:21.993" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1102" PostId="1569" Score="0" Text="I understand what PBR is and that it's a vague definition, as you pointed out I should have said BRDF. I get how basic Blinn-Phong shading works, but I'm wondering if (in probably naive terms) going for a (even simplistic) BRDF model is just a rewrite of shader + a slight change in the rendering pipeline, or something more complicated. As for my last question, I mean low end hardware with limited capabilities (think embedded devices)." CreationDate="2015-10-03T19:27:14.637" UserId="34" ContentLicense="CC BY-SA 3.0" />
  <row Id="1103" PostId="1569" Score="0" Text="Yes, implementing a BRDF is just a case of re-writing a shader and swapping out which textures are used as imputs. However, as mentioned above, making those textures is the hard part." CreationDate="2015-10-03T21:43:28.917" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1104" PostId="1569" Score="3" Text="Beyond just rewriting the BRDF, it's also important for the rendering pipeline to be gamma-correct and support HDR to some level. Those are definitely more involved changes if the engine doesn't have them already. Also, PBR tends to require more attention to indirect lighting (particularly specular, to look good with glossy materials), and adding various forms of real-time or semi-real-time indirect lighting to your engine can also be a huge task." CreationDate="2015-10-03T22:25:01.223" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1108" PostId="343" Score="0" Text="@joojaa Hi, May I ask you a question about `interpolate` a set of points with `closed B-spline curve`. Please see [here](http://scicomp.stackexchange.com/questions/20921/how-to-interpolate-a-set-of-points-with-a-continuous-closed-b-spline-curve)" CreationDate="2015-10-05T07:49:30.763" UserId="1796" ContentLicense="CC BY-SA 3.0" />
  <row Id="1109" PostId="1571" Score="0" Text="Thanks for your answer. I consider it to be incomplete though. Your statement from Interpretation 1 &quot;You perceive the spectrum as you would have perceived the rendered spectrum&quot; is arguably wrong. When perceiving the real spectrum, effects kick in that don't when using the conversion you described (e.g. you'd have to use a *scotopic standard observer* in low lighting conditions, as mentioned in Jameson, Hurvich: Visual Psychophysics). What you described is the idea of spectral rendering. Interpretation 2 is what I want to learn more about. The paper will be a good start, thanks for that." CreationDate="2015-10-05T08:30:23.777" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1110" PostId="1528" Score="1" Text="&quot;Note that lines, ... can be described by an implicit equation with integer coefficients.&quot; Do you mean that it is possible to write Bresenham's algorithm as a Diophantine equation?" CreationDate="2015-10-05T10:14:22.390" UserId="1786" ContentLicense="CC BY-SA 3.0" />
  <row Id="1111" PostId="1528" Score="1" Text="@AlexeyPopkov: in a way. In the first quadrant, the line equation is `Y-Y_0=(X-X0)(Y1-Y0)/(X1-X0)`, which gives rational `Y`'s. Setting `Z=Y(X1-X0)`, the equation becomes `Z-Z0=(X-X0)(Y1-Y0)`, of the form `Z=aX+b`." CreationDate="2015-10-05T10:23:54.207" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1112" PostId="343" Score="0" Text="@ShutaoTANG [image 1-2 postscript file](http://pastebin.com/Ncz4KxAC) as per request." CreationDate="2015-10-05T17:35:38.827" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1113" PostId="1575" Score="0" Text="Or you can just download the image files..." CreationDate="2015-10-05T17:46:18.773" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1114" PostId="1575" Score="0" Text="Thanks. I you hit the money when you said 'render each frame'. That is what I want to capture... the frames output by application. I'd like to 'grab' each rendered frame and pipe them to a client. I'll update my question." CreationDate="2015-10-05T18:14:36.430" UserId="1800" ContentLicense="CC BY-SA 3.0" />
  <row Id="1115" PostId="1575" Score="0" Text="Another question: Are you wanting it to be dynamic, or just a static animation? If it's static, @joojaa is right. Just render out all the frames, convert them to a movie file, then just serve the movie file from the server." CreationDate="2015-10-05T18:25:40.463" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1117" PostId="1569" Score="0" Text="Honestly, this is a bit off topic but to me it seems that focusing on crazy microscopic adjustments on the 5D space curves of BRDF by developping fitting models of microfacets integration, is just nit pick. It is much more important to focus on getting correct global illumination. it doesnt even have to be real time." CreationDate="2015-10-06T00:29:10.157" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1118" PostId="1576" Score="1" Text="There is a fourth way, artist supplied normals ;)" CreationDate="2015-10-06T04:19:45.657" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1119" PostId="1576" Score="0" Text="@joojaa: I assume you are referring to normal maps? I've never heard of manually authored normals otherwise." CreationDate="2015-10-06T05:09:13.187" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="1120" PostId="1576" Score="1" Text="No, manually authored normals. It sometimes happens that your artist knows more about how the normals should behave than the programmers models do. It is sometimes a bit problematic to the calculation engines if they assume normals come from underlying calculations. But certainly it happens and you save lot of time in mathematical modeling." CreationDate="2015-10-06T07:30:19.340" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1126" PostId="1579" Score="5" Text="Raytracers which render reflections and refractions have been ubiquitous for many many years." CreationDate="2015-10-06T13:59:46.793" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1128" PostId="1579" Score="0" Text="Path tracing does just that: bounces rays around the scene using the properties of the surfaces it hits to determine bounces.  http://www.thepolygoners.com/tutorials/GIIntro/GIIntro.htm http://www.iquilezles.org/www/articles/simplepathtracing/simplepathtracing.htm" CreationDate="2015-10-06T14:39:52.620" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1130" PostId="1582" Score="2" Text="The definition order did affect the layout. The relevant part here is the `s_buffer_load_dword` instructions - those are reading the input uniforms, and the last number in hex is the offset to read from. It shows in the first case `xy` is at offset 0 and `zw` at offset 16. In the second case you have `xy` at offset 0, `z` at offset 16, and `zw` at offset 32. It appears all the uniforms are individually 16-byte-aligned, and not packed together or reordered." CreationDate="2015-10-06T17:27:52.623" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1132" PostId="1585" Score="0" Text="This is the correct beginning for an answer to this question. But it lacks continuation, you should talk of modern unbiased rendering as well to conclude about the recent convergence of techniques toward Kajiya's equation." CreationDate="2015-10-07T01:23:15.187" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1133" PostId="1587" Score="4" Text="They're probably referring to the sRGB color space. https://en.m.wikipedia.org/wiki/SRGB" CreationDate="2015-10-07T01:29:55.510" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1135" PostId="1585" Score="1" Text="@v.oddou: feel free to enter your own. My point is about early techniques to show that this is a pretty old idea." CreationDate="2015-10-07T06:10:41.580" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1136" PostId="1575" Score="0" Text="@RichieSams it would be dynamic. I would like to allow for interaction." CreationDate="2015-10-07T08:35:04.333" UserId="1800" ContentLicense="CC BY-SA 3.0" />
  <row Id="1137" PostId="1590" Score="0" Text="How would I go about accessing the key frames? I'm not really sure where to start. The game would be streamed from the cloud to a client. Thanks" CreationDate="2015-10-07T11:29:39.523" UserId="1800" ContentLicense="CC BY-SA 3.0" />
  <row Id="1138" PostId="1592" Score="0" Text="Have you verified that mipmaps are actually used? The screenshot looks like bilinear filtering, instead of trilinear," CreationDate="2015-10-07T13:47:17.887" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="1140" PostId="1592" Score="0" Text="@JulienGuertault It may just be my poor eyesight but I can't see any distinct discontinuities that'd be indicative of just bilinear + nearest MIP map." CreationDate="2015-10-07T13:57:03.960" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1141" PostId="1592" Score="0" Text="Actually my sampler state is set to AnisotropicWrap, which, I believe, is the best. Also I am 100% sure, that I use mipmaps." CreationDate="2015-10-07T14:04:13.337" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="1142" PostId="1592" Score="0" Text="@SimonF: sorry for the confusion, I meant linear. But anyway, apparently the problem would be elsewhere." CreationDate="2015-10-07T14:06:34.733" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="1143" PostId="1593" Score="0" Text="Sorry for maybe stupid question, but what is '2x2 box filter'? Does it have any other name?" CreationDate="2015-10-07T14:18:29.363" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="1144" PostId="1593" Score="0" Text="I just meant the naive method of generating the next lower, X*Y MIP map level from the 2X *  2Y level above it. I.e.  that of averaging 2x2 texels in the upper level to produce the corresponding texel in the lower one.  It's cheap, but it's not great.  Even a simple 4x4 tent filter should produce a far better result." CreationDate="2015-10-07T14:25:08.200" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1145" PostId="1593" Score="0" Text="Just to make sure I fully understand you - now I am creating my mip maps like this: http://pastebin.com/nX1MVvEp  . Instead of that i should generate mipmaps from my original texture with original size yes?" CreationDate="2015-10-07T14:34:54.963" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="1146" PostId="1593" Score="0" Text="Well, I don't know what is inside the &quot;resize&quot; function so it's a little difficult to answer. The chaining, ie. 1024 generates 512, 512 generates 256, etc is not going to be a huge problem, as long as what's going on inside &quot;resize&quot; is ok.&#xA;As an experiment, could you create a 1024^2 texture that's all 0xFFFFFF _except_ for the top left pixel which you set to pure red, (0x0000FF).  What do you get in the 512^2  result?" CreationDate="2015-10-07T14:47:27.457" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1147" PostId="1593" Score="0" Text="My result: http://postimg.org/image/atd8nc4j3/3636ad65/" CreationDate="2015-10-07T14:55:04.413" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="1149" PostId="1593" Score="0" Text="Sorry, I wasn't very clear. What I meant was &quot;what is in the top left, say, 10x10 texels of 512x512 image&quot;." CreationDate="2015-10-07T15:40:53.390" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1150" PostId="1584" Score="0" Text="i wouldnt say we can render all effects. We can render most effects we know of. But rarely do we have the time to program in all effects." CreationDate="2015-10-07T16:34:09.477" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1151" PostId="1591" Score="1" Text="I think a image would do wonders." CreationDate="2015-10-07T16:40:41.067" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1152" PostId="1595" Score="1" Text="Do you mean that you want to detect areas of an image that are sharp and in-focus, versus blurred and out-of-focus?" CreationDate="2015-10-07T19:46:31.267" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1156" PostId="1595" Score="1" Text="How did you used edge detection algorithms? Not sure it works and can't test now (so no answer for now :P), but just off the top of my head, you can divide your image in subregions and measure the value of gradients. Based on a threshold you can then decide whether that subregion is &quot;in-focus&quot; or not. Then you can re-use the edge info to refine your first approximation I described before. It makes sense to me know, but it may very well be a brain-fart being it late here :) I'll give it a try tomorrow if I have a minute to." CreationDate="2015-10-07T21:13:59.867" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="1159" PostId="1434" Score="1" Text="Are you going to improve article on Wikipedia? I see question like yours pretty often in Comptuter Graphics SE. Maybe it's worth considering? :)" CreationDate="2015-10-08T05:24:39.500" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="1162" PostId="1595" Score="0" Text="@cifz - Thanks! Great idea to measure the gradiants, sounds like a simple and fast approach. But this will only work for a rough detection, right?" CreationDate="2015-10-08T10:24:07.547" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="1163" PostId="1595" Score="0" Text="@NathanReed Right. Sorry, not a native :) Do you think I should rephrase the question?" CreationDate="2015-10-08T10:28:27.593" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="1164" PostId="1599" Score="0" Text="Many thanks Fabrice! If possible, can you elaborate *local max frequency*?" CreationDate="2015-10-08T10:44:00.727" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="1165" PostId="1593" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/30017/discussion-between-simon-f-and-bartosz-baczek)." CreationDate="2015-10-08T12:05:38.610" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1166" PostId="1599" Score="0" Text="I don't remember how these papers were estimating the native focus before refocusing. But I could think of various solutions: poor-man wavelets by calculating the FFT in a grid of subimages (i.e. a grid of windowed FFT), estimating local autocorrelation, poor-man FFT by convolving with a set of Differential of Gaussians (to detect ranges of focus), etc. But this is kind-of instant-hacking, the best is to read what the experts did for real :-)" CreationDate="2015-10-08T12:58:38.223" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1167" PostId="1579" Score="0" Text="Youd likely be interested in reading about how the used raytracing techniques to render the black hole in the movie interstellar. They used realistic physics equations and the result was so interesting that they were able to publish a scientific research paper with the results.  http://www.wired.com/2014/10/astrophysics-interstellar-black-hole/" CreationDate="2015-10-08T14:26:19.093" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1168" PostId="1595" Score="0" Text="@poor Yes, I think it would be helpful to rephrase and edit the title to make it more clear." CreationDate="2015-10-08T17:10:30.513" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1169" PostId="1551" Score="1" Text="Wow, this should be a blog post somewhere! Great answer!" CreationDate="2015-10-08T18:39:32.033" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1171" PostId="210" Score="0" Text="I'm with Alan too, gamma can cause all sorts of contrast issues. But a uniformly emissive sphere will tend to produce very faint occlusions. You should try to use a falloff factor in the emission, such that vertical emissions are stronger than grazing emissions. Also your spheres are reflective, make then absorbing. but i guess your tracer does not take that into account otherwise we'd see more energy under your spheres." CreationDate="2015-10-09T01:24:09.873" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1172" PostId="1604" Score="0" Text="Interesting - thank you. For the additional light reaching the floor from the spheres, I am expecting this to be less than the light that would reach the floor directly from the sky if the spheres were not there, since they are convex. That is, I can't picture caustics resulting from only convex surfaces when the light is coming evenly from the sky. Even in the extreme case of all light being reflected (no absorption), I would expect simply no shadow, rather than a caustic." CreationDate="2015-10-09T02:47:27.653" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1173" PostId="361" Score="0" Text="Yes, the wording is quite strange (what is a rotating point for instance ? :-) ). It's not a mistake, it's poor wording. I'm afraid that once a guy quicky made the whole &quot;basic computer graphics&quot; content of wikipedia for some reason, letting a lot of polishing (or more) to be done. Seems like hot topics are well edited and completed (by academics and master/PhD students ?), but not basic topics (I did, for a very few)." CreationDate="2015-10-09T07:44:52.237" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1175" PostId="1551" Score="2" Text="Glad it's of help. As for a blog, I did write [this a decade ago](http://web.onetel.net.uk/~simonnihal/texcom/texcompcomp.html) but I really don't have time to do them." CreationDate="2015-10-09T08:01:52.133" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1177" PostId="1592" Score="0" Text="@bartosz.baczek It's great that you found a solution to your problem. But instead of editing the solution into your question, please post it as an answer to your question, as [answering your own question is perfectly fine](http://computergraphics.stackexchange.com/help/self-answer)." CreationDate="2015-10-09T15:41:33.163" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1178" PostId="1592" Score="1" Text="@Nero I did so :)" CreationDate="2015-10-09T15:56:13.237" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="1179" PostId="1605" Score="1" Text="Mirror is likely to define what happens when your texture repeats. In the normal mode (just considering horizontal only) if you had a texture that looked like &quot;&lt;&quot;  you'd get &quot;&lt;&lt;&lt;&lt;&lt;&quot;.  With mirroring you get &quot;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&quot;.  Obviously can also apply to vertical direction as well." CreationDate="2015-10-09T16:15:52.590" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1180" PostId="1605" Score="0" Text="It looks like increasing the resolution is the main helper. Moires patterns are just barely starting to form in the back corner of the rectangle." CreationDate="2015-10-09T16:32:09.603" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1181" PostId="1605" Score="0" Text="Actually no, mirroring does most of the job :) I tested it with mirroring only and with bigger texture only as well, and turned out that mirror provides great effect. Fortunatelly tile texture is simetric so it' s not a big deal." CreationDate="2015-10-09T16:38:44.877" UserId="205" ContentLicense="CC BY-SA 3.0" />
  <row Id="1184" PostId="1607" Score="2" Text="While this answers the question, instead of simply providing the solution to the specific problem it would be much better to describe how the OP could come up with this solution on his own. The question clearly is a homework related question. As of now, this answer does not help to solve the problem with different numbers." CreationDate="2015-10-09T18:51:18.327" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1185" PostId="1606" Score="1" Text="You can read off the matrix elements from the coefficients of x, y, z in the result. For instance if the result had 2x + 3y in one component, then the corresponding column of the matrix would have values (2, 3, 0). The problem in this case is a bit more complicated as it appears to be relying on homogeneous coordinates (since there's division by x and y in the result), but still you can basically read off the matrix elements from the form of the result." CreationDate="2015-10-09T18:51:22.410" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1186" PostId="1607" Score="0" Text="Thank you. I just investigated what &quot;perspective divide&quot; is and I found it has to do with finding a w for (x, y, z) in order to get (x, y, z, w). After finding such a value then everything is straight forward. Is this correct? Also, the issue here is to get rid of the variables in denominator." CreationDate="2015-10-09T19:02:02.020" UserId="1836" ContentLicense="CC BY-SA 3.0" />
  <row Id="1188" PostId="1607" Score="0" Text="I wonder if I can ask another question related to this same problem. &quot;Assume you have performed the projection on the canonical view volume.  Give the matrix you would use to make the final image of size 5 units wide and 3 units tall.&quot; I think the answer is [5, 0, 0, 0][0, 3, 0, 0][0, 0, 1, 0][0, 0, 0, 1] Am I correct?" CreationDate="2015-10-09T19:23:59.467" UserId="1836" ContentLicense="CC BY-SA 3.0" />
  <row Id="1189" PostId="1609" Score="0" Text="I am sorry but I do not get the idea. What do you exactly mean by &quot;reverse order&quot;?" CreationDate="2015-10-09T20:21:31.200" UserId="1836" ContentLicense="CC BY-SA 3.0" />
  <row Id="1190" PostId="1609" Score="1" Text="Multiply x by z instead of z by x;" CreationDate="2015-10-09T20:27:31.040" UserId="1826" ContentLicense="CC BY-SA 3.0" />
  <row Id="1191" PostId="1609" Score="1" Text="actually order is arbitrary one can model using row vectors and one can model column vectors. The computation yelds same result in both but the multiplication order changes. But yes this is sortof the right answer." CreationDate="2015-10-10T05:11:52.983" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1192" PostId="1607" Score="0" Text="@JORGE i guess you should update the original question. Please explain this topic or link useful resources. I am interested" CreationDate="2015-10-10T09:05:55.533" UserId="316" ContentLicense="CC BY-SA 3.0" />
  <row Id="1194" PostId="1607" Score="0" Text="@JORGE if it's a different question but relating to this one then you can post it separately as a new question but include a link to this question." CreationDate="2015-10-10T15:52:28.570" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1195" PostId="1609" Score="0" Text="Joojaa, thanks for making that clear! Row matrix means reversed order of multiplication, is that correct?" CreationDate="2015-10-10T19:55:37.973" UserId="1826" ContentLicense="CC BY-SA 3.0" />
  <row Id="1197" PostId="211" Score="0" Text="The trick of putting the sphere on the ground and looking at the contact point is a great one. Notice it works for debugging your (spherical) lights too, but in the opposite direction!" CreationDate="2015-10-11T05:04:43.907" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1200" PostId="210" Score="0" Text="Your sky may be casting a little penumbra instead of a shadow. Reduce it to a small sphere for a check. Also, the spheres are indirectly illuminating the background." CreationDate="2015-10-11T18:45:36.700" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1201" PostId="1606" Score="0" Text="[Relevant meta discussion on whether multi part questions should be in one post or separate questions](http://meta.computergraphics.stackexchange.com/questions/181/editing-a-question-to-add-a-second-part)" CreationDate="2015-10-11T20:02:02.933" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1202" PostId="1606" Score="0" Text="I'd recommend asking the second part as a separate question, since there is already an answer that was based on there only being one part. I've rolled back to the previous edit with only one question, but I've also raised this on Meta to see how others see it." CreationDate="2015-10-11T20:08:12.620" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1203" PostId="1606" Score="0" Text="I don't know what will be decided on Meta but I've rolled back the edit quickly rather than waiting for the outcome of the Meta discussion, in order to prevent new answers coming in for the second part that would then prevent the edit being rolled back. I don't mean this to seem pushy. If there's anything you'd like to discuss please feel free to join us in [chat]." CreationDate="2015-10-11T20:14:33.037" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1205" PostId="1590" Score="0" Text="maybe try to record it and than search them by hand. I know this is far away from any kind of automated test or even realtime feedback, but it seemed to be the rather uncomplicated" CreationDate="2015-10-12T07:28:21.890" UserId="1818" ContentLicense="CC BY-SA 3.0" />
  <row Id="1210" PostId="1614" Score="1" Text="Can you atomic-add 3 to the counter instead of incrementing it?" CreationDate="2015-10-12T22:38:52.533" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1211" PostId="1614" Score="1" Text="No, atomic counters can only be queried, incremented (by 1) or decremented (by 1). See [the Article](https://www.opengl.org/wiki/Atomic_Counter#Operations) in the opengl wiki. They are not the same as atomic Add etc on Images. You only have 8 (or so) of them and they are supposedly much faster when accessed extremely often (like when generating (and thus counting) thousands of ... things.)" CreationDate="2015-10-12T23:21:37.290" UserId="1699" ContentLicense="CC BY-SA 3.0" />
  <row Id="1212" PostId="1614" Score="0" Text="Yeah, I guess you would have to turn it from an &quot;atomic counter&quot; to just a variable in an SSBO. Would be interesting to see if that's actually any slower (it might not be, depending on HW). Other than that, the only thing I can think of is to do like you said and run a compute shader to multiply the value by 3." CreationDate="2015-10-13T00:35:03.507" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1213" PostId="1613" Score="1" Text="I would be very carrfull with that associativity comment its easy to misunderstand" CreationDate="2015-10-13T03:55:31.857" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1214" PostId="1613" Score="0" Text="@joojaa I don't know what exactly you mean, but I've tried to clarify that bit." CreationDate="2015-10-13T07:36:30.703" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1215" PostId="1613" Score="0" Text="Its hard for a layman to separate betveen order you multiply things and the order in which the elements are in multiplication." CreationDate="2015-10-13T11:25:49.913" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1216" PostId="1613" Score="0" Text="so they do not understand the difference between assiocative and commutative. so if you talk of the order of multiplication many may think of commutativity" CreationDate="2015-10-13T11:37:31.017" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1217" PostId="1615" Score="0" Text="I believe the question is seeking embedding of code in the image file, rather than in the image itself. If you're uncertain what a question is looking for you can also comment on the question once you have sufficient reputation." CreationDate="2015-10-13T13:58:22.043" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1218" PostId="1616" Score="0" Text="I like the second idea because it makes clear that I need two different things: A way to atomically add triangles and a way to get the final index count. Doesn't have to be the same mechanism. And yes, I'll try atomicAdd as well. As Nathan suggested in a comment above, it would be interesting to see if it really is slower. I'll report back with the findings." CreationDate="2015-10-13T14:33:49.207" UserId="1699" ContentLicense="CC BY-SA 3.0" />
  <row Id="1219" PostId="1615" Score="1" Text="thanks user1846, I was looking to embed the code inside the image file, as trichoplax mentions." CreationDate="2015-10-13T18:13:03.397" UserId="1806" ContentLicense="CC BY-SA 3.0" />
  <row Id="1220" PostId="1616" Score="1" Text="Can confirm that atomicAdd is indeed not slower for this use case on nvidia kepler ( ~1.5k indices: 0.1ms - probably needs larger size for meaningful benchmark). Binding the indirect command buffer as an SSBO and writing to the uint at position 0 works fine. Sometimes one should just try the easy solutions first..." CreationDate="2015-10-13T22:15:00.713" UserId="1699" ContentLicense="CC BY-SA 3.0" />
  <row Id="1223" PostId="1581" Score="1" Text="Something slightly unrelated, what about images that are code? Look at the esolang [piet](https://esolangs.org/wiki/Piet) which lets you program with images. And [this video](https://www.youtube.com/watch?v=FvS_DG8yIqQ) where an image is created and then built as an executable." CreationDate="2015-10-14T15:40:37.467" UserId="214" ContentLicense="CC BY-SA 3.0" />
  <row Id="1224" PostId="1619" Score="0" Text="Are you sure lerping the forward transformed object is the same as lerping the backward transformed ray? For example, I can renormalize the ray after the lerp (and scale the hit distance accordingly). This doesn't change the result." CreationDate="2015-10-14T19:41:56.740" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1225" PostId="1620" Score="0" Text="I break it down into a linear approximation, which is quite typical. One can handle more complex transforms with multiple such steps.¶ My problem is not in making the transform nonlinear, but in making the linear approximation to it correct." CreationDate="2015-10-14T19:45:49.290" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1226" PostId="1619" Score="0" Text="@imallett Lerping the ray should be equivalent to lerping the inverse matrices, but not necessarily to lerping the forward matrices or lerping the object (as inversion isn't a linear operation). And I don't think renormalizing the ray after the lerp fixes things entirely - you can still be in a sheared, nonuniformly-scaled coordinate system that can screw up the math in your intersection routines and suchlike." CreationDate="2015-10-14T20:46:39.937" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1227" PostId="1619" Score="0" Text="[See edit; better picture] At least, I do think renormalizing should rule out problems with the intersection--but that is what I thought; lerping the ray isn't lerping the object. In your answer you suggested lerping [inverse?] TRS and then recombining. Is this the way production renderers do it?" CreationDate="2015-10-14T21:01:21.383" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1228" PostId="1622" Score="0" Text="your second `glVertexAttribPointer` doesn't look right, it's missing a *5 in the stride param" CreationDate="2015-10-15T10:31:15.700" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1229" PostId="1622" Score="0" Text="@ratchetfreak Yes the *5 was missing, But the problem still exists." CreationDate="2015-10-15T12:53:33.407" UserId="1861" ContentLicense="CC BY-SA 3.0" />
  <row Id="1230" PostId="1623" Score="1" Text="What is missing for you from the SH wikipedia page ? https://en.wikipedia.org/wiki/Spherical_harmonics" CreationDate="2015-10-15T13:20:06.867" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1231" PostId="1623" Score="1" Text="Fourier basis functions are simply sines (or sines.x * sines.y). Like sound can decompose in spectrums (i.e. set of sines), so is it for a 1D drawing. For 2D data, you simply do that in x then y (but I find quite intuitive to extrapolate the notion of wavelength to 2D).  Maybe you should tell what is your background in maths ? (which school level ?)" CreationDate="2015-10-15T13:23:38.570" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1233" PostId="1604" Score="1" Text="I understand your intuition. I wanted to dare make an analytical computation for what energy we should expect on a pixel laying just under the vertical tangent of the sphere but I'm so slow at math I gave up lol. Indeed convex surfaces will reflect through only one path to any given singular emission source-point, so caustics seems implausible." CreationDate="2015-10-16T01:19:25.370" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1234" PostId="1627" Score="4" Text="I think you're going to have to ask this on a site about electrical engineering..." CreationDate="2015-10-16T01:48:37.990" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1235" PostId="1623" Score="0" Text="@FabriceNEYRET Thank you for the link. While I don't think this information will benefit other users, my background in maths is prépa(MP*) + engineering school (Master Degree) ; the program did not have in-depth covering of SH. I am indeed looking for material as rigorous as what I was used to study, although I now have less time to do so, so brevity is also important." CreationDate="2015-10-16T03:02:28.307" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="1236" PostId="1623" Score="1" Text="I read the paper you linked, it seems pretty solid." CreationDate="2015-10-16T05:10:24.150" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1237" PostId="1627" Score="0" Text="Agree with glampert but, assuming all are monochromatic, it's probably just the electronics which control the voltage patterns that, in turn drive the electromagnets (or charged plates??) that deflect the electron beam in order to sweep it across the display phosphor." CreationDate="2015-10-16T06:40:22.217" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1238" PostId="1605" Score="0" Text="It's hard to judge how much of an improvement this has made since the plane does not extend as far into the distance as in the example in the question." CreationDate="2015-10-16T07:55:13.733" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1239" PostId="1629" Score="1" Text="Indeed the way Fourier is treated at university or enginneering school can be totally different from place to place, from a pure fancy pure math toy in strange mathematical spaces (but here it's even worse for Finite Element methods) to something really connected to signal, theory and applications. Similarily SH are sometime presented as cool math object dedicated to the physics of electronic orbitals. In all these case it's not easy for (ex)students to transpose to CG or signal. -&gt; that's why it's important to tell where he starts from and where he seeks to." CreationDate="2015-10-16T08:06:35.957" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1240" PostId="1627" Score="0" Text="Please not that cathode ray tubes are at the brink of being discontinued as technology. Only very very special uses even can get hold of these things. Even oscilloscopes are using LCD monitors by now. Ancient tech." CreationDate="2015-10-16T16:03:11.870" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1241" PostId="1635" Score="0" Text="That is indeed a much better test. Actually, the Phong normalization I implemented is indeed based on Global Illumination Compendium 31a. It works out to be a 1D table which you precompute to any desired resolution for each BRDF.&#xA;&#xA;The other normalization term produces much weaker results, as might be expected in this glancing reflection case. I'm also trying to solve it myself--a similar 1D table approach should be possible." CreationDate="2015-10-17T16:53:08.640" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1242" PostId="1633" Score="0" Text="Is this per-frame? As in a prepass to fill the depth buffer?" CreationDate="2015-10-18T05:49:56.820" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1244" PostId="1585" Score="0" Text="If you don't count ray casting (which you shouldn't), this was the first ray tracing paper. So reflections/refractions/shadows have all been here since literally the very beginning." CreationDate="2015-10-18T06:20:46.497" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1245" PostId="1579" Score="0" Text="Because we currently render movies using . . . magic?" CreationDate="2015-10-18T06:21:44.070" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1246" PostId="1633" Score="0" Text="Nope, done in the loading screen. Not talking about a depth only render!" CreationDate="2015-10-18T14:16:14.810" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1247" PostId="1637" Score="0" Text="In my understanding, raycasting isnt very good for boundary information. Depthmaps also dont dont benefit from raycasting. Deepshadow maps might benefit from raycasting. Otherwise you would just be better of baking lightning to model uv maps or some kind of voxel tree. Possibly per vertex/face. The question is a bit hard to swallow, could you specify it a bit. Best is not defined, efficient in this case is not terribly defined either" CreationDate="2015-10-19T06:15:54.390" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1248" PostId="1637" Score="0" Text="Are you asking for something geometric such as Franklin Crow's [Projected Shadow Volumes](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.424.6834)?" CreationDate="2015-10-19T12:19:21.930" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1249" PostId="1599" Score="0" Text="Any work done on single image, not with Coded Aperture? Thank You." CreationDate="2015-10-19T15:08:50.363" UserId="234" ContentLicense="CC BY-SA 3.0" />
  <row Id="1251" PostId="1637" Score="0" Text="Please fill in the &quot;etc&quot; and tell us the purpose of storing the shadow data." CreationDate="2015-10-19T21:07:33.990" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1252" PostId="1637" Score="0" Text="I apologize for being so vague.. Its basically for an application where I want to be able to move the shadow around manually to see the change in the 3d objects position. Say I apply a force to the shadow of a ball, I can actually see the ball move and also the change in the shadow. So I need to be able to set up colliders for the shadow and hence need a way to be able to store the boundary of the shadow and information about the object that cast it to carry out the actions." CreationDate="2015-10-20T01:03:27.893" UserId="1879" ContentLicense="CC BY-SA 3.0" />
  <row Id="1253" PostId="1637" Score="0" Text="Can I infer that &quot;etc&quot; is void ? What other information is relevant ?" CreationDate="2015-10-20T08:49:06.520" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1255" PostId="1639" Score="0" Text="Why not trace on click? There sa really neat trick if there is less than 32-96 objects." CreationDate="2015-10-20T14:48:37.183" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1256" PostId="375" Score="0" Text="In a word: evil." CreationDate="2015-10-20T15:21:08.137" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1257" PostId="379" Score="4" Text="@ratchetfreak only if you're in space, which you're typically not. Our glorious atmosphere makes it appear yellowish (by scattering the blue)." CreationDate="2015-10-20T15:23:56.237" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1259" PostId="1637" Score="0" Text="@ichigo1191 your comments are giving a lot more detail than the question, which makes a big difference. I recommend you [edit] the question to include the extra detail all in one place. Comments are not guaranteed to last forever." CreationDate="2015-10-20T15:40:47.627" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1260" PostId="1639" Score="0" Text="@joojaa: no idea what you mean." CreationDate="2015-10-20T15:55:13.470" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1261" PostId="1641" Score="0" Text="Thank you for the reply. I am going to replicate what I did and post a screen as suggested. Anyway the final intent of this question isn't to debug my code, but to understand if what I am doing is right or there are others steps that I am missing in between!" CreationDate="2015-10-20T15:55:23.550" UserId="1895" ContentLicense="CC BY-SA 3.0" />
  <row Id="1262" PostId="1639" Score="0" Text="Why make a buffer of rays? If your only interested in one ray, shoot the ray when your needing it. Its fast enough. Why shoot a shadow buffer with rays just use a separate pass and use depth maps + index color. Again you can do this on demand. If you have 3 color channels you can use a bit mask for surfaces hit tahtway you can store ALL objects in pass of ray if you must etc. Too many open variables in question to nake any meaningful impact." CreationDate="2015-10-20T16:21:27.457" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1264" PostId="1639" Score="0" Text="@joojaa: I guess this comment belongs to the OP. I am answering the question &quot;how to store ?&quot;." CreationDate="2015-10-20T17:05:27.403" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1265" PostId="1642" Score="0" Text="Wow, i want to die now :(&#xA;I even said i wanted gl_GlobalInvocationID to behave like gl_FragCoord, which uses uses window coordinates.&#xA;This was one of the most stupid error ever.&#xA;&#xA;You saved my day and my sanity." CreationDate="2015-10-20T21:56:09.980" UserId="281" ContentLicense="CC BY-SA 3.0" />
  <row Id="1266" PostId="1641" Score="0" Text="added examples as suggested!" CreationDate="2015-10-21T11:38:50.710" UserId="1895" ContentLicense="CC BY-SA 3.0" />
  <row Id="1267" PostId="1644" Score="0" Text="and what's wrong with a 1024x512 texture that gets blitted to the display buffer by the hardware?" CreationDate="2015-10-21T16:03:15.850" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1268" PostId="1644" Score="0" Text="Nothing is inherently wrong with that, but what about smaller operations like outputting a single character? Also some smaller systems might not have a GPU, etc." CreationDate="2015-10-21T16:16:38.943" UserId="1902" ContentLicense="CC BY-SA 3.0" />
  <row Id="1269" PostId="1644" Score="4" Text="This used to be a thing in VGA days. 0xA0000 anyone?" CreationDate="2015-10-21T16:59:42.720" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1271" PostId="1641" Score="0" Text="The points from the poisson sampling are right. The algorithm that generates them is fully unit tested and the ones you see in the screens are spheres with the center in the sampled point which i programmatically created before calling Voronoi(points)! I am worried that I am not following the proper path or I am handling the Voronoi result in a wrong way" CreationDate="2015-10-21T20:10:46.213" UserId="1895" ContentLicense="CC BY-SA 3.0" />
  <row Id="1272" PostId="1641" Score="0" Text="The images you show have done the voronoi on the 2d function." CreationDate="2015-10-22T03:57:12.260" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1273" PostId="1641" Score="0" Text="@joojaa From the example images I expected that the Voronoi cell edges on the 2D surface were what was required (to give a collection of line segments connecting points on the sphere surface, rather than the collection of plane sections that would be given in 3D). However, [scipy.spatial.Voronoi](http://scipy.github.io/devdocs/generated/scipy.spatial.Voronoi.html) seems to be designed for N dimensional spaces rather than surfaces embedded in them. I can't immediately see how it would be used for 3D points constrained to a 2D surface." CreationDate="2015-10-22T10:07:57.903" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1274" PostId="1641" Score="0" Text="Is not a problem to me if I have to change library or to write my own implementation.  If you can suggest any source I need to look at I will be happy to do it!" CreationDate="2015-10-22T10:20:03.673" UserId="1895" ContentLicense="CC BY-SA 3.0" />
  <row Id="1275" PostId="1641" Score="0" Text="I'm guessing you would need to implement a distance function for two points on a surface. On the sphere you can get away with just using the 3D Euclidean distance but for arbitrary shapes you'll need something more specific otherwise points that are near to each other in 3D due to folds in the surface will appear nearer to each other than to points between them." CreationDate="2015-10-22T14:29:47.847" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1276" PostId="1641" Score="0" Text="Do you mean to sample points? I am using isotopic distance to accomplish this. Anyway as you see, my problem is with the Voronoi computation, not the initial sampling procedure, but maybe I didn't understand what you were saying!" CreationDate="2015-10-22T14:36:31.333" UserId="1895" ContentLicense="CC BY-SA 3.0" />
  <row Id="1281" PostId="1622" Score="0" Text="What if you remove the `layout` stuff from the shaders?" CreationDate="2015-10-22T18:47:39.383" UserId="1881" ContentLicense="CC BY-SA 3.0" />
  <row Id="1282" PostId="1641" Score="0" Text="No I meant the Voronoi step - converting cell centres (the Poisson sampled points) into cell edges. This also requires a length calculation in order to determine which line is equidistant from a given two points." CreationDate="2015-10-22T20:37:01.267" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1284" PostId="153" Score="0" Text="+1 So many answers to things I've always wondered about!" CreationDate="2015-10-22T20:53:48.117" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1287" PostId="1644" Score="0" Text="1024 x 512 x 4 bytes = 2MB. Peanuts in a 2GB address space." CreationDate="2015-10-23T13:37:03.110" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="1294" PostId="1651" Score="0" Text="Now this appears to be Mac only, which may not work for you with Direct3D, but [Syphon Inject](http://syphon.v002.info) which OBS uses on Mac can capture game windows that use OpenGL _much_ faster than standard video capture.  Syphon &quot;allows applications to share frames - full frame rate video or stills - with one another in realtime&quot; to quote the website." CreationDate="2015-10-24T22:45:32.967" UserId="1922" ContentLicense="CC BY-SA 3.0" />
  <row Id="1296" PostId="1652" Score="0" Text="Thanks for the answer. I think that ultimately, I will instead of saving images, stream them to a remote machine, so copying the framebuffer may be the way to go. You seem to have a fair bit of experience with this stuff. Are you able to expand on any of the questions that I posed? Thanks for the help!" CreationDate="2015-10-25T10:10:03.303" UserId="1800" ContentLicense="CC BY-SA 3.0" />
  <row Id="1297" PostId="1651" Score="0" Text="@Gliderman Thanks, I will check that out!" CreationDate="2015-10-25T10:10:32.563" UserId="1800" ContentLicense="CC BY-SA 3.0" />
  <row Id="1298" PostId="1652" Score="1" Text="@pookie Hey! Sure, I can expand this. Is there anything specifically you'd like me to comment on? For your questions, I think 1&amp;2 can be done with the DLL hooks, so be sure to read the links I've included (specially the one about starcraft). 3&amp;4 will be API specific, but on OpenGL, you can start with glReadPixels and work from there if you find it to be too slow. My experience is from recording frames from my own games plus some DLL hacking done in the past, but I've never combined both in the attempt of recording frames from a third party app ;)" CreationDate="2015-10-25T17:13:52.243" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1299" PostId="1652" Score="0" Text="Thank you! I've checked the links - windows DLL injection and starcraft link (pretty awesome) seem to be good starting points. I will take a deapre look into glReadPixels, too. In your opinion, which did you find easier to work with and which did you find faster: DirectX or OpenGL?" CreationDate="2015-10-25T17:23:39.877" UserId="1800" ContentLicense="CC BY-SA 3.0" />
  <row Id="1300" PostId="1652" Score="0" Text="@pookie, if you have some programming experience, I think Direct3D will be much easier to use, the libraries are better designed. OpenGL has too may &quot;rough edges&quot; in my opinion (this coming from someone that uses OpenGL more than D3D). D3D is Windows only, so that might be a negative for you, don't know. Performance-wise, I don't think there's going to make much of a difference which one you use. The best advice I can probably give you is: first figure out how to get it working, then worry about optimizations." CreationDate="2015-10-25T18:08:27.037" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1301" PostId="1652" Score="0" Text="Thanks, I've a fair bit of c# knowledge - not sure how I will take to D3D, but I will give it a stab." CreationDate="2015-10-25T18:11:45.210" UserId="1800" ContentLicense="CC BY-SA 3.0" />
  <row Id="1302" PostId="1652" Score="1" Text="Ah, ok, if you're familiar with OOP programming, then D3D will definitely be easier. OpenGL is all procedural and uses a bunch of global state, that's mostly what I meant about rough edges. Best of luck to you!" CreationDate="2015-10-25T18:30:15.723" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1306" PostId="1656" Score="2" Text="Code looks right on a first glance. Are you checking for GL errors? Do you have [KHR_debug](http://renderingpipeline.com/2013/09/opengl-debugging-with-khr_debug/) output set up?" CreationDate="2015-10-26T02:41:25.983" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1308" PostId="1641" Score="0" Text="These articles look relevant to what you are trying to achieve:&#xA;http://meshlabstuff.blogspot.com/2009/03/creating-voronoi-sphere.html&#xA;http://meshlabstuff.blogspot.com/2009/04/creating-voronoi-sphere-2.html" CreationDate="2015-10-26T03:19:08.577" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="1311" PostId="1657" Score="5" Text="I'm voting to close this question as off-topic because [current consensus](http://meta.computergraphics.stackexchange.com/q/146/16) seems to be that asking for literature (or similar) recommendations is not a good fit for the site." CreationDate="2015-10-26T08:51:24.840" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1312" PostId="1656" Score="0" Text="Updated my question." CreationDate="2015-10-26T09:11:02.463" UserId="214" ContentLicense="CC BY-SA 3.0" />
  <row Id="1314" PostId="1658" Score="0" Text="It's the book. :-)" CreationDate="2015-10-27T00:30:18.227" UserId="1886" ContentLicense="CC BY-SA 3.0" />
  <row Id="1316" PostId="1662" Score="0" Text="If it's a read-only lookup table, you can just use a buffer/texture. You could either pack it into one of the normal texture formats, or you can use some of the newer features of DX11 / OpenGL to have a custom format. UAV in DX11 land, or a texture / shader_image_load_store in OpenGL land." CreationDate="2015-10-27T17:33:49.673" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1318" PostId="1662" Score="0" Text="In addition, give this presentation a look: https://www.cvg.ethz.ch/teaching/2011spring/gpgpu/cuda_memory.pdf It's for CUDA, but it should give you a better idea of what is happening on the underlying hardware" CreationDate="2015-10-27T17:41:58.977" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1319" PostId="1663" Score="0" Text="In your code sample the instance buffer is empty, is this intended? Also there is no vertex buffer binding (either glBindBufferRange​ or the newer glBindVertexBuffer)." CreationDate="2015-10-27T22:21:27.677" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="1320" PostId="1663" Score="0" Text="I'm filling the data into the second paragraph, also, why would I need to call glBindBufferRange?&#xA;&#xA;Isn't that just needed for UBOs / SSBOs?" CreationDate="2015-10-28T11:32:11.837" UserId="1938" ContentLicense="CC BY-SA 3.0" />
  <row Id="1321" PostId="1665" Score="0" Text="Amazing, just finished implementing it and Element Buffers are the way to go." CreationDate="2015-10-28T16:46:49.410" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="1322" PostId="1663" Score="0" Text="Oh sorry, overlooked the buffer filling. And I also remembered the glBindBufferRange wrong since I was by now too much used to the newer glBindVertexBuffer semantics. But there should be a glBindBuffer for the per-vertex data somewhere between your VertexAttrib calls, right?" CreationDate="2015-10-28T19:43:50.567" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="1323" PostId="1663" Score="0" Text="Yes. As you can see, my per-instance's data's vertex attrib locations start at 3, that's because, previously, from 0 to 2, I've defined my per-vertex data. I just posted the instancing related parts of the code." CreationDate="2015-10-29T04:38:49.910" UserId="1938" ContentLicense="CC BY-SA 3.0" />
  <row Id="1324" PostId="1663" Score="0" Text="Sorry for double posting, however, strangely enough, [this](http://imgur.com/KgVN4L1) is what happens when I turn on the &quot;wireframe mode&quot;. The middle quad gets cut." CreationDate="2015-10-29T05:01:15.347" UserId="1938" ContentLicense="CC BY-SA 3.0" />
  <row Id="1326" PostId="1669" Score="3" Text="Any context? Or reference where you have read that? Because it does not make sense to me. Plus, if I'm not mistaken, Gn continuity is defined only for piece-wise polynomial surfaces, there is no reason for a surface to be polynomial and in practice most of the surfaces are piece-wise linear." CreationDate="2015-10-30T14:29:20.230" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="1327" PostId="1669" Score="2" Text="G2 just mentions the geometric n-derivability, independently of any parameterisation." CreationDate="2015-10-30T20:06:05.620" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1329" PostId="1671" Score="0" Text="I would be more modest on general claims about &quot;textures can be modeled as&quot;. I would rather say &quot;some (restricted) famillies of textures can&quot;." CreationDate="2015-10-31T09:20:29.293" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1330" PostId="1672" Score="0" Text="[Skeletal Animation](https://en.wikipedia.org/wiki/Skeletal_animation)" CreationDate="2015-10-31T17:48:22.650" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1331" PostId="1671" Score="0" Text="It would help if you could add some links to explain the concepts you are referring to, like Markov Random Fields and texture neighborhoods. Otherwise, we're just guessing what you're talking about." CreationDate="2015-10-31T21:08:47.123" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1332" PostId="1669" Score="0" Text="@tom He is talking of general surfece design like in CAD. No they dont need to be polynomials, but in practice they often are (except for circular arcs and conics)" CreationDate="2015-11-01T08:00:08.843" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1333" PostId="1672" Score="0" Text="I assume the question is really how are transformation matrices interpolated. http://stackoverflow.com/questions/3093455/3d-geometry-how-to-interpolate-a-matrix" CreationDate="2015-11-01T08:50:02.107" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1334" PostId="1673" Score="1" Text="I edited the question to include the image, code and link to the other question. However, in my opinion, it would have been better to edit the other question and include the new things you found out there. It is also encouraged to update the question to include what you have tried to solve the problem even after you posted it. [See our Help Center](http://computergraphics.stackexchange.com/help/no-one-answers)." CreationDate="2015-11-01T11:38:06.240" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1336" PostId="1673" Score="0" Text="Oh, I did not know about this. Thank you for both editing my questions and providing advice." CreationDate="2015-11-01T14:26:15.407" UserId="1938" ContentLicense="CC BY-SA 3.0" />
  <row Id="1337" PostId="1669" Score="0" Text="@joojaa Than I'm still puzzled why the use of special notation Gn. In mathematics there is standard notion of Cn differentiable manifold. So is Gn and Cn the same? I thought that Gn manifold is piece-wise polynomial, so it is C-infty manifold except at the patch seams." CreationDate="2015-11-01T16:48:00.427" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="1339" PostId="1672" Score="0" Text="Or have a look at screw theory." CreationDate="2015-11-01T22:06:49.970" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="1340" PostId="1675" Score="4" Text="your question is too vague. &quot;design&quot;, &quot;implement&quot; = the rendering only ? now there are several aspects in rendering: intersecting the shape, surface materials, volume materials, light transport... the strange thing here is that the caustics on the floor seems very accurate, but the glass aspect seem very non-physical. Anyway, yes, ray-based algorithms are involved at many place here.  Anyway people would more consider design = shape and motion. (+ textures, when present)." CreationDate="2015-11-02T13:11:45.747" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1341" PostId="1671" Score="0" Text="Do you mean &quot;Texture Synthesis&quot; in which a large texture can be generated from a small exemplar?" CreationDate="2015-11-02T15:14:52.437" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1342" PostId="1675" Score="0" Text="Do you mean the mathematical shape?" CreationDate="2015-11-02T18:36:39.023" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1344" PostId="1529" Score="0" Text="I have actually designed and implemented (with help of my colleague) an algorithm to create 3D mesh of the city from the data that is collected (for a competitor company). However, what you're asking would require breaking NDA and I don't believe any sane person would be willing to do that, so your question can only be truly answered by developers who have worked on something like this in their spare time and thus are not bound by legal paperwork (regardless of whether they still work for that company or not)." CreationDate="2015-10-07T18:35:40.373" UserId="1820" ContentLicense="CC BY-SA 3.0" />
  <row Id="1345" PostId="1562" Score="0" Text="Since you are using SceneKit, would `SCNShape` be useful to you? It can create a 3D mesh with normals and texture coordinates by extruding a Bézier path." CreationDate="2015-10-13T22:17:45.723" UserId="1852" ContentLicense="CC BY-SA 3.0" />
  <row Id="1347" PostId="1669" Score="0" Text="@tom C continuity is the parametric continuity and G is the geonetric continuity and in this case the continuity over 2 separate geometries." CreationDate="2015-11-03T05:19:53.993" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1348" PostId="420" Score="1" Text="@MichaelIV Nevertheless, Alan Wolfe has a point. You could greatly improve your answer by including relevant code in the answer itself to make it self-contained and future-proof against the link going dead some day." CreationDate="2015-11-03T08:45:30.887" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1349" PostId="1589" Score="0" Text="This seems to have a couple of problems. a) It's not entirely clear whether you're the developer or just a player of the game (in the latter case, this is clearly off-topic, as this site is about computer graphics *programming* and *research*). b) If you are the developer this doesn't seem to be actually about graphics but more about the network code of your game. In that case, you might want to try [GameDev.SE](http://gamedev.stackexchange.com/) or [so] but be sure to read their respective help centres to make sure your question is of high-quality and on-topic." CreationDate="2015-11-03T08:50:00.717" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1350" PostId="1624" Score="0" Text="(A belated) Welcome to Computer Graphics SE! I'm afraid questions about 3D printing are not on topic here, as this site is about computer graphics programming and research. Unfortunately, I can't think of any better Stack Exchange to redirect you to either. There *is* [a proposal for a 3D Printing SE](http://area51.stackexchange.com/proposals/82438/3d-printing-and-rapid-prototyping) in the commitment phase though, so you might want to keep an eye on that." CreationDate="2015-11-03T08:54:32.170" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1351" PostId="1672" Score="0" Text="Welcome to Computer Graphics SE! I think there's a good and interesting question there, but currently it's not clear what exactly is being asked. Please consider editing some more detail into your question and also show what you've tried/found out yourself." CreationDate="2015-11-03T08:56:22.190" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1352" PostId="1675" Score="0" Text="Welcome to Computer Graphics SE! As Fabrice's and joojaa's comments show it's not entirely clear what you're asking. Please include some more detail whether your question is about the rendering (refraction and caustics), or the geometry or something else. It might also help to include the source of the image." CreationDate="2015-11-03T08:59:23.493" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1353" PostId="1621" Score="1" Text="The original vector is  $(2/(x+y),(5y+z)/(2x+2y),3)$ while your expansion assumes  $(2/(x+y),5(y+z)/(2x+2y),3)$" CreationDate="2015-11-03T13:47:17.367" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1354" PostId="1486" Score="0" Text="@ratchet, MathJax, yeah!" CreationDate="2015-11-03T15:13:15.963" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="1355" PostId="1621" Score="0" Text="@ratchetfreak Thanks for your comment. I corrected my mistake." CreationDate="2015-11-03T15:36:09.700" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1356" PostId="258" Score="0" Text="N.B. this fact is also used when deriving theoretical bounds for [anisotropic diffusion](https://en.wikipedia.org/wiki/Anisotropic_diffusion)." CreationDate="2015-11-03T17:12:34.340" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="1358" PostId="317" Score="0" Text="Direct3D is also what you would use if you're programming for any of the Xbox consoles." CreationDate="2015-11-03T23:37:08.393" UserId="1989" ContentLicense="CC BY-SA 3.0" />
  <row Id="1359" PostId="295" Score="2" Text="ratchet freak's answer is a good one.  If you want to learn Direct3D you can read my free book [&quot;The Direct3D Graphics Pipeline&quot;](https://legalizeadulthood.wordpress.com/the-direct3d-graphics-pipeline/).  The API details are mostly Direct3D9, but the concepts are the same.  The thing is, the concepts for programming 3D graphics haven't really changed at all since the late 1960s.  The APIs for expressing those concepts have gotten much smarter." CreationDate="2015-11-03T23:38:58.073" UserId="1989" ContentLicense="CC BY-SA 3.0" />
  <row Id="1360" PostId="1687" Score="0" Text="obj is simple enough to write a parser for, collada also has free libs to parse it." CreationDate="2015-11-04T22:16:51.503" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1362" PostId="1691" Score="1" Text="Yes, it makes a difference. As an extreme example, suppose the incident light spectrum is a delta function at 600 nm and the reflectance of the surface is a delta function at 610 nm. Then the exitant light is zero, but if you convert both spectra to XYZ or RGB that's not what you will get." CreationDate="2015-11-05T11:01:38.407" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="1363" PostId="1691" Score="0" Text="@Rahul True. I changed the question to reflect that I'm less interested in these kind of contrived cases and more in the usual, real-world (somewhat) continuous spectra." CreationDate="2015-11-05T11:37:42.180" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1364" PostId="1691" Score="0" Text="I think this previous question covers a lot of similar ground: [Are there common materials that aren't represented well by RGB?](http://computergraphics.stackexchange.com/q/203/106) Most of the answers apply equally well to any three-component colour space, like XYZ." CreationDate="2015-11-05T12:22:43.103" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="1365" PostId="1691" Score="1" Text="I understand that there are &quot;Effects for which the path of a ray is dependent on its wavelength&quot;, and I've explicitly excluded those. I also understand that there are &quot;Colours that the human eye can detect that cannot be displayed in RGB&quot;. Since I will end up in sRGB anyway, I can't change that. The question holds: do the intermediate calculations yield the same results _in common cases_ (no sodium lamps, no delta functions, just wide-range-of-frequencies stuff)?" CreationDate="2015-11-05T12:43:04.497" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1366" PostId="1691" Score="3" Text="There will almost always be a difference in intermediate results when rendering using a three-component color space rather than the full spectrum. The pathological cases presented here just take that difference to an extreme. You say you don't care about sodium lamps, but you do care about wide-range-of-frequencies stuff - this makes this question very vague. Unless we know where your threshold is, we won't be able to answer this question." CreationDate="2015-11-05T13:08:26.880" UserId="79" ContentLicense="CC BY-SA 3.0" />
  <row Id="1367" PostId="1691" Score="0" Text="@BenediktBitterli I'd be happy about some mathematical formulation of how to quantify the error, or a detailed description of the sources of error. Is early integration the only source, or does the inclusion of a reference illuminant pose a problem itself?" CreationDate="2015-11-05T13:18:27.120" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1370" PostId="1693" Score="0" Text="It's entirely possible to run the pixel shader for overlapping triangles at the same time. All that matters is that the *blending* happens sequentially. Pixel shaders can kick off a lot of work to the blending unit in any order, which can then reorder all those blend operations depending on the triangle that it came from." CreationDate="2015-11-05T14:44:47.070" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="1371" PostId="1693" Score="0" Text="I've seen situations where flickering does occur due to multiple triangles lying in the same plane, but I don't remember if that happened while the camera/world/etc. transforms were static or if it required movement and recalculation. That's an edge case, though." CreationDate="2015-11-05T16:07:18.187" UserId="2000" ContentLicense="CC BY-SA 3.0" />
  <row Id="1372" PostId="1693" Score="0" Text="@JAB It sounds like you're referring to [z-fighting](https://en.wikipedia.org/wiki/Z-fighting).  This usually occurs due to the fact that even if two primitives are analytically co-planar, quantization in the interpolator produces a pattern where some pixels from both primitives are visible.  The pattern will shift, causing flickering, only if the vertices (or camera) are moved, otherwise the interference pattern should be consistent between frames." CreationDate="2015-11-05T17:46:51.760" UserId="1992" ContentLicense="CC BY-SA 3.0" />
  <row Id="1373" PostId="1693" Score="0" Text="@MooseBoys That's exactly it, yeah. It's been a few years since I did much with 3D graphics." CreationDate="2015-11-05T18:43:21.977" UserId="2000" ContentLicense="CC BY-SA 3.0" />
  <row Id="1374" PostId="1682" Score="0" Text="Thanks for the advice, I chose the lazy solution for now. I use a function in the vertex shader that determines the wave attenuation from the distance from tha camera." CreationDate="2015-11-05T18:52:22.637" UserId="1987" ContentLicense="CC BY-SA 3.0" />
  <row Id="1375" PostId="1693" Score="0" Text="Besides just running multiple vertices and pixels in parallel, it's also possible for multiple draw calls to be in flight at the same time, if enough GPU resources are available and there is no dependency between the draw calls." CreationDate="2015-11-05T21:15:30.600" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1376" PostId="1694" Score="0" Text="There are many possibilities, so you should describe more:&#xA;&#xA;- What kind of material you aim at (basic Lambertian, or mirror specular  + glossy specular + multi-transparent layers + diffuse + subsurface scattering ?)&#xA;&#xA;- What paradigm of &quot;light particule&quot; you want to stick with (monochromatic photons, polychromatic photons, chuck of light ray) &#xA;&#xA;- What paradigm of &quot;light transport algorithm&quot; you want to stick with (from pure stochastic with each basic behavior in 0/1, to full valuated)." CreationDate="2015-11-06T15:45:31.220" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1377" PostId="1694" Score="0" Text="@FabriceNEYRET I updated the post. But what do you mean under monochromatic photons, polychromatic photons, full valuated light transport algorithm?" CreationDate="2015-11-06T16:29:05.437" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="1378" PostId="1694" Score="4" Text="a monochromatic photon (i.e. a &quot;real photon&quot;) die or not, but cannot fade. a polychromatic photon is a bag of synchronized photon, each can have his own fate.  &quot;full valued&quot; means that you transport the faded amount of energy (i.e. the optical geometry flux) rather than throwing coins to either absorb or reflect unfaded." CreationDate="2015-11-06T16:37:54.630" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="1392" PostId="1701" Score="0" Text="Can you give an example with a 3d modeling software ?" CreationDate="2015-11-09T00:41:21.270" UserId="1636" ContentLicense="CC BY-SA 3.0" />
  <row Id="1393" PostId="1700" Score="3" Text="Following from your comment on the answer, could you clarify whether you want to know about a theoretical approach, how to achieve this programmatically or how to achieve this in modelling software? This Stack Exchange is specifically about computer graphics programming and theory. If your question is specifically about how to achieve the effect in modelling software, I'm afraid your question is off topic. It makes a good question as a general CG problem though." CreationDate="2015-11-09T09:14:42.337" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1397" PostId="1701" Score="4" Text="@Valerio this is not a forum for how to use modeling software." CreationDate="2015-11-09T14:03:21.607" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1399" PostId="138" Score="0" Text="[Also see this related question on GameDev SE.](http://gamedev.stackexchange.com/q/111051/19876)" CreationDate="2015-11-10T09:05:28.537" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1403" PostId="1709" Score="0" Text="Thanks, the docs recommended a fuzz factor. Combined with shaving border artifacts it worked perfectly. `convert original.png -shave 5x5 -fuzz 1% -trim +repage trimmed.png`" CreationDate="2015-11-12T19:13:30.373" UserId="2040" ContentLicense="CC BY-SA 3.0" />
  <row Id="1407" PostId="1711" Score="3" Text="You could improve this answer by elaborating a bit, or even just linking to something that explains further." CreationDate="2015-11-12T20:41:33.173" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1409" PostId="1711" Score="1" Text="One way of expanding this answer would be to address the drawbacks the author mentions, and explain how your approach helps with those." CreationDate="2015-11-12T20:46:54.930" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1410" PostId="1714" Score="1" Text="Note that I might have answered a question that is slightly different from yours, but I did that on purpose as yours is clearly an homework question and I preferred to give you the tools to come up with an answer by yourself rather than giving you one directly :)" CreationDate="2015-11-13T09:35:22.210" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="1412" PostId="1716" Score="0" Text="Yes, algorithms are on topic, software programs aren't, and so I guess libraries are in-between? I was hoping for an answer along the lines of &quot;Oh, you need the &lt;maximal euclidian distance in XYZ colorspace&gt; algorithm. There's an implementation in xyz.js; go an study it there.&quot;" CreationDate="2015-11-13T12:50:10.797" UserId="2040" ContentLicense="CC BY-SA 3.0" />
  <row Id="1414" PostId="1716" Score="0" Text="For your particular question, I suspect your main task will be choosing which [colour space](https://en.wikipedia.org/wiki/Color_space) best suits your purpose, which should simplify the remainder of the task." CreationDate="2015-11-13T14:18:27.197" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1415" PostId="1720" Score="0" Text="Not super experienced here either but I suppose you could be using a different shader in one. Lambert's would look like the one on the right. But gourands would average them and make it appear smoother. http://prosjekt.ffi.no/unik-4660/lectures04/chapters/jpgfiles/3-shadings.jpg" CreationDate="2015-11-14T04:07:38.430" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="1416" PostId="1720" Score="1" Text="Thanks for your comment! Quite frankly, the only thing I did was to apply the Smoother modifier. There was no added shader before and I did not add any shader after." CreationDate="2015-11-14T04:09:48.693" UserId="2061" ContentLicense="CC BY-SA 3.0" />
  <row Id="1417" PostId="1722" Score="0" Text="Great answer! That's exactly what I was looking for: a conceptual explanation that didn't leave crucial details out but was still clear and direct to the point. Many thanks" CreationDate="2015-11-16T00:47:13.523" UserId="2061" ContentLicense="CC BY-SA 3.0" />
  <row Id="1418" PostId="1708" Score="0" Text="I'm closing this question as off-topic because it appears to be about using image processing software, not about computer graphics programming and research. [This might be on topic on Super User](http://superuser.com/questions/tagged/imagemagick)." CreationDate="2015-11-16T09:49:11.447" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1419" PostId="1722" Score="3" Text="_&quot; The eye however is extremely sensitive to abrupt changes in color, and interprets that as a hard crease.&quot;_   Actually the human visual system is very good at detecting changes in the _derivative_ of the shading. The shading can be continuous but if there are discontinuities in the rate of change [as in this image](http://www.cs.ru.ac.za/research/Groups/vrsig/pastprojects/041machbands/image03.png) these can be surprisingly noticeable. Search for Mach band effect." CreationDate="2015-11-16T10:57:28.713" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1420" PostId="1722" Score="1" Text="@SimonF thats why they see a crease, because they derive it edge detection and all that. But the human brain does not really eveluate the gradient flow 2 smooths are allmost equal to most humans (theres no second derviate sensing for example). So having  smooth sphere is smooth, even if the normals do not behave entirely spherically just as long as they are smooth. Thets why we get away with the trick. Very few surfaces actually behave this way." CreationDate="2015-11-16T11:59:40.677" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1421" PostId="1722" Score="0" Text="_&quot;(theres no second derviate sensing for example)&quot;_ I just checked in  Glassner's &quot;Principles of Digital Image Synthesis&quot; (Volume 1 page 29)  ... and now I am more confused than ever." CreationDate="2015-11-16T12:41:54.293" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1422" PostId="1722" Score="0" Text="@SimonF You may be confused about the reflections that are naturally one derivative lower than what the surface is. Thus a human can under certain conditions sense the second derivative. But the point is rather that humans can see creases, but they don't make meaningful difference between all different changes, the fact that a reflection is slightly off or in wrong direction isnt automatically apparent to a human. Without deeper analysis. Just as long as there's no abrupt change is for the most part good enough in many circumstances. We should continue this in the chat room though" CreationDate="2015-11-16T12:49:24.150" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1424" PostId="1728" Score="1" Text="The algorithm in your link is very simple. It looks less sophisticated than some of the papers I found from the nineties. It looks like a good starting point, but I'm hoping for the highest-performance solution for a production system, not just a &quot;my first raytracer&quot;." CreationDate="2015-11-17T08:35:18.037" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1425" PostId="1725" Score="0" Text="Screen-space reflections: create a height-field using the depth and frame buffer, ray trace it to get crude reflections. I don't know about the details, but I'd imagine Crysis, Killzone, lately Frostbite etc. will have used some sophisticated technique to get it fast. Have you looked into this?" CreationDate="2015-11-17T10:15:27.807" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1426" PostId="13" Score="3" Text="&quot;Whereas, in Monte Carlo ray tracing or simply path tracing, you sample only one ray in a direction preferred by the BRDF.&quot; Per se, you don't know how the ray is selected. Naive approaches use random rays. Taking the BRDF into account is importance sampling and not inherent to Monte Carlo ray tracing or path tracing." CreationDate="2015-11-17T10:20:32.100" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1427" PostId="1729" Score="0" Text="dynamic indexing can cause issues in some drivers" CreationDate="2015-11-17T10:21:21.460" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1428" PostId="1725" Score="1" Text="@DavidKuri Thanks, that's a good pointer for how to get the core ray-marching fast. There should be a lot of optimizations possible for a more static height-field that don't work so well on screen-space tracing, such as pre-computing mipmaps or a min-max quadtree, so I'm still hoping for an answer that covers that." CreationDate="2015-11-17T10:38:27.833" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1429" PostId="1728" Score="0" Text="This stuff is used in demoscene code and screenspace reflections in the most advanced modern games. The fastest code is sometimes the simplest. I wouldn't dismiss it due to its simplicity. It'll be interesting to see if you get any other responses though." CreationDate="2015-11-17T14:18:41.427" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1430" PostId="1725" Score="0" Text="Hey Dan BTW are you looking for CPU or GPU solutions? And real time or non real time rendering?" CreationDate="2015-11-17T14:27:43.217" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1431" PostId="1725" Score="0" Text="@AlanWolfe My use is GPU and non-real-time (i.e. max throughput rather than best image quality you can manage in 16 ms), but I'll still upvote interesting answers that are fast on the CPU or primarily for interactive renderers." CreationDate="2015-11-17T14:46:44.920" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1432" PostId="1725" Score="0" Text="You could try to create a signed distance field from the height map. Thats basically a 3d texture that stores the distance to the next surface. This allows to &quot;travel the ray faster&quot;. Unreal Engine 4 uses this for mid-range ambient occlusion, soft shadows and terrain shadows in general" CreationDate="2015-11-17T15:34:12.997" UserId="1888" ContentLicense="CC BY-SA 3.0" />
  <row Id="1433" PostId="1730" Score="0" Text="is it possible for bode the modes to be set to 1? and if it shouldn't be are you guarding against it?" CreationDate="2015-11-17T16:25:10.333" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1434" PostId="1730" Score="0" Text="for now I am assuming that the user is either in editMode or deleteMode. he doesn't press 'c' and 'd' together. if 'c' is pressed, it is made sure he exits from editMode before pressing ' d'." CreationDate="2015-11-17T16:27:28.313" UserId="1588" ContentLicense="CC BY-SA 3.0" />
  <row Id="1435" PostId="1730" Score="0" Text="is that pseudo code or the actual code in your app (if so what language)?" CreationDate="2015-11-17T16:30:30.950" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1436" PostId="1730" Score="0" Text="its c++. and this is the pseudo code giving the general idea of what I want to achieve. The actual code has some other flags for calling drawBeizer fn etc." CreationDate="2015-11-17T16:51:53.837" UserId="1588" ContentLicense="CC BY-SA 3.0" />
  <row Id="1437" PostId="1721" Score="0" Text="Both answers are really good, it was hard for me to pick. Since I had asked about the simplest way, I think Nathan takes the cake." CreationDate="2015-11-17T17:28:29.103" UserId="14" ContentLicense="CC BY-SA 3.0" />
  <row Id="1438" PostId="1730" Score="1" Text="It may help to try to remove everything from your code that you can, without removing the problem. This may highlight the cause for you, but even if it doesn't you can then edit the minimal code into your question, which will increase the chance of someone being able to spot the underlying cause." CreationDate="2015-11-17T17:44:41.283" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1439" PostId="1730" Score="0" Text="Relevant discussion on Meta about [whether we should require a minimal working example](http://meta.computergraphics.stackexchange.com/questions/126/should-we-require-minimal-working-examples-mwes). This needs more attention to make clear the community consensus." CreationDate="2015-11-17T17:47:03.237" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1440" PostId="1728" Score="2" Text="What's missing in your response is that IQ uses a standard heightfield mesh as an initial guess to kickstart raymarching the actual terrain. He first renders a low-poly version of the terrain using standard rasterization, and then runs a pixel shader over the image that starts raymarching at the rasterized depth minus some conservative threshold. This is the only way to actually make this realtime." CreationDate="2015-11-17T19:23:46.443" UserId="79" ContentLicense="CC BY-SA 3.0" />
  <row Id="1441" PostId="1728" Score="0" Text="I believe that only part of what you are saying is true.  he does use heuristics based on terrain height (along with distance from camera) to come up with how far the ray can march, but as far as i have heard, he doesn't use rasterization.  Here is an example of his work, which does not use rasterization, but that isn't to say that there aren't implementations that DO use rasterization:  https://www.shadertoy.com/view/MdX3Rr" CreationDate="2015-11-17T19:27:12.690" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1442" PostId="41" Score="0" Text="don't forget to work in linear color space for correct results." CreationDate="2015-11-18T00:57:19.487" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1443" PostId="1731" Score="1" Text="I was considering a similar solution to Nero's answer, but you've tagged this [tag:bezier-curve]. Are you limited to using Bezier curves with thickness, or is drawing circular arcs actually an option?" CreationDate="2015-11-18T09:05:37.100" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1445" PostId="1581" Score="0" Text="Besides vulnerabilities in image decoders (e.g. GDI+) it is possible to embed code e.g. AFTER the EOF marker on a JPEG. In this case the custom code is in same image file but technically not part of the image. See example in this document re: &quot;hammertoss&quot; malware: https://www2.fireeye.com/rs/848-DID-242/images/rpt-apt29-hammertoss.pdf" CreationDate="2015-11-18T17:12:26.290" UserId="2081" ContentLicense="CC BY-SA 3.0" />
  <row Id="1446" PostId="1581" Score="0" Text="Related: http://security.stackexchange.com/questions/55061/can-malware-be-attached-to-an-image" CreationDate="2015-11-18T17:12:55.610" UserId="2081" ContentLicense="CC BY-SA 3.0" />
  <row Id="1447" PostId="1732" Score="0" Text="Yes, that's what I'm after, but there's still an overlap. If it helps, I'm using Adobe After Effects’ shape layers and expressions to create this. Rectangle 1 is driving Rectangle 2's round corner value.&#xA;&#xA;e.g.&#xA;&#xA;Rectangle 1&#xA;Round Corners = 30&#xA;&#xA;Rectangle 2&#xA;Stroke width = 40&#xA;Round Corners = 60 (30*2)&#xA;&#xA;produces this:&#xA;![rectangles2](https://dl.dropboxusercontent.com/u/1414976/rectangles2.png)&#xA;&#xA;I've set the opacity of Rectangle 1 to 50% so you can see the overlap. The darker purple is the edge of Rectangle 2's stroke.&#xA;&#xA;I can supply by After Effects file if it will help." CreationDate="2015-11-18T20:37:37.240" UserId="2076" ContentLicense="CC BY-SA 3.0" />
  <row Id="1448" PostId="1732" Score="0" Text="@GregGunn If your inner/blue box has a border radius of $30px$ and the red stroke's width is $40px$, then the radius in the middle of the red stroke needs to be $30px + (40px/2) = 50px$." CreationDate="2015-11-18T20:46:16.047" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1449" PostId="1732" Score="0" Text="@Nero Nailed it. That equation works perfectly—thank you." CreationDate="2015-11-18T20:58:43.250" UserId="2076" ContentLicense="CC BY-SA 3.0" />
  <row Id="1454" PostId="1737" Score="2" Text="Thanks Nathan that's exactly what I was after, where did you get this diagram may I ask, is it in any public documentation from oculus ?" CreationDate="2015-11-19T05:34:35.090" UserId="288" ContentLicense="CC BY-SA 3.0" />
  <row Id="1455" PostId="1737" Score="2" Text="@GarryWallis Cass [tweeted it](https://twitter.com/casseveritt/status/608677561674149889) a few months ago." CreationDate="2015-11-19T05:37:24.127" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1457" PostId="1734" Score="4" Text="Pro tip: `x * x` is much faster than `pow(x, 2.0f)`." CreationDate="2015-11-19T09:49:11.077" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1459" PostId="1728" Score="0" Text="I'm a little confused that the question is about ray tracing, and this answer is about ray marching. There is a fundamental difference between the two and what they can achieve." CreationDate="2015-11-19T12:35:05.170" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="1460" PostId="1674" Score="0" Text="What does, &quot;The change has to not only be locally satisfied but also satisfied on the retina,&quot; mean?" CreationDate="2015-11-19T13:05:03.053" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1461" PostId="1674" Score="1" Text="The eye is not recording a continious signal.  Its discrete, so even if your surface might technically meet the condition presented on a mathematical level. It might not be enough if the dicrete sample spacing does not see the change. So the slope still has to be big enough for human eye to notice." CreationDate="2015-11-19T13:38:05.733" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1462" PostId="1674" Score="0" Text="It sounds like you're saying the derivative (of the normal) doesn't just have to be continuous, but its derivative has to be below some limit. If that's what you mean, I think that last paragraph of your answer could be clearer." CreationDate="2015-11-19T13:45:04.373" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1464" PostId="1674" Score="0" Text="@DanHulme its not a limit the derivate, its not a question of slope, only, but the interwall of the slope. So it is about a discrete sampling. So a very sharp angle but small difference in slope might seem continious. Likewise continious changes under a short interwall might seem sharp. Its not about mathematics its about sampling. Its just hard to qantify as its a biological system." CreationDate="2015-11-19T14:41:26.223" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1465" PostId="1728" Score="0" Text="If you notice, the question mentions ray marching (through a grid) as an example algorithm that he has read about in the past." CreationDate="2015-11-19T14:46:45.220" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1466" PostId="1738" Score="0" Text="Thank you very much for your help, I really appreciate you taking the time to write it.  I have implemented your suggestions.  Unfortunately, my shadows are still missing.  My most recent output is in the original qustion.  I'm assuming this means the problem is somewhere upstream in my code.  Of course, that's about 200-300 lines of setting my stage.  If you've got the stomach for it, I've put it on [pastebin](http://pastebin.com/03PHrUWJ).  If you don't, I totally understand.  I really appreciate your assistance; I've been so close to this for so long I think I've become blind to my mistakes" CreationDate="2015-11-19T23:55:15.083" UserId="2084" ContentLicense="CC BY-SA 3.0" />
  <row Id="1467" PostId="1743" Score="4" Text="did you check the numbers with a simple triangle? 1,0,0; 0,1,0; 0,0,1 and ray from origin to 1,1,1" CreationDate="2015-11-20T08:54:33.727" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1468" PostId="1741" Score="0" Text="Could you maybe add an example mesh and how it's being transformed? If you're actually moving the *vertices*, not the individual polygons, I'm not sure why they would pass through each other from a simple bending operation. Shouldn't the polygons on the concave side just shrink a bit?" CreationDate="2015-11-20T09:12:56.430" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1469" PostId="1738" Score="1" Text="If you're still having trouble, just apply the usual debugging techniques. Keep simplifying your code to cut the problem space in half. Try removing all the shading computation and just set the colour to `intersections / 999999`, so your unshadowed intersections are just white. Draw an object at the light position to make sure it's correct: in both your renders, it looks like it might be in the bottom-right of the image, between the two objects. Put a breakpoint conditional on the shading co-ordinates and step through the shading of one particular point that you know ought to be in shadow." CreationDate="2015-11-20T09:19:35.913" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1470" PostId="1741" Score="0" Text="@MartinBüttner By the sound of it, he/she will get a similar problem to that of doing offset curves when the offset exceeds the radius of curvature. e.g. [look at the inner set of  green curves](https://en.wikipedia.org/wiki/Parallel_curve#/media/File:Evolute_and_parallel.gif) which have been displaced too far from the red." CreationDate="2015-11-20T10:44:10.333" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1471" PostId="1741" Score="0" Text="Seems to me the subject lines ask about recalculating normals while the body  asks about self intersection." CreationDate="2015-11-20T12:50:07.063" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1472" PostId="1744" Score="1" Text="This is a sensoring problem. The damage is done in the scanner. Photographees know a lot about these things. Its not really a computer graphics problem as a image capture problem. How to use software is out of scope." CreationDate="2015-11-20T12:52:50.583" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1473" PostId="1744" Score="0" Text="@joojaa Maybe you have idea where to look out for help? Tried to find desktop publishing and scanning and graphic design forums, but no luck so far." CreationDate="2015-11-20T14:52:54.637" UserId="2100" ContentLicense="CC BY-SA 3.0" />
  <row Id="1474" PostId="1744" Score="0" Text="Well there is [GD.Se](http://graphicdesign.stackexchange.com/)" CreationDate="2015-11-20T15:25:07.923" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1475" PostId="1741" Score="1" Text="I am sorry, my subject and body are not coherent. This is because I am not sure of the exact terminology to use.  I think Simon F has interpreted my question as I intended though; I need to figure out how to handle the situation where the offset exceeds the radius of curvature.  I will upload a sketch momentarily." CreationDate="2015-11-20T16:16:57.443" UserId="2091" ContentLicense="CC BY-SA 3.0" />
  <row Id="1478" PostId="1741" Score="0" Text="Ah yes, theres really nothing you can do about this kind of things except not bend too much." CreationDate="2015-11-20T20:53:45.187" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1479" PostId="1745" Score="2" Text="I think you should add more details about how you actually created your color palette/table so users might be able to help you.  You might also consider one of the computer science-based Stack Exchanges." CreationDate="2015-11-19T20:29:48.510" UserDisplayName="honeste_vivere" ContentLicense="CC BY-SA 3.0" />
  <row Id="1482" PostId="1730" Score="0" Text="This question is not exactly about OpenGL and Graphics but about GLUT and Input. Might be better suited for http://gamedev.stackexchange.com/" CreationDate="2015-11-21T23:31:38.017" UserId="528" ContentLicense="CC BY-SA 3.0" />
  <row Id="1483" PostId="351" Score="0" Text="I don't think I get this. Isn't it basically just a minor difference in this case? E.g. what you said implies that the only difference would be that with albedo, the diffuse reflection is `(albedo * (1 - specular))` and specular is `albedo * specular`, instead of flat diffuse and specular numbers? I really don't get it :(" CreationDate="2015-11-22T04:11:00.777" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1484" PostId="1747" Score="0" Text="1080p is the new pixel art." CreationDate="2015-11-22T06:56:28.460" UserId="504" ContentLicense="CC BY-SA 3.0" />
  <row Id="1485" PostId="351" Score="0" Text="@Llamageddon there are a number of differences covered in the answer but as a simple example: the albedo of a surface could be 0.8, but the RGB value of it's diffuse component could be (0.6, 0.5, 0.9). The albedo is generally just a single scalar value, whereas the diffuse component may have multiple values to give colour rather than just brightness." CreationDate="2015-11-22T12:44:58.633" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1487" PostId="1729" Score="0" Text="Well, I guess that's the case, can't find a better explanation." CreationDate="2015-11-23T15:18:30.047" UserId="2064" ContentLicense="CC BY-SA 3.0" />
  <row Id="1489" PostId="1751" Score="0" Text="What about when vertex shaders are used to deform meshes? (which I assume is possible... I'm a newbie to graphics programming)" CreationDate="2015-11-24T03:21:18.113" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1490" PostId="1751" Score="0" Text="Also, do you have any papers on GPU-backed occlusion culling? Is it a built-in feature of modern GPUs, or..?" CreationDate="2015-11-24T04:54:26.810" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1491" PostId="1751" Score="0" Text="@Llamageddon On modern GPU architectures, the vertex shader is always run, and is perfectly capable of deforming meshes—that's just transforming vertices non-rigidly. A more expensive vertex shader that does more work is, of course, more work that will be skipped by culling. I don't have any papers on GPU occlusion culling, academia seems to not be very enthralled with it. It is not a built-in feature of GPUs, just a creative use of compute." CreationDate="2015-11-24T08:02:07.733" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="1492" PostId="1751" Score="0" Text="Oh, I meant what about culling meshes that will be deformed? Do you cull them after running the vertex shaders? That sounds convoluted." CreationDate="2015-11-24T10:01:09.083" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1493" PostId="1751" Score="0" Text="@Llamageddon Generally you just cull them against a conservative volume. Or several smaller volumes that you deform with the mesh (for example, when skinning you can attach culling volumes to joints)." CreationDate="2015-11-24T16:43:05.913" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="1494" PostId="1751" Score="0" Text="Ah, I see. Would you mind getting in touch via some sort of IM/mail/direct messaging? I would really appreciate a mentor/tutor in graphics programmings stuff, since it's a topic that fascinates me greatly, but... various reasons, curb my ability to pursue it on my own." CreationDate="2015-11-25T04:58:56.910" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1496" PostId="1751" Score="0" Text="@Llamageddon The chat room for this site http://chat.stackexchange.com/rooms/26589/the-cornell-box is probably a good venue." CreationDate="2015-11-25T06:57:46.010" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="1497" PostId="1751" Score="0" Text="I know but I just am not good at chatrooms, I much prefer having someone to talk to and get to know x.x It's fine if you don't want to do anything such." CreationDate="2015-11-25T07:26:33.150" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1498" PostId="1757" Score="2" Text="I found this while searching around: http://neil-strickland.staff.shef.ac.uk/courses/algtop/pictures/sphere/ I think the animations help picture the homeomorphism between a sphere without poles and the plane/cylinder, so I thought you might want to include it" CreationDate="2015-11-26T09:02:46.857" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1499" PostId="1757" Score="0" Text="Yeah i was planning on drawing a picture once in front of a computer." CreationDate="2015-11-26T09:10:07.183" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1500" PostId="1754" Score="0" Text="If you solve it for still images, it ought to be the same solution for video frames right?  Also just to make sure, you are just trying to avoid the distortion at the poles when naively putting a texture on a sphere?" CreationDate="2015-11-26T18:12:40.187" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1501" PostId="1759" Score="8" Text="+1 for showing that homework questions can be high-quality questions. :)" CreationDate="2015-11-27T12:55:58.820" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1502" PostId="1754" Score="0" Text="I believe you are correct with the still images. However, I wasnt able to find any good material about that either.&#xA;&#xA;And yes, I'm trying mainly to solve the distortion at poles problem, since the fact that left side has to be identical to right side of the video for a good transition is a trivial problem with easy solution." CreationDate="2015-11-27T14:32:42.717" UserId="2138" ContentLicense="CC BY-SA 3.0" />
  <row Id="1503" PostId="1754" Score="0" Text="Does this info help any? http://blender.stackexchange.com/questions/10741/what-is-the-best-way-to-unwrap-a-sphere" CreationDate="2015-11-27T15:54:58.697" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1506" PostId="1761" Score="0" Text="Lines or line segments?" CreationDate="2015-11-29T18:33:25.253" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="1507" PostId="1762" Score="0" Text="Can we assume that the corners are 90 degree angles" CreationDate="2015-11-29T18:42:42.237" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1508" PostId="1763" Score="0" Text="How would one calculate the aspect ratio if we weren't so close to the center?" CreationDate="2015-11-29T21:15:03.770" UserId="2162" ContentLicense="CC BY-SA 3.0" />
  <row Id="1509" PostId="1763" Score="0" Text="@succubus there is a lengthy explanation [here](http://www.handprint.com/HP/WCL/perspect3.html) but you can do this with matrix calculation. Just didnt have time to outline the math." CreationDate="2015-11-29T22:20:47.103" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1510" PostId="1761" Score="0" Text="It is only important, that the two new points are connected, therefore segments of curves are also OK. Question edited." CreationDate="2015-11-29T22:33:43.360" UserId="2161" ContentLicense="CC BY-SA 3.0" />
  <row Id="1511" PostId="1756" Score="0" Text="&quot;..of a plane, cylinder or torus&quot; Klein bottle and real-projective plane feels left out :( Have a look at https://en.wikipedia.org/wiki/Fundamental_polygon" CreationDate="2015-11-29T22:58:50.367" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="1512" PostId="1756" Score="0" Text="Mobius band is now crying..." CreationDate="2015-11-29T23:06:49.073" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="1513" PostId="1757" Score="0" Text="Well your answer is not very consistent. If the sphere is not a sphere but a cylinder, because you are missing those two points, than the cylinder is not a cylinder but it is a plane, because you are missing the whole edge. And there are more than 3 topological families, by gluing different edges you can get sphere, cylinder, torus, mobius band, klein bottle, real-projective plane. Have a look at wiki page about Fundamental polygon." CreationDate="2015-11-29T23:13:41.173" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="1514" PostId="1763" Score="0" Text="Thanks for your help, unfortunately I can't accept your answer because I messed up with my stackexchange account." CreationDate="2015-11-29T23:20:19.373" UserId="2162" ContentLicense="CC BY-SA 3.0" />
  <row Id="1515" PostId="1761" Score="2" Text="You might look into path-finding algorithms for this. Use existing segments as obstacles and find a path between the two new endpoints. Maybe apply some smoothing to the resulting path to make it a nicer-looking curve." CreationDate="2015-11-29T23:36:38.467" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1516" PostId="1764" Score="0" Text="Other tools include yEd (Free to use but no free licese), gephi... this is a NP Hard problem." CreationDate="2015-11-30T03:45:31.187" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1517" PostId="1763" Score="0" Text="@succcubbus please refer to [official help page](http://computergraphics.stackexchange.com/help/merging-accounts) about merging your account to regain the ownership of the question." CreationDate="2015-11-30T10:38:01.683" UserId="2170" ContentLicense="CC BY-SA 3.0" />
  <row Id="1518" PostId="1741" Score="0" Text="Well I'm pretty sure there is something that can be done, I just don't want to re-invent the wheel.  I'm pretty sure that professional applications like Maya have this solved.  Right now I'm thinking of something along the lines of checking normal directions before and after the transform to identify problem polys.  Perhaps the blender source code has something." CreationDate="2015-11-30T20:29:36.923" UserId="2091" ContentLicense="CC BY-SA 3.0" />
  <row Id="1519" PostId="1767" Score="0" Text="Isn't this slightly under-constrained?   If you only have 3  coplanar(?),  non-colinear points, won't that allow you to have a circle? (Or do you also have the centre?)" CreationDate="2015-12-01T13:20:31.557" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1520" PostId="1767" Score="0" Text="Are the three points any particular points on the ellipse? e.g. if one is guaranteed to be one end of the major axis, and another is one end of the minor axis, then the problem is trivial." CreationDate="2015-12-01T15:16:01.610" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1521" PostId="1767" Score="0" Text="Yeah, someone elsewhere pointed out it's underconstrained, and the Keplerian elements are probably more usable. That said, I can arrange for the points to be the end of the major and minor axes if that trivializes the problem." CreationDate="2015-12-01T16:43:47.290" UserId="1634" ContentLicense="CC BY-SA 3.0" />
  <row Id="1522" PostId="1767" Score="3" Text="If you have the center and major/minor axis vectors, then the corners of the quad will just be center ± major ± minor." CreationDate="2015-12-02T01:10:19.490" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1523" PostId="1766" Score="0" Text="[Please explain why you downvote](http://meta.stackexchange.com/questions/135/encouraging-people-to-explain-downvotes) when you do." CreationDate="2015-12-02T09:58:10.617" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1524" PostId="1770" Score="0" Text="I'm not, I'm referring to virtual texturing, most well known as idTech 5's [MegaTexture](https://en.wikipedia.org/wiki/MegaTexture) technology. Also see [this](http://holger.dammertz.org/stuff/notes_VirtualTexturing.html) and [this](http://silverspaceship.com/src/svt/). I've seen it mentioned in overview of many modern engines' rendering pipelines, and in a few papers that use a similar approach for shadowmaps. It does have a lot in common with texture atlases, yes, it uses them, in a way, but I'm not confusing it with texture atlases." CreationDate="2015-12-02T18:18:08.880" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1525" PostId="1770" Score="0" Text="Ahh. Thanks for the links. Can you add them to the question. I will update my answer accordingly" CreationDate="2015-12-02T18:49:41.500" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1526" PostId="1770" Score="3" Text="IMO, the main drawback of simple texture atlases (not virtual textures) is you lose wrap modes like repeat and clamp, and bleeding occurs due to filtering/mipmapping - not floating-point precision. I'd be surprised to see float precision becoming a problem for ordinary (non-virtual) textures; even a 16K texture (the max allowed by current APIs) isn't big enough to really strain float precision." CreationDate="2015-12-02T20:58:17.627" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1527" PostId="1765" Score="1" Text="&quot;What is the best solution&quot; is mostly a matter of opinion. Does your current solution work well enough, or are there problems with it that you're looking to solve? If so, what are those problems specifically?" CreationDate="2015-12-02T21:50:53.767" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1528" PostId="1765" Score="0" Text="I wanted to make sure my method is fine. But I have a problem to project my 3D direction vector on the screen, so I asked [this question](http://computergraphics.stackexchange.com/questions/1766/how-to-use-getviewprojmatrix-transformvectorlinedirection-in-ue4)." CreationDate="2015-12-02T22:28:05.527" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1529" PostId="1766" Score="0" Text="I dont know why it was downvoted. but this seems like a close dupllicate to your other question." CreationDate="2015-12-02T23:48:17.757" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1530" PostId="5" Score="0" Text="Hey OP, if possible, could you post stats of the results you got out of the accepted answer, if you did?" CreationDate="2015-12-03T09:00:19.020" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1531" PostId="5" Score="0" Text="@Llamageddon To be honest, I had this actual problem maybe two years ago or so and just dug it up from a past CG projects to come up with decent questions during the private beta. I haven't yet revisited said project, so I didn't get around to trying out the answer." CreationDate="2015-12-03T09:02:28.797" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1532" PostId="1770" Score="0" Text="@RichieSams Btw, I think your answer is a good one, even if to a different question. You should make a Q&amp;A post." CreationDate="2015-12-03T09:29:11.247" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1533" PostId="1773" Score="0" Text="Hey, thank you for the excellent answer. I know this is typically frowned upon, but I have various issues, so I mostly just skim through things - to get an intuitive overview of topics for the future(I'm afraid properly learning and implementing things is out of my reach for the moment) - anyway, if possible, could you post a pseudocode example outlining the process itself, ideally, but not necessarily, illustrated?" CreationDate="2015-12-03T09:33:14.113" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1534" PostId="1774" Score="1" Text="about saving processing power: http://computergraphics.stackexchange.com/q/259/137" CreationDate="2015-12-03T10:22:03.977" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1535" PostId="1772" Score="0" Text="Ok, thanks for this answer! I used `TransformVector` after reading the first paragraph of [this page about homogenous coordinates](http://www.opengl-tutorial.org/beginners-tutorials/tutorial-3-matrices/). I understood I should not worry about W when projecting a direction, but maybe I'm wrong." CreationDate="2015-12-03T11:30:33.810" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1536" PostId="1772" Score="0" Text="I thought about projecting both points on my screen, but I changed my mind because I thought it was not necessary." CreationDate="2015-12-03T11:34:06.963" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1537" PostId="1774" Score="0" Text="@ratchetfreak Interesting post. Stenciling isn't conditionals, though." CreationDate="2015-12-03T11:57:17.630" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1538" PostId="1774" Score="1" Text="But the GPU still groups (at least) 2x2 pixels together even if 3 out of 4 will fail the stencil test." CreationDate="2015-12-03T12:02:05.197" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1539" PostId="1774" Score="0" Text="Are you certain about that? I'd imagine that that's what happens for conditionals, while with stenciles, the GPU is a bit smarter about it." CreationDate="2015-12-03T12:36:46.807" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1540" PostId="1774" Score="0" Text="Given that it will calculate pixels just off the triangle I doubt that it will cull the singular stencil-failing pixels." CreationDate="2015-12-03T14:40:19.997" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1541" PostId="1774" Score="2" Text="@Llamageddon Pixel shaders are always packed together in 2x2 quads, because a pixel shader that doesn't sample at least one texture is a rare thing indeed. But if you are doing depth-only rendering, that matters less. Also, for what it's worth, shadow rendering is traditionally bound by vertices, so in traditional high-vertex-count situations this probably won't help. Also don't forget that you'd be writing to every pixel's stencil just to skip writing to that same pixel's depth (unless your pixel shader is complex)." CreationDate="2015-12-03T15:37:16.540" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="1542" PostId="1774" Score="0" Text="@JohnCalsbeek I see. What about other hypothetical scenarios, such as ones involving more complex shader code, or a more regular stencil, that discards, for example, the left half of the screen? What situations can stencils be used in to accelerate things?" CreationDate="2015-12-03T18:11:52.060" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1543" PostId="1772" Score="0" Text="@arthur.sw That article is OK as far as affine transforms go, but when projections get involved, things are more complicated." CreationDate="2015-12-03T18:40:23.763" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1544" PostId="1773" Score="1" Text="@Llamageddon, it just so happens that I still had a diagram at hand ;) I'm afraid pseudo-code is going to bit a bit hard to provide, since there's quite a bit of real code to it. But I hope the expanded answer helps giving a general idea of the technique." CreationDate="2015-12-03T18:47:15.340" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1545" PostId="1773" Score="0" Text="Amazing answer, though I still find some details unclear: If the pre-pass is low-res, isn't it possible to miss some textures altogether? What happens then? How do the shaders for the pre-pass and final render look? Is the pre-pass used only for fetching the textures, or...?" CreationDate="2015-12-03T22:03:09.053" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1546" PostId="1773" Score="3" Text="It's worth noting that most modern hardware now exposes programmable page tables, eliminating the need for a redirection texture.  This is exposed through e.g. [tag:directx12] [reserved resources](https://msdn.microsoft.com/en-us/library/windows/desktop/dn899181(v=vs.85).aspx), which builds on [tag:directx11] [tiled resources](https://msdn.microsoft.com/en-us/library/windows/desktop/dn786477(v=vs.85).aspx), or [tag:opengl] [sparse textures](https://www.opengl.org/registry/specs/ARB/sparse_texture.txt)." CreationDate="2015-12-04T02:32:18.077" UserId="1992" ContentLicense="CC BY-SA 3.0" />
  <row Id="1547" PostId="1773" Score="1" Text="@Llamageddon, the feedback pre-pass can be done at a lower res to save as much computing and memory as possible, since pixels for a page will generally repeat (you can notice the big colored squares in my demo). You're correct that it might eventually miss a visible page like that, but that's not usually going to have a big visual impact because the system should always keep at least the lowest mipmap of the whole VT available in cache. That second paper I linked has all the shader examples in the appendix, you can also refer to the repo for my own project, they are similar." CreationDate="2015-12-04T03:15:23.287" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1548" PostId="1773" Score="0" Text="The feedback pre-pass is only useful for determining the visible set of pages needed for a view/frame, but you could also combine something like depth pre-pass to it." CreationDate="2015-12-04T03:16:55.707" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1549" PostId="1770" Score="0" Text="Hmm, this explains it quite well, though I don't really understand how it works with mip levels. I wish I could write down my specific problem with understanding it down, but it kinda eludes me..." CreationDate="2015-12-04T10:29:13.167" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1550" PostId="1775" Score="2" Text="cos and sin in the std lib take the angle in radians" CreationDate="2015-12-04T12:40:35.183" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1551" PostId="1774" Score="0" Text="@Llamageddon The canonical example for stencil these days is discarding all visible sky when doing a lighting full-screen pass. Lighting is expensive, and the sky often large." CreationDate="2015-12-04T15:48:26.510" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="1552" PostId="1773" Score="0" Text="BTW, why do you say that &quot;Virtual Texturing also doesn't handle transparency in an easy way&quot;? Transparency has nothing to do with how textures are stored in memory..." CreationDate="2015-12-04T19:52:20.630" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1553" PostId="1773" Score="0" Text="@NathanReed, it's because of the page id pre-pass. It's impossible to handle transparency there, because blending 2 or more colors would produce a different page number. What I mean is, suppose object A gets assigned page X which when encoded into a color makes a red tone, now object B gets assigned page Y, which results on green when encoded in the feedback pass. If A and B were to get blended, the color output in the pre-pass for those pixels would be a shade of yellow, translating to the wrong page id/number." CreationDate="2015-12-05T00:00:57.850" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1554" PostId="1773" Score="1" Text="@glampert Ahh, I see; that makes sense. Still, I think there are lots of options for handling transparencies; in the page ID pass, you could dither (so histogramming would see all the pages, unless there were a huge number of transparent layers), or use a [k-buffer approach](http://www.sci.utah.edu/~csilva/papers/i3d2007.pdf), or even just base transparent texture residency on which objects are near the camera (as opposed to rendering them in a feedback pass)." CreationDate="2015-12-05T02:12:20.280" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1555" PostId="1772" Score="0" Text="Yes, I would like to have more details about this, maybe I should ask this question on [math.stackexchange](http://math.stackexchange.com/)?" CreationDate="2015-12-05T15:34:24.700" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1557" PostId="1772" Score="0" Text="@arthur.sw What details are you interested in? I could add a quick example showing how the divide-by-W affects vectors, if that would be helpful." CreationDate="2015-12-05T21:12:09.347" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1558" PostId="1772" Score="0" Text="Well I just wanted to make sure that projecting two points is the only or best way to do it." CreationDate="2015-12-06T19:26:40.830" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1563" PostId="1786" Score="4" Text="This sounds more like a forum discussion than a specific question. This site works best with questions that can be answered with a single, factual answer. The &quot;what is your wishlist&quot; part is definitely not suitable for an SE site." CreationDate="2015-12-08T15:09:24.190" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1565" PostId="1786" Score="1" Text="You can come to [the cornell box](http://chat.stackexchange.com/rooms/26589/the-cornell-box) our chatroom and fire up a discussion" CreationDate="2015-12-08T18:45:12.097" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1567" PostId="1785" Score="0" Text="Scrolling through your output quickly, it looks reasonable. Although it's hard to tell since it's not plotted as an image. Can you clarify what the problem is?" CreationDate="2015-12-08T19:02:08.877" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1568" PostId="1789" Score="0" Text="Ok this does answer the question. Hovewer, the explanation probably reqjires one to knpw the answer before understanding. Yould you drop down the abstraction a bit." CreationDate="2015-12-09T05:59:02.037" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1569" PostId="1789" Score="0" Text="The theory is clear, but how would one go about implementing it? The 2 rotations I used in my example could be done without releasing the mouse inbetween, so there isn't really a clear cut line between them. The user may even go crazy and rotate up left down left which could yield something like cameraRotation.X = 0 cameraRotation.Y = 200, which says nothing about the actual ordner in which the transformations happened." CreationDate="2015-12-09T07:00:28.523" UserId="2219" ContentLicense="CC BY-SA 3.0" />
  <row Id="1570" PostId="1791" Score="0" Text="Nice ! Your explanation gives me the intuition about it. Do you happen to have some sources where I can read a bit more about rasterization? I suppose this n-separating stuff comes from 2D where is easier to understand and then I can do more thinking to grasp the 6- and 26-separating in 3D." CreationDate="2015-12-09T14:47:13.873" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="1571" PostId="1785" Score="0" Text="Hope the addition make it more clear.." CreationDate="2015-12-09T16:44:38.587" UserId="2226" ContentLicense="CC BY-SA 3.0" />
  <row Id="1572" PostId="1785" Score="0" Text="Hmmm...it's still not very clear to me. Is the problem that e.g. at a 45 degree angle, the line only goes out to (7, 7) and stops, instead of going all the way out to the corner at e.g. (10, 10)?" CreationDate="2015-12-09T18:07:36.510" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1573" PostId="1785" Score="0" Text="yes exactly..   but also that the method only works for slopes within 0-1" CreationDate="2015-12-09T18:11:14.893" UserId="2226" ContentLicense="CC BY-SA 3.0" />
  <row Id="1574" PostId="1791" Score="1" Text="@BRabbit27 I don't think the &quot;n-separating&quot; terminology is used much in 2D rasterization; I've only seen it when discussing voxelization. It just refers to the number of neighbors. I'll add a bit to the answer about that." CreationDate="2015-12-09T18:11:56.777" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1575" PostId="1795" Score="0" Text="Nice..  it works.. :)" CreationDate="2015-12-09T19:37:20.910" UserId="2226" ContentLicense="CC BY-SA 3.0" />
  <row Id="1576" PostId="1789" Score="0" Text="@Patrick Moving the mouse back without releasing the mouse button while rotating an object usually leads to the same result as if the mouse had not been moved e.g. up and back down at all. I added a few words about this (pretend the mouse moved instantly to the current position)." CreationDate="2015-12-09T21:50:46.807" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1577" PostId="1795" Score="0" Text="well.. partly seem to have an issue with negative values.. Some end position contains negative values..&#xA;&#xA;the center is here 10,10.. thereby making the matrix sized 20X20.." CreationDate="2015-12-09T22:12:46.037" UserId="2226" ContentLicense="CC BY-SA 3.0" />
  <row Id="1578" PostId="1795" Score="0" Text="endPos: (15,-2) Angle: 293" CreationDate="2015-12-09T22:17:29.263" UserId="2226" ContentLicense="CC BY-SA 3.0" />
  <row Id="1579" PostId="1798" Score="0" Text="Thanks @NathanReed. 1 more question. Say I want to traverse the tree breadth first, I am at level 0 (i.e at the root of the tree, I haven't divided the tree yet, I have a bounding box including all the scene primitives). I divide the bounding box along an axis such that the left child node is the one closer to the camera. Then I find that a certain ray R intersects the left node. R should be early terminated with regard to the right child node and to all of its future children nodes. But I still have to test the intersection of R with the future children nodes of this left node, am I?" CreationDate="2015-12-10T07:29:19.003" UserId="2233" ContentLicense="CC BY-SA 3.0" />
  <row Id="1580" PostId="1793" Score="2" Text="I don't think the question's necessarily too open-ended, but any numeric answer is going to be wrong within 12 months." CreationDate="2015-12-10T09:51:04.117" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1581" PostId="1799" Score="0" Text="I already asked this question here http://gamedev.stackexchange.com/questions/112165/brdf-and-spherical-coordinate-in-ray-tracing. Nobody seems to have an answer. I think that the arguments in my &quot;UPDATE 2&quot; could be the way to follow (because it seems that the project of vector in this way described could be the correct way to calculate the azimuth angle). Anyone could help me with an answer and maybe a canonical reference to be used as study reference?" CreationDate="2015-12-10T13:10:30.087" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="1582" PostId="1796" Score="1" Text="Could you provide more detail on what the exact problem is? The only problem you describe is about negative coordinates. A circle of radius 10 around $(10, 10)$ barely reaches zero. Even if there are numerical errors leading to values slightly below zero, after rounding these should be gone." CreationDate="2015-12-10T16:24:12.387" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1583" PostId="1793" Score="0" Text="@DanHulme Yeah, but the approaches used to reach that kind of efficiency stay the same. And when not, I've seen questions that require updating answers periodically on other stackexchange sites, so I think that's fine." CreationDate="2015-12-10T18:47:56.763" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1584" PostId="1798" Score="1" Text="@user2651062 Normally you would have built the whole tree before you start traversing it. Your comment makes it sound like you're trying to build and traverse at the same time? Or else what do you mean by &quot;future child nodes&quot;? In any case, you have to traverse all child nodes that intersect the ray or segment. If it intersects the left child, you descend into the left child and repeat the process for its children. You might need to descend into both children if they both intersect the ray/segment." CreationDate="2015-12-10T20:01:27.557" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1585" PostId="1796" Score="0" Text="It would also be useful to see the output for a variety of different input values." CreationDate="2015-12-11T00:22:00.550" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1586" PostId="1775" Score="0" Text="This appears to be the same user and question as [Draw angles lines in raster graphics using bresenham line algorithm](http://computergraphics.stackexchange.com/questions/1785/draw-angles-lines-in-raster-graphics-using-bresenham-line-algorithm)" CreationDate="2015-12-11T06:29:34.403" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1588" PostId="1793" Score="9" Text="This is really impossible to answer. First of all, what is &quot;realtime&quot;—60fps? 30? Less? Second, the answer will vary hugely based on what GPU you have and what resolution you're rendering at. Third, the answer will vary hugely depending on the details of how the rendering works. Limits on scene complexity are more complicated than just the number of polygons per se, but involve such things as the number of draw calls, state changes, render passes and so on—which are affected by how the engine works, how the artists constructed the scene, and so on..." CreationDate="2015-12-11T06:39:54.973" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1589" PostId="1800" Score="0" Text="Thank you so much for your help @NathanReed. One last question: do you have any good reference material on how to calculate the tangent space and convert my vectors wi and wo to the coordinate space for ray tracing/BRDF? At the moment I didn't find any useful one. In this way I would be able to do some comparison between the way with tangent space and the way using vectors math." CreationDate="2015-12-11T12:19:49.197" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="1590" PostId="1793" Score="0" Text="@NathanReed And those are precisely what I'm asking about, the number itself isn't important, and likewise, realtime is loosely defined here as well, although personally I'd go with being able to maintain a framerate above 30. The question itself is about techniques and approaches used to minimize costs per frame, I just couldn't think of a better way to title and phrase it." CreationDate="2015-12-11T12:31:49.687" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1591" PostId="1800" Score="0" Text="@FabrizioDuroni I assume you're familiar with how to convert between coordinate systems in general? For tangent space you just have to set up coordinates using the surface normal plus some two vectors perpendicular to it as the axes. For normal mapping, the two vectors are often chosen to match the texture space U and V axes (as mapped to the particular surface). For isotropic BRDFs without normal mapping, it doesn't really matter." CreationDate="2015-12-11T19:11:04.707" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1593" PostId="1793" Score="1" Text="@Llamageddon Considering your comments, I'm not quite sure what you actually ask for. On one hand, your question title is quite clear (max out geometry and how to do so), but as Nathan pointed out, this is kind of impossible to answer. On the other hand, in your comments you say you want to know how to minimize cost per frame. This is an extremely broad question, because you could improve/optimize your shaders, scene graph, models, textures, API usage, simply everything that does some part of your rendering. You could probably write entire books about this (if not done by someone already)." CreationDate="2015-12-12T00:39:01.593" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1596" PostId="1800" Score="0" Text="yes I'm familiar with how to convert between coordinate system (so I know that a particular matrix that use the component of the basis vector must be used) but I'm not so familiar with tangent space and texture mapping. Searching the web it seems that the calculation vary between type of objects (sphere, triangle...). You say &quot; For isotropic BRDFs without normal mapping, it doesn't really matter.&quot;, what do you mean? So how do I choose them? Thank you again, I accepted your answer because your the only one that really give me some good hints." CreationDate="2015-12-12T17:43:10.420" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="1597" PostId="1793" Score="0" Text="@Nero As I said, it's a broad question, I'm interested in all kinds of techniques utilized to push hardware to the limits - occlusion culling, scene graph management, batching, instantiation, virtual texturing, deferred rendering, etc. Setting aside implementing more than just the baseline of geometry, shadows, basic shading, but inclusive of everything leading up to that. I'm sorry if it's a bad question." CreationDate="2015-12-12T23:32:10.617" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1598" PostId="1788" Score="0" Text="This seemed a good question - did you delete it because you found the solution? If so you could undelete it and post an answer with your solution. Answering your own question is encouraged and you gain reputation for both the question and the answer. Plus it may help someone else who has a similar problem in future..." CreationDate="2015-12-13T14:11:21.907" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1599" PostId="1793" Score="0" Text="The more you stuff every technique in one program (what usually an engine does) you are not pushing the hardware to the limits, on the contrary you give it many opportunities to breathe. The way to use the hardware at max capacity is what OCCT does. only a benchmark can do this." CreationDate="2015-12-14T02:19:57.877" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1600" PostId="1801" Score="0" Text="if c is a union of the projected pixels, when s1 or s2 completely obstructs the other sphere, it does not mean c gets empty. please clarify." CreationDate="2015-12-14T02:24:53.623" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1601" PostId="1793" Score="0" Text="@v.oddou I'd say it's still pushing it to its limits, just in a smarter way. I'm not necessarily interested in super-advanced techniques, just how to... ugh, honestly, I thought this question would be self-explanatory, I even gave examples of what I mean. I just give up, honestly. It's like you guys are literally trying to interpret the question word-by-word instead of actually considering it. Should I just write a wiki answer and let others correct me where I'm wrong or miss something? :/" CreationDate="2015-12-14T02:56:24.510" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1602" PostId="88" Score="1" Text="I'm going to give you a straightforward answer: GPU are turing complete. what do you conclude from this ? second answer: lux render http://www.luxrender.net/wiki/SLG" CreationDate="2015-12-14T05:13:53.773" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1603" PostId="1806" Score="1" Text="A million seems a bit low to me." CreationDate="2015-12-14T09:00:57.703" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1604" PostId="1806" Score="1" Text="just take how many MPoly/s the card is capable of, and that's the FPS at which it will render 1 million. I just recalled an experiment for a terrain renderer on an ATI4800HD. If you take this list https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units they don't give Vertices/s info starting from the era of unified architecture. but 10 year old hardware seems to advertise about 40 FPS for 1 million triangles. + c.f. edit in my answer" CreationDate="2015-12-14T09:30:38.697" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1606" PostId="1806" Score="1" Text="@v.oddou Yeah, but to get near that number you need to do batching of geometry, or instancing, in case of dynamic scenes, and **that** is what I'm asking about. How to not bottleneck yourself 2% of the way towards what hardware can do." CreationDate="2015-12-14T13:49:23.330" UserId="2111" ContentLicense="CC BY-SA 3.0" />
  <row Id="1607" PostId="1788" Score="1" Text="Hello @trichoplax actually I found the solution, I'll share the answer with everyone answering my own question. Honestly I deleted my question because I thought none cares about this issue." CreationDate="2015-12-14T21:56:19.720" UserId="2228" ContentLicense="CC BY-SA 3.0" />
  <row Id="1608" PostId="1788" Score="1" Text="BTW, instead of editing the question with &quot;SOLVED&quot; in the title, it's preferable to just accept your own answer. (The site might make you wait a day after posting to do that; I don't remember.)" CreationDate="2015-12-15T00:37:03.437" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1609" PostId="1788" Score="0" Text="Hey! @NathanReed  I'll change the title, thanks about that :)" CreationDate="2015-12-15T00:38:57.447" UserId="2228" ContentLicense="CC BY-SA 3.0" />
  <row Id="1610" PostId="1797" Score="0" Text="I'm curious - what languages like this are out there? I know there are engine-specific ones like Unity's or UE4's shader systems, plus some academic researchy things like [Spark](https://graphics.stanford.edu/papers/spark/), but I'm not aware of anything else in current use in this space." CreationDate="2015-12-15T01:22:18.650" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1611" PostId="1806" Score="0" Text="@Llamageddon aaah, I see, THAT is a question indeed. Let me see what I can say about it. (EDIT2)" CreationDate="2015-12-15T01:47:59.120" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="1612" PostId="1797" Score="0" Text="@NathanReed, probably Apple's [Metal](https://developer.apple.com/library/ios/documentation/Metal/Reference/MetalShadingLanguageGuide/Introduction/Introduction.html) is one of the most notable, as of now, but I haven't looked into much detail..." CreationDate="2015-12-15T02:42:46.927" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1613" PostId="1797" Score="0" Text="Oh, OK. Metal isn't on top of HLSL or GLSL though; it's a primary shading language for Apple GPUs that compiles directly to HW microcode (via a proprietary LLVM backend I believe)." CreationDate="2015-12-15T04:42:36.130" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1617" PostId="1762" Score="0" Text="The table's corners are, yes." CreationDate="2015-12-16T11:56:41.987" UserId="2162" ContentLicense="CC BY-SA 3.0" />
  <row Id="1619" PostId="1806" Score="0" Text="Great in depth answer! I've made a few minor edits, as a user rather than a moderator. Feel free to roll back any/all if they don't match your intention." CreationDate="2015-12-17T12:50:50.273" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1624" PostId="1816" Score="0" Text="You can answer [here](http://stackoverflow.com/questions/34357110/how-to-triangulate-from-a-vorono%C3%AF-diagram) as well or I will delete my other question." CreationDate="2015-12-18T20:48:24.100" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1625" PostId="1816" Score="3" Text="@arthur.sw Cross-posting is generally discouraged on SE, so I suppose deleting it there would be the better option." CreationDate="2015-12-18T20:49:20.453" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1626" PostId="1816" Score="0" Text="an interactive voronoï diagram creator: http://alexbeutel.com/webgl/voronoi.html" CreationDate="2015-12-19T00:47:21.030" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1633" PostId="1822" Score="2" Text="Sorry, but that's plain wrong. OpenGL and DirectX use approximations which are inherently faster than precise raytracing. The whole point of accelerated 3D graphics is having algorithms which balance between realism and speed, looking good enough for most practical uses: gaming, CAD, etc." CreationDate="2015-12-21T15:16:53.507" UserId="2312" ContentLicense="CC BY-SA 3.0" />
  <row Id="1634" PostId="1822" Score="2" Text="@IMil OpenGL can be used for raytracing. Its faster because it is optimized for the hardware in question. But Maya does NOT have to ray trace. Maya and Max can use openGL and directX just as much as your game. Mayas (and 3ds) viewport is opengl or directX (your choice). The fact that your processor is slower in certain parallel processing loads is another thing. So the answer stands. The standard settings of maya is no more realistic than a standard scanline." CreationDate="2015-12-21T15:36:03.813" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1635" PostId="1818" Score="1" Text="The short answer is that OpenGL takes shortcuts." CreationDate="2015-12-21T18:59:57.240" UserId="2316" ContentLicense="CC BY-SA 3.0" />
  <row Id="1636" PostId="1821" Score="5" Text="This answer would be even better if you explained what the problem turned out to be and what you changed to fix it..." CreationDate="2015-12-21T20:25:45.710" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1637" PostId="1820" Score="0" Text="The problem seems to be that all variables are `int`. In particular, `dx` and `dy` will probably get 0." CreationDate="2015-12-22T01:50:08.923" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="1645" PostId="1824" Score="4" Text="Others common renderers have this conflation problem too. See http://w3.impa.br/~diego/projects/GanEtAl14/sample.html?contour." CreationDate="2015-12-22T10:15:25.367" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="1650" PostId="1821" Score="1" Text="Ill update the question and answer now with more info." CreationDate="2015-12-22T21:22:27.333" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1651" PostId="1827" Score="0" Text="Its also a question of time. Even if you could render at say 60 fps and get acceptable results it rarely pans out to optimize for it. Say it takes 3 minutes per frame and you have 200 frames to render. You might be able to get the 60 fps by hiring a shader writer and by optimizing but then that takes atleast a day or two of your time. But  200 frames at 3 mins only takes 10 hours so you save that cost. In practice its cheaper to buy more hardware and not worry too much about it. Games simply can not take this approach." CreationDate="2015-12-23T05:02:36.703" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1652" PostId="1827" Score="0" Text="@joojaa It's also a little bit more complex though. Just doing really good real-time shaders for Maya might take a year or so at the very, very least, even from an experienced shader developer (with lesser gains), because the flexibility of the nodal system there is targeted towards production rendering. It would take a reverse engineering mindset and kind of new kind of GLSL/HLSL code generation technique (like a meta programming system) to translate these general-purpose shader nodes into a real-time shading system that captures the range of effects of UE 4, e.g." CreationDate="2015-12-23T05:06:44.513" UserId="2247" ContentLicense="CC BY-SA 3.0" />
  <row Id="1653" PostId="1827" Score="0" Text="@joojaa UE 4's shader engine is directly targeted towards an heavily-approximated PBR mindset (a very small subset of Disney's PBR shader). They designed even their material system for a fast, real-time purpose, instead of starting with something like Maya's material system which isn't at all (designed for raytracing). Even if the brightest of the UE 4 worked on VP 2.0, they'd have to work night and day for possibly years to achieve the same results against a design not intended to do this sort of stuff." CreationDate="2015-12-23T05:08:30.277" UserId="2247" ContentLicense="CC BY-SA 3.0" />
  <row Id="1654" PostId="1827" Score="0" Text="but thats a onetime cost even if youd have that pipeline in a VFX app each scene might need that extra optimization. Theres no reason why a maya user could't render in UDK for example for same shader dev platform." CreationDate="2015-12-23T05:15:20.407" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1655" PostId="1827" Score="0" Text="They dont have to allways be top notch just good enough for the job at hand. But yes i agree that my time factor is off by a magnitude. But even if it would take days its still not worth it. Games do not have the option." CreationDate="2015-12-23T05:17:28.650" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1656" PostId="1827" Score="0" Text="@joojaa Yeah, tricky part is that we're often talking about production renderers that are 10+ years old, Maya which is 17+ years old -- huge legacy attached ranging from content to studio pipelines to plugins and so on. It's hard to modernize it -- game engines are always cutting-edge, always purging their previous generation engines and kind of starting a new canvas with each engine generation (though they might reuse or continue a lot of previous work). VFX companies just keep poking at their old codebase." CreationDate="2015-12-23T05:19:29.040" UserId="2247" ContentLicense="CC BY-SA 3.0" />
  <row Id="1657" PostId="1827" Score="1" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/33352/discussion-between-joojaa-and-ike)." CreationDate="2015-12-23T05:20:41.790" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1659" PostId="1828" Score="0" Text="Does this do the same thing except with an increase on the Z height on each iteration of the for loop?" CreationDate="2015-12-24T09:37:43.297" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1660" PostId="1828" Score="0" Text="Pretty much, except its always symmetrical (istead of the original a and b, I used a uniform `size` param)." CreationDate="2015-12-24T09:54:29.600" UserId="2317" ContentLicense="CC BY-SA 3.0" />
  <row Id="1661" PostId="1830" Score="0" Text="Are you aiming for a solution with a specific programming language / library?" CreationDate="2015-12-24T13:13:16.813" UserId="2317" ContentLicense="CC BY-SA 3.0" />
  <row Id="1662" PostId="1830" Score="0" Text="solution  can be in any language .... what is more important is algorithm" CreationDate="2015-12-24T13:23:49.630" UserId="2335" ContentLicense="CC BY-SA 3.0" />
  <row Id="1664" PostId="1831" Score="2" Text="Something looks wrong with your first image pair...the result is mostly complete black, which doesn't seem correct. Also, it would be good to handle divide-by-zero so rows that are complete black in the input don't flip out. Maybe just replace the row by all 128s in that case?" CreationDate="2015-12-24T21:30:50.837" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1665" PostId="1825" Score="1" Text="BTW, for a more detailed look at why alpha blending doesn't &quot;do the right thing&quot; in cases like this, check out this paper: [Interpreting Alpha](http://jcgt.org/published/0004/02/03/) by Andrew Glassner." CreationDate="2015-12-24T21:34:26.007" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1666" PostId="1825" Score="0" Text="@NathanReed Will look thnx, but here it is simply that even if alpha worked right coverage wont know which parts cover the pixels and which not. So  Two layers with 50% alpha could mean fully opaque or only one layer visible because objects fill identical region we just dont know." CreationDate="2015-12-24T22:06:05.467" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1667" PostId="1831" Score="0" Text="Same algorithm for 1st and 2nd img. Will check though, makes sense about the flipping, thanks." CreationDate="2015-12-24T22:20:57.027" UserId="2317" ContentLicense="CC BY-SA 3.0" />
  <row Id="1668" PostId="1825" Score="2" Text="@joojaa Yes that's basically the point the paper is making: alpha can represent either opacity, coverage, or a combination of both. :)" CreationDate="2015-12-25T00:15:09.077" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1669" PostId="1833" Score="3" Text="Very easy way to achieve a similar look is to just add a small value to the color or each pixel, then clamp to max (255 or 1). It will make the image look more &quot;washed out&quot;." CreationDate="2015-12-25T22:39:48.917" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1672" PostId="1834" Score="0" Text="I edited my question so that it is more clear. I am not working in 3d btw." CreationDate="2015-12-26T18:54:49.437" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="1674" PostId="1840" Score="0" Text="I tried `float dirX = -(a + b * angle) * sin(angle) + b * cos(angle);` and `float dirZ = (a + b * angle) * cos(angle) + b * sin(angle);&#xA;` then `float newAngle = atan2(dirZ, dirX);` but this does not set the angle correctly." CreationDate="2015-12-27T14:38:35.250" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1676" PostId="1840" Score="0" Text="I'm multiplying the result by 180 * Pi to convert to radians" CreationDate="2015-12-27T14:52:42.093" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1678" PostId="1840" Score="0" Text="Programming language is c++" CreationDate="2015-12-27T14:57:16.397" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1679" PostId="1840" Score="0" Text="Atan2 c++ http://www.cplusplus.com/reference/cmath/atan2/" CreationDate="2015-12-27T15:02:54.920" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1681" PostId="1840" Score="0" Text="ill update the question with what I have so far can you have a look?" CreationDate="2015-12-27T15:13:48.623" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1682" PostId="1839" Score="0" Text="It would be nice if you could include in your post what libraries you use. Your using Unity right?" CreationDate="2015-12-27T15:35:39.203" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1683" PostId="1839" Score="0" Text="Using Bullet physics and freeglut" CreationDate="2015-12-27T15:39:25.653" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1684" PostId="1840" Score="0" Text="Ive just noticed that your circle is spiraling in the opposite direction to mine" CreationDate="2015-12-27T16:33:02.170" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1685" PostId="1840" Score="0" Text="@damorton It depends on how your csys is defined and from which side of the spiral you look from. These are arbitrary definitions. I just have a right handed coordinate system in a 2d space." CreationDate="2015-12-27T17:51:48.493" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1686" PostId="1839" Score="0" Text="Not Unity https://github.com/damorton/bullet-dominos" CreationDate="2015-12-27T22:06:55.920" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1688" PostId="1843" Score="1" Text="Kinda looks like you changed it to `3.2`." CreationDate="2015-12-28T07:35:46.130" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1689" PostId="1845" Score="1" Text="'Processes' is very vague. Is that vertex shader ops? Raterizer? Shading? All of the above? None of these are meaningful, because they have a massive scene dependence. FLOPS is kind of better, but still not great because it doesnt take into account register pressure, memory latency, etc." CreationDate="2015-12-28T16:52:43.583" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1690" PostId="1846" Score="1" Text="Hello and welcome. Are you trying to circumvent the [future MIT license](http://meta.stackexchange.com/questions/271080/the-mit-license-clarity-on-using-code-on-stack-overflow-and-stack-exchange?cb=1) by posting a image or what up with that?" CreationDate="2015-12-28T17:10:03.210" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1691" PostId="1840" Score="0" Text="Turned out that the above was perfect, the problem was with the dimensions of each domino. Swapping the width and height values fix it :/ thanks @joojaa" CreationDate="2015-12-28T17:52:47.107" UserId="2248" ContentLicense="CC BY-SA 3.0" />
  <row Id="1693" PostId="1843" Score="0" Text="Well, in my code its 4.1 haha" CreationDate="2015-12-28T18:32:20.553" UserId="2355" ContentLicense="CC BY-SA 3.0" />
  <row Id="1694" PostId="1849" Score="0" Text="That's interesting, though I see from the photon-mapping link that what I'm asking about is how to approach Spectral Rendering.  Do you think it's reasonable to change the question so far as to update it with this term?  I still think I'm asking the same thing." CreationDate="2015-12-28T23:14:10.250" UserId="2360" ContentLicense="CC BY-SA 3.0" />
  <row Id="1695" PostId="1849" Score="0" Text="@NewAlexandria Hmm...I interpreted it as being primarily about caustics. If you're really asking about spectral rendering, that's a different question than the one I answered. :) I think it wouldn't be a bad idea to post a new question, if you want to. Maybe edit this one to be specifically about caustics?" CreationDate="2015-12-29T04:13:59.447" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1704" PostId="1845" Score="0" Text="I understand that there are all of these factors. Nonetheless, I'd be interested to know about how many triangles per second can be drawn assuming modest/reasonable/typical choices for the various factors (simple/default vertex and pixel shaders, simple lighting, big model being textured by some reasonable texture sheets)." CreationDate="2015-12-29T18:32:58.440" UserId="2358" ContentLicense="CC BY-SA 3.0" />
  <row Id="1706" PostId="1849" Score="0" Text="Thanks, [I did, here](http://computergraphics.stackexchange.com/questions/1854/how-is-spectral-rendering-handled)" CreationDate="2015-12-29T20:25:53.253" UserId="2360" ContentLicense="CC BY-SA 3.0" />
  <row Id="1708" PostId="1852" Score="0" Text="Thanyou so much! Now I know the how alpha blending works and fixed the particle stuff!  The solution I found worked best was making sure the function got called 60 times a second regardless of the framerate.  Thankyou much!" CreationDate="2015-12-29T23:51:03.110" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="1714" PostId="1842" Score="0" Text="you can use glGetString(GL_SHADING_LANGUAGE_VERSION) to see which glsl versions are available to you. printf(&quot;Supported GLSL version is %s.\n&quot;, (char *)glGetString(GL_SHADING_LANGUAGE_VERSION));" CreationDate="2015-12-29T15:10:54.010" UserId="2366" ContentLicense="CC BY-SA 3.0" />
  <row Id="1722" PostId="1831" Score="0" Text="The above solution seems correct at first glance but what if user enter the output image as input then it shows distorted output. The solution should be adaptive and learn when not to perform calculation as the output of first input is the solution so when output is again input no changes should be seen." CreationDate="2016-01-01T07:45:53.923" UserId="2383" ContentLicense="CC BY-SA 3.0" />
  <row Id="1723" PostId="1831" Score="0" Text="@SandeepNeupane that makes sense, although that's not what the author of the question asked. I'm sure this answer can serve as the basis for a more adaptive, complete solution." CreationDate="2016-01-01T18:16:25.310" UserId="2317" ContentLicense="CC BY-SA 3.0" />
  <row Id="1724" PostId="67" Score="1" Text="[Related question on SuperUser.](http://superuser.com/q/1019825/215723)" CreationDate="2016-01-01T19:27:16.227" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1725" PostId="1857" Score="0" Text="Welcome to Computer Graphics Stack Exchange. You could improve your question by adding what information about the polygon and the red box you have. Do you e.g. know the precise coordinates of the vertices, or do you only have the image? Also, please describe what you already tried to do. These information will help us to write good answers and especially also one that helps you best. Some more tips can be found on our [ask] page in our Help Center." CreationDate="2016-01-01T22:09:06.727" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1726" PostId="1859" Score="0" Text="Note that the question also requires the specified edge to be at the bottom." CreationDate="2016-01-02T23:40:30.977" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1727" PostId="1859" Score="0" Text="@trichoplax, right. Added that bit of code." CreationDate="2016-01-03T05:34:53.337" UserId="2317" ContentLicense="CC BY-SA 3.0" />
  <row Id="1728" PostId="1862" Score="5" Text="There are scaling algorithms like bicubic scaling which use splines to approximate the color of pixels when scaled to any size." CreationDate="2016-01-03T13:09:54.743" UserId="1683" ContentLicense="CC BY-SA 3.0" />
  <row Id="1729" PostId="1861" Score="3" Text="These &quot;textures&quot; are called light cookies, which essentially is a texture which shows the strength of light on the projected area. This is actually really simple to implement. When the light attenuation is calculated in the shader, it looks up the cookie texture and returns the strength (which can be stored in the R, G, B, or A channels)." CreationDate="2016-01-03T13:13:00.867" UserId="1683" ContentLicense="CC BY-SA 3.0" />
  <row Id="1730" PostId="1859" Score="0" Text="From the example in the question, I believe &quot;at the bottom&quot; means that the horizontal edge becomes the base of the shape, rather than moving it to the bottom of the screen." CreationDate="2016-01-03T14:14:13.190" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1731" PostId="1859" Score="0" Text="For instance, your side AD is at the top of the shape, so that even when it is moved to the bottom of the screen, the rest of the shape is still below it, off screen." CreationDate="2016-01-03T14:15:43.820" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1732" PostId="1835" Score="1" Text="You may have to use culling and only draw visible cubes if you can't do a million cubes.  Another option is to merge cubes into larger rectangular shapes." CreationDate="2016-01-05T01:16:56.063" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1733" PostId="1865" Score="0" Text="Interesting. Could you point us to any sources, examples or results of this approach?" CreationDate="2016-01-05T09:11:27.457" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1734" PostId="1865" Score="0" Text="I'm on my phone so can't take a screenshot, but this shadertoy uses the method and looks pretty decent: https://www.shadertoy.com/view/ltfXDM" CreationDate="2016-01-05T12:30:40.763" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1735" PostId="1865" Score="2" Text="POV-Ray is an open-source ray-tracer that uses a similar method to simulate dispersion. It's not a ray per channel: you can configure how many rays are used, spread equally across the spectrum." CreationDate="2016-01-05T15:19:28.693" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1736" PostId="1854" Score="0" Text="I feel like this question is way too broad as it stands. Whole books have been written on the subject. Perhaps you could narrow it down to a specific question that's not covered by existing resources?" CreationDate="2016-01-05T15:20:30.733" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1737" PostId="1854" Score="0" Text="I can see this being answered along the lines of &quot;There are hundreds of ways, each of which falls into one of the following N broad categories. If you want to know specific detail about one of these categories you can ask a new question.&quot;" CreationDate="2016-01-05T16:17:38.973" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1738" PostId="1862" Score="0" Text="@EvilTak, can you expand your comment into an small answer?" CreationDate="2016-01-05T16:37:23.030" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1739" PostId="1866" Score="0" Text="There is a affine transform that will map each corner to its texture coordinate, you can use that to map P to its uv." CreationDate="2016-01-05T16:59:28.540" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1740" PostId="1866" Score="0" Text="@ratchetfreak could you provide me a link plz ?" CreationDate="2016-01-05T17:06:38.397" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="1745" PostId="1866" Score="0" Text="There is a good write up on how to do the intersection point calculation as well as barycentric cord calculation in one go [in this paper](http://www.cs.virginia.edu/~gfx/courses/2003/ImageSynthesis/papers/Acceleration/Fast%20MinimumStorage%20RayTriangle%20Intersection.pdf). This essentially amounts to transforming the triangle." CreationDate="2016-01-06T07:26:36.763" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1746" PostId="1867" Score="0" Text="i sthere a error in $Bary_B$? Should the first term be $(A_y-C_y)$ or am i wrong?" CreationDate="2016-01-06T07:35:25.327" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1747" PostId="1867" Score="0" Text="@joojaa I don't think so. It's the same in the Wikipedia article, and it seems correct from a test calculation I did." CreationDate="2016-01-06T07:52:02.270" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1748" PostId="1862" Score="0" Text="@glampert did that. Do you want me to remove my comment?" CreationDate="2016-01-06T08:00:26.423" UserId="1683" ContentLicense="CC BY-SA 3.0" />
  <row Id="1749" PostId="1867" Score="0" Text="ah, so it is $-(A_y-C_y)$, might be good to point out as you would pre calculate $AC_y = (A_y-C_y)$." CreationDate="2016-01-06T08:11:20.477" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1750" PostId="1867" Score="1" Text="@joojaa The entire denominator and some of the terms in the nominator can be precalculated for each triangle, only few of the terms depend on $P$. I've added a link to a question dealing with methods of calculation. In this formula I thought it would be better to keep the notation simple and uniform rather than efficient." CreationDate="2016-01-06T08:15:03.043" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1753" PostId="1861" Score="0" Text="I'm not really sure what part of this you don't understand. It's a relatively simple lighting environment and doesn't present any problems for a real-time renderer. Is it the way lights overlap that seems hard, or the shapes of the lights, or something else completely?" CreationDate="2016-01-06T10:13:16.560" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1754" PostId="1862" Score="0" Text="@EvilTak, I think you can leave it. Nice answer btw, thanks!" CreationDate="2016-01-06T14:27:11.127" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="1755" PostId="1874" Score="2" Text="The question is about software (read: CPU) rasterization. Some of the information you gave are about rasterization in general, some techniques - in my book - have nothing to do with rasterization at all. Could you please clarify in your answer how the techniques use or benefit from software rasterization?" CreationDate="2016-01-07T12:42:47.580" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1762" PostId="1878" Score="0" Text="show us what your output is and describe what you expect it to be" CreationDate="2016-01-08T14:33:11.280" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1763" PostId="1878" Score="0" Text="It just wont compile. It is a version of a basic pixel ilumination shader. The previous version just supports 1 light. I am trying to adapt it so that it can process 8 lights." CreationDate="2016-01-08T14:44:49.503" UserId="2425" ContentLicense="CC BY-SA 3.0" />
  <row Id="1764" PostId="1878" Score="0" Text="we'll need to see the info log from the failed compilation/linking. It's a good practice to always log it while developing." CreationDate="2016-01-08T14:55:00.770" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1765" PostId="1878" Score="0" Text="I am pretty much just starting with shaders...  I do get :&#xA;&#xA;&#xA;	&#xA;Cannot compile frgament shader: 0(24) : error C1101: ambiguous overloaded function reference &quot;mul(mat4, vec3&quot;) (0): mat3x4 mul(mat3x1, mat1x4) (0): mat3 mul(mat3x1, mat1x3) (0): mat3x2 mul(mat3x1, mat1x2) ..." CreationDate="2016-01-08T15:01:46.820" UserId="2425" ContentLicense="CC BY-SA 3.0" />
  <row Id="1766" PostId="1879" Score="0" Text="I do not get matrix errors anymore but It just wont compile. I have posted the original working code for a single light. How should I approach it?" CreationDate="2016-01-08T15:36:28.617" UserId="2425" ContentLicense="CC BY-SA 3.0" />
  <row Id="1767" PostId="1880" Score="1" Text="This question is about image / signal processing, which is not within the scope of this site. However, there is a SE for signal processing here: http://dsp.stackexchange.com/" CreationDate="2016-01-08T20:41:33.267" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1768" PostId="1880" Score="0" Text="increase contrast add more frames from video feed. Most of it remains guesswork though" CreationDate="2016-01-08T21:19:06.850" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1769" PostId="1880" Score="0" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/201/is-this-graphics-or-image-processing-is-there-an-overlap) on whether image processing is on topic." CreationDate="2016-01-08T23:15:37.607" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1770" PostId="1881" Score="0" Text="I haven't heard of anyone doing it yet, but I really think temporal techniques could be used to get sub pixel accuracy from video streams." CreationDate="2016-01-09T02:18:06.283" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1771" PostId="1881" Score="0" Text="@AlanWolfe Definitely. Googling &quot;video super-resolution&quot; turns up a number of papers on that idea." CreationDate="2016-01-09T04:53:26.357" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1772" PostId="1880" Score="0" Text="Is there a way to move this question to the other SE or should I just cut/paste it over there ?" CreationDate="2016-01-09T07:13:44.533" UserId="2427" ContentLicense="CC BY-SA 3.0" />
  <row Id="1774" PostId="1883" Score="0" Text="Very interesting answer! I thought about this, but did not take the time to explain it. &#xA;&#xA;In my opinion, the best solution is to combine both methods: use the first method when possible and handy (when the selected axis does not point to the camera), and your method otherwise. There should be an angle threshold (the angle between the axis and the camera forward vector) to switch between those two methods. In both cases, the GUI should give a good feedback of what's going on (direction and amount of the extrusion, maybe with graduation)." CreationDate="2016-01-09T13:11:37.553" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1775" PostId="1883" Score="0" Text="I feel like I must accept the answer which would explain both solutions, do you want to write it?" CreationDate="2016-01-09T13:12:29.870" UserId="2173" ContentLicense="CC BY-SA 3.0" />
  <row Id="1776" PostId="1883" Score="0" Text="Dont recommend dual approach. This kind of twitching between the modes makes worst UXp" CreationDate="2016-01-09T13:15:55.557" UserId="2433" ContentLicense="CC BY-SA 3.0" />
  <row Id="1777" PostId="1880" Score="0" Text="Per these comments and @trichoplax 's meta discussion I went ahead and cross-posted this in two other SE forums. Here are the links to those discussions: http://dsp.stackexchange.com/questions/28168/what-is-the-state-of-the-art-on-using-computers-to-clean-up-videos?noredirect=1#comment52603_28168 and http://video.stackexchange.com/questions/17363/what-is-the-state-of-the-art-on-using-computers-to-clean-up-videos" CreationDate="2016-01-09T15:55:40.803" UserId="2427" ContentLicense="CC BY-SA 3.0" />
  <row Id="1778" PostId="1880" Score="2" Text="Although this was with good intentions, note that [cross posting on several SE sites is not recommended](http://meta.stackexchange.com/questions/64068/is-cross-posting-a-question-on-multiple-stack-exchange-sites-permitted-if-the-qu)." CreationDate="2016-01-09T16:00:30.437" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1779" PostId="1880" Score="0" Text="@O.M.Y. note that despite the comment about being off topic, the only response so far on meta is to say that this question is definitely on topic here. It will take time to see what other responses come in, but I don't see any reason to move this question at present unless you have your own reasons to." CreationDate="2016-01-09T16:02:31.760" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1780" PostId="1880" Score="1" Text="Also cross posting is considered rude." CreationDate="2016-01-09T16:16:52.227" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1781" PostId="1880" Score="0" Text="@trichoplax While it is likely that some of the people in the three SE communities will be joined in all three, there is no guarantee that experts in VIDEO will be part of DSP or CG and vice-versa. As the question seems to be on topic for all three I would like to *gather wool* from each community, give credit where credit is due to each expert, then merge the information into a really good answer for all three SE communities. I am a futurist and it seems reasonable to me that these three fields will simultaneously become both more *integrated* and more *specialized* as we progress." CreationDate="2016-01-09T16:17:25.070" UserId="2427" ContentLicense="CC BY-SA 3.0" />
  <row Id="1782" PostId="1880" Score="0" Text="@joojaa I asked about that in a meta discussion and was told to go ahead and post it in the other forum. If I was given bad information then I apologize, but others have not complained and one even helped me to add the cross post links into the comments." CreationDate="2016-01-09T16:19:46.647" UserId="2427" ContentLicense="CC BY-SA 3.0" />
  <row Id="1783" PostId="1880" Score="2" Text="You were not asked to post you were told there are other avenues by one user. Cross posting is not the stackexhange way never. If you have a question you need to make it worthy of the site your asking. So you cant ask the same question on video for example." CreationDate="2016-01-09T16:22:05.063" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1784" PostId="1880" Score="1" Text="@O.M.Y. the meta post I linked to about cross posting shows community consensus is against cross posting, but there are also answers in favour of cross posting in certain rare circumstances, which are worth reading. Even then they recommend tailoring your question to different sites rather than just copy and pasting." CreationDate="2016-01-09T16:23:34.663" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1785" PostId="1880" Score="1" Text="The reason that meta post exists is because it is not immediately obvious why cross posting would be a bad thing. It's well worth reading through. If there is going to be ill feeling on our site due to this it might be worth having a local meta discussion to see how our particular community feels about when and whether to cross post." CreationDate="2016-01-09T16:25:40.903" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1786" PostId="1880" Score="0" Text="@joojaa This is where I was told to &quot;go ahead and post&quot; in VIDEO ... http://meta.video.stackexchange.com/questions/1467/about-asking-a-question-in-two-different-se-communities/1468#1468" CreationDate="2016-01-09T16:28:00.320" UserId="2427" ContentLicense="CC BY-SA 3.0" />
  <row Id="1787" PostId="1880" Score="1" Text="Still i think you should re word your question anyway. Video will get you a different answer." CreationDate="2016-01-09T16:28:56.570" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1788" PostId="1880" Score="0" Text="Also user @OlliNiemitalo (2000+ rep) helped me add the cross links in DSP without complain... again if I was provided bad information I apologize but I did ask and I did feel it was okay based on those responses. Now I feel confused and wonder if I should just delete it all and shut up." CreationDate="2016-01-09T16:31:39.993" UserId="2427" ContentLicense="CC BY-SA 3.0" />
  <row Id="1789" PostId="1880" Score="1" Text="@O.M.Y. you certainly shouldn't feel bad about posting as you have sought advice in advance. There will always be conflicting opinions about things. If you want to talk about it further I recommend [chat] rather than trying to fit it into comments here." CreationDate="2016-01-09T16:35:06.137" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1790" PostId="1881" Score="0" Text="I did toyed with SISR a while ago, but my mind cleverly decided to erase that information from the brain. If I can recall though one important bit (mentioned in the paper) that I would explicit in your excellent answer is that the patterns or substructures are to be searched not only in the original image but also in multiple scales of it. That IIRC lead to significantly better results, although as I said I don't remember that much details :(" CreationDate="2016-01-09T17:40:34.587" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="1792" PostId="1888" Score="1" Text="Interesting question. If there is a specific reason that you don't want a shape with planar quads, like a [rhombic dodecahedron](https://en.wikipedia.org/wiki/Rhombic_dodecahedron), mentioning that reason and explaining your motivation for using non-planar quads might help give insight into what is required." CreationDate="2016-01-10T16:12:58.640" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1793" PostId="1889" Score="0" Text="Thanks for the answer! But I am not looking for a simple 2D shadows algorithm (there are lots on the web), I am looking for a shadow/light algorithm that can handle volumetric fog and colored transparent surfaces (as seen in the Youtube video)." CreationDate="2016-01-10T17:57:52.223" UserId="2437" ContentLicense="CC BY-SA 3.0" />
  <row Id="1794" PostId="1888" Score="1" Text="@trichoplax Ok, updated the question :)." CreationDate="2016-01-10T17:57:59.480" UserId="2440" ContentLicense="CC BY-SA 3.0" />
  <row Id="1795" PostId="1889" Score="0" Text="@sydd that is just a shader on the shadow volume. Easy to extend." CreationDate="2016-01-10T18:17:25.673" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1797" PostId="1892" Score="1" Text="It's unfortunately not uncommon for drivers to contain bugs (or &quot;features&quot; to let Game X play optimally even though it doesn't use the API correctly)" CreationDate="2016-01-11T12:01:05.807" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1799" PostId="1892" Score="0" Text="I hope it's not driver bug, but my bug with padding in that struct." CreationDate="2016-01-11T13:46:39.523" UserId="2413" ContentLicense="CC BY-SA 3.0" />
  <row Id="1800" PostId="1892" Score="0" Text="and if you use std430 layout?" CreationDate="2016-01-11T13:50:07.660" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1801" PostId="1892" Score="0" Text="@ratchetfreak OS X only support OpenGL [4.1](https://developer.apple.com/opengl/capabilities/) ( also, ARB_gpu_shader_fp64 is not supported on some 4.1 Mac Radeons, but it should be on my Intel ), and std430 was introduced in OpenGL 4.3" CreationDate="2016-01-11T13:59:30.073" UserId="2413" ContentLicense="CC BY-SA 3.0" />
  <row Id="1802" PostId="1892" Score="0" Text="I've tested it now on one iMac, and it's working there ( but super slow )." CreationDate="2016-01-11T14:39:47.893" UserId="2413" ContentLicense="CC BY-SA 3.0" />
  <row Id="1803" PostId="1892" Score="0" Text="So it looks like this shader is not compatible with Intel cards or it's bug in OS X Intel driver." CreationDate="2016-01-11T14:43:31.763" UserId="2413" ContentLicense="CC BY-SA 3.0" />
  <row Id="1806" PostId="1889" Score="0" Text="@sydd added colored example" CreationDate="2016-01-11T17:43:40.670" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1807" PostId="1893" Score="2" Text="What units is the radius measured in? It doesn't make a lot of sense to have a radius of 0.12 pixels (it would just be the one center pixel). It's probably 0.12 centimeters, or 0.12 ems, or something like that." CreationDate="2016-01-11T19:05:36.990" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1808" PostId="1893" Score="0" Text="Also what does it mean to 'pick a pixel'? Given a pixel coordinate, do you want to know if it is inside the circle?" CreationDate="2016-01-11T20:44:44.263" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1809" PostId="1893" Score="0" Text="By &quot;to pick a pixel&quot; I meant to randomly choose a pixel inside the circle. The units are not mentioned. The circle has a center C(Cx, Cy), which is a pixel. My guess is, a pixel inside the circle has coordinates P(Px, Py) such that sqrt((Px-Cx)^2+(Py-Cy)^2) &lt; 0.12 (the distance between P and the center C is less than the radius) . That's the most logical way to think about it in my opinion." CreationDate="2016-01-11T21:59:37.363" UserId="2233" ContentLicense="CC BY-SA 3.0" />
  <row Id="1810" PostId="1896" Score="0" Text="So what's the problem, specifically? &quot;I cannot make it work&quot; doesn't give us much to go on. Are you getting compiler errors? Post them. Bad output? Show us a screenshot." CreationDate="2016-01-11T22:11:03.257" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1811" PostId="1896" Score="0" Text="I have just started with shaders. Sorry about that, I am a little bit desperate right now...  It compiles just fine. But won't  affect the lighting in any way. The objects just stays there unaffected by the shader, same as it it was not even loaded." CreationDate="2016-01-11T22:13:12.750" UserId="2425" ContentLicense="CC BY-SA 3.0" />
  <row Id="1812" PostId="1896" Score="0" Text="as a test, you might make the sphere have a color based on some value that you are using to calculate your lighting.  This is a really primitive way of getting an idea of the value of your variables.  Doing this, you might notice that something is constant across the sphere which shouldn't be, or a similar problem, which will then help point you in the direction of what is going wrong specifically." CreationDate="2016-01-12T01:16:59.820" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1813" PostId="1895" Score="0" Text="Is there a reason nobody responds? Should I reformulate the question?" CreationDate="2016-01-12T08:36:11.910" UserId="2447" ContentLicense="CC BY-SA 3.0" />
  <row Id="1815" PostId="1895" Score="1" Text="Its early days there are only so many people who have time to answer and you've only so far reached 5 views. This stackexchange is still an infant and has not got many users give it time." CreationDate="2016-01-12T10:52:36.993" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1816" PostId="1895" Score="0" Text="OK. Thanks. I did not realize so little people were here." CreationDate="2016-01-12T12:10:08.863" UserId="2447" ContentLicense="CC BY-SA 3.0" />
  <row Id="1817" PostId="1899" Score="1" Text="Thanks! A, B, C are vectors right? Also, I am using the scanline method because it allows me to get the exact number of points I need. Could you take a look at the code and guess why it is not working? Just the formulas. Also I would upvote the response, but I don't have 15 reputation." CreationDate="2016-01-12T13:04:15.660" UserId="2447" ContentLicense="CC BY-SA 3.0" />
  <row Id="1818" PostId="1899" Score="0" Text="Yes, A B &amp;C are vectors; 2D in your case but it applies equally well to N dimensions. As for going through the code... as I said I don't know Lua and even getting a standard polygon scanline renderer correct can be tricky - e.g. You have to be very careful when counting crossings which lie exactly on vertex positions. When you extend that to handling Beziers directly (which I did about 20+ years ago) it's more difficult still. I'm sorry I don't have the time." CreationDate="2016-01-12T14:09:28.923" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1819" PostId="1899" Score="1" Text="Thanks for the help. Just found the issue. The a and c in the quadratic equation were inverted." CreationDate="2016-01-12T16:22:31.000" UserId="2447" ContentLicense="CC BY-SA 3.0" />
  <row Id="1820" PostId="1889" Score="0" Text="thanks, it looks nice! But as I wrote my post is about porting the algorithm I described to the GPU. Actually someone gave an answer on SO at http://stackoverflow.com/questions/34708021/how-to-implement-2d-raycasting-light-effect-in-glsl . This answer is what im looking for, but its slow, thus I dont think that this algorithm is feasible for a real game." CreationDate="2016-01-12T17:23:18.417" UserId="2437" ContentLicense="CC BY-SA 3.0" />
  <row Id="1821" PostId="1889" Score="0" Text="@sydd raycasting can be accelerated by precomputing the data into a better datastructure." CreationDate="2016-01-12T18:19:55.377" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1822" PostId="1903" Score="0" Text="in graph (A), y range is   -1.0 ~ 1.5," CreationDate="2016-01-13T08:25:40.240" UserId="2459" ContentLicense="CC BY-SA 3.0" />
  <row Id="1825" PostId="1903" Score="3" Text="Welcome to Computer Graphics SE! I tried to improve the readability of your post a bit. However, I'm not sure it's a great fit for this site. You could probably just ask this on [so] instead. I've [started a discussion on meta](http://meta.computergraphics.stackexchange.com/q/205/16) whether questions about plotting simple graphs are considered on topic or not." CreationDate="2016-01-13T12:42:17.800" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1828" PostId="1904" Score="0" Text="I've made some minor edits that hopefully do not change your original intention. Feel free to roll back anything that changes your question." CreationDate="2016-01-13T21:59:34.110" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1829" PostId="1903" Score="3" Text="Given that this is specifically about R, it's likely a better fit for [Cross Validated](https://stats.stackexchange.com/)." CreationDate="2016-01-14T00:02:58.473" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1830" PostId="1901" Score="0" Text="What's the problem with doing a dynamic loop? That seems like the natural way to solve this kind of problem. If the board has a fixed 8x8 size, the longest ray would only pass through 16 squares at most, so it's not a crazy iteration count. And if you restrict the camera angles like you said, so that people can't look straight across the board, you can cut down the longest ray length considerably." CreationDate="2016-01-14T00:09:16.400" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1831" PostId="1901" Score="0" Text="I've been going down that route without having a better solution, so it's nice to hear your support of it, thanks Nathan." CreationDate="2016-01-14T00:17:58.143" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1835" PostId="1909" Score="1" Text="Hello, it could be a particular issue so, can you edit your post and add your GPU code (vertex/fragment shaders) as well as your specific CPU's code on which you are dispatching your shadow map to GPU? this is in order to do a fast debug." CreationDate="2016-01-15T04:07:54.017" UserId="2228" ContentLicense="CC BY-SA 3.0" />
  <row Id="1836" PostId="1909" Score="0" Text="I added the code but as you can see, there's nothing really advanced here. I also precise that I am using Unity." CreationDate="2016-01-15T07:27:49.133" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="1838" PostId="1901" Score="0" Text="Have you tried using raymarching instead of raytracing? Raytracing is more of a CPU/software rendering oriented algorithm, whereas raymarching with signed distance fields is much more easy to implement on a GPU, IMO." CreationDate="2016-01-16T06:24:50.037" UserId="1683" ContentLicense="CC BY-SA 3.0" />
  <row Id="1839" PostId="1901" Score="0" Text="For sphere and axis aligned box, directly finding the intersection with raytracing is the better way to go.  For more complex shapes, sure, ray marching is definitely a nice option.  Ray marching exists to get around having to analytically solve line segment vs arbitrary shape." CreationDate="2016-01-16T06:35:37.547" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="1840" PostId="1909" Score="0" Text="This should be the same issue as in http://stackoverflow.com/questions/1513383/texturing-error-on-a-sphere" CreationDate="2016-01-16T13:53:15.863" UserId="2476" ContentLicense="CC BY-SA 3.0" />
  <row Id="1841" PostId="1912" Score="0" Text="transform feedback buffers" CreationDate="2016-01-16T17:02:05.970" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="1842" PostId="1912" Score="0" Text="Sounds very good. The constraints of the cloth simulation make it necessary that I can access all neighbours of a particle (to calculate the spring forces). Can this be done with such buffers? In a geometry shader maybe? to pervent the feedback buffer to print each vertex multiple times I would like to use GL_POINTS. After a first look it seems that this makes it harder to calculate the springs." CreationDate="2016-01-16T19:18:54.543" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="1845" PostId="1907" Score="0" Text="This seems to be helpful .... Thanks cheers!!!!!!!1" CreationDate="2016-01-18T03:34:44.603" UserId="2383" ContentLicense="CC BY-SA 3.0" />
  <row Id="1846" PostId="1911" Score="1" Text="Your question could probably be improved by including some more information about the actual algorithm instead of just linking to it. Posts should generally be self-contained so as not to be susceptible to link rot (even though that's unlikely for Wikipedia) and so that people can understand the full question without having to read an external link first." CreationDate="2016-01-18T09:10:28.977" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1849" PostId="1916" Score="1" Text="Depends heavily on what you trace, but this isnt a good fit for the site in my opinion written in this form. Edit the post to be about backface culling in raytracing." CreationDate="2016-01-18T16:08:59.780" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1856" PostId="1921" Score="1" Text="I think even in the silhouette case, using *signed* projected area (as you noted) means that assumption 3 isn't violated, so long as the microsurface's boundaries match the macrosurface's. Even if there are overhangs beyond the silhouette, the signed projected area of facets on the front and back sides of the overhang will cancel out." CreationDate="2016-01-19T18:30:58.430" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1857" PostId="1921" Score="0" Text="(Also, maybe this goes without saying, but I think the assumptions also guarantee that the microsurface is a nice, 2-manifold surface without any holes or other weird stuff.)" CreationDate="2016-01-19T18:51:22.653" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1858" PostId="1924" Score="2" Text="Could you elaborate on what you mean by sampling? The data is just a LUT. The simple way to use it would be to look up the closests input / output vectors and LERP the spectral data between them. Or do you mean how to importance sample?" CreationDate="2016-01-19T23:32:20.063" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1859" PostId="1924" Score="0" Text="Hi @RichieSams i update my question with more specific problems. Ok for the linear interpolation (I want to keep thing simple at the moment :))" CreationDate="2016-01-19T23:55:21.643" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="1860" PostId="1925" Score="2" Text="Modern cards differ from previous generation cards as they nolonger have a fixed pipeline for triangles and thus its harder to say what the rate is as it waries with conditions outside the card." CreationDate="2016-01-20T06:22:05.103" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1861" PostId="1925" Score="0" Text="I was worried that might be the case for NV. AMD did have a fixed triangle setup engine in Hawaii, but I wouldn't be surprised if it went away in the next architecture revision." CreationDate="2016-01-20T06:39:17.713" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1862" PostId="1926" Score="0" Text="Thank you @RichieSams. So what about that transformation matrix? Is it some kind of change of coordinate matrix, for example like the one used for transform input data to a camera coordiante system?" CreationDate="2016-01-20T07:23:31.213" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="1863" PostId="1921" Score="0" Text="@NathanReed That's true, I should have been more precise about that. As for what the assumptions guarantee, I think of it the other way round: the fact that a surface, however faceted, has to be the whole of a boundary between some &quot;inside&quot; and some &quot;outside&quot; forces it to have the three properties." CreationDate="2016-01-20T09:58:42.097" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="1864" PostId="1926" Score="1" Text="Exactly. It's a change of coordinate matrix. The data is in world coordinate space. And specifically, the input vector is a small portion of the space. So transform to world, then rotate to the input vector domain" CreationDate="2016-01-20T14:24:10.900" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1865" PostId="1927" Score="2" Text="How are you representing the rectangle, and how are you moving it? Can you share the relevant sections of your code?" CreationDate="2016-01-20T18:53:01.593" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1866" PostId="1927" Score="1" Text="We will need more detail on what you are trying to achieve, and what you have tried so far so we don't duplicate effort." CreationDate="2016-01-20T20:20:24.237" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1867" PostId="1927" Score="0" Text="I'm new to this code base so I'm still figuring things out. I'll update the question with some more information when I get time." CreationDate="2016-01-20T22:27:34.393" UserId="2506" ContentLicense="CC BY-SA 3.0" />
  <row Id="1868" PostId="1925" Score="0" Text="Again it seems to me that you can make certain default assumptions that take a lot of the uncertainly out of what's &quot;outside the card&quot; for the purpose of comparing card speeds (simple/default vertex and pixel shaders, simple lighting, big model being textured by some reasonable texture sheets). But I'll take this fixed-pipeline estimate of 2 billion per second as being similar to what you'd get with those assumptions. Thanks." CreationDate="2016-01-21T01:26:13.003" UserId="2358" ContentLicense="CC BY-SA 3.0" />
  <row Id="1869" PostId="1925" Score="0" Text="I'm going to study up on NV's white papers - I need to catch up on their architecture (I worked for AMD a few years back). Ideally, there would be be synthetic benchmarks which would help everybody understand where the bottlenecks are, but there was a long history of cheating on them..." CreationDate="2016-01-21T03:31:10.087" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1870" PostId="1927" Score="1" Text="There is no such thing as a straight line on a sphere. Thus no rectangle. But most likely also your coordinate system is cylindrical causing the cordinates not to be uniform." CreationDate="2016-01-21T05:25:11.487" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1871" PostId="1925" Score="0" Text="If it's an operation that always (or nearly always) needs to be done (and triangle set up is one such thing) then there can be large power/area/efficiency reasons for using dedicated hardware." CreationDate="2016-01-21T10:31:56.017" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1872" PostId="1927" Score="0" Text="@joojaa: A rectangle is mapped on to a sphere. The coordinates on the globe are LatLon but from what I can tell, the X and Y used aren't related." CreationDate="2016-01-21T15:26:45.447" UserId="2506" ContentLicense="CC BY-SA 3.0" />
  <row Id="1873" PostId="1931" Score="0" Text="Is the dipole approximation at all related to the Beer–Lambert law?" CreationDate="2016-01-22T03:16:56.483" UserId="2457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1874" PostId="1931" Score="0" Text="@maogenc The dipole approximation concerns homogeneous materials with a very high scattering coefficient, such that they're dominated by multiple scattering. The Beer–Lambert law would also apply to such materials." CreationDate="2016-01-22T06:38:14.047" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1875" PostId="1937" Score="0" Text="How sure are you that the light source is parameterized in the same way?" CreationDate="2016-01-22T09:27:04.150" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1876" PostId="1937" Score="0" Text="Pretty sure, because and Lux uses diffuse IES data on light sources by default and I only support diffuse material. Also, both light sources have the same strength/color of 6.0." CreationDate="2016-01-22T09:49:04.440" UserId="2448" ContentLicense="CC BY-SA 3.0" />
  <row Id="1877" PostId="1937" Score="1" Text="Still it might be that you are using a different [photometric quantity](http://mentalraytips.blogspot.de/2007/03/understanding-photometric-and.html) somewhere, that your tone mapping is still different despite Gamma 2.2, that you're [missing a pi](https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/) somewhere, or any other reason. It's hard to tell, please give some more details about your shading, show some code etc. Besides, the camera is not the same in both images, you might wanna adjust that too :)" CreationDate="2016-01-22T10:20:14.420" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1879" PostId="1937" Score="0" Text="I have edited in some code. Hopefully it helps. Also, here is the whole repo. More code could be found here. (Please use the &quot;dev&quot; branch. It's where all the new codes are.) https://bitbucket.org/Seanstone5923/simpleray5cpu. OK, I'll fix the camera as soon as possible." CreationDate="2016-01-22T15:03:49.677" UserId="2448" ContentLicense="CC BY-SA 3.0" />
  <row Id="1880" PostId="101" Score="0" Text="When they are available you can use SSBOs to get a more flexible output format where you don't need to encode in colors. However, the big drawback of this approach is that it alters the code which can hide/alter bugs, especially when UB is involved. +1 Nevertheless, for it is the most direct method that is available." CreationDate="2016-01-22T15:10:21.610" UserId="2521" ContentLicense="CC BY-SA 3.0" />
  <row Id="1882" PostId="1940" Score="3" Text="You may need to add relevant code but this should be pretty easy to paralellisize and use the GPU for calculation. How many objects are we talking about." CreationDate="2016-01-22T16:39:56.777" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1885" PostId="1939" Score="1" Text="Before/after screenshots would be helpful, otherwise we're all trying to imagine what you mean. Also, is the question about how to enlarge a known region (i.e. you have already identified where the eyes are), or about how to find/track the objects to enlarge? Because those are very different questions. :)" CreationDate="2016-01-22T19:13:21.767" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1886" PostId="1939" Score="0" Text="Welcome to Computer Graphics Stack Exchange. Asking for off-site resources is off-topic for this site, that's why I removed that part of your question." CreationDate="2016-01-22T19:18:26.767" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1887" PostId="1940" Score="1" Text="Welcome to Computer Graphics Stack Exchange. As joojaa already said, it's quite hard to help you to improve your code without seeing it. Please edit your question and add relevant parts of your code. Additionally, a screenshot of the result then also helps understanding what your code does." CreationDate="2016-01-22T19:26:54.350" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1888" PostId="1940" Score="0" Text="Off the cuff, I'd say render the objects in a floating-point texture using additive blending to accumulate their [gravitational potential](https://en.wikipedia.org/wiki/Gravitational_potential). Then use a second pass to convert the texture into a color map." CreationDate="2016-01-22T19:39:39.613" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1889" PostId="1946" Score="0" Text="Hi @Dragonseel thank you for your answer. I read somewhere on the web that you could also need the texture coordinate to calculate the tangent space. Are there other procedure that could be used to calculate the tangent space?" CreationDate="2016-01-22T20:49:11.507" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="1890" PostId="1946" Score="1" Text="Nice answer. I edited it to apply MathJax. A minor note: You can't use $DIR$ for the pixel that looks directly at the sphere, because of linear dependency to $N$. But actually you could use any vector instead of $DIR$ that is linearly independent. Or am I missing something?" CreationDate="2016-01-22T20:51:29.750" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="1891" PostId="1946" Score="0" Text="I don't know about the texture coordinate, but given that you already have the intersection-point and the normal calculating two cross-products seems to be very cost effective way. Yes, any vector that is linearly independent can be used. I wrote $DIR$ because it is an easily usable vector. You could just invent a way to generate a non-linear dependent vector (switch two components comes to mind, but I'm not that sure)." CreationDate="2016-01-22T21:18:56.590" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="1892" PostId="1946" Score="0" Text="If the tangent space is to be used for texture mapping (e.g. filtering, normal mapping etc) then it's convenient for the tangent space axes to be aligned with the texture UV axes. But if any arbitrary tangent space is fine, then [an arbitary vector orthogonal to the normal](http://lolengine.net/blog/2013/09/21/picking-orthogonal-vector-combing-coconuts) can be used." CreationDate="2016-01-22T23:15:56.423" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1893" PostId="1939" Score="0" Text="@Nero no worries." CreationDate="2016-01-23T04:56:08.257" UserId="2520" ContentLicense="CC BY-SA 3.0" />
  <row Id="1894" PostId="1939" Score="0" Text="@NathanReed I have already tracked the region of the video for enlargement. I am only looking at enlarging the regions are blend well with the rest of the video content." CreationDate="2016-01-23T04:58:07.287" UserId="2520" ContentLicense="CC BY-SA 3.0" />
  <row Id="1895" PostId="1940" Score="0" Text="@NathanReed Right my thoughts exactly..." CreationDate="2016-01-23T07:57:50.153" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1896" PostId="1948" Score="2" Text="Your point is well taken as a matter of practice: when I used to work at AMD, to test architectural limits, I would do things like clock down the GPU so I could simulate &quot;infinitely&quot; fast memory. However, such architectural limits are all part of a well designed, balanced GPU; understanding gives insight into GPU design, implementation and programming." CreationDate="2016-01-23T15:58:04.627" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1897" PostId="1951" Score="1" Text="Just out of curiosity didnt we agree that [software recommendations are offtopic](http://computergraphics.stackexchange.com/help/on-topic)? I dont nesseserily mind but..." CreationDate="2016-01-24T01:21:46.423" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1898" PostId="1951" Score="1" Text="@joojaa The question is about using scripting to automate image operations, which seems on-topic and generally useful to me. Moreover I don't think I should hold back my answer just because it refers to some external software." CreationDate="2016-01-24T03:04:22.223" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1900" PostId="1951" Score="0" Text="Love you man. You saved so much of my time :) Hope you have a great day!" CreationDate="2016-01-24T08:27:08.883" UserId="2537" ContentLicense="CC BY-SA 3.0" />
  <row Id="1901" PostId="1951" Score="1" Text="No thats not nesseserily what im saying, im just pointing out that have closed questions of simililar nature due to them being software recommendations." CreationDate="2016-01-24T08:40:07.400" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1904" PostId="1953" Score="0" Text="I read that in a raster display a picture is kind-of jaggy but in random we get a smooth line. why and how does this happen when both work on pixel level anyhow?" CreationDate="2016-01-24T14:13:13.327" UserId="2540" ContentLicense="CC BY-SA 3.0" />
  <row Id="1905" PostId="1953" Score="0" Text="Both systems produce some aliasing (I think this is what you mean with 'jaggy') on pixel-level since there is only a finite discrete resolution available. I don't know how the amount of aliasing compares, since in the background of a rasterizer geometric primitives also get converted into pixel-grid, and a random scan has to do the same conversion. I can only assume that because a random scan can directly follow the line it is easier to anti-alias." CreationDate="2016-01-24T14:36:38.283" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="1906" PostId="1957" Score="1" Text="I dont think the memory type matters, however you may want to get a new chipset so that you can do most of the state of the art stuff. Since the memory type may a indicator of the chipset... Memory per see should not matter." CreationDate="2016-01-25T07:32:58.423" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1911" PostId="1959" Score="0" Text="Statistical tests for random number generators should be useful. Computing the expected number of in order (reverse order) pairs might be a good place to start with a test. This paper has lots of references: http://csrc.nist.gov/groups/ST/toolkit/rng/documents/nissc-paper.pdf." CreationDate="2016-01-25T10:30:09.687" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1912" PostId="1960" Score="0" Text="Welcome to Computer Graphics SE! Re your second point, I looked at the cyclic decomposition of the permutation table used by Perlin. It consists of multiple cycles of lengths `{4, 121, 89, 12, 4, 15, 4, 6}`, so apparently that's good enough? (Or maybe it isn't and a different permutation table would be even &quot;better&quot;? Although I'm not sure a human could perceive the difference. Or is actually better to have multiple cycles?)  I'm not following your third point. A uniform random distribution of what? And what step distance do you mean?" CreationDate="2016-01-25T11:29:40.773" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1913" PostId="1960" Score="0" Text="Thanks! Yeah, that was quite cryptic I guess. It was years ago I experimented with this, so I don't recall the exact implementation, but when you have the implementation for the optimal path generation, it should become evident that you pick random positions of what is left of unused indices - it is that random step length I refer to. I updated the answer" CreationDate="2016-01-25T11:53:09.523" UserId="1790" ContentLicense="CC BY-SA 3.0" />
  <row Id="1914" PostId="1957" Score="0" Text="&quot;Maxwell&quot; is NV's current architecture - it has some new graphics features which are described in NVs GeForce GTX 980 Whitepaper. I believe the lowest end Maxwell is the 950." CreationDate="2016-01-25T12:23:23.247" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1915" PostId="1936" Score="0" Text="Note that there is a fundamental difference: supersampling (with jittering or any other technique) requires _additional samples_, while a post-filter (like the suggested low-pass filter) works on the _rendered image_. Nevertheless, this is valuable additional information and both techniques go hand in hand." CreationDate="2016-01-25T12:47:23.523" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1916" PostId="1936" Score="0" Text="I answered initially without seeing yours (antialias with some kind of stochastic supersampling), but your answer was better. I still thought the older paper was still worth referencing, so I changed my answer instead of deleting it. I hope that's reasonable etiquette?" CreationDate="2016-01-25T17:34:39.693" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1917" PostId="1957" Score="0" Text="It's really very simple now, especially for Nvidia GPUs: the more you pay, the more you get. Check out this [Price/performance ratio table](http://www.videocardbenchmark.net/gpu_value.html). It roughly shows that if you are low on budget, GeForce GTX 950 is the best you can get. Also it's based on the latest architecture, so you get DirectX 12 and CUDA compute capability 5.2. Alternatively you may look into laptops. Low-end discrete GPUs are almost free there." CreationDate="2016-01-25T17:45:15.380" UserId="2115" ContentLicense="CC BY-SA 3.0" />
  <row Id="1918" PostId="1937" Score="0" Text="I'm afraid I dont know enough about it to be really helpful, however, I would probably try to adjust the light intensity so the images look equal, then see with which factor you needed, maybe it can give a clue as what went wrong. Or maybe you cant get it to look the same, like the whole image gets to light to get the centre the right brightness, then maybe some angle calculation is not the same.. just some ideas to try out" CreationDate="2016-01-25T19:11:02.740" UserId="2422" ContentLicense="CC BY-SA 3.0" />
  <row Id="1919" PostId="1958" Score="0" Text="So there're no architectural differences whatsoever?" CreationDate="2016-01-26T01:31:12.990" UserId="1870" ContentLicense="CC BY-SA 3.0" />
  <row Id="1920" PostId="1958" Score="1" Text="It's just memory. Of course there are some hardware-level differences in how the chip talks to the memory, but it makes no difference for programming it. In CPU programming, you don't write different code for DDR3 vs DDR4 memory either." CreationDate="2016-01-26T02:15:08.723" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="1922" PostId="1963" Score="3" Text="Is your actual texture also just black dots on white? If so, you might be able to generate the texture procedurally, e.g. with [Poisson disc sampling](http://devmag.org.za/2009/05/03/poisson-disk-sampling/) with varying radii. Otherwise, if your texture is actually something more continuous, blending might be an option (that wouldn't work well for crisp textures like your example though). So if you could clarify what your actual pattern/texture looks like that might help getting more helpful answers." CreationDate="2016-01-26T07:37:53.980" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="1923" PostId="1936" Score="0" Text="Totally, your answer is a fine addition :) I just wanted to make sure nobody gets confused." CreationDate="2016-01-26T08:43:32.147" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1924" PostId="1951" Score="0" Text="The question is about a scripting tool, perhaps making it an &quot;API recommendation&quot; question? It would probably be at home in game development. Maybe I've got a meta meta question here - can questions be recommended for transfer to or crossposted on other stack exchange sites?" CreationDate="2016-01-26T12:53:28.380" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1925" PostId="1937" Score="0" Text="Have you tried setting `FACTOR_CUT_OFF` to 0 (introduces bias, LuxRender is unbiased)? Have you tried excluding techniques like Russian Roulette, to see where the culprit may be?" CreationDate="2016-01-26T13:31:53.990" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="1926" PostId="1936" Score="0" Text="Understood. It really sinks in that aliasing is a result of the initial samples being at regular intervals: if every sample hits the pickets of a fence, you're stuck with a white wall. Applying a low pass filter to a regularly sampled image just isn't universally effective (the high frequency picket fence aliases down to a low frequency artifact - a white wall)." CreationDate="2016-01-26T14:00:57.493" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1927" PostId="1936" Score="0" Text="The post filtering approach also seems like a really interesting modern CPU (with integrated GPU) load balancing question. Ray trace using CPU cores, which walk such data structures efficiently, post process the images on the GPU. Fun project!" CreationDate="2016-01-26T14:02:20.203" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1928" PostId="1937" Score="0" Text="Yes, I did test that. In fact, the image I shown is rendered under a unbiased condition. Also, Russian Roulette is enabled in the image that I shown. I have set `BOUNCE_DEPTH` to -1(which makes my renderer to ignore bounce limits)." CreationDate="2016-01-26T15:10:53.090" UserId="2448" ContentLicense="CC BY-SA 3.0" />
  <row Id="1929" PostId="1967" Score="0" Text="So essentially, you want to remove the hand and pencil from the drawing? So you can have just a time-lapse of the drawing over time? Did I understand you correctly?" CreationDate="2016-01-26T16:37:49.393" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1930" PostId="1967" Score="0" Text="Actually, I want to know how the painter draw. That is, the sequence of its drawing stroke. The painter will draw by hands on paper. Like that http://imgur.com/FeN1IPy" CreationDate="2016-01-26T16:40:09.933" UserId="2551" ContentLicense="CC BY-SA 3.0" />
  <row Id="1931" PostId="1967" Score="0" Text="Remove the hand and pencil may benefit to analyze and it is good also." CreationDate="2016-01-26T16:40:52.333" UserId="2551" ContentLicense="CC BY-SA 3.0" />
  <row Id="1932" PostId="1963" Score="1" Text="Post process? Something like a variant on edge detection: write out the distance to the nearest dot center then, in screen space, search each pixel's dot sized neighborhood for any (close to) zero values. This gives screen space dots, so you'd have to write and mess with UV derivatives to get back to texture space... Note that you still have a continuity problem - as the p high/low boundary moves across the surface, dots appear and disappear. If p can be continuous (a soft boundary) you might be able to shrink/grow the dots along the boundary..." CreationDate="2016-01-26T16:50:03.830" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1933" PostId="1967" Score="0" Text="So, sort of a vector field drawing showing the path the pencil takes on the paper? For example, each stroke would be an individual curve/arrow." CreationDate="2016-01-26T19:17:59.540" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1935" PostId="1963" Score="1" Text="Another option is to look into Wang tiling.  [introduction](http://procworld.blogspot.com/2013/01/introduction-to-wang-tiles.html), [tile genetics](http://procworld.blogspot.com/2013/01/tile-genetics.html), [GPU gems](http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter12.html)" CreationDate="2016-01-27T02:06:31.343" UserId="2457" ContentLicense="CC BY-SA 3.0" />
  <row Id="1936" PostId="1963" Score="0" Text="@MartinBüttner good point, I've added a line about the type of texture" CreationDate="2016-01-27T04:13:21.460" UserId="31" ContentLicense="CC BY-SA 3.0" />
  <row Id="1937" PostId="1967" Score="0" Text="Yes. I just record how painters they paint in daily life actually." CreationDate="2016-01-27T07:02:16.060" UserId="2551" ContentLicense="CC BY-SA 3.0" />
  <row Id="1938" PostId="1963" Score="1" Text="Have you looked at texture bombing? http://http.developer.nvidia.com/GPUGems/gpugems_ch20.html" CreationDate="2016-01-27T08:24:37.303" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="1939" PostId="1967" Score="0" Text="Any demo videos so we can test this? You could simply track the ne dof the pencil. But if it gets occluded then no dice. You could also track the evolution of the shape over frames by difference keying. Or just put the paper on a wacom tablet and change to a wacom ink pen." CreationDate="2016-01-27T08:33:28.030" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1940" PostId="1971" Score="0" Text="One could paint on a semitransparent surface and capture image from behind, use wacom tablet trough paper and so on... Reverse difference matte should also give the evolution." CreationDate="2016-01-27T08:42:03.433" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1941" PostId="1967" Score="0" Text="Its my test video, a very simple one. No overlap. http://1drv.ms/1UpF4x3" CreationDate="2016-01-27T09:06:40.550" UserId="2551" ContentLicense="CC BY-SA 3.0" />
  <row Id="1942" PostId="1971" Score="0" Text="I understood he had the finished video from somewhere and now wants to reconstruct the process." CreationDate="2016-01-27T10:14:24.280" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="1943" PostId="1971" Score="0" Text="Sure but sometimes, one really can not get a good measurement in hindsight. Anyway for the test video tracing works well, and since he has a test video maybe he also hasnt set up the system yet." CreationDate="2016-01-27T10:21:52.227" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1944" PostId="1973" Score="1" Text="Thanks for the reply Nathan, I did expect that MSDN page was incorrect from my initial experiments. I might give the clip planes solution another try when I get some time, i'll post another question when I have more information on what's going wrong. Thanks again for your help." CreationDate="2016-01-27T11:40:47.010" UserId="288" ContentLicense="CC BY-SA 3.0" />
  <row Id="1945" PostId="1974" Score="0" Text="Im not sure it exists as such, determining wetehr or theres a possibility for 0-1 or 2 or more is pretty trivial but the formulation does not really make it ieasy to make sure its 0 or 1 without actually checking." CreationDate="2016-01-28T13:44:13.640" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1946" PostId="1974" Score="0" Text="What is the runtime requirements? An solution that should be able to produce pretty accurate results would be to approximate both curves by a large number of short straight segments and then intersecting them in a pairwise fashion. But that costs much time and memory." CreationDate="2016-01-28T14:05:10.967" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="1947" PostId="1974" Score="0" Text="@Dragonseel Well, I would be happy for any solution, really, but since you asked O(1) would be nice. But approximating the curves with line segments leads to the same problems as the test for bounding box overlap..." CreationDate="2016-01-28T15:00:20.193" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="1948" PostId="1974" Score="0" Text="Interesting problem. I don't think there's an easy answer but I'd like to be wrong. Do you have a link for the Sederberg and Meyers paper?" CreationDate="2016-01-28T15:21:11.820" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1949" PostId="1974" Score="0" Text="@DanielMGessel Yes, see the edit above." CreationDate="2016-01-28T15:25:10.350" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="1950" PostId="1977" Score="0" Text="Using the identity matrix is equivalent to placing the object at the origin, with no scaling, and no rotation." CreationDate="2016-01-28T19:29:07.043" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="1951" PostId="1977" Score="0" Text="@RichieSams yes, same as putting object to world coordinates (or other parent), except theres a matrix in between to manipulate." CreationDate="2016-01-28T19:46:41.473" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1952" PostId="1967" Score="1" Text="[Meta discussion](http://meta.computergraphics.stackexchange.com/questions/210/is-reconstruction-of-pen-strokes-from-real-life-video-on-topic) on whether this is on topic." CreationDate="2016-01-28T21:42:55.493" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1953" PostId="1974" Score="0" Text="Are the control points of your curves 2D or 3D (or, for that matter, greater)?" CreationDate="2016-01-29T09:41:23.870" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1954" PostId="1974" Score="0" Text="@SimonF 2D, sorry, see the edit." CreationDate="2016-01-29T09:48:31.333" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="1955" PostId="1974" Score="0" Text="(Sigh. Caught by the comment edit timeout). You also say _&quot;I also would like to use floating-point numbers in the implementation&quot;_  I suspect that you might be limited on how &quot;reliable&quot; the test will be by whatever rounding goes on in floating point. For example, if there is a huge dynamic range in the control point values, you may lose too much precision." CreationDate="2016-01-29T09:50:10.977" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1956" PostId="1974" Score="0" Text="@Eric: Being pedantic here:  By planar do you mean the _same_ plane?" CreationDate="2016-01-29T09:55:32.927" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1957" PostId="1974" Score="0" Text="@SimonF Yes, same plane." CreationDate="2016-01-29T09:58:28.413" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="1958" PostId="1974" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/35018/discussion-between-simon-f-and-ecir-hana)." CreationDate="2016-01-29T10:03:58.910" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1959" PostId="1963" Score="1" Text="Can you give an idea of the range of shapes/patterns you need to cover and how much flexibility there can be in the approach? @DanielMGessel's suggestion of changing the circle radii instead of their density might make things simpler and give smoother animation but it's not yet clear how much your approach is constrained." CreationDate="2016-01-29T12:19:35.967" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="1963" PostId="1979" Score="0" Text="Do you understand how frame buffers work? https://en.wikipedia.org/wiki/Framebuffer (Caveat, it's got alot of history.) Is that some of what you are looking for?" CreationDate="2016-01-29T14:11:53.177" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1964" PostId="1982" Score="0" Text="Yes, but im personally more interested in the case where  about the case where the Bm and/or B0 are Both within the volume of A's max and min bound but does not pierce it then you need to subdivide and in worst case scenario calculate the intersection point. Better ways would be to use the minimum bounding box also known as thick line approximation." CreationDate="2016-01-29T15:23:46.557" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1965" PostId="1982" Score="0" Text="Given that, with every binary subdivision, the difference between the curve and the segment connecting the end points goes down by reasonable factor (and, off the top of my head, I think it might have been 4x for quadratics) surely the bounds are going to converge to a &quot;thin&quot; ribbon fairly rapidly." CreationDate="2016-01-29T15:44:00.973" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1966" PostId="1982" Score="0" Text="Yes but worst case scenario is that the other bezier starts at the other." CreationDate="2016-01-29T15:54:48.087" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1967" PostId="1982" Score="0" Text="You mean, for example, _An_ == _B0_.  Do you define that as an intersection or not?" CreationDate="2016-01-29T16:01:40.370" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="1968" PostId="1982" Score="0" Text="No more like B0 is At somewhere on the curve. Or even a just minimally crossing" CreationDate="2016-01-29T16:08:39.820" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1969" PostId="1982" Score="0" Text="Thanks a lot for the effort but I think the problem is the 6th case. If you propose to subdivide the curves further then this is not better than the overlapping bboxes because it could happen than after many subdivisions we still wont be able to answer &quot;yes&quot; or &quot;no&quot; definitely. But in ordered not to end up in infinite loop or floating point rounding errors we would have to stop at some point, basically guessing." CreationDate="2016-01-29T16:54:07.137" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="1971" PostId="1984" Score="0" Text="Blender supports Nurbs surfaces and a &quot;spin&quot; operator for curves. Is that the operation you want? If so you could maybe lookup how Blender implemented the operation." CreationDate="2016-01-29T23:53:59.457" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="1972" PostId="1973" Score="0" Text="Finally figured out why the VR prototype I did wasn't working. Turns out my prototype was a bit too simple, I was just doing the squish and move code to all vertex shaders and also doing all draws with an instance count of 2. This meant that when the last BLIT ran to copy the render target to the back buffer happened it also squish, moved and rendered twice causing the 4 views :) figured it out pretty quick once I'd captured in RenderDoc." CreationDate="2016-01-30T00:40:49.583" UserId="288" ContentLicense="CC BY-SA 3.0" />
  <row Id="1973" PostId="1973" Score="0" Text="BTW: Thanks again for the information on the SV_RenderTargetArrayIndex in the pixel shader now I've thought about it it makes sense that the RT output would need to be consistent across all pixels." CreationDate="2016-01-30T00:41:03.577" UserId="288" ContentLicense="CC BY-SA 3.0" />
  <row Id="1975" PostId="1984" Score="0" Text="@Dragonseel No, i know how to make a revolved surface, im only interested in how to make a silhouette curve in my own 2d software. I can do a discrete solution for this like blender would in maya, max, creo, solidworks. The only app that ive ever seen to do this acceptably is Rhino. But since this is a special case seems to me there should be a analytic solution." CreationDate="2016-01-30T06:56:11.890" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1976" PostId="1984" Score="1" Text="@Dragonseel updated the question to refelect what i said" CreationDate="2016-01-30T08:29:24.650" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1978" PostId="1984" Score="0" Text="If you figure out what happens in your first image when rotating around a vertical axis, the answer will generalize to all cases (others are rotations of that case in the image plane) - is that right?" CreationDate="2016-01-31T11:10:45.940" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1979" PostId="1984" Score="0" Text="@DanielMGessel Most likely yes, at least in the case where your shape gets thinner towards you. When the situation gets thinker towards you you get all kinds of other possible self occlusion problems. Basically this case is most likely some evolution of the swept ovals shape. But im not really certain how to find the intersection point other than by binary search which isn't really good for analytical stuff ;)" CreationDate="2016-01-31T11:16:24.907" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="1995" PostId="1984" Score="0" Text="It's a good problem. I don't know the answer - obviously ;) - just feeling it out as a problem to solve. Self occlusion - of course. That'll introduce discontinuities." CreationDate="2016-01-31T17:10:12.063" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="1996" PostId="1986" Score="0" Text="Why is it 6th degree polynomial, and not 3rd, if we are talking about cubic Beziers? And the two methods you linked to, are they amenable to finding solutions only in $[0,1]^2$, as opposed to whole $R^2$?" CreationDate="2016-01-31T19:51:51.933" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="1997" PostId="1940" Score="0" Text="I'd still have to calculate for each point gravity for each object. This seems to be the expensive part" CreationDate="2016-01-31T20:33:44.037" UserId="2524" ContentLicense="CC BY-SA 3.0" />
  <row Id="1998" PostId="1986" Score="1" Text="@EcirHana It's 6th degree because it's the squared distance. (You could square-root it, but then it's no longer polynomial, and will be non-smooth at the zeroes.) Note that the $[0,1]$ is the parameter space, not the space the splines live in, i.e. these are splines with endpoints. In any case, the methods will work fine in $\mathbb{R}^2$, but they only &quot;travel downhill&quot; from an initial guess and find a local minimum; something more is needed to examine the whole parameter region and find the _global_ minimum. Constraining the parameter space is probably helpful there." CreationDate="2016-01-31T22:00:52.463" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2000" PostId="1986" Score="1" Text="Nathan - nice formulation! I'm rusty, but: I think you can divide each bezier curve into at most 5 segments, by where $x$ or $y$ change direction in the curve. $x$, as a function of $c_i$ changes direction at most twice (roots of the derivative) breaking the curve into 3 segments, 2 of which may be divided again by changes in direction of $y$. Now you have, not straight segments, but segments that &quot;don't curve too much&quot;. I think if you start your search at 25 points, chosen by segment pairs, you could be always find the global minima, but I can't quite see how to prove (or disprove) it." CreationDate="2016-02-01T03:47:09.483" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2001" PostId="1982" Score="0" Text="Oops! I'd renumbered the initial list but not the solution cases!" CreationDate="2016-02-01T08:50:38.250" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2002" PostId="1986" Score="0" Text="@Nathan: I had considered that but, having spent much time writing code to find minima in texture compression formats, it all seemed a bit hideous." CreationDate="2016-02-01T13:30:27.697" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2003" PostId="1987" Score="0" Text="What's the paper you mentioned in your first sentence? It might give helpful context. Otherwise, it does sound like these are two mutually incompatible uses of the word &quot;fluence&quot;." CreationDate="2016-02-01T17:29:09.283" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2004" PostId="1987" Score="0" Text="I made an edit to include a link to the paper. Is there a preferred format for referencing papers?" CreationDate="2016-02-01T18:00:59.183" UserId="2457" ContentLicense="CC BY-SA 3.0" />
  <row Id="2007" PostId="1493" Score="1" Text="This should go somewhere in this thread, so adding it here :) Inigo Quilez's analytic sphere projection: https://www.shadertoy.com/view/XdBGzd" CreationDate="2016-02-02T12:11:49.200" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="2008" PostId="1995" Score="0" Text="The only thing I can think of is doing the perspective projection on the CPU, and using an ortho matrix... Does that work?" CreationDate="2016-02-03T14:07:19.980" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2009" PostId="1995" Score="5" Text="why not use glsl? You are going to need to learn to program a shader eventually why not now." CreationDate="2016-02-03T14:54:31.353" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2010" PostId="1995" Score="5" Text="It should be possible to do this using projective texture mapping, which can be done in fixed-function by providing 4-dimensional texcoords. I'd have to sit down and do the math to work out the details though." CreationDate="2016-02-03T16:48:34.523" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2021" PostId="1999" Score="0" Text="Awh shoot, just realized [Geometry shaders aren't supported in WebGL](http://stackoverflow.com/questions/8641119/webgl-geometry-shader-equivalent). :|" CreationDate="2016-02-04T05:57:24.597" UserId="71" ContentLicense="CC BY-SA 3.0" />
  <row Id="2022" PostId="1999" Score="2" Text="Why not make that your answer. Then this question is not as purposeless as it seems. (or delete offcourse)" CreationDate="2016-02-04T08:26:01.440" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2023" PostId="1999" Score="0" Text="@joojaa done and done :)" CreationDate="2016-02-04T09:29:33.060" UserId="71" ContentLicense="CC BY-SA 3.0" />
  <row Id="2024" PostId="1997" Score="0" Text="@trichoplax which part of it you did not understand?" CreationDate="2016-02-04T23:53:47.483" UserId="2602" ContentLicense="CC BY-SA 3.0" />
  <row Id="2025" PostId="1997" Score="0" Text="`transform bounding sphere radius and center of sphere in 0-1` doesn't make clear whether you want the radius normalised to 1, or transformed to some value between 0 and 1 (in which case, how is that value determined?). The same for the centre - is it to be moved to the origin or translated to somewhere within a distance 1 from the origin or something else?" CreationDate="2016-02-05T04:56:14.470" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2026" PostId="1997" Score="0" Text="`somebody pointed out to me that its not possible to transform radius into 0-1` From the comments on your [initial question](http://computergraphics.stackexchange.com/questions/1996/calculate-bounding-sphere-radius-in-normalized-space) it doesn't appear that anyone is saying that it is not possible to transform the radius. We just need to know what you mean by &quot;0-1&quot;." CreationDate="2016-02-05T05:00:12.693" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2027" PostId="1997" Score="0" Text="@trichoplax okay. I changed my algorithm. Can you check it" CreationDate="2016-02-05T05:35:09.910" UserId="2602" ContentLicense="CC BY-SA 3.0" />
  <row Id="2028" PostId="2004" Score="0" Text="I'm accepting this answer because the paper itself is good and its &quot;Related work&quot; section seems pretty comprehensive. Even if I don't end up using this technique exactly, I'm sure I'll be able to tailor something for my use case from this and its references." CreationDate="2016-02-05T09:42:17.403" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2029" PostId="1963" Score="0" Text="How is define the pattern ? as an image of pixels, or as a noise function, a procedural field, etc, with some threshold to make in B&amp;W ? in the letter case you could adapt the value of the threshold.&#xA;&#xA;Anyway, we need more information about what is the texture to be able to answer." CreationDate="2016-02-06T06:25:19.577" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2030" PostId="1997" Score="1" Text="This seems clearer now. I've edited to add in links and hovertext for the acronyms. If this changes your intention please edit to correct this." CreationDate="2016-02-06T13:18:48.847" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2035" PostId="1995" Score="0" Text="@DanielMGessel I think that's a good idea. I'll try." CreationDate="2016-02-07T05:05:58.510" UserId="2600" ContentLicense="CC BY-SA 3.0" />
  <row Id="2036" PostId="1995" Score="0" Text="@ratchetfreak Actually it's a project requirement..." CreationDate="2016-02-07T05:06:37.673" UserId="2600" ContentLicense="CC BY-SA 3.0" />
  <row Id="2037" PostId="2008" Score="2" Text="Maybe your model of the box has a wrong normal." CreationDate="2016-02-07T10:05:01.643" UserId="1888" ContentLicense="CC BY-SA 3.0" />
  <row Id="2038" PostId="1995" Score="1" Text="Nathan's right - you can undo perspective projection with a 4th coordinate (I too would have to work out the math); look into texture coordinate generation. I have vague memories that on (very) old hardware precision can crop up as an issue (I think implementing stipple this way was error prone - I never had to do it personally) - so if old hardware is why you're not able to use shaders, you may bump into some oddities at the pixel level. But it is a better approach, especially if you have alot of geometry or if you are using vertex buffers." CreationDate="2016-02-07T12:54:25.723" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2039" PostId="2008" Score="2" Text="This might not be the error, but you have a uniform &quot;viewPos&quot; which is, I assume, the camera position. But you do all calculations in Camera-Space, so this should always be $(0.0, 0.0, 0.0)$ and thereby the **viewDir** becomes $-1 * fragPos$." CreationDate="2016-02-07T14:30:11.230" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2040" PostId="2011" Score="1" Text="the first parameter of glVertexAttribPointer should be queried with glGetAttribLocation" CreationDate="2016-02-07T23:50:21.737" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2042" PostId="2012" Score="0" Text="Do you see that working well when you have N grid cells, each of which may or may not have a sphere in it that you need to test your ray against?  Maybe a bvh would be better when the grid is mostly empty, but have a worse worst case? Hrm.." CreationDate="2016-02-08T02:40:57.633" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2043" PostId="2012" Score="0" Text="I was thinking using it for the (real) sphere set only, after a first cut to the plate &quot;skin&quot;, as you said.&#xA;&#xA;Note that dynamics loops are not totally an issue as long as they are bounded; you can break/return/continue using an if inside. only the longest length for a given warp will be applied to all of its pixel.s The only issue is the code length, since loops and functions are unrolled." CreationDate="2016-02-08T03:13:24.527" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2044" PostId="2014" Score="0" Text="Is it possible that the solid red is a hole?" CreationDate="2016-02-08T14:12:08.473" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2046" PostId="2014" Score="0" Text="No I have checked that, and it is not" CreationDate="2016-02-08T15:14:53.877" UserId="2630" ContentLicense="CC BY-SA 3.0" />
  <row Id="2047" PostId="2014" Score="1" Text="Can you upload your code somewhere (gist, pastebin, etc.) and link it? We might be able to spot the error in the code." CreationDate="2016-02-08T15:37:48.797" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2048" PostId="2008" Score="0" Text="@Dragonseel  Yup!! That was the problem! Thank you for taking out some time for this question!" CreationDate="2016-02-08T17:41:38.177" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="2049" PostId="2007" Score="0" Text="I can not calculate min max corner in modelspace as after multiplying with view and projection matrices, min and max corner does not remains the same. that is why I am iterating over all the vertices.Previously I thought just using bounding box would suffice but looks like it is changing in clip coordinate space." CreationDate="2016-02-08T18:09:17.637" UserId="2602" ContentLicense="CC BY-SA 3.0" />
  <row Id="2050" PostId="2014" Score="4" Text="Problem is most likely in your normal computation. The end is a singularity in the ubderlying parametric shape which can yeld bad results if you dont know what your doing. The visible artefact number 2 is pretty common in these cases." CreationDate="2016-02-08T22:25:12.420" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2051" PostId="2007" Score="0" Text="when I checked NDC coordinates at the end in both ways, I get very different numbers. with iteration over vertices I get vertex cooridinates in range of minus thousands to plus thousands whereas with bounding boxes they are in range of -10 plus 10 approx." CreationDate="2016-02-09T00:48:54.703" UserId="2602" ContentLicense="CC BY-SA 3.0" />
  <row Id="2053" PostId="2014" Score="0" Text="@joojaa, could you explain more on the second sentence?" CreationDate="2016-02-09T08:06:49.157" UserId="2630" ContentLicense="CC BY-SA 3.0" />
  <row Id="2055" PostId="1995" Score="0" Text="@DanielMGessel I figured it out. Just use(sz,tz,0,z) and it works, where z is z-coordinate in eye space." CreationDate="2016-02-09T08:48:56.447" UserId="2600" ContentLicense="CC BY-SA 3.0" />
  <row Id="2056" PostId="2016" Score="2" Text="The normal depth value is not linear and goes against 1 relatively quickly. Are you sure it is really white as in value 1.0? Could be that the values are just close to 1.0 so that you cannot visibly distinguish them? (there are only 255 white-gray-black values but far more values in a big depth-buffer)." CreationDate="2016-02-09T10:30:13.477" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2058" PostId="2018" Score="2" Text="Oh dear, 2 answers and no picture computer graphicists on their best." CreationDate="2016-02-09T14:03:32.957" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2064" PostId="2020" Score="1" Text="@Nicol Thank you. I edited the answer accordingly." CreationDate="2016-02-09T23:54:48.630" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2067" PostId="2023" Score="2" Text="The range of numbers on the axes change between the three diagrams, and the 2nd and 3rd have more vertices labelled. Are these transformed versions of the same cube, or three separate shapes?" CreationDate="2016-02-10T03:43:37.407" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2068" PostId="2023" Score="0" Text="There is [information about syntax highlighting on Meta Stack Exchange](http://meta.stackexchange.com/questions/184108/what-is-syntax-highlighting-and-how-does-it-work) but it appears that Matlab is not supported." CreationDate="2016-02-10T03:54:42.653" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2071" PostId="2023" Score="0" Text="The first shape is an example of one of the composite worldspace shapes in its local coordinates with normals drawn on. The middel and right hand plots is the same worldspace shape transformed" CreationDate="2016-02-10T11:38:40.393" UserId="2646" ContentLicense="CC BY-SA 3.0" />
  <row Id="2072" PostId="2023" Score="3" Text="Just FYI, in a GPU, the back face culling is done (or can be considered to be done) on the post-projection values of the vertices, and thus only needs to consider the X&amp;Y values of the 3 triangle vertices. (There are a few reasons for this: avoiding problems with floating point rounding being one)" CreationDate="2016-02-10T11:53:46.063" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2073" PostId="2026" Score="0" Text="Thanks for the comment, updated my post. This is my first post on stackoverflow so I appreciate all constructive criticism haha" CreationDate="2016-02-10T15:26:21.163" UserId="2651" ContentLicense="CC BY-SA 3.0" />
  <row Id="2074" PostId="2026" Score="1" Text="This is a significant improvement! Note that [so] and [computergraphics.se] are different sites. They are both part of the Stack Exchange network but they have different rules about what is on topic. If there's any confusion feel free to drop in to [chat]." CreationDate="2016-02-10T15:37:30.790" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2075" PostId="2029" Score="1" Text="In principle, I think automatic has an advantage: when using the same vertex shader with a variety of fragment shaders, some attributes may not propogate to the output of a given pixel shader and the compiler can detect this and inform the application, allowing optimization of the vertex layout. In practice, I don't take advantage of this (for a few reasons). I too am curious to hear what the downside to automatic is..." CreationDate="2016-02-11T05:21:48.507" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2076" PostId="2030" Score="1" Text="That's just a warning that the vram is getting over-committed and could cause more memory transfers." CreationDate="2016-02-11T11:06:37.830" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2077" PostId="2014" Score="1" Text="Your question would definitely be more answerable if you included relevant parts of the code." CreationDate="2016-02-11T12:36:04.533" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2078" PostId="2031" Score="2" Text="Where have you seen that term? Stereo 3D reconstruction is a special case of multi-view reconstruction as far as I know. I have never seen the term &quot;multi-view stereo&quot;." CreationDate="2016-02-11T14:23:38.730" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2080" PostId="2031" Score="2" Text="For example http://dl.acm.org/citation.cfm?id=1153518" CreationDate="2016-02-11T14:31:27.930" UserId="2670" ContentLicense="CC BY-SA 3.0" />
  <row Id="2081" PostId="2016" Score="0" Text="I tried it from different view points and with different entities. But still all I can see is a wait blank texture." CreationDate="2016-02-11T14:59:10.137" UserId="2637" ContentLicense="CC BY-SA 3.0" />
  <row Id="2082" PostId="2016" Score="0" Text="How do you check the framebuffer texture? And have you tried to leave out the 'out_color' from the fragment shader?" CreationDate="2016-02-11T15:16:53.333" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2083" PostId="2030" Score="0" Text="@ratchetfreak: That's just a side effect then? I still don't understand why the program stops working though." CreationDate="2016-02-11T17:08:21.130" UserId="2666" ContentLicense="CC BY-SA 3.0" />
  <row Id="2084" PostId="2030" Score="0" Text="Can you share OS + GPU details?" CreationDate="2016-02-12T02:19:05.593" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2086" PostId="2034" Score="6" Text="https://en.wikipedia.org/wiki/ClearType or more generally https://en.wikipedia.org/wiki/Subpixel_rendering" CreationDate="2016-02-12T08:44:20.430" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="2088" PostId="2014" Score="0" Text="What is the source of your teapot? Glut-teapot?" CreationDate="2016-02-12T09:57:09.787" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="2090" PostId="2035" Score="0" Text="If you know the final display size, scaling your original, highest quality image to the display size with a good graphics application should produce as good as or better than letting the display system do it for you (which is likely to cut corners for performance/simplicity)." CreationDate="2016-02-12T15:08:23.783" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2091" PostId="2016" Score="0" Text="I'm using a gui system to show the texture and then I screenshot it to check it with better detail in a editor program. I removed the 'out_color' but still it is just blank white space." CreationDate="2016-02-12T15:35:00.743" UserId="2637" ContentLicense="CC BY-SA 3.0" />
  <row Id="2092" PostId="2030" Score="0" Text="@DanielMGessel: Yes, I am using Linux Mint 17.2 and my GPU is a GTX 980Ti with 6GB of VRAM." CreationDate="2016-02-12T17:00:26.943" UserId="2666" ContentLicense="CC BY-SA 3.0" />
  <row Id="2093" PostId="2037" Score="1" Text="The object in the right hand image does appear to be at a similar angle to the middle image. It's just that the axes are not the same. Is it only the axes you are asking about, or does the object itself have a problem?" CreationDate="2016-02-12T19:20:25.223" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2094" PostId="2030" Score="0" Text="The message seems to be that a buffer in VRAM is getting straight up discarded - unless you are using a ton of VRAM for something else, or you have locked up the GPU - this could be a symptom what would be a TDR on windows - if you submit a command buffer that takes too long. But that should just kill your command buffer, not your data. You could try changing the bits passed to glBufferStorage to make it dynamic and see what happens. Crosspost on an NV message board and/or file a bug report." CreationDate="2016-02-12T20:02:26.917" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2095" PostId="2037" Score="0" Text="It's essentially the axes - I would have thought that the axes should be rotated to look like the central figure. Instead, it's slightly off, as in the right figure" CreationDate="2016-02-12T20:36:08.070" UserId="2646" ContentLicense="CC BY-SA 3.0" />
  <row Id="2096" PostId="2016" Score="0" Text="Okay.  My last idea is that you try to bind a color attachment that you write a static random color into. Maybe there is some optimisation bug. I got my shadow mapping running with such a &quot;fake&quot; texture attached." CreationDate="2016-02-12T21:20:02.317" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2097" PostId="2016" Score="0" Text="When you say GUI-system do you mean some software or a self written texture renderer with orthogonal projection? If self written you may have a slight error there an hour could give the relevant shader that reads the shadow map." CreationDate="2016-02-12T21:22:34.177" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2101" PostId="2042" Score="0" Text="I was writing ray-tracing instead of raytracing, that was the error" CreationDate="2016-02-14T00:52:34.470" UserId="2684" ContentLicense="CC BY-SA 3.0" />
  <row Id="2103" PostId="2041" Score="2" Text="As for the code, there are many examples online. Take a look at SO: http://stackoverflow.com/questions/1659440/32-bit-to-16-bit-floating-point-conversion" CreationDate="2016-02-14T16:57:40.007" UserId="54" ContentLicense="CC BY-SA 3.0" />
  <row Id="2105" PostId="2045" Score="0" Text="Cool - it's nice to have an alternative to AVX. f11_f11_f10 is my priority right now, as I use it for all my linear space/HDR rendering." CreationDate="2016-02-14T22:52:45.997" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2108" PostId="2030" Score="0" Text="@DanielMGessel: I think it has something to do with how long my compute shader is working. When I am using a simple compute shader where I iterate through the whole buffer and compute an average over all the values inside, then it works with bigger buffer sizes. Are there any rules on how long a shader can be working? My error appeared when the shader was working for more than ~8 seconds." CreationDate="2016-02-15T07:55:24.873" UserId="2666" ContentLicense="CC BY-SA 3.0" />
  <row Id="2109" PostId="2045" Score="1" Text="I've added some links to R11G11B10_Float conversion implementations." CreationDate="2016-02-15T08:29:16.037" UserId="93" ContentLicense="CC BY-SA 3.0" />
  <row Id="2115" PostId="2043" Score="1" Text="&quot;The only difference is that half edge uses directional edge and winged edge uses undirectional edge.&quot; From my understanding, more like: [Half-edge](https://en.wikipedia.org/wiki/Doubly_connected_edge_list) is doubly linked (and each direction may contain additional information), while [winged edge](https://en.wikipedia.org/wiki/Winged_edge) is, most commonly, counter-clockwise only." CreationDate="2016-02-15T11:28:57.173" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="2116" PostId="88" Score="1" Text="The best way to do this is probably using atomic operations for the scattered accumulation operations (when multiple rays may hit the same point), but you could render your entire &quot;ray cloud&quot; as points and do the intersection and occlusion calculations in a vertex shader, use a passthrough pixel shader and let the HW blender do the accumulation." CreationDate="2016-02-15T13:55:00.150" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2117" PostId="2049" Score="0" Text="It is customary for the camera to face in the negative z direction. Perhaps you have view in positive direction? Sorry no time to debug." CreationDate="2016-02-15T20:57:51.090" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2118" PostId="2049" Score="0" Text="I'm unsure if I got the camera facing in the right direction - my main thought was that the view frustum was still point towards the object from the origin so that should be satisfactory? Either way I dont think it transforms correctly...." CreationDate="2016-02-15T21:16:40.003" UserId="2646" ContentLicense="CC BY-SA 3.0" />
  <row Id="2120" PostId="2046" Score="0" Text="Links aren't allowed to be answers by themselves,  but here is a link that explains it pretty well, and has simple working source code: http://blog.demofox.org/2015/07/28/rectangular-bezier-patches/" CreationDate="2016-02-16T00:37:18.353" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2121" PostId="2050" Score="1" Text="Follow your answer, I found out that the Modified Conjugate Gradient method is proposed to deal with the non-symmetric semi-positive-definite linear equation(section 5.2). Thanks a lot." CreationDate="2016-02-16T01:14:47.317" UserId="2593" ContentLicense="CC BY-SA 3.0" />
  <row Id="2123" PostId="2043" Score="0" Text="So, you mean the way they use doubly linked simply to add more info explicitly? Because I think by using Half Edge there might be some performance gained for specific query from the mesh. But till now, I still cannot figure which query.." CreationDate="2016-02-16T07:28:43.660" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="2124" PostId="2047" Score="0" Text="I don't think it is what I had to begin with - although on a brief glance it should seem that e &lt;=&gt; C and  x,y,z &lt;=&gt; U,V,N, when I change the code to use your method, the results are starting to seem more intuitively correct!" CreationDate="2016-02-16T10:13:23.190" UserId="2646" ContentLicense="CC BY-SA 3.0" />
  <row Id="2125" PostId="2046" Score="0" Text="@AlanWolfe , that's a great article which explains much, but nevertheless it says about 16 control points for bicubic surface, which I'm trying to evade" CreationDate="2016-02-16T10:43:47.063" UserId="2694" ContentLicense="CC BY-SA 3.0" />
  <row Id="2126" PostId="2052" Score="0" Text="I've performed the perspective divide - (see update) - and my result is now a cube that has coordinates of +/- 1 in all axes (seems right!) although the z plane has been reflected such that the far plane is now at +1 and the near plane at -1 - is this correct? Thanks very much" CreationDate="2016-02-16T10:58:02.583" UserId="2646" ContentLicense="CC BY-SA 3.0" />
  <row Id="2127" PostId="2047" Score="2" Text="In your matrix you have a &quot;c&quot; superscript on the x, y and z components (camera space). I added a description of how to derive the eye local space based on the gluLookat parameters. I think that pyramid in front is pretty confusing; I'd use one box to start with. Try plotting looking down the axi first, make sure you get the correct rectangles. Also, anything that can let you animate changing the parameters will help clear things up immensely. A single wireframe picture is always hard to decode..." CreationDate="2016-02-16T12:31:51.497" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2128" PostId="2046" Score="0" Text="Ah, sorry for missing that part" CreationDate="2016-02-16T14:25:53.657" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2129" PostId="2052" Score="0" Text="Additionally - should the aspect ratio of the view volume be taken into account in the size - or does this simply lead to a slight deformation of the shape rather than the [1, -1] cube deformation?" CreationDate="2016-02-16T15:16:48.743" UserId="2646" ContentLicense="CC BY-SA 3.0" />
  <row Id="2130" PostId="2048" Score="0" Text="thank you. Do I understand you correctly, that in my case using NURBS the user could first specify points on surface, and then fine-tune the surface by fine-tuning section curves? And I still cannot get how I can make Bezier (or NURBS) patch with control points and knots only on edges. Can you show your second image with control points and edges?" CreationDate="2016-02-16T16:43:55.223" UserId="2694" ContentLicense="CC BY-SA 3.0" />
  <row Id="2131" PostId="2048" Score="0" Text="@morodeer theres are infinite ways of doing this. But i was drawing a image for you allready just dont have time to complete it" CreationDate="2016-02-16T17:19:11.770" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2132" PostId="2052" Score="1" Text="@davidhood2 That sounds right, yes. Post-projective space is left-handed (+x = right, +y = up, +z = into screen). As for the aspect ratio, that should be handled by x and y scale factors in the projection matrix. Post-projection, x and y should be in [−1, 1] regardless of aspect ratio." CreationDate="2016-02-16T17:28:29.150" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2133" PostId="2048" Score="0" Text="I think, I understood almost everything: should I use the same algorithm for mid-points? PS: I think there are some mistakes in points and lengths, but image makes it clear." CreationDate="2016-02-16T18:40:20.657" UserId="2694" ContentLicense="CC BY-SA 3.0" />
  <row Id="2134" PostId="2048" Score="0" Text="@morodeer yes same algo works fine" CreationDate="2016-02-16T19:00:10.883" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2135" PostId="2048" Score="0" Text="ok, and the algorithm you described looks more like blending between 3 curves, but not as interpolating between curve network, right? Do you know equally elegant way to interpolate mid-points in case of curve network?" CreationDate="2016-02-16T19:02:51.053" UserId="2694" ContentLicense="CC BY-SA 3.0" />
  <row Id="2136" PostId="2048" Score="0" Text="@morodeer Thats what you asked. It is forming a interpolation, curve networks just split up and insert knots in the underlying surface least if its a square case," CreationDate="2016-02-16T19:27:37.363" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2137" PostId="2043" Score="0" Text="While we're on edge-representations, this is a great paper, generalising a lot of them: http://graphics.cs.ucdavis.edu/~joy/ecs178/Unit-9/resources/Tahoe-Paper.pdf" CreationDate="2016-02-16T19:56:33.423" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="2138" PostId="2058" Score="0" Text="Though in general it is far easier to just use the construction joojaa explained in his answer." CreationDate="2016-02-16T23:27:35.177" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2139" PostId="2060" Score="3" Text="I don't know a lot about voxel-based rendering, but I always got the impression voxel-based raytracers intersected the rays directly with the voxels, instead of constructing a polygonal representation of the voxels first." CreationDate="2016-02-17T15:14:39.980" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2140" PostId="2060" Score="0" Text="Neither am I, I just read about rendering using voxelization so I might have a wrong idea on the technique." CreationDate="2016-02-17T15:26:39.277" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="2141" PostId="1861" Score="0" Text="The problem for me is projecting the actual beam, like if the light is passing through fog, from the source to the floor... The simplest way i think would be &quot;projecting&quot; a fake beam, a cone of a fixed lenght, made of triangles, from the lens of the light, towards the direction in which the light is facing, with some alpha blending." CreationDate="2016-02-17T17:28:01.917" UserId="2388" ContentLicense="CC BY-SA 3.0" />
  <row Id="2142" PostId="1861" Score="0" Text="But this way the beam would pass through the objects, so i need a way to stop it at the first intersection... a very simple way would be raytracing the distance from the center of the lens to the floor, and then making the cone of that lenght. but its not a good way..." CreationDate="2016-02-17T17:34:00.240" UserId="2388" ContentLicense="CC BY-SA 3.0" />
  <row Id="2143" PostId="2053" Score="0" Text="Where the lens are located in your model? Onto the screen plane? Where is the aperture size?" CreationDate="2016-02-17T18:15:08.913" UserId="2684" ContentLicense="CC BY-SA 3.0" />
  <row Id="2144" PostId="2059" Score="0" Text="Thank you very much for your answer, could you tell me more about stratified sampling and low-discrepancy sequence ?" CreationDate="2016-02-17T18:38:36.007" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="2145" PostId="2029" Score="0" Text="The accepted answer on this post argues for the pro-explicit side, maybe you could take a look at why he thinks so.&#xA;http://stackoverflow.com/questions/4635913/explicit-vs-automatic-attribute-location-binding-for-opengl-shaders" CreationDate="2016-02-17T19:30:35.047" UserId="2651" ContentLicense="CC BY-SA 3.0" />
  <row Id="2146" PostId="2029" Score="1" Text="Reusing VAOs with multiple shaders that use the same vertex layout. If you let the compiler decide, you have potentially need a VAO for each VB for each compiled program, even if they use exactly the same attributes. Sounds good to me! I should look up what Vulkan does, now that the spec is out..." CreationDate="2016-02-18T02:01:04.087" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2147" PostId="2063" Score="0" Text="What transformations are you applying? Or are these coordinates directly in clip space with no transformations? And how are you triangulating the square?" CreationDate="2016-02-18T04:21:46.547" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2148" PostId="2063" Score="0" Text="No transformations, I edited my post to show code" CreationDate="2016-02-18T04:25:14.960" UserId="2651" ContentLicense="CC BY-SA 3.0" />
  <row Id="2157" PostId="2068" Score="0" Text="Wouldn't `norm` be better at normalizing also you can write `R[U, 0; V, 0; -N, 0; 0, 0, 0, 1]`" CreationDate="2016-02-18T22:23:28.900" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2158" PostId="2068" Score="1" Text="You are right about norm, but sqrt(dot(x,x)), is the same (I do not know octave/matlab very good, so I use math). Regarding matrix - not really, because U, V, N are columns, correct way would be R = [U', 0; V', 0; -N', 0; 0,0,0,1] :-) but I tried to keep original formatting." CreationDate="2016-02-18T22:34:29.107" UserId="43" ContentLicense="CC BY-SA 3.0" />
  <row Id="2159" PostId="2068" Score="0" Text="Its the same but really ease of reading would make the code much better... As the original code is a totall mess. Which is mostly why there were not many takers. And the fact that its matlab..." CreationDate="2016-02-18T22:47:25.633" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2160" PostId="2062" Score="0" Text="Thanks for the great answer! Would it be possible to allocate texture or geometry data to just individual cards then? The link seemed to indicate that computing could be distributed, which is makes sense, but I'm still confused as to how or if geometry or textures could be split." CreationDate="2016-02-19T02:12:22.923" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="2161" PostId="2062" Score="0" Text="@aces Yes, you can allocate geometry or textures to just individual cards—you'd just create those resources only on one node. Every time you create a resource, you specify which node to put it on, so you have total control over which GPUs get copies of which resources." CreationDate="2016-02-19T02:20:00.133" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2162" PostId="2062" Score="0" Text="That makes sense, but would that be feasible in a game? I would think you would have to synchronize all the fragments if you did that somehow." CreationDate="2016-02-19T02:22:04.977" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="2163" PostId="2062" Score="0" Text="@aces &quot;Synchronize all the fragments&quot;? You've lost me. :)" CreationDate="2016-02-19T02:22:49.977" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2164" PostId="2062" Score="0" Text="Sorry :). In a simple example, if you have one model with a certain texture on one GPU and another model with a different texture on another GPU, they would both need to be rasterized and shaded, so the frame buffer would need to contain both of this information in the end for depth tests. I guess I'm just confused on how that could utilize VRAM stacking." CreationDate="2016-02-19T02:25:53.217" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="2165" PostId="2062" Score="3" Text="@aces OK, yeah. If you distribute the rendering across GPUs in some way, you have to put the results back together somehow afterward. So in your example, after rendering, you could copy the color &amp; depth buffers from GPU2 back to GPU1, and use GPU1 to composite the two frames together. That would take some extra time, which eats into the time you saved by distributing the rendering in the first place, so it might or might not be an overall perf win depending on circumstances." CreationDate="2016-02-19T02:33:24.280" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2166" PostId="2062" Score="0" Text="Ah ok, so I guess if the bus latency/bandwidth isn't too bad that could work. Thanks for the clarification!" CreationDate="2016-02-19T02:36:16.770" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="2167" PostId="2071" Score="0" Text="Ah cool neat idea. And yeah, I'm just doing nearest neighbor sampling. I'll edit the question to reflect that." CreationDate="2016-02-19T05:14:05.223" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2168" PostId="2071" Score="1" Text="Given you are storing 0-1 values where, probably, having high precision near 0 is of little value, the exponents of the f16 may be wasted space. Something to think about if you are looking to tighten things up as much as possible. Bit packing can be especially useful in vertex data before any interpolation happens; in one case, I store texture array coordinates in 4 bytes and use the alpha channel to add extra bits to x &amp; y; this gives me texel precise addressing for up to a 4k x 4k x 256 texture." CreationDate="2016-02-19T05:29:18.677" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2169" PostId="2071" Score="0" Text="Good to know! In my case unfortunately all I have is a full screen pixel shader, I don't have geometry. But, on the plus side, the buffer can store values outside of 0 to 1.  It's a full 16 bit floating point number which is nice." CreationDate="2016-02-19T05:33:37.603" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2170" PostId="2071" Score="0" Text="When you talk about the exponent bits being wasted are you talking about doing something like storing two values in a single float, where you make them be of different scales and add then, so that you can use mod and div to get the values out again? Do you have any working details of that by chance?" CreationDate="2016-02-19T05:52:11.697" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2171" PostId="2072" Score="0" Text="Slerp is a possibility for normals. It stands for &quot;spherical lerp&quot;—unit vectors lie on a sphere as much as unit quaternions do." CreationDate="2016-02-19T07:08:04.263" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="2172" PostId="2071" Score="1" Text="Basically, the exponent scales an otherwise fixed point value (the mantissa) with the size of the value. If you don't benefit from that, then fixed point numbers (ints or GL &quot;norms&quot;) utilize the bits better. It may not apply in your case for any number of reasons (does WebGL support integers? I'm out of date). I do use mod and div to unpack when I need to (also, before GLSL supported ints)." CreationDate="2016-02-19T13:28:07.083" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2173" PostId="2071" Score="1" Text="I can rant on floats, so I won't go there. Fixed point can be a better choice (ask if more precision near 0 is useful - with positional values fp is often more of a convenience; in a linear color space, fp matches our perceptual system beautifully). Just something to consider when concerned with bandwidth." CreationDate="2016-02-19T13:56:30.313" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2174" PostId="2072" Score="1" Text="I've found out that nlerp is a decent method: just interpolate your components and re-normalize.  I can do a bicubic version of this by doing each component individually.  Found some decent links too: https://keithmaggio.wordpress.com/2011/02/15/math-magician-lerp-slerp-and-nlerp/  and http://number-none.com/product/Understanding%20Slerp,%20Then%20Not%20Using%20It/ ." CreationDate="2016-02-19T14:50:07.500" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2175" PostId="2072" Score="1" Text="If you know your grid vectors don't vary more than, say, 120 degrees  from grid point to grid point (i.e. trying to avoid problems with &quot;which way do you interpolate), why wouldn't bicubic interpolation with renormalisation be &quot;good enough&quot;?" CreationDate="2016-02-19T14:55:46.257" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2176" PostId="2069" Score="0" Text="I am currently using the code to provide 2d rendering support for sprites that position the image pixel perfect to the desired position on the screen in pixel coordinates instead of opengl coordinates. As such I currently do not plan to allow batch rendering of multiple quads but instead use the quad as a &quot;stamp&quot; to render the image to the screen. As such each quad drawn has of course a different texture or different texture coordinates to read from. As such I also never know how many quads are drawn beforehand." CreationDate="2016-02-19T19:19:33.343" UserId="2623" ContentLicense="CC BY-SA 3.0" />
  <row Id="2177" PostId="2069" Score="1" Text="@salbeira OK. You could still combine sprites in an atlas to allow instancing. Even if you don't do that, though, you can still batch the uniform buffer updates. And you can batch even while not knowing how many quads are drawn beforehand—I added a paragraph to the answer about that." CreationDate="2016-02-19T20:08:47.280" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2178" PostId="2069" Score="0" Text="I can imagine how this should work but it still seems a bit like a mystery to me. Like when I supply a method to &quot;render a single spirte&quot; that has a texture attached to it and information about where to find the sprite within that texture, I'd need to pile that data up and store it in a huge prepared array. When I want to draw that I'd still need to call a drawElements for each sprite, and bind each texture that contains that sprite sparately. Do I magically combine each texture with a framebuffer to a huge atlas before rendering? Or should I just say to the application dev to use an atlas?" CreationDate="2016-02-20T03:04:03.583" UserId="2623" ContentLicense="CC BY-SA 3.0" />
  <row Id="2179" PostId="2074" Score="0" Text="After you fix the main problem, if you want better image quality you should try bicubic interpolation instead of bilinear.  http://blog.demofox.org/2015/08/15/resizing-images-with-bicubic-interpolation/" CreationDate="2016-02-20T05:41:31.877" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2180" PostId="2069" Score="0" Text="@salbeira Yeah, designing the interface for this sort of thing is tricky. You could supply a method to &quot;add a sprite to the render list&quot; and another method to &quot;render all the sprites on the list&quot;, so the user would be responsible for calling that when the rendering was finished. If the rendering order of the sprites doesn't matter, you could also sort them by texture, so that you don't have to switch textures so many times and you can use instancing in a group of sprites with the same texture. Then your system would work with or without a user-provided atlas (but faster with)." CreationDate="2016-02-20T07:45:05.573" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2181" PostId="2068" Score="0" Text="Thanks very much - I know the original code is a mess - please accept my apologies! I've tried to do it using only simple matlab functions because I will be porting it to an FPGA (in VHDL) - and am simply trying to model the maths!" CreationDate="2016-02-20T18:26:26.127" UserId="2646" ContentLicense="CC BY-SA 3.0" />
  <row Id="2185" PostId="2060" Score="1" Text="You don't want to do voxel rendering, right ? since you would no longer need triangles. You wan't to do an optimisation structure sorting the scene triangles withing regular grid cells, right ? (but there are several variants, e.g. spliting the triangles or not. which is yours ?)" CreationDate="2016-02-21T00:49:57.720" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2186" PostId="2069" Score="0" Text="I guess this will also not work since I'd like the user of these functions to be able to apply the painters algorithm if needed and also the bound framebuffer, color modifiers and stencil information all need to be applied in order" CreationDate="2016-02-21T04:17:07.203" UserId="2623" ContentLicense="CC BY-SA 3.0" />
  <row Id="2187" PostId="2081" Score="0" Text="how the objects in the scene are defined? In ray tracing you calculate intersections using the implicit function, ie, sphere, plane, cone, etc, how is it in ray marching? also can you expaling the meaining of the variables that you wrote in your post?" CreationDate="2016-02-21T04:21:20.363" UserId="2684" ContentLicense="CC BY-SA 3.0" />
  <row Id="2188" PostId="2081" Score="0" Text="I mimicked your very definition, which did not defined meshes :-). For volume rendering, the scene is made of density stored in each voxel, i.e. each cell of the 3D grid." CreationDate="2016-02-21T09:39:25.080" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2189" PostId="2081" Score="0" Text="I modified the equation and completed the definition. Now for such basic definitions, you should rather google or open a book. ;-) https://en.wikipedia.org/wiki/Volume_rendering" CreationDate="2016-02-21T09:42:41.673" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2191" PostId="2078" Score="0" Text="What you are saying may be a valid thing I haven't heard of before, but for the purposes of high performance, I've always heard of scan line conversion working like you convert 3d object to 2d screenspace objects and then use something like bressenham (for triangles, drawing a line down each edge, one y step at a time) to know where to start and end on the x axis for the current y.  In your case I could see similar working, where you do that N times, where each index of N is a slice of the Z axis.  I haven't ever done what you are trying though so can't say for sure if that is a good idea." CreationDate="2016-02-21T21:31:20.850" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2192" PostId="2083" Score="1" Text="NB:  they don't answer emails about these questions :-)" CreationDate="2016-02-21T21:45:19.160" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2193" PostId="2078" Score="0" Text="Whenever it comes to scanline conversion for 2D graphics, definitely it is not a good idea. But in a particular technology of 3d printing, this type of printing head manuevoring has been deemed to effective to generate better printing outputs. This is why I want to find out intersection between scan line segment [(x1,y1)------(x2,y2)] parallel to x-axis or y-axis and polygon edges." CreationDate="2016-02-21T23:33:27.227" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2194" PostId="2073" Score="0" Text="Thanks for taking the time, on both the answer and the ShaderToy example!" CreationDate="2016-02-22T14:16:06.993" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="2196" PostId="397" Score="0" Text="Why do you think Wavelet based Denoising would be best?" CreationDate="2016-02-23T07:04:54.733" UserId="234" ContentLicense="CC BY-SA 3.0" />
  <row Id="2204" PostId="2088" Score="0" Text="Even if we know the order theres till a infinite number of curves that fit trough the points. Even if we add additional constraints, then the open ends are problematic as their tangent orientation can be arbitrary. A [picture here](http://i.imgur.com/nlWqkRT.png)" CreationDate="2016-02-23T16:17:59.463" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2207" PostId="2088" Score="0" Text="@joojaa Yes, you are right. But since the packing of points is very dense, I don't expect it to be exact. If I do get to have the right order, I was planning to connect the sequence of points as a polyline." CreationDate="2016-02-23T16:41:32.440" UserId="2562" ContentLicense="CC BY-SA 3.0" />
  <row Id="2209" PostId="2088" Score="0" Text="In the code that needs to order the points, are you even aware of the parametric form of the curve? (If not, I'll delete my first answer, because it requires you to know the parametric form.)" CreationDate="2016-02-23T16:49:33.153" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2210" PostId="2088" Score="0" Text="@MartinBüttner Yes, I do have access to the parametric form of the curve, if it's needed." CreationDate="2016-02-23T16:51:17.707" UserId="2562" ContentLicense="CC BY-SA 3.0" />
  <row Id="2211" PostId="2088" Score="0" Text="@andrea.al i do secondary curve fitting for cad data all the time and I was not entirely happy with the results every second time. Thats why I have started to inject normal's to the starting points for the fitting. Problem is that curves tend to lose it in certain situations." CreationDate="2016-02-23T17:29:49.320" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2214" PostId="397" Score="0" Text="@poor, What make you think that (Not that I think the other way, I'm just wondering)?" CreationDate="2016-02-24T06:42:29.020" UserId="234" ContentLicense="CC BY-SA 3.0" />
  <row Id="2215" PostId="2088" Score="1" Text="Please show a typical point set !" CreationDate="2016-02-24T09:26:57.800" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="2217" PostId="2078" Score="0" Text="Your question is still unclear but I figure that you are trying to fill a polygon with a Hilbert curve, right ? So you are asking about an algorithm that finds the useful portions of a Hilbert curve inside a given polygon ? If yes, do you want whole segments only or precise clipping to the polygonal window ?" CreationDate="2016-02-24T11:03:19.857" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="2218" PostId="397" Score="0" Text="@Drazick I've read this somewhere 2-3 years ago. Not sure, but I guess it's probably a mixture of different algorithms for different situations. May I ask why you are interested in this? Is there any better approach? *Note: I'm not a physicist. However I'm using denoisers for video very frequently and I'm just curious how they work* :) Also see: http://dsp.stackexchange.com/questions/20086/denoise-images-with-wavelets" CreationDate="2016-02-24T12:26:51.637" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="2219" PostId="2078" Score="0" Text="You guessed right!. I am looking for the whole segments that reside inside the given polygon contour. In the usual scan line conversion we have parallel scan lines that fill the inside  region and in my specific case I am using Hilbert curve to fill the inside region.  Here goes a sample [image](http://i.imgur.com/NTtPYoB.jpg) that shows the pattern. As you can see that intersection test has to be performed with hilbert line segments and polygon outer and inner borders to define the inner region and I need tips in intersection test." CreationDate="2016-02-24T21:07:50.960" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2220" PostId="397" Score="0" Text="I would also guess Wavelets, but here - https://ni.neatvideo.com/overview/how-does-it-work the imply something else. Regarding the approach, I'm not sure, Non Local Means should also be good but harder to tune and slower to run." CreationDate="2016-02-25T06:56:27.483" UserId="234" ContentLicense="CC BY-SA 3.0" />
  <row Id="2221" PostId="397" Score="0" Text="@Drazick That's neat image, a stand alone app to denoise *images*." CreationDate="2016-02-25T10:46:52.927" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="2222" PostId="397" Score="0" Text="It is the same for Video. But you deleted the comment which creates the context." CreationDate="2016-02-25T11:02:13.980" UserId="234" ContentLicense="CC BY-SA 3.0" />
  <row Id="2224" PostId="2094" Score="0" Text="Everything looks ok. Bugs like these are the worst. :( Your quad coordinates are ok. DX Clip space is from (-1.0, 1.0). 2 suggestions: First, enable a debug Device using D3D11_CREATE_DEVICE_DEBUG. And see if anything interesting pops up in the output. Second, try capturing a frame in RenderDoc: https://github.com/baldurk/renderdoc Look at the call graph and see if anything is fishy." CreationDate="2016-02-25T23:28:56.167" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2225" PostId="2096" Score="0" Text="Jason Allen is also correct. You will need to do both to get your program rendering." CreationDate="2016-02-26T00:25:59.260" UserId="2752" ContentLicense="CC BY-SA 3.0" />
  <row Id="2226" PostId="2095" Score="0" Text="Fixed it, no change. See my comment on icStatic's answer" CreationDate="2016-02-26T01:41:21.753" UserId="2726" ContentLicense="CC BY-SA 3.0" />
  <row Id="2227" PostId="2096" Score="0" Text="I added more code that is in my project. My D3D11 Initialization and my swap chain resize. Initialization creates rasterization states and also sets one of them. Swap chain makes/remakes the viewport according to screen current size." CreationDate="2016-02-26T01:42:39.307" UserId="2726" ContentLicense="CC BY-SA 3.0" />
  <row Id="2228" PostId="2094" Score="0" Text="Have you tried temporarily turning off back-face culling (`D3D11_CULL_NONE` instead of `D3D11_CULL_BACK`) to see if there's a winding order problem?" CreationDate="2016-02-26T04:00:33.473" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2229" PostId="2094" Score="0" Text="just did, didn't work" CreationDate="2016-02-26T04:04:04.400" UserId="2726" ContentLicense="CC BY-SA 3.0" />
  <row Id="2233" PostId="2094" Score="0" Text="I posted a picture, could those warnings be causing an issue?" CreationDate="2016-02-26T12:18:30.087" UserId="2726" ContentLicense="CC BY-SA 3.0" />
  <row Id="2234" PostId="2096" Score="0" Text="I posted a picture, could those warnings be causing an issue?" CreationDate="2016-02-26T12:18:34.900" UserId="2726" ContentLicense="CC BY-SA 3.0" />
  <row Id="2238" PostId="2099" Score="0" Text="Its a bit hard to say what something looks like depends on where your camera is and whet Leese you have going on in the scene." CreationDate="2016-02-26T14:20:59.543" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2239" PostId="2099" Score="0" Text="There is only this one cube in the scene. It is slowly rotating around its z axis and slowly going into the distance (this seems to work fine)." CreationDate="2016-02-26T14:39:16.877" UserId="2508" ContentLicense="CC BY-SA 3.0" />
  <row Id="2240" PostId="2099" Score="1" Text="Yes, but that does not help us if we cant see your matrix, etc. You dont have any code that i can help you with. Your supposed to give minimal code to reproduce the problem otherwise theres no debugging to be had. Yes cubes can look very elongated if they are very close to the camera plane." CreationDate="2016-02-26T14:58:51.237" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2241" PostId="2099" Score="0" Text="@joojaa sorry. I was a bit reluctant to provide any additional code because I am writing all of this in assembly. I supposed that would not help if I posted it here :)" CreationDate="2016-02-26T15:52:36.163" UserId="2508" ContentLicense="CC BY-SA 3.0" />
  <row Id="2245" PostId="2104" Score="2" Text="To expand a little on the solution you found: with values like 1366 and 768 in `glFrustum`, you were effectively setting an extremely wide field of view (nearly 180 degrees), which caused extreme perspective distortion; that's why the cube looked weirdly stretched out. You might prefer to use `gluPerspective` instead of `glFrustum`, as the parameters there are more intuitive." CreationDate="2016-02-27T01:22:06.027" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2246" PostId="2103" Score="0" Text="I don't get it. What is the relation between the MERL database (and the mystery of it's references for colors) and BRDF viewer ? Or do you think there is a special mode dedicated to MERL ? but just browsing the BRDF does not require to accurately depict the colors, right ?" CreationDate="2016-02-27T02:55:41.603" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2247" PostId="2103" Score="0" Text="I hoped they would interpret the colours giving you a hint.  They also use two databases so the might have to interpret to unify. Sounds like they don't." CreationDate="2016-02-27T03:01:03.483" UserId="2748" ContentLicense="CC BY-SA 3.0" />
  <row Id="2248" PostId="2093" Score="0" Text="What do you consider ligtweight and why not use opengl or matplotlib?" CreationDate="2016-02-27T09:49:06.657" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2249" PostId="2106" Score="0" Text="How are the objects represented? Surfaces modeled with triangles?" CreationDate="2016-02-28T03:44:26.493" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2250" PostId="2106" Score="0" Text="Yes, they are triangular poly meshes." CreationDate="2016-02-28T04:18:50.750" UserId="2748" ContentLicense="CC BY-SA 3.0" />
  <row Id="2251" PostId="2105" Score="0" Text="Are you working with integers or floating point numbers?" CreationDate="2016-02-28T04:34:39.430" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2252" PostId="1662" Score="0" Text="Not a full answer but the smaller amount of memory you use the better, as it will be more likely to fit in caches and have fewer cache misses.  If you have interpolatable values, like you are baking out points on a curve into textures, you might check this out as a way to get higher quality curve lookup tables with less memory: http://blog.demofox.org/2016/02/22/gpu-texture-sampler-bezier-curve-evaluation/" CreationDate="2016-02-28T04:51:46.943" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2253" PostId="2105" Score="0" Text="Floating point numbers." CreationDate="2016-02-28T11:44:50.393" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2254" PostId="2105" Score="0" Text="ah ok, assumed so.  The answer I gave is appropriate for your usage case then.  If using integer coordinates there would be a way to get an EXACT answer without thresholding, but with floating point it's basically unavoidable." CreationDate="2016-02-28T21:04:44.877" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2255" PostId="2108" Score="0" Text="The above explanation is focused on clarity, but there are some other optimizations you can do if you need it to be even faster.  For instance, instead of using a distance threshold, you can use a squared distance threshold, to avoid a square root.  You could also avoid the normalization step, and clamp between 0 and 1 instead of from 0 to length.  The above explanation should be plenty fast for most needs though!" CreationDate="2016-02-28T21:06:22.633" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2256" PostId="2107" Score="0" Text="This is for the LUT, but it tells nothing for the gamut, right ? or does it ? (moreover I understood that their capture device was more home-made than the standard camera - and not all camera captors have the same raw gamut anyway)." CreationDate="2016-02-28T21:19:20.050" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2257" PostId="2106" Score="0" Text="approximate (conservative or not ?) or exact ? GPU assisted of pure CPU ? rigid object or deformable ? are the 2 objects arbitrary complex or at least one of both has a simple shape ?" CreationDate="2016-02-28T22:02:38.030" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2258" PostId="2106" Score="0" Text="Minkowski portal refinement and also GJK  are algorithms commonly used for this." CreationDate="2016-02-29T04:47:15.370" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2260" PostId="2103" Score="0" Text="If the tool you mention can be used to provide an answer, it would help to explain how. As it stands, this is more of an additional question than an answer." CreationDate="2016-02-29T12:05:49.693" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2262" PostId="2108" Score="0" Text="You are suggesting a dot product between a point and vector. Should it not be between two vectors. In that case, we have to derive vector from point. We can do it by generating a vector between the point and the start point of the line segment" CreationDate="2016-02-29T16:15:17.363" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2265" PostId="2108" Score="0" Text="You are right, good eye.  Fixed!" CreationDate="2016-02-29T16:23:50.907" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2266" PostId="2093" Score="0" Text="@joojaa Saying &quot;lightweight&quot;, I mean not a game framework, because I don't need a lot of features just to visualize models. Matplotlib took several seconds to render the model; mayavi does it fast, but I don't need a frame with image on it -- I need just a matrix with colors. I started meeting with PyOpenGL and didn't find, how to render triangulated model and get buffer as a matrix instead of displaying it." CreationDate="2016-02-29T17:09:13.360" UserId="2750" ContentLicense="CC BY-SA 3.0" />
  <row Id="2267" PostId="2117" Score="0" Text="Here is the texture I'm using: &#xA;http://i.stack.imgur.com/SEi0G.png" CreationDate="2016-02-29T19:19:59.807" UserId="2775" ContentLicense="CC BY-SA 3.0" />
  <row Id="2268" PostId="2119" Score="1" Text="I've started experimenting with some of this already.  I've found that sampling in a + sign (5 reads) instead of the full 9 showed no differences in my testing, but im sure with more complex situations, there would be differences.  Doing a full x jfa and then a full y jfa does make lots of errors.&#xA;&#xA;I'll be interested to hear more details/info if you have it, but accepting your answer :P" CreationDate="2016-02-29T20:43:56.783" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2269" PostId="2119" Score="1" Text="Forgot, here's link to one of my experiments:&#xA;https://www.shadertoy.com/view/Mdy3D3" CreationDate="2016-02-29T20:44:08.737" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2270" PostId="2119" Score="0" Text="Interesting that it works apparently just as well with only 5 reads - especially since they can't be parallelised. Since the paper lists the cases that lead to error maybe you could deliberately set these up and see if 5 jump directions is still as good." CreationDate="2016-02-29T20:49:41.167" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2271" PostId="2119" Score="0" Text="Sounds like you are ready to post your own answer..." CreationDate="2016-02-29T20:50:13.827" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2272" PostId="2118" Score="1" Text="Thanks, I'll try this. It was one of my original methods of building the polygon, however much more complex mathematically so I went with the simpler method. I'll give it another shot and report back." CreationDate="2016-02-29T20:58:35.013" UserId="2775" ContentLicense="CC BY-SA 3.0" />
  <row Id="2273" PostId="2119" Score="0" Text="my info supplements yours :P" CreationDate="2016-02-29T21:07:27.703" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2274" PostId="2108" Score="0" Text="Dont we have to normalize the vector &quot;PRelative&quot; ?" CreationDate="2016-02-29T21:10:42.860" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2275" PostId="2108" Score="0" Text="No, you don't and shouldn't!  The length of PRelative is what is projected onto the direction vector (direction must be normalized though).  Are you having problems getting it working still?  If easily done you might try visualizing the various steps of finding the closest point on the line to see where it's breaking down." CreationDate="2016-02-29T21:14:21.913" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2276" PostId="2108" Score="0" Text="I am defining the vector between closestPoint and P and then getting the length of the vector . If the length of the vector is less than equal to std::numerical_limit&lt;float&gt;::epsilon() , I return true, else return false. Is there anything worng with the concept ? I am still getting the wrong answer." CreationDate="2016-02-29T22:25:58.587" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2277" PostId="2108" Score="0" Text="i would make the epsilon larger than that.  The error that creeps in from floating point calculations is likely much larger than the smallest possible difference between two floating point numbers.  I'd try a larger than needed number to start out, and then shrink it down to the smallest value that feels right." CreationDate="2016-02-29T22:27:40.800" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2280" PostId="2123" Score="9" Text="Don't apologise, I think it's a fair question, seeing that GLM's function naming is quite misleading here. I expect this could be a useful (and concise) reference in the future." CreationDate="2016-03-01T09:52:48.017" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2282" PostId="2108" Score="0" Text="It works!, Some of the issues are not clear though. When to or not to normalize vectors. Since the normalization of a vector does not change its direction other than clamping its value between zero and one, it would be nice if some one elaborate more over this issue of when to normalize." CreationDate="2016-03-01T13:54:24.103" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2283" PostId="2108" Score="1" Text="You might consider asking some new questions about that, perhaps even on the math stack exchange sites.  You might also give dot product / vector projection a Google and read up on that a little bit, there is some great info on the net on that stuff.  Congratulations though, I'm really glad to hear you got it working (:" CreationDate="2016-03-01T14:53:03.910" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2284" PostId="2117" Score="1" Text="I assume the issue is the shearing of the image? I see this happens where you have inconsistent spacing between the vertices of the triangular subdivision. My guess is you are using consistent steps for your uv mapping, giving you the shearing effect." CreationDate="2016-03-01T23:15:41.800" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2285" PostId="2128" Score="3" Text="I don't really think you can be more efficient than a equality check for one coordinate of two points. And as far as I see this check is correct. So... What is your issue with this solution?" CreationDate="2016-03-02T11:57:06.680" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2286" PostId="2128" Score="2" Text="It may (or may not) be *slightly* faster to subtract the points and check whether one component of the difference is zero. Dragonseel has a valid question though." CreationDate="2016-03-02T12:26:55.480" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2288" PostId="2128" Score="1" Text="@MartinBüttner Yes, its also easier to deal with floating point inaccuracy that way. Possibly more readable algo... Sounds like premature optimization to me." CreationDate="2016-03-02T13:01:06.953" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2289" PostId="2130" Score="0" Text="Just thinking aloud... Could you model the negative part of a filter separately to generate two sets of samples, one to be treated as positive and the other as negative? Would this allow arbitrary filters for your second approach (generate in the shape of a filter)?" CreationDate="2016-03-02T20:02:08.893" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2290" PostId="2073" Score="0" Text="Lerp and nlerp seem “good enough” for angles &lt;90°, but when you start getting higher (especially around 180°), the inaccuracies of lerp/nlerp become extremely pronounced.  I modified your ShaderToy example to have the vectors be 178.2° (180° * 0.99) apart from each other and the lerp/nlerp results are garbage: https://www.shadertoy.com/view/4sGGWd  If you can guarantee that the angle delta will be &lt;90° lerp/nlerp may be a fair optimization; if not, I think using slerp in combination with heavy memoization or a pre-computed lookup table is a safer and fast-enough way to go." CreationDate="2016-03-02T20:23:53.287" UserId="2785" ContentLicense="CC BY-SA 3.0" />
  <row Id="2291" PostId="2073" Score="0" Text="I think you missed the fact that you can click the mouse to change the vector angles! and yeah, it gets real bad at larger angles.  You can even see that in the top image in my answer!" CreationDate="2016-03-02T20:40:26.713" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2292" PostId="2073" Score="0" Text="@AlanWolfe: Side-note: I can write up an answer than pre-computes the `slerp` results into a texture and looks-up the values at runtime, if you're interested.  Given that the color value is the output normal, and if we can assume both lookup vectors are normal-or-nearly-normal-length, I believe the lookup table can be reduced to only plausible inputs, without covering the whole input space (4-dimensional, 2 for each input vector) and without resorting to using vector angles as inputs (2-dimensional, but still incurs the `atan2` cost)." CreationDate="2016-03-02T20:40:34.917" UserId="2785" ContentLicense="CC BY-SA 3.0" />
  <row Id="2293" PostId="2073" Score="0" Text="@AlanWolfe Ha, yah, I totally did.  You're one step ahead of me!  _Deleting my shadertoy clone (link above is now dead)._" CreationDate="2016-03-02T20:41:45.603" UserId="2785" ContentLicense="CC BY-SA 3.0" />
  <row Id="2294" PostId="2130" Score="0" Text="Maybe? Lemme fiddle with it for a bit" CreationDate="2016-03-02T20:47:44.223" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2295" PostId="2130" Score="1" Text="Ok, if you track the zeros of the function, you can abs() the output into the pdf. Then when sampling, you can check if you're negative. Sample code here: https://gist.github.com/RichieSams/aa7e71a0fb4720c8cb41" CreationDate="2016-03-02T22:31:26.867" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2296" PostId="2117" Score="0" Text="I actually fixed that shortly after posting this by making the subdivisions consistent and smaller, it fixes the shearing somewhat (although not completely). However the texture is still very distorted. I'm working on @nathan suggestion below, which at the very least will allow for a much finer subdivision." CreationDate="2016-03-03T00:06:08.263" UserId="2775" ContentLicense="CC BY-SA 3.0" />
  <row Id="2297" PostId="2135" Score="0" Text="Try a game engine like unity, etc... not sure this is the best place for this question though" CreationDate="2016-03-03T12:37:20.770" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2298" PostId="2135" Score="0" Text="Thanks, I have thought of Unity already. But I think it would be overkill for my purpose. I was hoping to find something not so complex." CreationDate="2016-03-03T14:11:00.490" UserId="2789" ContentLicense="CC BY-SA 3.0" />
  <row Id="2299" PostId="2134" Score="2" Text="Are you looking to measure details smaller than a pixel by looking at several different instances of the pattern that have different offsets from the pixel grid? Or are you looking to use the arrangement of the red, green and blue subpixels to improve accuracy with a single instance of the pattern?" CreationDate="2016-03-03T14:48:16.543" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2300" PostId="2137" Score="0" Text="Have you tried tessellating the environment map and associating depth with each vertex? Then crossfading as you move from one point to the other." CreationDate="2016-03-03T15:43:11.610" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2301" PostId="2132" Score="0" Text="Thanks! These are great resources. So, in the end, there are 3 methods? 1. Generate and Weigh with splatting 2. Generate and Weigh without splatting 3. Generate in the Shape of a Filter" CreationDate="2016-03-03T15:54:38.817" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2302" PostId="2132" Score="0" Text="Do you know of any papers, blogs, etc. that explore how to parallelize Generate and Weight *with* splatting? Off the top of my head, you could have a mutex per tile, or make each pixel atomic." CreationDate="2016-03-03T15:57:59.273" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2304" PostId="2131" Score="0" Text="It isn't clear why static objects and empty cells should allow the deletion of rows and columns. Are you setting these rows and columns to zero or removing them altogether to give a smaller matrix?" CreationDate="2016-03-03T17:13:24.183" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2305" PostId="2131" Score="0" Text="In case the problem is somewhere other than where you guess, it would help to see the code, if this is something you are happy to share. Ideally an [MCVE](http://stackoverflow.com/help/mcve)" CreationDate="2016-03-03T17:14:50.840" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2307" PostId="2131" Score="0" Text="Hey trichoplax. A matrix with an all zero row or column would be singular, as far as I know, so I instead remove them from the matrix to make a smaller matrix (as well as their corresponding entries in the b vector)." CreationDate="2016-03-03T17:57:59.703" UserId="2786" ContentLicense="CC BY-SA 3.0" />
  <row Id="2308" PostId="2131" Score="0" Text="I will edit an MCVE in tonight when I am near my computer with the source." CreationDate="2016-03-03T17:58:37.703" UserId="2786" ContentLicense="CC BY-SA 3.0" />
  <row Id="2309" PostId="2131" Score="0" Text="I also suspected that I was maybe making a wrong assumption somewhere else in the code, however this only pertains to the matrix structure (and whether or not it's singular). The only thing I can think of is what qualifies as a &quot;surface cell&quot; vs an air cell or a liquid cell. If this is a liquid cell adjacent to an air cell, is there something different that I should be doing with its corresponding columns/rows?" CreationDate="2016-03-03T18:00:50.033" UserId="2786" ContentLicense="CC BY-SA 3.0" />
  <row Id="2310" PostId="2132" Score="2" Text="@RichieSams I don't know why you'd use &quot;generate and weigh without splatting&quot;, actually—that seems like it would be worse in any case than filter importance sampling. I was assuming that &quot;generate and weigh&quot; implies splatting. As for parallelization of splatting, off the top of my head, one way would be to split the image into tiles, but give each tile a 2‒3 pixel border to catch splats that cross the tile edge. Then in a final pass, additively composite the bordered tiles together into the final image." CreationDate="2016-03-03T18:20:25.620" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2311" PostId="2131" Score="0" Text="I edited the question to include my code for generating the matrix. It is in Processing/Java." CreationDate="2016-03-03T19:49:29.360" UserId="2786" ContentLicense="CC BY-SA 3.0" />
  <row Id="2312" PostId="2134" Score="0" Text="I'm trying to measure details smaller than a pixel so by several instance you mean shifting it to left or right and then match .Thanks for your response." CreationDate="2016-03-04T03:24:09.633" UserId="2788" ContentLicense="CC BY-SA 3.0" />
  <row Id="2314" PostId="2135" Score="0" Text="if it's really for trivial shapes and to play with positions and orientations, why base OpenGL is not good ?" CreationDate="2016-03-04T07:06:06.737" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2315" PostId="2139" Score="0" Text="I don't see a problem, doubles can represent numbers as small as 10^-308." CreationDate="2016-03-04T09:00:09.867" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="2316" PostId="2128" Score="0" Text="You will get the most appropriate answers by tell us **why** you want to detect such segments." CreationDate="2016-03-04T09:03:50.460" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="2317" PostId="2134" Score="0" Text="By several instances I mean an image with a repeat pattern as described [here](http://computergraphics.stackexchange.com/questions/1880/what-is-the-state-of-the-art-on-using-computers-to-clean-up-images/1881#1881). By red, green and blue subpixels I mean taking advantage of the placement of different colours within a single pixel (the pixel geometry) as described [here](http://computergraphics.stackexchange.com/questions/424/subpixel-rendering-for-a-ray-tracer). It sounds like you probably mean the first approach." CreationDate="2016-03-04T12:30:21.587" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2319" PostId="2118" Score="0" Text="Thank you. I was able to modify your suggestion into a solution. See my edit above." CreationDate="2016-03-04T17:16:09.480" UserId="2775" ContentLicense="CC BY-SA 3.0" />
  <row Id="2320" PostId="2131" Score="0" Text="I added more information to the post." CreationDate="2016-03-04T18:02:45.977" UserId="2786" ContentLicense="CC BY-SA 3.0" />
  <row Id="2321" PostId="2139" Score="1" Text="Perhaps your problem might stem from the order of ops in your calculation. You might have something like  (BigValue + SmallValue) - BigValue, and instead of getting &quot;SmalValue&quot; you get 0." CreationDate="2016-03-04T18:29:46.107" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2322" PostId="2139" Score="1" Text="Just occurred to me that you seem to, effectively, have a rotation, R, followed a translation T.  If you can keep these separate, to get Inverse(R*T) you can just do Inverse(T)*Inverse(R), both of which are trivial. Would that help?" CreationDate="2016-03-04T18:46:03.690" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2323" PostId="2139" Score="0" Text="Simon, you're right about the problem stemming from multiplying numbers of different magnitudes. The matrix is more complex than an R*T; it's also a scale, and can be a multiplication of any affine.. I managed to fix the problem, however, see below. Thanks for your help." CreationDate="2016-03-04T21:10:12.203" UserId="2792" ContentLicense="CC BY-SA 3.0" />
  <row Id="2324" PostId="2139" Score="0" Text="You should ask on the math forum !" CreationDate="2016-03-04T21:16:23.010" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2325" PostId="2144" Score="0" Text="But the other line x = x1 is an infinite line parallel to the y-axis. Is not there a significant difference between a line and a line segment where both are parallel to y-axis ?" CreationDate="2016-03-04T21:55:48.563" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2326" PostId="2144" Score="0" Text="woops, you want segments. then verify that y match, and that $\lambda$ match." CreationDate="2016-03-05T00:47:07.520" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2329" PostId="2140" Score="0" Text="Is it known which line segment is axis parallel and which axis it is parallel to? Are you given two line segments, where one of them is parallel to one of the axes but you don't know which line segment or which axis, or are you given the first line segment, knowing it is always parallel to the x axis, and the second one may be any arbitrary line segment?" CreationDate="2016-03-05T18:04:10.747" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2330" PostId="2140" Score="0" Text="These seemingly subtle differences may make a large difference to the approach, and so answerers will need to know which is the case." CreationDate="2016-03-05T18:05:11.483" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2331" PostId="2140" Score="0" Text="It is confirmed that one of the line segment is either parallel to x-axis or y-axis and the other line segment may be or may be not parallel to either of the axis." CreationDate="2016-03-05T18:28:54.393" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2332" PostId="2133" Score="0" Text="24 bit shifters are used in single precision floating point to align mantissas, so the compiler might generate a few, but I don't think you'll see 30." CreationDate="2016-03-05T18:34:46.297" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2333" PostId="1893" Score="1" Text="I'm betting the 0.12 is in &quot;uv space&quot; meaning it's 12% of the image size (on each axis)." CreationDate="2016-03-07T04:11:23.850" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2337" PostId="2139" Score="0" Text="@solendil sorry yes I meant to mention scaling as well, but missed the edit timeout. Scaling is, obviously, also trivial to include particularly if it's the same for both dimensions. FWIW I used this trick in an API for some early PC graphics chips. Anyway, glad you've resolved your issues." CreationDate="2016-03-07T08:52:34.917" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2338" PostId="316" Score="0" Text="Of course I don't have all the details, but it seems to me that this person merely used a pseudo-distance field in place of a regular one, which has already been demonstrated in a 2006 paper by Qin, McCool and Kaplan, &quot;Real-time texture-mapped vector glyphs&quot;, which is also referenced in the Valve paper. It only affects the miters of outlines and does nothing to improve the appearance of corners. I suspect the reason it looks sharp is because he uses unpractically large distance field textures. I might be wrong though." CreationDate="2016-03-07T12:21:17.253" UserId="2811" ContentLicense="CC BY-SA 3.0" />
  <row Id="2339" PostId="2151" Score="5" Text="Great first answer, welcome to the Computer Graphics SE! :) Is your thesis publicly available? (Or will it be after you've finished said paper?) If so it would probably be very helpful to link to that, too." CreationDate="2016-03-07T13:57:41.343" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2340" PostId="2151" Score="0" Text="It is supposed to be publicly available, but it seems the school hasn't put it up yet. Anyway, I would prefer not to spread it right now, since the article I'm writing will really explain the important parts much better and focus on how to implement it, and it should be complete very soon." CreationDate="2016-03-07T14:26:47.413" UserId="2811" ContentLicense="CC BY-SA 3.0" />
  <row Id="2341" PostId="2151" Score="0" Text="@Detheroc Please notify here and on the gamedev Q when you are done with the article. Explanation's still not 100% clear for me. I would suggest showing the composition step by step in images." CreationDate="2016-03-07T15:09:17.600" UserId="101" ContentLicense="CC BY-SA 3.0" />
  <row Id="2342" PostId="2151" Score="1" Text="would love to be able to replicate your current results even if they are not as good as your future results, +1 to sharing whatever details you can.  very exciting.  Have you considered either technique's application towards ray marching (sphere tracing)?  In volume textures or similar..." CreationDate="2016-03-07T19:36:45.700" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2343" PostId="2151" Score="0" Text="Also, obviously would love to see how it compares to just having a single channel texture that has 3x as many pixels (;" CreationDate="2016-03-07T23:18:19.107" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2344" PostId="2150" Score="1" Text="You might try asking on the math stack exchange site if you don't get an answer here." CreationDate="2016-03-08T04:43:30.923" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2345" PostId="2150" Score="2" Text="I must say that i dont understand what the inaccuracy is. So I cant help. Basically your saying Im doing A but A does not work. Without explaining what thing that does not work is." CreationDate="2016-03-08T07:17:27.507" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2346" PostId="2150" Score="0" Text="@joojaa, When i fit single surface from a set of sampled points (Each sampled point was sampled from one of the source surface), the resulting surface fails to achieve good accuracy (In sense of maximum deviation between resulting surface and the set of sampled points). So I'm asking if there is some another method to do same thing (Get single surface from different trimmed surfaces) with smaller loss in accuracy, because least squares method gives too rough results." CreationDate="2016-03-08T11:37:15.227" UserId="2644" ContentLicense="CC BY-SA 3.0" />
  <row Id="2347" PostId="2150" Score="1" Text="Make a picture showing the error." CreationDate="2016-03-08T11:42:02.893" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2348" PostId="2150" Score="0" Text="@joojaam, it's just an error of NLib library, which tells that i cannot approximate set of points with desired tolerance. It's programming related, not from CAD package, sorry, i forgot to mention this." CreationDate="2016-03-08T16:02:01.437" UserId="2644" ContentLicense="CC BY-SA 3.0" />
  <row Id="2350" PostId="2150" Score="1" Text="Even if the error is not clearly visible, it may help to include in the question an image of a curved surface for which the accuracy is unacceptable, and the (possibly textual) evidence that the accuracy is not sufficient. This will give answerers a better idea of what you are dealing with." CreationDate="2016-03-08T20:00:50.473" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2351" PostId="2153" Score="2" Text="What are you trying to do with the array and loop? I'm asking because this somehow sounds like an [XY Problem](http://meta.stackexchange.com/questions/66377/what-is-the-xy-problem) to me. Since the best way to use conditions and loops on the GPU is to refrain from using them, maybe there are even better ways instead of using arrays and loops in your case." CreationDate="2016-03-08T21:38:13.670" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="2352" PostId="2153" Score="0" Text="I am implementing a screenspace subsurface scattering effect which currently work. But I have some doubts about the way I use the kernel according to performances. I've choose to do a maximum array size and fill only a part and use a dynamic loop with a dynamic number of iteration which is related to the currently used array content.&#xA;I think that there are things to do or know when programming shaders according to performances for example. And in my opinion loops is a common performance topic which might follow some rules and maybe &quot;good practices&quot; but I didn't found any good answer about it." CreationDate="2016-03-09T07:38:31.413" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="2354" PostId="2156" Score="0" Text="Out of curiosity, what was the precision before? Low or Medium?" CreationDate="2016-03-09T09:33:26.220" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2355" PostId="2152" Score="0" Text="Doh:  actually I'm wrong about the method not being uniform...working on a rewrite." CreationDate="2016-03-09T10:57:04.507" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="2356" PostId="2152" Score="0" Text="OK, I put up a first revision." CreationDate="2016-03-09T12:17:30.033" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="2363" PostId="2156" Score="0" Text="I found a document saying it's low for samplers and high for float/int by default on ES. Oh, ints depend on shader type I've added link to my answer." CreationDate="2016-03-09T21:16:58.943" UserId="2840" ContentLicense="CC BY-SA 3.0" />
  <row Id="2364" PostId="2163" Score="0" Text="To be clear, are you looking for an existing software renderer for vector graphics, or are you looking to learn how to write a software renderer for vector graphics on your own?" CreationDate="2016-03-10T04:58:27.120" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2365" PostId="2163" Score="0" Text="i am looking to write a software renderer for vector graphics on my own" CreationDate="2016-03-10T04:59:42.887" UserId="2853" ContentLicense="CC BY-SA 3.0" />
  <row Id="2366" PostId="2163" Score="0" Text="Are you trying to make a program that outputs an image file? Or are you trying to make a program that shows the vector shapes on the screen and lets you edit them in real time?  What operating system are you using?" CreationDate="2016-03-10T06:19:25.380" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2367" PostId="2152" Score="0" Text="Added the area-distortion note" CreationDate="2016-03-10T09:13:11.253" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="2368" PostId="2159" Score="0" Text="The scene graph technique that I am looking forward to implement is very similar to the http://www.openscenegraph.org/ . I have used it for a while and I am familiar to it. The scene that you created with it can be stored as a trivial file format which can be processed further by this library." CreationDate="2016-03-10T10:53:03.073" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2369" PostId="2163" Score="1" Text="I'm afraid it is not possible for us to narrow the question down for you, as we do not know specifically what you want. Is there a similar application that already exists for comparison, so we can see what you intend to build? What will the finished program be used for? Will a shape be displayed as a wire frame, a solid, with flat shading or realistic lighting? Do you want to produce images in real time or slowly generate high quality images?" CreationDate="2016-03-10T12:08:33.670" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2370" PostId="2156" Score="0" Text="OK. Was just wondering if you'd tried mediump as there could be performance (and power) benefits." CreationDate="2016-03-10T12:30:13.953" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2371" PostId="2163" Score="0" Text="i wan't to create a 3D program where the only thing it does is showing a box with 8 vertex points , 12 edges in wire frame . This program will be an executable for windows operating system 10 . I wan't to create this 'sample' structure program without using OpenGL, DirectX or Vulkan , only with custom code for the entire input , output of the program . That means creating the class methods for vertex and edges and a ouput system to the screen with vector graphics. It may be in CLI or MFC , it does not matter for me." CreationDate="2016-03-10T14:03:53.507" UserId="2853" ContentLicense="CC BY-SA 3.0" />
  <row Id="2372" PostId="2168" Score="0" Text="The option to queue the frame but not wait on it, would that be considered tripple (and higher) buffering?" CreationDate="2016-03-10T14:19:45.650" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2373" PostId="2168" Score="0" Text="@AlanWolfe possibly, you need at least 3 buffers for a non-tearing and non-blocking view, one to display, one as the next to display and one that's being rendered to." CreationDate="2016-03-10T14:21:21.993" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2375" PostId="2159" Score="0" Text="I would either look into the javascript library http://osgjs.org/, which also is based on openscenegraph.org. But if you want to have more control over the interaction with the library i would suggest trying to compile the openscenegraph using emscripten (as it is open source), or leaving the compiled library as is and creating javascript to c++ bindings for it (also possible with emscripten iirc)." CreationDate="2016-03-10T16:31:28.303" UserId="64" ContentLicense="CC BY-SA 3.0" />
  <row Id="2376" PostId="2162" Score="0" Text="Thanks for your suggestion. Must try it, there are a good few demos on youtube, but I had never seen any mentioning live imput from mike." CreationDate="2016-03-10T19:23:51.333" UserId="2848" ContentLicense="CC BY-SA 3.0" />
  <row Id="2377" PostId="2151" Score="0" Text="@Detheroc This is great. I'm also interested to get notified when you release it publicly." CreationDate="2016-03-11T00:55:40.090" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="2378" PostId="2163" Score="0" Text="what is your reasoning for not wanting to use a graphics API? Without a graphics API you will be doing CPU software rendering which is slower and not the modern way to do graphics. The graphics APIs utilize the graphics card's hardware, that is all.  You really are best off using opengl, directx or similar, unless you have a really strange reason why you can't use them." CreationDate="2016-03-11T03:37:11.020" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2379" PostId="2163" Score="0" Text="i want to learn the work flow and understand the logic in details. It's only for educational purpose . There is not any big work going on or idea . You can say that i am a maniac in knowing this things how they work down in detail and code. I can use a API and i know how , it's just for my own personal reason." CreationDate="2016-03-11T05:30:16.603" UserId="2853" ContentLicense="CC BY-SA 3.0" />
  <row Id="2380" PostId="2163" Score="0" Text="So I have re-interpreted the question as follows: *How to make the rasterizer of vector graphics yourself?* Does that sound about right? It hard to find a modern operating system functionality that allows you to manage everything yourself. Even the lowest level modern apis know how to draw lines for you." CreationDate="2016-03-11T08:42:40.920" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2382" PostId="2170" Score="1" Text="Will you want to repeatedly test many different angles of ray from the same starting point, against the same polyline? Or will the starting point and angle both be variable? (What I'm getting at is it's probably possible to build an acceleration structure that would speed up these queries, but which structure is best will depend on how it's going to be used.)" CreationDate="2016-03-11T21:45:17.817" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2386" PostId="2172" Score="0" Text="Putting online and use the google image advanced search ? :-p ( beside kidding, they might have publish (white) papers on that )." CreationDate="2016-03-12T00:10:26.250" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2387" PostId="2175" Score="1" Text="I really like this idea, I'll have to try it out. It makes perfect sense. Thanks!" CreationDate="2016-03-12T02:49:48.353" UserId="31" ContentLicense="CC BY-SA 3.0" />
  <row Id="2388" PostId="2170" Score="0" Text="Nathan, I will be testing many from a single starting point. And then test multiple angles from another starting point and another and etc. I actually have segments of two contour lines and am trying to draw a line in between the two of them. My current strategy is to draw a series of lines at semi-regular intervals between the two lines. I can then create a polyline between all the short crossing lines.  I'm currently working on how to create the lines that cross between the two contours, which is surprisingly difficult due to how curvy and convoluted some of them are." CreationDate="2016-03-12T03:41:19.340" UserId="2863" ContentLicense="CC BY-SA 3.0" />
  <row Id="2390" PostId="2172" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it." CreationDate="2016-03-12T11:25:11.537" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2391" PostId="2172" Score="0" Text="As it stands, this question does not specify what types of images will need to be considered, so I am closing as unclear. The question may be reopened if it can be edited to clarify what types of non-photographic images will be presented, and to request an algorithm rather than an off site resource recommendation." CreationDate="2016-03-12T11:28:02.647" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2392" PostId="2172" Score="0" Text="See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T11:29:43.333" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2394" PostId="2135" Score="0" Text="Not sure if this is what you're looking for, but [OpenSceneGraph](http://www.openscenegraph.org/) or [Vtk](http://www.vtk.org/) are [scene graph toolkits](https://en.m.wikipedia.org/wiki/Scene_graph). You may want to check licensing. I think they are LGPL but I'm not sure." CreationDate="2016-03-05T13:39:03.330" UserId="2562" ContentLicense="CC BY-SA 3.0" />
  <row Id="2395" PostId="2135" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T11:38:51.463" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2397" PostId="1758" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-12T12:23:28.780" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2399" PostId="2170" Score="0" Text="So your triong to find the closest point on the curve?" CreationDate="2016-03-12T15:51:57.763" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2400" PostId="2170" Score="0" Text="I tested out closest point on the curve and while it can work there are too many cases where it doesn't work at all or works poorly. The approach I'm currently considering is based on angles. When a line is drawn between two other lines 4 angles are created. I'm thinking the &quot;best&quot; line is created when all 4 angles are as close to 90 degrees as possible. So at a given position I intend to draw every possible line (intervals of 5 or 10 degrees or similar) and select the best line based on a ranking of the 4 angles." CreationDate="2016-03-12T16:51:03.637" UserId="2863" ContentLicense="CC BY-SA 3.0" />
  <row Id="2401" PostId="2163" Score="0" Text="joojaa , yes that is about what i wan't ." CreationDate="2016-03-12T17:02:06.673" UserId="2853" ContentLicense="CC BY-SA 3.0" />
  <row Id="2402" PostId="2170" Score="0" Text="Would you be interested in solutions that use precomputed acceleration structures? Bsp trees and possibly vornoi diagrams seem likely useful (:" CreationDate="2016-03-12T17:26:06.243" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2403" PostId="2163" Score="0" Text="Ah ok Roger.  To help you out in googling or asking further questions, this might be referred to as software rendering or software rasterization." CreationDate="2016-03-12T20:08:41.263" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2406" PostId="2178" Score="1" Text="If you take a screenshot using Alt and printscreen, instead of printscreen alone, it will just copy the current window instead of the whole screen, so you can show just the relevant part of your screen." CreationDate="2016-03-13T13:51:29.133" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2408" PostId="1614" Score="0" Text="Hi cupe, is it possible to take a look to the code somewhere? Ps: are you checking also for multiple entries?" CreationDate="2016-03-13T18:35:18.960" UserId="1561" ContentLicense="CC BY-SA 3.0" />
  <row Id="2409" PostId="1983" Score="0" Text="@trichoplax 1. Why now, after all this time? 2. Moreover, the response to the [meta-question](http://meta.computergraphics.stackexchange.com/questions/211/are-requests-for-reputable-sources-on-topic/212#212), while not firm, was leaning towards allowing the question. 3. Finally, as I have explained there, the argument about &quot;opinionated answers and spam&quot; is completely inapplicable in the case of requests for reputable sources for a specific fact. Either there is such a source or not." CreationDate="2016-03-13T20:29:40.783" UserId="2574" ContentLicense="CC BY-SA 3.0" />
  <row Id="2412" PostId="1983" Score="0" Text="If anyone disagrees that off site resource requests should be off topic, they can have their say in [Are questions asking for off site resources on topic?](http://meta.computergraphics.stackexchange.com/questions/146/are-questions-asking-for-off-site-resources-on-topic) If anyone thinks requests for reputable sources should be an exception to the off site resources rule, they can have their say in [Are requests for reputable sources on topic?](http://meta.computergraphics.stackexchange.com/questions/211/are-requests-for-reputable-sources-on-topic)" CreationDate="2016-03-14T00:10:10.030" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2415" PostId="2181" Score="0" Text="You might also want to read up on signed distance fields in general by the way. Seems relevant as you might possibly be able to skip the distance transform step and just draw sdf's" CreationDate="2016-03-14T03:12:19.650" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2416" PostId="2177" Score="0" Text="Thank you for the suggestions Nathan, I'll look into BVH. Delaunay triangulation could be useful, but I'm seeing potential issues in the example you posted. I'm having the greatest issues where one contour is significantly longer than the other. On the ENE ridge in the image the triangles are spanning the same contour, which is the type of location that I really need the lines to span both contours to interpolate a smooth curve. I may have to go with a ray tracing type approach." CreationDate="2016-03-14T17:05:09.320" UserId="2863" ContentLicense="CC BY-SA 3.0" />
  <row Id="2418" PostId="2183" Score="0" Text="I got the problem!" CreationDate="2016-03-14T18:26:00.437" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="2420" PostId="2177" Score="0" Text="@MikeBannister, yes, there are places where triangles span the same contour, but you could detect those and filter them out, and only look at triangles that span two adjacent contours." CreationDate="2016-03-14T21:17:01.287" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2421" PostId="2156" Score="0" Text="mediump doesn't help in my case, and also I don't notice any significant performance difference either." CreationDate="2016-03-14T21:22:20.267" UserId="2840" ContentLicense="CC BY-SA 3.0" />
  <row Id="2422" PostId="2181" Score="0" Text="Thanks, it seems to work quite well. Using OpenCV, applying a distance transform, thresholding the image and finally normalizing it back to 0 to 255 values seems to give quite a good result. Some jagged lines are really visible though. I tried a Gaussian blur which kind of works but I would really like to know if there is a better solution." CreationDate="2016-03-15T02:54:12.533" UserId="2319" ContentLicense="CC BY-SA 3.0" />
  <row Id="2423" PostId="2181" Score="0" Text="There is a better solution.  The topic kind of warrants it's own question but here's a short answer.  First step is to make it fade from white to black over a specific distance.  Like if 10 was the distance that went from black to white, you could make it fade between 9 and 11 where you use the distance value to figure out how light or dark to make the pixel based on distance.  Next, you take that pixel shade, which should be between 0 and 1, and put it through the smoothstep function.  That will turn it from a linear fade to something a lot more appealing. Adjust fade distance to taste (:" CreationDate="2016-03-15T03:10:46.917" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2424" PostId="2181" Score="0" Text="Here is the details of the smoothstep function:  https://en.m.wikipedia.org/wiki/Smoothstep  for a better or more detailed answer you might ask how to do anti aliasing when rendering using a distance field." CreationDate="2016-03-15T03:12:21.340" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2425" PostId="2181" Score="0" Text="oh sorry, to address more towards your original question.  I believe the curve just says how to turn the linear distance into a pixel shade.  like if your width was 10 pixels, and you were 5 pixels away (either on the positive or negative side), that it would take that to mean you were at 50% distance, and do a lookup on the curve at 50% to see how white or black to make the output pixel." CreationDate="2016-03-15T03:47:51.517" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2426" PostId="2189" Score="2" Text="You can (and probably should) include images straight in the post (which I've done for your post now). Just hit Ctrl+G while editing the post and drag your image file into the browser. That way it will be hosted on imgur (on a specific Stack Exchange subdomain), which is a lot less likely to be subject to link rot at some point in the future (plus, people don't have to follow a link to your image to see what the problem)." CreationDate="2016-03-15T15:37:43.417" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2427" PostId="2189" Score="0" Text="@MartinBüttner oh, thanks! I didn't know that I can do that here :)" CreationDate="2016-03-15T15:40:11.260" UserId="2508" ContentLicense="CC BY-SA 3.0" />
  <row Id="2428" PostId="2188" Score="0" Text="Given this and your previous answers is there a reason why you do not use nurbs and B-Rep?" CreationDate="2016-03-15T17:35:16.007" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2429" PostId="1963" Score="0" Text="I just came across this paper from SIGGRAPH 2006 that talks about a way to do this using wang tiles.&#xA;Recursive Wang Tiles for Real-Time Blue Noise&#xA;http://johanneskopf.de/publications/blue_noise/" CreationDate="2016-03-15T18:53:54.283" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2430" PostId="2181" Score="0" Text="Sorry if my comment was a bit unclear.&#xA;&#xA;here is an [image](http://i.imgur.com/RKxyKtH.png) of the result I get.&#xA;&#xA;As you can see in the result, jagged strips are quite visible.&#xA;Like I mentioned, I used the distance transform + threshold + normalize to &quot;make it fade from white to black over a specific distance&quot;.&#xA;&#xA;You mentioned &quot;anti aliasing when rendering using a distance field&quot;. Is this what corresponds to the issue here?" CreationDate="2016-03-15T20:24:52.497" UserId="2319" ContentLicense="CC BY-SA 3.0" />
  <row Id="2431" PostId="2181" Score="0" Text="Did you put the shade color through smoothstep to make the blending non linear? If so that is really weird. Probably worth another question about just that specifically. (Using terminology of rendering using a distance field)" CreationDate="2016-03-15T20:28:58.600" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2432" PostId="2188" Score="0" Text="Yes. Whatever I build must then be uploaded in CSG format to a very old piece of software that then does raytracing on it. And the old software is setting up some STRONG restrictions on functions. However I seem to have found a workaround. As soon as I get approval on the figures I'll post my current solution." CreationDate="2016-03-16T08:14:08.810" UserId="2858" ContentLicense="CC BY-SA 3.0" />
  <row Id="2433" PostId="2188" Score="0" Text="Yes but if you could tell what the old ray tracer is then we can come up with better solutions. I mean first you tell use it has to be torus and sphere cylinder etc. than you relax it to hull and Malinowski functions. Neither one of those alone are perfect solutions. But given artificial restrictions that are there because you think you are restricted in this way is not effective communication.   B-Rep is for most part CSG." CreationDate="2016-03-16T08:28:55.827" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2434" PostId="2188" Score="0" Text="You are indeed correct. The final object must be constructed in Craig E. Kolb's Rayshade program and it must be inputted as a text file. I am still not allowed to use commands such as Hull or Minkowski since they are not available in Rayshade. I was looking for a way to simulate them. Up to now I've had (in my opinion) relative success by taking three projections of the objects and then intersecting them.   Documentation about Rayshade can be found in here: http://graphics.stanford.edu/~cek/rayshade/doc/guide/guide.html" CreationDate="2016-03-16T09:19:58.120" UserId="2858" ContentLicense="CC BY-SA 3.0" />
  <row Id="2435" PostId="2190" Score="1" Text="I've been thinking about this myself and I'm not entirely sure, but here's some inspiration that might or might not be valid... So can there be a continuity? Ontologically no. Phenomenologically yes." CreationDate="2016-03-16T09:26:56.747" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="2436" PostId="2188" Score="0" Text="Howabout using sweptsph that gives you quite some options." CreationDate="2016-03-16T10:32:09.303" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2437" PostId="2188" Score="0" Text="It turns out that intersecting the projections from three different views gives me an acceptable quality. I will check swepthsph if I need extra smoothness in the figures but for now the method I'm using is enough. Thank you very much for taking the time to help." CreationDate="2016-03-16T12:58:44.057" UserId="2858" ContentLicense="CC BY-SA 3.0" />
  <row Id="2438" PostId="2190" Score="2" Text="BRDF space is definitely continuous. The classes of BRDFs you mention are simplified slices of BRDF space and it would take careful analysis of the formulas to decide if there is a parameter for specular in Cook-Torrence that gives a lambertian result when used in a physically based renderer. I think of a surface with both specular and diffuse reflection as being &quot;layered&quot; - like a gloss coating on a magazine, a polished outermost layer (specular) with crevices - all plastics seem translucent, underneath a shiny layer, light bounces around before being reemitted. Thus the divided model." CreationDate="2016-03-16T13:31:28.293" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2440" PostId="2193" Score="0" Text="And this discrete nature of depth map is created because of floating point precision. Right?" CreationDate="2016-03-16T20:12:31.963" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="2441" PostId="2193" Score="1" Text="No its created because images are discrete as in have only one value for a area that varies." CreationDate="2016-03-16T21:00:08.700" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2442" PostId="2193" Score="0" Text="Images are different in camera and light space?" CreationDate="2016-03-16T21:04:51.713" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="2443" PostId="2176" Score="0" Text="thanks a lot for your help in this subject . It covers the basics that i need to continue what im searching for . i am grateful  for the help." CreationDate="2016-03-17T00:03:55.267" UserId="2853" ContentLicense="CC BY-SA 3.0" />
  <row Id="2444" PostId="2176" Score="0" Text="Glad to hear! Sometimes it's hard to know what to search for." CreationDate="2016-03-17T00:14:55.893" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2445" PostId="2189" Score="2" Text="I'd suggest to proceed step by step: try to display the framebuffer (directly with a quad) to validate its content first. Also, it looks like the UV on your cube might be wrong, so you may want to display your cube with its texture coordinates (`gl_FragColor = vec4(gl_TexCoord[0].xy, 0., 1.);`)." CreationDate="2016-03-17T03:01:52.287" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="2446" PostId="2187" Score="0" Text="Great question, I'm also curious." CreationDate="2016-03-17T03:48:40.693" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2447" PostId="2187" Score="0" Text="I suggest you do a test by outputting a grey scale band, and see how it looks on all your available platforms. then try the same thing with `pow(c, 1/2.2)` at the end of the pipeline. Your trained eye will immediately see which is good and which is over-done. Over-done gamma should result in banding." CreationDate="2016-03-17T08:23:56.873" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="2449" PostId="2187" Score="0" Text="I came here *because* of the test I've done :-) https://www.shadertoy.com/view/4stSRN&#xA;The real world of webGLSL is incredibly messy and unrobust: behaviors can depend on driver, browser, OS, settings (Angle vs native OpenGL, display settings - soft and hard), plus the versions of all these. ( Of course here I had to trust people telling there system is well calibrated. )" CreationDate="2016-03-17T08:28:42.067" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2450" PostId="2181" Score="0" Text="Smoothstep does not solve the jagged lines problem which is not really surprising. It remaps the values in a smoother curve but it does not fix the aliasing in any way. I will address another question targeting this specific issue." CreationDate="2016-03-17T11:51:21.250" UserId="2319" ContentLicense="CC BY-SA 3.0" />
  <row Id="2451" PostId="2193" Score="0" Text="Ok is that zigzag line represents depth map?" CreationDate="2016-03-17T14:14:02.127" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="2452" PostId="2193" Score="0" Text="It represents the function of the depth map, the dashed lines represent the pixel samples of the depth map." CreationDate="2016-03-17T14:16:55.647" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2453" PostId="2197" Score="0" Text="One can also be more clever and render mid distance maps." CreationDate="2016-03-17T14:18:53.710" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2454" PostId="2193" Score="0" Text="Ok so when we sample from a depth map..Its not necessary you will sample from the exact same position..but the different one which can be either higher or lower than the depth map?" CreationDate="2016-03-17T14:25:36.870" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="2455" PostId="2197" Score="1" Text="I would have thought it's called Peter Panning [because some films depict Peter Pan's shadow as having a mind of its own and detaching itself from Peter](http://movies.stackexchange.com/q/13552/15400)." CreationDate="2016-03-17T14:27:12.473" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2456" PostId="2181" Score="0" Text="I'm looking forward to seeing the details in the new question. The artifacts you are seeing is strange. Fwiw the smoothstep-ing of the fade is a common technique. It might not help the specific issue you are hitting but it is useful / widely used. Just wanted to let you know I didn't just make it up :p" CreationDate="2016-03-17T15:04:59.550" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2457" PostId="2197" Score="0" Text="@MartinBüttner Well, yes. That seems to be a sensible reason to call it that way. The tutorial I refered to uses the explanation that I gave." CreationDate="2016-03-17T16:12:43.823" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2458" PostId="2181" Score="0" Text="Don't  worry! I believe you haha! I read about it and it seems to be used in many situations. This is simply not useful in this case since I need to preserve the linear interpolation. Here is [another post](http://dsp.stackexchange.com/questions/530/bitmap-alpha-bevel-algorithm) I found that has similar request and technology in use. I think the artifacts could be related to the implementation of distance transform in OpenCV." CreationDate="2016-03-17T18:27:42.700" UserId="2319" ContentLicense="CC BY-SA 3.0" />
  <row Id="2459" PostId="2187" Score="0" Text="You cold add that to your post ;) Unfortunately this is all just a case of futility." CreationDate="2016-03-17T20:23:01.877" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2460" PostId="2198" Score="0" Text="Do you want the insets to travel on the surface itself?" CreationDate="2016-03-17T21:41:28.000" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2461" PostId="2198" Score="0" Text="Yes, on the surface.&#xA;&#xA;I've found papers for insetting 3D shapes in 3D, but I need to adhere to the surface." CreationDate="2016-03-17T22:35:39.817" UserId="2896" ContentLicense="CC BY-SA 3.0" />
  <row Id="2462" PostId="2193" Score="0" Text="Yes you got it." CreationDate="2016-03-18T05:39:50.807" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2465" PostId="2188" Score="0" Text="The reason nobody suggest the 3 projection boolean is because it does in fact not work for nearly any sensible case." CreationDate="2016-03-18T10:07:37.767" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2467" PostId="2198" Score="0" Text="This is easier for cad applications as they have mathematically better surfaces than polygon meshes. But yes you could extrude tubes of varying sizes along edges then repeat and you'd get a good approximation. Its just that defining what distance along internal polygons is shortest and valid one is a bit hard." CreationDate="2016-03-18T18:37:10.940" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2468" PostId="2201" Score="0" Text="OpenGL doesn't seem like it would help here; it's a computational geometry problem, not rendering. Can you edit the question and define your terms better? What do you mean by a &quot;maximum continual convex patch&quot; exactly?" CreationDate="2016-03-19T02:22:18.800" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2469" PostId="2194" Score="0" Text="What is a &quot;dioptre material&quot;? AFAIK a dioptre is a unit of measurement of optical power. :)" CreationDate="2016-03-19T02:25:40.427" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2471" PostId="2194" Score="1" Text="a dioptre between material 1 / material 2 is the surface between  2 optical materials of different index of refraction.  You confuse with the dioptry." CreationDate="2016-03-19T03:28:32.193" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2472" PostId="2194" Score="0" Text="Diopter/dioptre is [a unit of measurement](https://en.wikipedia.org/wiki/Dioptre). The surface between two materials is usually called an &quot;interface&quot;, AFAIK...I've never heard any term similar to &quot;diopter&quot; for that..." CreationDate="2016-03-19T03:30:54.830" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2473" PostId="2194" Score="1" Text="possibly a problem of translation, then. The french wikipedia offer no english equivalent: https://fr.wikipedia.org/wiki/Dioptre  . And online translation suggest to use the same world is english. :-/ . Ok, I replace by &quot;interface&quot; but this world is less precise." CreationDate="2016-03-19T04:42:14.507" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2475" PostId="2189" Score="0" Text="@JulienGuertault I tried rendering the FB directly to screen and I've attached the output of this to my original post (at the end). It seems to be drawing to the FB just fine - I'm still not getting what is wrong here. Also, I switched to quads (2 triangles) and put in the coords by hand just to be sure - so the coords are now right, but the texture, when applied to the second quad, seems to be stretched badly in the Y direction - the square looks like a very tall rectangle. I don't get it..." CreationDate="2016-03-19T14:57:51.800" UserId="2508" ContentLicense="CC BY-SA 3.0" />
  <row Id="2477" PostId="2201" Score="0" Text="@nathanreed opengl will be used to render the objects, processing algorithms are to be done in C++. I have to apply different colors to the different convex patches on the object to signify the selection. Say I have a sphere then the whole sphere is one maximal convex patch. Any portion of the sphere surface will be a convex patch, by maximal I mean the maximum continuous convex patch that can be found. Well in the rendering, depending on the viewing angles, the maximal convex patches visible to the viewer will have to colored." CreationDate="2016-03-20T04:01:00.113" UserId="2898" ContentLicense="CC BY-SA 3.0" />
  <row Id="2478" PostId="2201" Score="0" Text="@nathanreed Below I have posted an answer that should work, can you improve upon it, or can you provide a better solution?" CreationDate="2016-03-20T04:02:01.553" UserId="2898" ContentLicense="CC BY-SA 3.0" />
  <row Id="2483" PostId="2204" Score="0" Text="If a patch ends, and not all triangles have been visited/tested, you have to start anew with another random triangle that is not part of a patch yet. So you find all convex patches and not just the one you happen to begin with. Then chose the biggest one. (Note: Patches can be just a single triangle)" CreationDate="2016-03-20T13:13:42.493" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2484" PostId="1983" Score="1" Text="@trichoplax You yourself were uncertain whether this question is really a request for off-site resources, and you say that uncertainty remains. Is it a standard practice on this stackexchange that, when there is uncertainty about whether a question is off-topic, to err on the side of it being off-topic?" CreationDate="2016-03-20T14:23:28.137" UserId="2574" ContentLicense="CC BY-SA 3.0" />
  <row Id="2490" PostId="2204" Score="0" Text="@dragonseel well for practical purposes I think single triangle patches should be ruled out, as for us to determine convexity, it has to span across several triangles.&#xA;well we do get all patches, but when a convex patch continues further, it's size keeps on increasing by including the neighbouring triangles that satisfy convexity, until the patch reaches the maximum size it can attain." CreationDate="2016-03-20T16:41:13.333" UserId="2898" ContentLicense="CC BY-SA 3.0" />
  <row Id="2492" PostId="2204" Score="0" Text="Yea. I just wanted to emphazise that in order to find the biggest convex patch, you have to keep sampling until you tested everything, since after the first one there might be a even bigger one disconnected that you haven't found jet." CreationDate="2016-03-20T20:30:55.647" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2493" PostId="2206" Score="0" Text="It is perfectly OK if your material looks dark gray if you illuminate it with weak light source. What is the brightness of the light source?" CreationDate="2016-03-20T23:14:39.753" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2494" PostId="2206" Score="0" Text="@ivokabel but my materials should look white and light gray. The SPD of the illuminant used is the D65. Do i need to tweak the spd of the light in some way?" CreationDate="2016-03-20T23:17:02.907" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2495" PostId="2206" Score="0" Text="@ivokabel Do i need to define a brightness paramter and use it somewhere?" CreationDate="2016-03-20T23:17:50.843" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2496" PostId="2206" Score="0" Text="If I am not mistaken, D65 only defines the shape of the spectrum, not the intensity. Therefore, you will really have to add a parameter telling the amount of emitting radiance, or something similar. Related topic is the renderer exposure value, but I saw that you take 1 as the limit value, so you don't have to bother with this one." CreationDate="2016-03-20T23:37:29.820" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2497" PostId="2206" Score="0" Text="Thank you @ivokabel for the suggestion about the parameter radiance. Could it be just a constant that will be multiplied with the spd of the illuminant during the tracing of rays? Or do i need to multiply the spd of the illuminant during the conversion from spd to cie xyz? Also I don't understand what you mean with renderer exposure value. Where do I take 1 as its value?" CreationDate="2016-03-20T23:45:26.890" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2498" PostId="2206" Score="0" Text="Yes, multiplying the SPD of your illuminant with a value (whether constant or variable) during or before the ray tracing is the way to go." CreationDate="2016-03-20T23:55:55.353" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2499" PostId="2206" Score="0" Text="@ivokabel what about the render exposure value? Where do i take 1?" CreationDate="2016-03-20T23:58:54.817" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2500" PostId="2206" Score="0" Text="...and sorry for the confusion about exposure. What I meant is the image value which maps onto maximum value in the resulting picture, 255 in your case. Don't worry about that at this point." CreationDate="2016-03-20T23:58:58.063" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2501" PostId="2206" Score="0" Text="@ivokabel can you just write a response to my question? In this way your answer an the other (if someone else would response) will remain as reference, and the user will not have to search in the  :) thank you." CreationDate="2016-03-21T00:05:41.957" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2502" PostId="2206" Score="0" Text="I might get to it tomorrow..." CreationDate="2016-03-21T00:11:36.470" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2503" PostId="2206" Score="0" Text="After re-reading your code, it is unclear to me what is the relation between scene-&gt;light-&gt;spectrum and material-&gt;le in `PathBRDF::shade`. Is it the same value? If yes, do you allow just one light source in your scene? Moreover, why do you normalize your XYZ values with illuminant luminance in `CIE1931XYZ::tristimulusValues()`?" CreationDate="2016-03-21T10:01:09.643" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2504" PostId="2206" Score="0" Text="@ivokabel scene-&gt;light-&gt;spectrum and material-&gt;le contain the same value, the D65 SPD. In the scene init I give to the material of light object the SPD of the scene light. My scenes supports only one light. The normalization in CIE1931XYZ::tristimulusValues() follows the standard conversion from spd to CIE XYZ that you can found here http://www.scratchapixel.com/old/lessons/3d-basic-lessons/lesson-5-colors-and-digital-images/color-spaces/ or on the wiki CIE XYZ page. Is this passage not correct? I don't think so because my engine supports also the whitted ray tracing model that seems to be ok." CreationDate="2016-03-21T10:09:52.457" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2505" PostId="2206" Score="0" Text="This is an example scene generated with a different illuminant SPD and the Whitted model https://raw.githubusercontent.com/chicio/Spectrum-Clara-Lux-Tracer/master/Screenshots/03_scene4_whittedSpectrum_fl9.png" CreationDate="2016-03-21T10:10:25.363" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2506" PostId="2206" Score="0" Text="In fact if I multiply the material-&gt;le with a multiplier the scene become more bright. Here are some rendered images https://drive.google.com/drive/u/1/folders/0BxeVnHLvT8-7Ty1jTVM5U1JJdms. They are not totally correct (as I expect the floor to be white), but maybe i just need a higher multiplier. Do you see any error in code that could avoid this multiplier (some error in the pdf/BRDF calculation)? Thank you very much again @ivokabel." CreationDate="2016-03-21T10:13:59.067" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2508" PostId="2206" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/37281/discussion-between-ivokabel-and-fabrizio-duroni)." CreationDate="2016-03-21T12:29:26.397" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2509" PostId="2211" Score="1" Text="Not to mention it needs to render the image from 2 slightly different perspectives to accommodate 3D." CreationDate="2016-03-22T09:09:30.180" UserId="2933" ContentLicense="CC BY-SA 3.0" />
  <row Id="2510" PostId="2211" Score="2" Text="@Tom.Bowen89 I'd file that under needs higher resolution or higher framerate. Given that you are either using geom shader to emit 2 sets of vertices to the rasterizer or simply rendering twice." CreationDate="2016-03-22T09:12:49.417" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2513" PostId="2210" Score="1" Text="My laptop which has dual 980m cards is inferior too fyi. The mobile cards add latency apparently due to Intel optimus technology. I was very sad to find that out." CreationDate="2016-03-22T16:49:55.247" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2514" PostId="2160" Score="0" Text="Questions asking us to recommend or find a book, tool, software, tutorial or other off-site resource are off-topic as they tend to attract opinionated answers and spam. Instead, describe your problem or need and the steps, if any, you've taken to solve it. See also [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic)" CreationDate="2016-03-22T17:19:54.807" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2517" PostId="1983" Score="0" Text="You make a good point, which I've taken some time to think about. In the absence of any community support for excluding such questions, I have reopened this one." CreationDate="2016-03-23T15:34:44.407" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2519" PostId="2222" Score="0" Text="This question is a cross-post from http://gamedev.stackexchange.com/questions/118744/how-to-convert-non-axis-aligned-bounding-boxes-to-aabb" CreationDate="2016-03-23T21:30:58.277" UserId="2954" ContentLicense="CC BY-SA 3.0" />
  <row Id="2521" PostId="1720" Score="0" Text="The historical terms are &quot;that's the difference between flat and gouraud's shading&quot;. In practice this relates to normals like explained by joojaa" CreationDate="2016-03-24T05:17:13.580" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="2522" PostId="13" Score="5" Text="I believe this answer is completely false. You are free to multisample outgoing rays and weight their result when combining. You obtain the same truth than the russian roulette technique, but it's generally accepted that the former method is more expensive." CreationDate="2016-03-24T05:27:59.740" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="2523" PostId="1983" Score="0" Text="This is allmost certainly never going to be answered. Besides what does it have to do with 3d graphics." CreationDate="2016-03-24T05:29:00.690" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2524" PostId="2223" Score="0" Text="This is (too) many questions rolled into one. Should it possibly be split up?" CreationDate="2016-03-24T05:33:11.800" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2525" PostId="2223" Score="0" Text="They are all algorithms which apply to the same topic in computational geometry, specifically, computation concerning non-convex enclosed polyhedrons. (Thank you, @nathan-reed, for adding that tag.)" CreationDate="2016-03-25T01:28:16.253" UserId="2957" ContentLicense="CC BY-SA 3.0" />
  <row Id="2526" PostId="1983" Score="0" Text="@joojaa 1. you are probably right that if it hadn't been answered up to this point, it's has low probability of ever being answered. But (a) &quot;low&quot; is not zero, and (b) there was no way to estimate this probability before posting the question." CreationDate="2016-03-25T02:07:53.613" UserId="2574" ContentLicense="CC BY-SA 3.0" />
  <row Id="2527" PostId="1983" Score="0" Text="@joojaa 2. I have explained my rationale for submitting this question to this stackexchange in the post itself, section **The reason I'm posting in Computer Graphics stackexchange**. Moreover, a book (_Multiple View Geometry in Computer Vision_), which isn't quite computer graphics but is surely quite related to it, does come pretty close to giving the answer, I would suggest that this is a reasonably strong argument that the question was indeed appropriate (see section **Two sources that come close**)." CreationDate="2016-03-25T02:09:05.650" UserId="2574" ContentLicense="CC BY-SA 3.0" />
  <row Id="2529" PostId="2093" Score="0" Text="Can i ask you how you defined colors for each triangle when you have already a variable defined in 3D-space ? Cheers, Aurel" CreationDate="2016-03-24T22:51:04.317" UserId="2962" ContentLicense="CC BY-SA 3.0" />
  <row Id="2530" PostId="2232" Score="6" Text="`See the number of pixels you need to update grows exponetially when the sides grow.` Quadratically, i think." CreationDate="2016-03-25T11:45:34.313" UserId="2970" ContentLicense="CC BY-SA 3.0" />
  <row Id="2531" PostId="2232" Score="0" Text="&quot;Copying the data over from the CPU to the graphics card is a relatively slow operation.&quot; This is true, but irrelevant. Copying over a few million pixels at 60 fps is easily achievable over even modest PCI-E links (it would only require a few hundred megabytes per second.)" CreationDate="2016-03-25T13:06:31.973" UserId="2972" ContentLicense="CC BY-SA 3.0" />
  <row Id="2532" PostId="2234" Score="0" Text="My laptop's APU has 256 shader cores @ 686MHZ while my tablet has 192." CreationDate="2016-03-25T13:38:36.217" UserId="2964" ContentLicense="CC BY-SA 3.0" />
  <row Id="2533" PostId="2232" Score="0" Text="So APIs make things faster by letting the GPU do the work?" CreationDate="2016-03-25T13:42:15.433" UserId="2964" ContentLicense="CC BY-SA 3.0" />
  <row Id="2534" PostId="2232" Score="0" Text="Yes - the GPU is capable of doing hundreds of times as many calculations as the CPU." CreationDate="2016-03-25T13:44:59.973" UserId="2971" ContentLicense="CC BY-SA 3.0" />
  <row Id="2535" PostId="2232" Score="0" Text="@pjc50 its not so much that the cpu is faster per core it just has extrenely many cores. Since pixels pushing is a embarassingly parallel business it works." CreationDate="2016-03-25T14:19:36.793" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2536" PostId="2232" Score="0" Text="@Coxy yes but on top of that copying you need to prepare that image. So it limits what i can prepare. Nothing stops you from doing this, with opengl. Prepare image upload prepare upload." CreationDate="2016-03-25T14:21:10.750" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2537" PostId="2232" Score="2" Text="@pjc50 That's not wrong, but also not exactly true. A GPU is specialized to run a single program (usually a shader) in parallel on a large amount of data. So, you need to perform the same operations on lots of data to actually use the computing power of a GPU. If your program doesn't, you better run the program on the CPU." CreationDate="2016-03-25T14:22:53.633" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="2538" PostId="2189" Score="1" Text="Other debugging techniques: Use a non-FBO texture to see if it shows up as you'd expect. Make sure that your FBO is complete by calling [`glCheckFramebufferStatus`](http://docs.gl/gl3/glCheckFramebufferStatus)" CreationDate="2016-03-25T14:57:08.820" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="2539" PostId="1983" Score="0" Text="Have you tried posting this in some approriate journal. This should immediately bring oout the trolls that know it has been addressed." CreationDate="2016-03-25T15:25:50.633" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2540" PostId="2212" Score="0" Text="Did this solve the problem?" CreationDate="2016-03-25T17:06:21.783" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2541" PostId="2232" Score="0" Text="@Cthulhu, quadratic can equal exponential (think about it, exponential = `y=x^2` and quadratic = `y=x^2+2x+1`, they looks the same, but they're in different places on the graph)" CreationDate="2016-03-25T18:00:36.997" UserId="2977" ContentLicense="CC BY-SA 3.0" />
  <row Id="2542" PostId="2234" Score="0" Text="Hey, the Titan X has 5760 cores in it." CreationDate="2016-03-25T18:02:04.110" UserId="2977" ContentLicense="CC BY-SA 3.0" />
  <row Id="2543" PostId="2232" Score="2" Text="@Daniel $x^2$ is a quadratic function. An exponential function would be $2^x$. And exponential functions grow way faster than quadratic functions." CreationDate="2016-03-25T18:14:13.497" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="2544" PostId="2232" Score="0" Text="Gah, my bad - this is what I get for not waking up fully before typing." CreationDate="2016-03-25T18:14:43.663" UserId="2977" ContentLicense="CC BY-SA 3.0" />
  <row Id="2545" PostId="2232" Score="0" Text="many words, don't go to the point. @pjc50 answer is most to the point and understandable" CreationDate="2016-03-25T20:18:55.960" UserId="2981" ContentLicense="CC BY-SA 3.0" />
  <row Id="2546" PostId="2234" Score="0" Text="@Daniel Are there any games that the Titan X goes &lt;  30fps at ultra high .The Titan X is very powerful" CreationDate="2016-03-26T02:33:38.857" UserId="2964" ContentLicense="CC BY-SA 3.0" />
  <row Id="2547" PostId="2234" Score="0" Text="Ummm, One of these, perhaps: http://www.maximumpc.com/10-most-graphically-demanding-pc-games/" CreationDate="2016-03-26T02:46:36.110" UserId="2977" ContentLicense="CC BY-SA 3.0" />
  <row Id="2548" PostId="2231" Score="0" Text="In general doing one operation on a lot of things is more efficient and easier to think about than doing it on every thing individually." CreationDate="2016-03-26T03:29:37.357" UserId="2988" ContentLicense="CC BY-SA 3.0" />
  <row Id="2549" PostId="2231" Score="2" Text="Because every game would need to be rewritten for every graphics card. Unless they didn't use the graphics card, but then they'd be slow." CreationDate="2016-03-26T04:18:47.940" UserId="2316" ContentLicense="CC BY-SA 3.0" />
  <row Id="2550" PostId="2231" Score="0" Text="I think that GPU companies have to create their own DirectX drivers" CreationDate="2016-03-26T04:35:21.333" UserId="2964" ContentLicense="CC BY-SA 3.0" />
  <row Id="2551" PostId="2235" Score="0" Text="So not good for excel :)" CreationDate="2016-03-26T05:31:50.900" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2552" PostId="2093" Score="0" Text="@AurelienSanchez sure. I want to use normal map as a color of pixels, and it's another problem -- to find normal vectors fast. Now I use glfw to render the model and planning to use glReadPixels to achieve matrix with pixels, and calculating normal vectors by myself" CreationDate="2016-03-26T06:35:48.780" UserId="2750" ContentLicense="CC BY-SA 3.0" />
  <row Id="2553" PostId="2232" Score="1" Text="You talk about copying and moving, but what's more important is the actual image generation. You have to determine which object will be visible at which point, how it will be lighted, what will be the effects of smoke, etc, etc. GPUs are highly optimized to perform these operations fast and in parallel. APIs make it easy to express common operations." CreationDate="2016-03-26T06:47:47.153" UserId="2312" ContentLicense="CC BY-SA 3.0" />
  <row Id="2554" PostId="2232" Score="0" Text="@IMil yesIi commented about this. It is more meant to be a lies to children type of answer than a this is the exact reason. 3D is by no reason the only reason we use graphics accelerators." CreationDate="2016-03-26T07:32:34.387" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2555" PostId="2232" Score="0" Text="The operating system could a API which allows everything to be drawn by pixels" CreationDate="2016-03-26T07:48:40.713" UserId="2964" ContentLicense="CC BY-SA 3.0" />
  <row Id="2556" PostId="2232" Score="0" Text="@SuiciDoga there is an api for that both directX and opengl can do this as can direct2D, GDI and numerous others. Just because the api can do more complex things does not mean it cant do simple things too. Just you do not write directly to the buffer on screen somebody else does that for you." CreationDate="2016-03-26T07:58:57.253" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2557" PostId="2232" Score="0" Text="@joojaa what do you mean by 'lies to the children'? There are two options. 1) you calculate every pixel by hand, 2) you say: here are the coordinates, here is the texture, here are the light sources, go ahead and draw them. Option 2 is easier, and it allows graphic chip designers and driver developers to make even old games faster." CreationDate="2016-03-26T08:11:28.833" UserId="2312" ContentLicense="CC BY-SA 3.0" />
  <row Id="2558" PostId="2232" Score="0" Text="@IMil Lies to children is a simplified reality, i know full and well that the answer is not this simple. Its not granted that the CPU is slower than the GPU. It can be faster IF the image you make is not parallelizable. Its just that in general pixels are independent of each other, if this is not the case then it does not hold true. So it is not certain and definitive that this is fastest in a general sense. But since it is there you rarely see other kinds of things. Too many dimensions to consider." CreationDate="2016-03-26T08:27:29.920" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2559" PostId="2234" Score="0" Text="@Daniel The developers of those games must have need about 2-4 Titan X cards :)" CreationDate="2016-03-26T13:30:40.113" UserId="2964" ContentLicense="CC BY-SA 3.0" />
  <row Id="2560" PostId="2234" Score="0" Text="Yep.  I'm going to start developing in UE4, and that engine is a beast.  For max settings in a big world you basically need 4 Titan X's to get 60fps @ 1080p." CreationDate="2016-03-26T14:48:27.263" UserId="2977" ContentLicense="CC BY-SA 3.0" />
  <row Id="2561" PostId="2231" Score="1" Text="&quot;But why would we need all these frameworks and GPU features when we could just draw everything pixel by pixel?&quot; *that's* how it was done in good ol' days. Wolfenstein 3D, Doom, Duke Nukem 3D, Quake and most other games of late '90 used pure software rendering (Quake offered OpenGL renderer as an option)." CreationDate="2016-03-26T17:07:22.950" UserId="2997" ContentLicense="CC BY-SA 3.0" />
  <row Id="2564" PostId="1983" Score="0" Text="@joojaa No, but I would be thankful indeed if your can think of some journal which might be appropriate---I'm certainly willing to try...  By the way, I'm not familiar with journal websites offering this sort of discussion as an option... do you happen to know of examples?" CreationDate="2016-03-26T19:52:22.037" UserId="2574" ContentLicense="CC BY-SA 3.0" />
  <row Id="2566" PostId="2240" Score="1" Text="Are you sure you want N·H, or do you want R·L i.e. reflection vector dotted with light source vector? The latter would be the classic Phong equation (N·H is Blinn), and the figure appears to mark the R·L angle: 15 degrees." CreationDate="2016-03-27T06:01:07.813" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2568" PostId="2240" Score="0" Text="Should the question not point out which Bidirectional Reflectance Distribution Function (BRDF) you need to use? You have Phong, Blinn-Phong, Modified Phong, Modified Blinn-Phong and a whole variety of physically-based ones." CreationDate="2016-03-27T10:17:51.400" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="2569" PostId="2240" Score="0" Text="@NathanReed how can i find R.L if i dont know the vectors? Do i just use 15 degrees?" CreationDate="2016-03-27T13:28:42.073" UserId="2359" ContentLicense="CC BY-SA 3.0" />
  <row Id="2570" PostId="2240" Score="2" Text="@user2976568 The dot product is the cosine of the angle between the vectors (for unit vectors). Just as you remarked that N·L = cos(theta). :)" CreationDate="2016-03-27T17:24:35.710" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2571" PostId="2234" Score="0" Text="I am a beginner with developing games and creating some Unity games using my laptop's APU.I haven't experienced much lag because my laptop has a dedicated class APU which almost as good as a dedicated video card.My APU is a AMD A8-4500M.I do not think that this laptop can use UE4 without a external GPU card (using USB3)." CreationDate="2016-03-28T02:44:14.047" UserId="2964" ContentLicense="CC BY-SA 3.0" />
  <row Id="2572" PostId="2234" Score="0" Text="And to get 4K they must need about 8 Titan X GPUs" CreationDate="2016-03-28T02:46:51.953" UserId="2964" ContentLicense="CC BY-SA 3.0" />
  <row Id="2573" PostId="2231" Score="1" Text="@MatthewRock You're not being helpful. You can certainly bundle an OS with a game inside a docker container and distribute the container. That way, the user doesn't need to install library dependencies for their distro." CreationDate="2016-03-28T03:55:44.500" UserId="2968" ContentLicense="CC BY-SA 3.0" />
  <row Id="2574" PostId="67" Score="1" Text="FWIW, I've heard of displays (usually very large displays in stadiums) that use a delta-nabla configuration. (Named for the Greek delta letter &quot;Δ&quot; and the Hebrew nabla letter &quot;∇&quot; because the pixels were alternating triangles with the point going up, then down, then up, then down.) One example is the Philips Vidiwall." CreationDate="2016-03-28T05:07:44.823" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="2575" PostId="1741" Score="0" Text="@user3531082 Maya does not in fact solve this problem" CreationDate="2016-03-28T08:52:08.703" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2576" PostId="1892" Score="0" Text="@Marqin Could you convert your last comment into an aswer so we can get this post out of the unanswered queue?" CreationDate="2016-03-28T09:03:15.333" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2578" PostId="1892" Score="0" Text="@joojaa But frankly it's not valid answer. Some time ago I've reorganized my shader, changed types/order of  struct fields and now it &quot;miraculously&quot; work. And I still want to know why it was not working the old way - maybe I just made some error with padding? I cannot find that bug :/" CreationDate="2016-03-28T09:36:20.210" UserId="2413" ContentLicense="CC BY-SA 3.0" />
  <row Id="2579" PostId="1892" Score="0" Text="@Marqin then answer that, the question does not contain your code so theres no way to verify your bug from this anyway." CreationDate="2016-03-28T09:41:19.163" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2580" PostId="2243" Score="0" Text="Thanks for the reference !  In case of web application, this adds one more level of difficulties. The fact is that as shadertoy shows, not all people see the same result. Plus in all API already providing a texture loading, I guess we can just hope a linearization is done." CreationDate="2016-03-28T10:43:15.020" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2581" PostId="2243" Score="0" Text="Beside, when preparing a slide on googleDoc comprising imported images, exporting in pdf, and displaying via acroread on the same screen, I already don't have the same gamma for the 2 copies of the image !" CreationDate="2016-03-28T10:44:21.600" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2583" PostId="2240" Score="1" Text="@NathanReed sounds like the answer to me" CreationDate="2016-03-29T08:07:37.823" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2584" PostId="2246" Score="0" Text="They may do some edge checking to maintain the crispness" CreationDate="2016-03-29T12:54:16.693" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2585" PostId="2247" Score="1" Text="What level of authenticity to early-90s games are you looking for? You could just alpha-blend the sprites over the background and it will look &quot;right&quot;. If it must be done using palettes, it's going to be a lot more difficult and limited. Or are you asking about how to do alpha blending? If so, we'll need more info about how your renderer works, which API(s) you're using, etc." CreationDate="2016-03-29T16:26:05.513" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2586" PostId="2247" Score="0" Text="Alpha blending should work(it's not 100% implemented yet), by the way of 50% blending every two color combination in the palette, finding the best fit color from the palette and writing that to an array. This array is then indexed by the background color and the new pixel's color to get the color that is then put on the screen." CreationDate="2016-03-29T17:37:58.307" UserId="3020" ContentLicense="CC BY-SA 3.0" />
  <row Id="2587" PostId="2247" Score="0" Text="OK, assuming you have a palette that has close enough fits for all the blended colors, that could work. But then, what's the question about? Sounds like you already have a plan for how to do it." CreationDate="2016-03-29T17:44:44.280" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2588" PostId="2247" Score="0" Text="Whether to first apply lighting to the sprite texel and then blend it with the background or first blend with the background and then apply lighting. (Or possibly something else entirely?)" CreationDate="2016-03-29T19:56:43.433" UserId="3020" ContentLicense="CC BY-SA 3.0" />
  <row Id="2589" PostId="1741" Score="0" Text="...that you know of." CreationDate="2016-03-29T21:15:07.007" UserId="2091" ContentLicense="CC BY-SA 3.0" />
  <row Id="2590" PostId="2234" Score="0" Text="Further to [pcj50's answer](http://computergraphics.stackexchange.com/a/2234/3007), you had to account for the peculiarities of every graphics card out there. While they claimed to support standards like EGA, VGA &amp; VESA VBE, there were peculiarities &amp; limitations with their implementations that you had to account for. I remember several graphics programs (not games) didn't work properly on my ATI Mach 32 card because ATI's implementation of VGA (or was that VESA VBE) was notoriously dodgy." CreationDate="2016-03-27T12:23:36.130" UserDisplayName="user3007" ContentLicense="CC BY-SA 3.0" />
  <row Id="2592" PostId="2030" Score="0" Text="It would be useful to include the additional information (such as the comment mentioning OS and GPU) in the question so it is more accessible. Also, comments are not intended to last long term. If there's any information you want to add to the question, just [edit]." CreationDate="2016-03-30T02:52:37.057" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2593" PostId="2252" Score="0" Text="Thanks. After reading of Vulkan doc I could find the similar for directx [fine](https://msdn.microsoft.com/en-us/library/windows/desktop/hh446950(v=vs.85).aspx) and [coarse](https://msdn.microsoft.com/en-us/library/windows/desktop/hh446948(v=vs.85).aspx) derivatives." CreationDate="2016-03-30T12:00:36.940" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="2595" PostId="2256" Score="1" Text="Sorry wrote this quickly on my phone. Need to add links and stuff. Maybe organize and a few rounds of google. Feel free to fix my typos." CreationDate="2016-03-30T21:37:12.947" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2597" PostId="1983" Score="0" Text="Not sure if this helps, but the book [3D Engine Design for Virtual Globes](http://www.virtualglobebook.com/) includes a chapter on GPU ray-casting the globe, which involves computing the horizon in the fragment shader.  Disclaimer, I know the authors of this book, after they wrote it they went on to be the founders of [Cesium](http://cesiumjs.org)." CreationDate="2016-03-31T21:08:16.023" UserId="1908" ContentLicense="CC BY-SA 3.0" />
  <row Id="2600" PostId="2030" Score="0" Text="There are usually two limits, one having &quot;native&quot; in the name. Beyond native limits the driver is likely to emulate behavior in software. MB of shader data storage is never good. I had similar problem in DX11 a few years back. You should try move that data to a buffer object of some kind or divide the issue into more passes." CreationDate="2016-03-31T18:53:43.617" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2601" PostId="2030" Score="0" Text="Why is a buffer object better than a shader data storage? I thought it was the other way around." CreationDate="2016-03-31T23:39:04.643" UserId="2666" ContentLicense="CC BY-SA 3.0" />
  <row Id="2602" PostId="2030" Score="0" Text="@gartenriese Better? How do you mean?" CreationDate="2016-04-01T06:07:30.627" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2603" PostId="2030" Score="0" Text="@Andreas: If you look [here](https://www.opengl.org/wiki/Shader_Storage_Buffer_Object), three out of four points are in favor of SSBOs over UBOs." CreationDate="2016-04-01T12:08:45.787" UserId="2666" ContentLicense="CC BY-SA 3.0" />
  <row Id="2604" PostId="2201" Score="0" Text="The comments on your answer suggest some confusion over what is meant by &quot;maximal&quot;. I believe you are using &quot;maximal&quot; to mean a convex surface which is not a subset of a larger convex surface, rather than to mean the largest convex surface that exists in the triangle mesh. This is covered in your question but due to the confusion it might be worth editing to clarify." CreationDate="2016-04-02T07:51:33.137" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2605" PostId="2201" Score="0" Text="It would be helpful to know what the purpose is, to give a better idea of exactly what is required. For example, does a convex patch need to be strictly convex (never flat), or does it count as convex provided it is nowhere concave? A cylinder is nowhere concave. It is convex everywhere, but it is not strictly convex in every direction (it is flat in the axial direction). Do you want the surface of a cylinder to count as one convex patch?" CreationDate="2016-04-02T07:59:20.887" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2606" PostId="2259" Score="0" Text="Could you clarify what you mean by &quot;360 degree stereo video&quot;? You can create a stereo image by taking two images from  different positions (approximating one from each eye). However, if you take two 360 degree images then the stereo effect will be strongest in the directions perpendicular to the offset between the images, and zero in the directions parallel to the offset. To get a stereo effect in all directions I would expect two images in each of a wide variety of directions to be necessary. Answering your question will require knowing more about the input that is provided." CreationDate="2016-04-02T08:12:36.327" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2608" PostId="2259" Score="0" Text="In the sense of what Kolor Eyes uses. You have a video, e.g. 1920x1080, split into two halves (side by side), each half (960x1080) then needs to be mapped onto a sphere and then you render the viewpoint of a viewer at the centre of that sphere looking out in a particular direction." CreationDate="2016-04-03T00:51:46.103" UserId="3038" ContentLicense="CC BY-SA 3.0" />
  <row Id="2611" PostId="2201" Score="0" Text="@trichoplax thanks for the suggestion! yes, a convex surface which is not a subset of a larger convex surface, is what is meant. And yes, it needs to be strictly convex. For a cylinder, there would be one convex patch in the whole figure which is maximal, and this patch would not include the axial flat surfaces." CreationDate="2016-04-03T08:43:49.370" UserId="2898" ContentLicense="CC BY-SA 3.0" />
  <row Id="2613" PostId="2259" Score="0" Text="Kolor Eyes appears to provide 360 degree video, but not in stereo. Are you looking to make your 360 degree stereo video by combining two 360 degree videos from two similar viewpoints, or by combining a large number of 360 degree videos from many similar viewpoints?" CreationDate="2016-04-03T10:54:20.407" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2614" PostId="2259" Score="0" Text="You mention rendering in your comment. Are you working with recorded real world video here, or a rendered artificial scene?" CreationDate="2016-04-03T10:54:59.813" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2615" PostId="2259" Score="0" Text="Note that requests for software recommendations and off site resources are off topic. See [What topics can I ask about here?](http://computergraphics.stackexchange.com/help/on-topic) I recommend editing to remove the request for existing software or libraries, keeping just the request for how to program this yourself, in order to avoid the question being closed." CreationDate="2016-04-03T11:05:03.730" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2616" PostId="2201" Score="0" Text="I meant that the curved surface of the cylinder is itself flat in the direction parallel to its central axis. Depending on whether you move to the next triangle around the cylinder or along the cylinder, it will appear to be either strictly convex (moving around), or flat (moving along). Do you require the surface to be strictly convex in every direction, or only one direction?" CreationDate="2016-04-03T11:15:01.443" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2617" PostId="2204" Score="0" Text="A possible problem with this approach is that for some surfaces there will exist a path between two adjacent triangles that is made up only of convex steps, even though the two adjacent triangles touch along a concave edge. In sufficiently smooth examples this may not be a problem as the inaccuracy will only tend to be one triangle wide, but it is possible to construct examples with arbitrarily large concavities that are accessible by purely convex paths. For example, imagine the shape made by pushing a pin into a balloon so that the surface dips inwards (assuming it doesn't burst)." CreationDate="2016-04-03T11:30:31.910" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2618" PostId="2201" Score="0" Text="It may help to consider a torus. Do you require an algorithm that makes the outer half of the torus a single convex patch, with the triangles on the inner half (around the hole) not counted as being in any convex patch? Or do you require an algorithm that makes the whole surface of the torus a single convex patch, since even on those parts of the surface that are concave in one direction, they are still convex in the perpendicular direction?" CreationDate="2016-04-03T11:38:01.053" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2620" PostId="2189" Score="1" Text="Assuming that the content of the FBO is correct, and based on your comment and the look of the cube on your screenshot, it sounds to me your texture coordinates are simply incorrect. You could use a regular texture as Mokosha suggests, and switch to FBO only after you have the cube textured correctly." CreationDate="2016-04-03T12:32:00.377" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="2622" PostId="2266" Score="1" Text="You can actually turn Photoshop effects to be profile  aware its just not on by default because of backwards compatibility and least surprise for old users. Mind you though its not perfect. I would say that in general ALL of our color correction workflow is totally whacked because if the tacked on by later date nature. so for 5 i propose (d) because its hard and time consuming to rebuild a new all encompassing standard that can replace the old ones." CreationDate="2016-04-04T07:12:00.370" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2623" PostId="2201" Score="0" Text="@trichoplax strictly convex is what I am looking for. In the example of the Torus, I require an algorithm that makes the outer half of the torus a single convex patch, with the visible (the inner surface of the other side of the torus will be visible to the viewer) triangles on the inner half (around the hole) , that forms a concave lateral arc to the viewer, it should show convex strips perpendicular to the concave arc. I hope I was able to communicate the idea effectively." CreationDate="2016-04-04T07:57:35.103" UserId="2898" ContentLicense="CC BY-SA 3.0" />
  <row Id="2624" PostId="2204" Score="0" Text="@trichoplax I am myself not sure that the algorithm I have posted is optimal or the best one. If you have a better algorithm, please do share. I would love receive it! If another answer turns out to be better, I will unaccept my answer the accept the better one. And for your example, as I said above in coment to the question, if there are local convex strips, they should be shown as local convex strips." CreationDate="2016-04-04T08:00:49.620" UserId="2898" ContentLicense="CC BY-SA 3.0" />
  <row Id="2627" PostId="2201" Score="0" Text="Your description of how the torus should be divided is perfectly clear, but I can't see a way of translating that into a rigorous requirement that would apply to other types of surface. The strips are distinct from each other due to concave edges, but they all seem to be connected to the large outer convex patch, and therefore part of it. I think the main question here is how to unambiguously define what a maximal convex patch is." CreationDate="2016-04-04T12:39:02.793" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2629" PostId="2204" Score="0" Text="I understand that your answer is just an example to get things started. I was just giving some feedback in the hope that it will trigger an idea for improvement from someone." CreationDate="2016-04-04T12:40:59.857" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2635" PostId="2271" Score="0" Text="For VR you need super low latency, any type of network hop will be too long." CreationDate="2016-04-05T10:18:00.380" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2636" PostId="2271" Score="0" Text="Only if the cloud is very close to the vr gear like, within tens of meters. But given that everything and their dad is going to be a computer that is certainly possible." CreationDate="2016-04-05T12:59:35.223" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2639" PostId="2272" Score="0" Text="There are two causes of tearing: rendering to the visible buffer (front buffer rendering) and swapping to the back buffer in the middle of a display refresh. Vsync assumes you are doing at least double buffered rendering and holds the swap until vblank - there is no tearing whatsoever. Since rendering rarely completes during vsync, two buffers can be held up waiting on display (one currently displayed, one waiting for vsync) thus requiring triple+ buffering to keep rendering continuous..." CreationDate="2016-04-05T17:50:14.567" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2644" PostId="2271" Score="0" Text="@ratchetfreak,  I added more detail info above. That explains what I'm thinking" CreationDate="2016-04-06T00:59:47.590" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2645" PostId="2273" Score="0" Text="I added more detail info above. That explains what I'm thinking" CreationDate="2016-04-06T01:00:01.760" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2646" PostId="2271" Score="0" Text="@HaoZhang well if your calculations that the total time to display a frame to the user from the cloud is 15.5ms then things should work out. In order to have a game running at 60FPS you need to display a frame on the screen every 16.6ms. Are you sure its going to take 15.5ms to display both frames(one for each screen of the VR system)?" CreationDate="2016-04-06T07:18:05.233" UserId="204" ContentLicense="CC BY-SA 3.0" />
  <row Id="2647" PostId="2271" Score="0" Text="I give it 5ms for transmission to OLED display of 2K*2@200Hz, that would need at least 40Gbps interface and cable, as display port 1.4. (https://en.wikipedia.org/wiki/DisplayPort) supports. Yes it would be a little costly but it is achievable." CreationDate="2016-04-06T08:45:59.250" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2648" PostId="2272" Score="0" Text="Yes, I heard triple buffer technology. That's cool. Thanks." CreationDate="2016-04-06T09:00:12.287" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2650" PostId="2275" Score="1" Text="The reason they're using a FPGA, is they require hardware features that the GPU does not have. So they emulate a GPU using a FPGA." CreationDate="2016-04-06T14:24:19.277" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2653" PostId="2008" Score="3" Text="@Dragonseel: since it was the correct observation, could you turn the comment into an answer?" CreationDate="2016-04-07T02:53:05.380" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="2654" PostId="2276" Score="0" Text="Hi Nathan, nice to meet you here. I have read some of your presentations on Nvidia Gameworks VR. &quot;Timewarp&quot; can reduce the latency but with image quality loss. And I think, if the head rotated very fast,  it's very hard to warp the 2D image to cheat the eyes. Motion blurring may be help in this case. Local client or remote server could send an unclear image to the display in very low latency. You mentioned Cloud could do some offloadings on VR. That's a cool. What's the requirement for the network bandwidth and latency? Does it still need high bandwidth and low latency? Thanks." CreationDate="2016-04-07T06:08:09.030" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2655" PostId="2277" Score="0" Text="The second paper use &quot;ray casting&quot; -- a simplified version of  &quot;ray tracing&quot;. It could reduce the computing power dramatically. I think the rendering quality is not as good as full ray tracing rendering. But, is it acceptable in most cases? And, as what you said, frameless rendering, or racing the beam, could be implemented as spliting the screen into several blocks and &quot;racing the block&quot; instead of &quot;racing the beam&quot;. Of course it need modify the GPU and display. It seems wonderful, does it?" CreationDate="2016-04-07T06:31:30.850" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2656" PostId="2275" Score="0" Text="It seems that in second paper the authors used &quot;ray casting&quot; instead of rasterization. So they used FPGA directly, not emulate a GPU? And it also seems they didn't support any API like OpenGL or DirectX. Do you mean the authors just need FPGA to &quot;race the beam&quot;?" CreationDate="2016-04-07T06:38:59.827" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2657" PostId="2276" Score="0" Text="You are right, current internet access are mostly based on dialup or DSL. But, in the near future, e.g. 5-10 years later, could something be changed? Google has deployed 1Gbps fiber in some cities and I heard Google also tried to introduce 10Gbps to home." CreationDate="2016-04-07T06:48:39.943" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2658" PostId="2276" Score="0" Text="haha, Nathan, I got some information from your blog: (http://www.reedbeta.com/blog/2014/04/03/vr-and-multi-gpu/#more-591), &quot;That being said, what if the high-spec dGPUs were far away from you? Cloud-based rendering is a topic that’s been getting interest lately, and the same kind of latency reduction strategies could be applicable there. Imagine having your VR headset driven by the little GPU in your phone, but streaming down source frames or shading cache updates from a dGPU in the cloud (or your home PC)! &quot;" CreationDate="2016-04-07T07:23:32.787" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2660" PostId="2279" Score="0" Text="I can see shadows. Therefore i do not concurr with your analysis. However secondary light might have too high reflectivity. Could you try using a simpler scene." CreationDate="2016-04-07T13:10:39.740" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2661" PostId="2279" Score="1" Text="@joojaa edited the description with a target reference image" CreationDate="2016-04-07T13:31:07.520" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2662" PostId="2279" Score="0" Text="Using a simpler scene helps you debug the effect. To me it looks like the shadows are there your scene is just not color corrected and you bounce too nuch energy." CreationDate="2016-04-07T13:57:06.897" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2663" PostId="2275" Score="0" Text="As Nathan mentioned in his anwer, in order to &quot;race the beam&quot;, you need a very tight synchronization of the &quot;graphics hardware&quot; and the OS/CPU. Current GPUs and Graphics APIs don't have this support. So the authors created the hardware themselves using a FPGA. In theory, the hardware they created in FPGA could be manufactured into &quot;real&quot; hardware." CreationDate="2016-04-07T14:59:28.613" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2664" PostId="2008" Score="0" Text="@JulienGuertault yes. Done" CreationDate="2016-04-07T14:59:52.257" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2666" PostId="2271" Score="0" Text="Although this question as phrased appears to be broad and opinion based, the answers show that the possibilities are not as arbitrary as they may at first appear, and objective restrictions can be placed on what is likely to happen." CreationDate="2016-04-07T16:21:10.270" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2667" PostId="2279" Score="0" Text="@joojaa I just posted some of the code because I am using a much more complex framework. That's why I was avoiding rendering more simple scenes, because it requires me some effort finding out how exactly the scene loader works.  Ok, may you suggest me code-wisely what I could change to color the scene correctly?" CreationDate="2016-04-07T16:27:09.340" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2668" PostId="2281" Score="0" Text="First of all thanks for your answer :)&#xA;&#xA;So, the method to check the intersection between the ray and the square light is right. I have already been asking about it cause of a small error I couldn't find. The method and the relative solution can both be found here:&#xA;&#xA;http://stackoverflow.com/questions/36180741/intersection-of-ray-and-rectangle-in-c/36186088#36186088&#xA;&#xA;I also edited the question adding a picture that shows the result in case I omit the **depth &gt; 1** check. Maybe it can help understanding where the problem is.. thanks once more" CreationDate="2016-04-07T19:52:56.443" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2669" PostId="2279" Score="0" Text="It might help to restrict your image to a very small region in which you expect to have a very obvious shadow. Then you can get a more detailed look at that region in a much shorter rendering time, to see if it really is zero shadow, or just a less obvious softened shadow." CreationDate="2016-04-07T23:24:54.983" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2670" PostId="2275" Score="0" Text="You say &quot;real hardware&quot;, does it mean ASIC?" CreationDate="2016-04-08T00:31:58.077" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="2671" PostId="2283" Score="2" Text="Pro tips, unrelated to your question: you can expect `(foo * foo)` to be a lot faster than `pow(foo, 2.0f)`; if your data is organized row by row (as opposed to column by column), you should swap the for loops to traverse data in a more coherent order." CreationDate="2016-04-08T02:02:55.560" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="2672" PostId="1519" Score="0" Text="Do you have sources to recommend on the fact that metals do transmit light? What is the order of magnitude of the contribution to the perceived color?" CreationDate="2016-04-08T08:03:34.527" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="2676" PostId="1519" Score="0" Text="@JulienGuertault It's fairly standard knowledge, and Google returns useful results. E.g. [this](https://www.reddit.com/r/askscience/comments/37ktye/would_it_be_possible_to_make_a_thin_enough_sheet/crnsmgw) &quot;Below 50 nm thickness, the light transmitted through gold is green (imagine all the colours that aren't reflected are coming through . . .).&quot; Probably appearance depends. Metals reflect a lot, even with thin coatings. The amount refracted depends on the extinction coefficient and thickness." CreationDate="2016-04-08T16:25:32.930" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="2682" PostId="2288" Score="1" Text="Could you narrow things down a bit. Like what have you tried. What is your level of expertise. Think of it thisway, if I were to answer you as briegly as you ask would you be happy?" CreationDate="2016-04-09T05:14:46.607" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2683" PostId="2288" Score="0" Text="I am pretty much a newbie, never touch cg before, just try to get some keywords so that I can google around and move forward. But details are welcomed too." CreationDate="2016-04-09T10:27:03.123" UserId="3074" ContentLicense="CC BY-SA 3.0" />
  <row Id="2684" PostId="2288" Score="1" Text="Problem is that since your question is high level you also get a high level answer that pobably does not help you much. The first step is hard. Not the what to do once you have it done." CreationDate="2016-04-09T10:54:49.477" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2685" PostId="2030" Score="0" Text="@gartenriese (Sry for delay, didnt have rep to comment) Oh I see. SSBO have additional features, and that is &quot;better&quot;. Ok. My idea of a &quot;buffer storage&quot; was not SSBO/UBO. This post mentions &quot;texture buffer objects&quot; that are quite large without the requirement to be writeable: http://stackoverflow.com/questions/7954927/glsl-passing-a-list-of-values-to-fragment-shader" CreationDate="2016-04-09T15:15:36.510" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2687" PostId="2294" Score="0" Text="Did you get to try this? Did it work?" CreationDate="2016-04-10T07:33:33.293" UserId="3083" ContentLicense="CC BY-SA 3.0" />
  <row Id="2691" PostId="2275" Score="0" Text="No, just any chip. It could be a modified GPU. When I say 'real', I just mean that it's silicon logic, rather than programmed logic, ie. a FPGA." CreationDate="2016-04-10T15:17:58.693" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2692" PostId="2294" Score="0" Text="I just did it now. I originally had the pos defined in the 4x4 view matrix, but i took it out and did it at the end, and it worked! Thankyou!" CreationDate="2016-04-10T16:05:34.780" UserId="2953" ContentLicense="CC BY-SA 3.0" />
  <row Id="2693" PostId="2291" Score="0" Text="your answer was very useful, thanks first of all ! You where right, **distFromLight** and  **distFromObj** where wrongly calculated. I fixed it. Something has been improved but the problem is not fixed yet. I posted one picture showing the new results. I thought maybe the error relies on too soft shadows due to a too big light, therefore I tried to lower the dimension of the light. No matter how increase the power of the light, but in this case results are bad (posted a second picture about it). Hope it helps you. In the meantime I am working on the .obj loader for a simpler scene." CreationDate="2016-04-10T17:15:00.353" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2695" PostId="2300" Score="0" Text="Looks fine to me. Is anything not working as it should, or do you have any specific questions about the code or Euler angles? Asking for &quot;your opinion about this approach&quot; is very vague and will probably lead to questions like this being closed on this site." CreationDate="2016-04-10T19:45:35.710" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2696" PostId="2300" Score="1" Text="Thanks @NathanReed for your reply and reviewing my code. You were right about the 'open endedness' of the question, thus, rephrased it." CreationDate="2016-04-10T20:40:28.353" UserId="3098" ContentLicense="CC BY-SA 3.0" />
  <row Id="2697" PostId="2291" Score="1" Text="A smaller light means more of the rays will never reach the light, so you will need to increase the number of rays to give a better quality image. This will also increase the time it takes to render. It might be worth instead making the light as large as possible (an infinite or very large plane). This way, if the problem is transparent surfaces, you should see even more evidence for it, with the added benefit that the largest possible light will reduce the graininess of the image." CreationDate="2016-04-10T22:45:39.950" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2698" PostId="2291" Score="0" Text="I've added an additional final paragraph about putting the light underground as a test (see edited answer) that may help if you are unable to change the scene, but able to move the light." CreationDate="2016-04-10T22:49:17.140" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2699" PostId="2279" Score="1" Text="Seeing your updated screenshot (3rd edit), is it still rendered with 16spp? With a only small area light, and no bidirectional path tracing or next event estimation, it's expected to have a lot of noise. Have you tried a much higher number, like 1000spp?" CreationDate="2016-04-11T01:55:11.087" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="2700" PostId="2301" Score="3" Text="Your question looks like it would belong to StackOverflow rather than here." CreationDate="2016-04-11T04:40:07.833" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="2701" PostId="2030" Score="0" Text="@Andreas: Thanks for the link, very informative. However the top answer says that SSBOs are even larger than TBOs, so I don't think this should be the problem. But I can try anyways!" CreationDate="2016-04-11T06:24:15.123" UserId="2666" ContentLicense="CC BY-SA 3.0" />
  <row Id="2702" PostId="2030" Score="0" Text="@gartenriese That's not what I read. SSBO are limited to 16MB. TBO is limited to VRAM. And you do have more VRAM than that, right? In any case you should check those parameters for you platform (glGetInteger)." CreationDate="2016-04-11T06:34:53.510" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2703" PostId="2300" Score="1" Text="[Meta discussion about review questions.](http://meta.computergraphics.stackexchange.com/q/223/16)" CreationDate="2016-04-11T07:04:09.383" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="2704" PostId="2030" Score="1" Text="@Andreas: No, they are not limited to 16MB. 16MB is the minimal size it is guaranteed to have. Usually they are limited to VRAM. If you look at my question I already checked the size with `GL_MAX_SHADER_STORAGE_BLOCK_SIZE` and it's 2GB (which isn't actually my VRAM size, but still big enough)." CreationDate="2016-04-11T09:53:47.163" UserId="2666" ContentLicense="CC BY-SA 3.0" />
  <row Id="2705" PostId="2030" Score="0" Text="@gartenriese Oh right, my bad." CreationDate="2016-04-11T09:55:06.207" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2706" PostId="2279" Score="0" Text="@JulienGuertault yes it is still rendered with 16 spp. No I didn't try with 1000spp because it would take a huge amount of time and this project is about a research topic which aims to good quality images with low number of spp." CreationDate="2016-04-11T12:59:48.033" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2707" PostId="2291" Score="0" Text="Ok, I followed your suggestion. I tried to put the light beneath the scene and nothing is lit up. No light passes through surfaces and therefore surfaces are not transparent. At least we can exclude this possibility.. so it can be that shadows are too soft or washed out by too much light reflected from the rest of the scene. How do you suggest me to proceed?" CreationDate="2016-04-11T13:06:12.773" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2708" PostId="2291" Score="0" Text="Have you tried rendering just a small section of the image yet? For example, the bottom right sixteenth of the image contains are strong shadow in the target image. Rendering just that small image would allow you to use a much higher number of samples per pixel, to give a clearer image and more idea of what is going on." CreationDate="2016-04-11T14:03:53.410" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2709" PostId="2291" Score="0" Text="just did it, shadows are not there.. you can find the piece of image above at EDIT 4" CreationDate="2016-04-11T14:38:21.090" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2710" PostId="2291" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/38237/discussion-between-tarta-and-trichoplax)." CreationDate="2016-04-11T15:39:07.487" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2736" PostId="2302" Score="3" Text="I would just add that Euler angles work very well for FPS-style or orbiting cameras, where you can only rotate on 2 axes, and controls map directly to the yaw and pitch angles. There are no gimbal lock problems then. If your camera can rotate on all 3 axes, then quaternions may make more sense." CreationDate="2016-04-13T06:06:54.480" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2738" PostId="2287" Score="0" Text="But how does the OS ask the GPU for memory to write to when you plugged in an HDMI port. Do OSes manage their own framebuffer when doing &quot;naive&quot; drawing operations and send the 1920*1080 pixel big framebuffer to the GPU with some arbitrary and common standard that can be implemented or does one need a driver for each GPU type so that one can send a simple text message over HDMI ? How does  a tty or xserver send it's content through HDMI to the screen?" CreationDate="2016-04-13T10:06:14.970" UserId="2623" ContentLicense="CC BY-SA 3.0" />
  <row Id="2739" PostId="2287" Score="0" Text="The video card in a PC will handle encoding VRAM to a video signal, the CPU is never involved in this. For unaccelerated OS's they will keep a system memory based frame buffer and use the CPU to draw into it, and then send regions of it over the PCIE bus to the card (typically 'dirty' regions or just areas of the screen that need updating), so some GPU driver/motherboard driver is required to do this." CreationDate="2016-04-13T10:19:22.433" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="2740" PostId="2287" Score="0" Text="So implementing this in my very own OS will not really be possible using open and known standards?" CreationDate="2016-04-13T10:34:27.023" UserId="2623" ContentLicense="CC BY-SA 3.0" />
  <row Id="2741" PostId="2305" Score="2" Text="If you plan to immediately answer your own question please at least mention it in the question so others don't waste their time." CreationDate="2016-04-13T12:50:45.033" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="2742" PostId="2305" Score="2" Text="Every good question should have 2 or 3 good answers. You did not waste your time and I really appreciated your effords. I need this thread to explain the logic to a project partner and your post will help him alot." CreationDate="2016-04-13T12:53:50.467" UserId="361" ContentLicense="CC BY-SA 3.0" />
  <row Id="2743" PostId="2279" Score="0" Text="Comments are not for extended discussion; this conversation has been [moved to chat](http://chat.stackexchange.com/rooms/38357/discussion-on-question-by-tarta-path-tracer-not-rendering-shadows)." CreationDate="2016-04-13T22:48:22.993" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2744" PostId="2279" Score="0" Text="@SlippD.Thompson and Tarta, this is not an appropriate place to have a discussion about behaviour on an external site. You are both very welcome on this site, but complaints relating to other sites will need to be addressed there, not here. If you use the chat room created to continue discussing the original path tracing question, please do so respectfully. Heated conflict over differences tends to decrease understanding rather than increase it." CreationDate="2016-04-13T23:00:09.740" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2745" PostId="2307" Score="0" Text="Thanks for your reply.&#xA;Idea with writing directly to color buffer sounds great. I will give it a try." CreationDate="2016-04-14T05:13:19.750" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="2746" PostId="2309" Score="0" Text="Yes this tracing works, but if the object was sealed in the first place wouldt the contour have a sidedness?" CreationDate="2016-04-14T09:17:09.850" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2747" PostId="2309" Score="0" Text="I'm not sure I follow. Isn't the point of the question to determine this sidedness? It's not like you magically get it as a parameter after the CSG slice." CreationDate="2016-04-14T09:23:23.287" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="2748" PostId="2309" Score="1" Text="You do get the sidedness of  the model, 3d already has a sidedness which is almost allways the case. When you slice the model you know which side is outwards so for each loop you also know which side of it is inside (jsut record normal in 2D also). This means that if you loop around the contour clockwise (from above) and the predominate cross product of each edge is away from you then its a interior edge. But the trace trick can be faster as its a O(log(n)) operations while this test is n." CreationDate="2016-04-14T09:43:29.437" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2749" PostId="2309" Score="0" Text="Ah, so you basically mean using the surface normal for that? I can see that working, but only if you have a way of knowing what &quot;away&quot; means. :) And while it's trivial for convex meshes (centroid should be fair enough), concave ones will be tricky. The ray trace method sidesteps this issue." CreationDate="2016-04-14T10:37:14.187" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="2750" PostId="2311" Score="0" Text="Yes i know this, but wondering if there could be a alternate strategy to avoid division by 0." CreationDate="2016-04-14T10:37:54.963" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2751" PostId="2309" Score="1" Text="that's whats i was alluding winding rules can handle this for you but tracing is less work." CreationDate="2016-04-14T10:40:02.013" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2752" PostId="2287" Score="0" Text="You could have a look here for some more specifics: http://wiki.osdev.org/VGA_Hardware" CreationDate="2016-04-15T06:26:36.063" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="2759" PostId="2305" Score="2" Text="@5chdn [Self answering is highly encouraged](http://blog.stackoverflow.com/2012/05/encyclopedia-stack-exchange/). Thanks for adding this answer. If you have an answer in mind at the time you post a question, there is a tick box below the question which will allow you to write your answer before posting the question, so both will appear at the same time. This is just to let you know in case it is useful - you certainly don't need to do this and self answers are still very welcome at any interval of time after posting the question." CreationDate="2016-04-16T09:48:50.590" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2762" PostId="2311" Score="0" Text="You could avoid divide by zero by pushing in or out values that would cause one." CreationDate="2016-04-17T00:24:11.360" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2763" PostId="2318" Score="0" Text="Also look into GJK and MPR (minkowski portal refinement)" CreationDate="2016-04-17T01:05:03.843" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2764" PostId="2291" Score="0" Text="EDIT 6 contains the final explanation" CreationDate="2016-04-17T10:22:27.487" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2765" PostId="2310" Score="0" Text="Imagine having a 3d object INSIDE your eye. How would that look like in your opinion? What do your buddies think?" CreationDate="2016-04-17T11:06:14.913" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2766" PostId="2310" Score="0" Text="@Andreas yes but that only how the linear algebra trick works we could use numerous other tricks." CreationDate="2016-04-17T11:17:16.863" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2767" PostId="2310" Score="0" Text="What other tricks?" CreationDate="2016-04-17T11:21:15.167" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2768" PostId="2310" Score="1" Text="@Andreas we could do spherical projections for example. Just because we have one mathematical model that works does not mean there are no other ways to solve problems." CreationDate="2016-04-17T11:50:04.330" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2772" PostId="2310" Score="0" Text="Removed my answer" CreationDate="2016-04-17T14:59:11.590" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2773" PostId="2311" Score="0" Text="@joojaa Raytracing does not divide by zero." CreationDate="2016-04-17T15:01:07.537" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2774" PostId="2311" Score="0" Text="@Andreas Yes i know that neither does reality ;)" CreationDate="2016-04-17T15:01:44.980" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2776" PostId="2326" Score="0" Text="Temporal algorithms ought to help, by reusing information from frame to frame, thus amortizing rendering costs over time." CreationDate="2016-04-18T04:45:53.690" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2777" PostId="2323" Score="0" Text="Ok now we are getting somewhere. I am not really concerned in how most rasterizers work but rather if there is a novel way to do it that does not have this problem. I am thinking of accepting this answer as it hints at the answer, and would avoid me having to show that in fact one can do this by approaching the problem differently than how current pipelines do it. Its not a feature of projection but rather the matrix operations and how we read the data in the projection." CreationDate="2016-04-18T07:18:21.897" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2778" PostId="2332" Score="1" Text="Are you looking to replace regions of pure red with pure blue? Or are you looking to change the red component of each colour to blue (so an orange region becomes cyan)? Somewhere in between these two approaches, you could identify regions of the image that are &quot;sufficiently red&quot; and only apply the change to those regions. If you edit the question to describe more specifically what you require, we will be better able to suggest algorithms." CreationDate="2016-04-18T09:33:19.480" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2779" PostId="2332" Score="0" Text="I want to change pure red with pure blue maintaing the lightning and shades of color" CreationDate="2016-04-18T09:40:28.387" UserId="2383" ContentLicense="CC BY-SA 3.0" />
  <row Id="2780" PostId="2332" Score="0" Text="This sounds like you want to change more than just one precise colour. In this [picture of an apple](https://upload.wikimedia.org/wikipedia/commons/2/20/Civni-Rubens_apple.jpg) would you want all of the reddish parts to become bluish? Or would you want only the very reddest parts to become blue, and the rest of the reddish parts to remain reddish?" CreationDate="2016-04-18T11:13:05.583" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2781" PostId="2332" Score="0" Text="i want to all red to become blue? for both wouldn't the algorithm differ only in the threshold taken for red" CreationDate="2016-04-18T11:33:21.457" UserId="2383" ContentLicense="CC BY-SA 3.0" />
  <row Id="2782" PostId="2332" Score="0" Text="Do you have an example before and after image?" CreationDate="2016-04-18T13:31:29.027" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2783" PostId="2167" Score="0" Text="Very interesting post. Could you post some images using different constants?" CreationDate="2016-04-18T18:07:41.437" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2785" PostId="2334" Score="0" Text="What I am trying to do is Color Replacement Tool in Photoshop. I donot know what Channel Mixer does. I am trying to replicate the Color Replacement Tool." CreationDate="2016-04-19T03:36:26.743" UserId="2383" ContentLicense="CC BY-SA 3.0" />
  <row Id="2786" PostId="2332" Score="0" Text="What i am trying to do is done in this tool http://explorug.net/coloranything/   I want to know what are the best methods to achieve this. This is same as PhotoShop's Color Replacement Tool." CreationDate="2016-04-19T03:37:48.830" UserId="2383" ContentLicense="CC BY-SA 3.0" />
  <row Id="2788" PostId="2343" Score="1" Text="some type of [linear regression](https://en.wikipedia.org/wiki/Linear_regression) would help find the parameters" CreationDate="2016-04-19T14:04:13.627" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2789" PostId="2342" Score="0" Text="Do you have any physical interpretation for the two normal maps? e.g. one is high-frequency and one is low-frequency and applied &quot;on top of it&quot; (which is the Reoriented Normal Mapping case)?&#xA;&#xA;I would expect that you would need to feed the mesh's normal/tangent/bitangent into the equation somehow. You might consider simply transforming the world space normal into tangent space, applying RNM, then transforming the result to world space—that's only one extra 3x3 mul." CreationDate="2016-04-19T14:17:41.263" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="2790" PostId="2342" Score="0" Text="Thanks for your answer @JohnCalsbeek but using baked tangent space normal doesn't provide good enough results that's why I am using a world space normal, especially a world space bent normal map, and I try to apply on top of if the regular normal map." CreationDate="2016-04-19T19:24:20.670" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="2791" PostId="2342" Score="0" Text="I've added some screenshot to illustrate the issue I am having." CreationDate="2016-04-19T20:01:02.903" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="2793" PostId="2346" Score="4" Text="Maybe a silly question, but you're setting up two separate VAOs and two separate arrays of VBOs for the two meshes, right? If you're accidentally re-using them, that would explain why setting-up B screws up the rendering of A." CreationDate="2016-04-19T22:46:37.597" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2794" PostId="2341" Score="0" Text="Hi, we try to avoid link only answers. Can you summarize the contents of the link. Thisway your answer is not nearly as prone to link rot." CreationDate="2016-04-20T04:58:39.273" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2795" PostId="2341" Score="0" Text="@joojaa I've corrected the answer." CreationDate="2016-04-20T05:44:47.307" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="2797" PostId="2350" Score="0" Text="What Icetigris said, with the correction that although nails aren't always reflective, they have a high Fresnel constribution, especially on the tops of the lengthwise ridges. These ridges tend to be polished through normal wear, even if the valleys of the ridges are more diffuse." CreationDate="2016-04-20T17:58:44.800" UserId="3184" ContentLicense="CC BY-SA 3.0" />
  <row Id="2798" PostId="2328" Score="0" Text="Thanks for your answer Julien! I do agree with you, most likely there is some error in the code.. I am not sure whether I am doing some wrong calculation or there is some error at the level of the algorithm itself. That's why I posted my code.. unfortunately I tried to implement some variance reduction algorithm like next event prediction.. the results are not better. Indeed those algorithm should give back a &quot;better&quot; image, but something is wrong at the base" CreationDate="2016-04-21T07:53:12.640" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2800" PostId="2208" Score="0" Text="Questions asking us to **recommend or find a book, tool, software, tutorial or other off-site resource** are off-topic for Computer Graphics as they tend to attract opinionated answers and spam. I have edited to remove that request, so that the question need not be closed. If this changes your intention in other ways, please edit to correct this." CreationDate="2016-04-21T11:20:42.770" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2801" PostId="2355" Score="1" Text="There is [rotating calipers](https://en.wikipedia.org/wiki/Rotating_calipers) but that only works for convex polygons. Otherwise you can use it to as a base for a brute force solution." CreationDate="2016-04-22T09:51:58.693" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2802" PostId="2355" Score="3" Text="Well if O(n^2) isnt a problem then test all point pairs" CreationDate="2016-04-22T09:52:26.363" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2804" PostId="2355" Score="0" Text="Yes, diameter, intra-shape width, not sure if theres a proper name for it. E.g. how'd you measure the maximum length of a 5-point star shape?" CreationDate="2016-04-22T10:18:05.417" UserId="3204" ContentLicense="CC BY-SA 3.0" />
  <row Id="2805" PostId="2355" Score="0" Text="was hoping for a simpler solution than this: https://gis.stackexchange.com/questions/32552/how-to-calculate-the-maximum-distance-within-a-polygon-in-x-direction-east-west" CreationDate="2016-04-22T11:18:34.263" UserId="3204" ContentLicense="CC BY-SA 3.0" />
  <row Id="2806" PostId="2355" Score="2" Text="Actually it's a bit more involved: imagine 2 rooms connected by a narrow corridor. The largest diameter will end on the walls in the different rooms and won't end on any points." CreationDate="2016-04-22T11:19:25.323" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2807" PostId="2355" Score="1" Text="Are you looking for an algorithm that works in the most general case or can it be restricted to e.g. the 2D case? This might be easier to solve with some more information or restrictions about the input. You use the word polygon which may hint at 2D-only, also the question you linked suggests the 2D case. Also, is it enough to consider vertex-vertex distances or do you need correct results for cases like ratchet freak mentioned in [his comment](http://computergraphics.stackexchange.com/questions/2355/find-the-longest-straight-line-between-two-points-on-surface-of-polygon#comment2806_2355)?" CreationDate="2016-04-22T20:02:12.510" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="2808" PostId="2355" Score="0" Text="@Nero 2D is fine. Shapes are mostly organic. Cancer nodules in ultrasound. I suppose vertex vertex is OK." CreationDate="2016-04-23T05:26:46.540" UserId="3204" ContentLicense="CC BY-SA 3.0" />
  <row Id="2810" PostId="2355" Score="0" Text="Is your raw data raster (pixel) data rather than a list of vertices? If so a raster approach which cuts out the intermediate step of converting to vertices may increase accuracy." CreationDate="2016-04-23T11:58:07.300" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2811" PostId="2309" Score="0" Text="Let me see if I have understood you correctly. Ray tracing parallel to the slicing planes seems to not only help me get the inside/ouside of the contour , but also pave a way to implement GPU slicer for 3D model made of triangular meshes." CreationDate="2016-04-23T13:04:45.360" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2812" PostId="2355" Score="0" Text="@trichoplax yes, but I thought that would be slower and messier. The ROI was drawn in paint by hand. Convert to vertexes was just noting the coordinates of each white pixel." CreationDate="2016-04-23T14:10:34.943" UserId="3204" ContentLicense="CC BY-SA 3.0" />
  <row Id="2813" PostId="2320" Score="1" Text="In your `Sample` function, there is the statement `float ndotl = dot(I.getNormal(), L);`. Not sure, but `I.getNormal()` looks a bit like it denotes the normalized `I` vector, instead of the surface normal at position `I`." CreationDate="2016-04-25T08:03:36.617" UserId="1835" ContentLicense="CC BY-SA 3.0" />
  <row Id="2814" PostId="2212" Score="0" Text="Hi @ivokabel, sorry for the late reply. You advice was good, and point me in the right direction, but was not enough. Indeed adding a brightness parameter to the light to manage the spectrum of the light. But i also added another parameter to manage the total luminance of a pixel: the tristimulus value obtained from the path tracer is multiply with the total radiance of the pixel (sum of each SPD wavelenght of the pixel). In this way the image brightness is good." CreationDate="2016-04-26T11:49:18.877" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2815" PostId="2212" Score="0" Text="This is an example: https://drive.google.com/open?id=0BxeVnHLvT8-7MFNuN1pDcG1DZjg. Anyway thank you again. I hope to find a completely physically based solution before the end of my thesis time." CreationDate="2016-04-26T11:49:24.143" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="2816" PostId="2151" Score="0" Text="@Detheroc Looks really inteestingif you could notify me too when  you release publicly." CreationDate="2016-04-26T14:18:21.603" UserId="204" ContentLicense="CC BY-SA 3.0" />
  <row Id="2817" PostId="2151" Score="0" Text="@UriPopov Well, I have released the source code publicly, please see my new answer." CreationDate="2016-04-26T15:28:18.787" UserId="2811" ContentLicense="CC BY-SA 3.0" />
  <row Id="2818" PostId="2360" Score="5" Text="You can use any standard image downsampling method. There's nothing special about raytraced images that requires a different method." CreationDate="2016-04-26T17:29:20.133" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2819" PostId="2360" Score="1" Text="Is part of the question whether or not downsampling should be done in linear or gamma corrected space? Linear space is always appropriate - GPUs can apply a degamma-regamma when resolving gamma-corrected multisample buffers." CreationDate="2016-04-26T18:08:32.773" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2820" PostId="2326" Score="0" Text="The question states &quot;acoording to some sources, less than 1% of computers in use...&quot;. Could you be more precise in what sources you are thinking of?" CreationDate="2016-04-26T18:50:18.367" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2821" PostId="2326" Score="1" Text="@Andreas http://goo.gl/YqMpwQ&#xA;http://goo.gl/2oQBPw&#xA;&#xA;I did not believe that this statement was relevant enough to the question to require a source. Especially considering that (while the actual numbers are debatable), it is obvious and well known that VR is performance heavy and thus won't run on slower PCs." CreationDate="2016-04-26T19:29:28.087" UserId="3156" ContentLicense="CC BY-SA 3.0" />
  <row Id="2822" PostId="2360" Score="0" Text="I'm not asking about whether it should be done in linear or gamma space (I know that it should be done in linear space). @NathanReed could you suggest an easy to implement one?" CreationDate="2016-04-26T19:42:32.240" UserId="1924" ContentLicense="CC BY-SA 3.0" />
  <row Id="2823" PostId="2326" Score="0" Text="I was not sure if &quot;some&quot; were some guys you met on reddit rambling or, as the links points out, quite respectable companies and news networks. As for obvious well that is matter of today and tomorrow. Today is 1%. 2020 is estimated 8%. The latter may well cover everyone still interested in VR by that time so it may or may not be a problem." CreationDate="2016-04-26T19:55:03.180" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2825" PostId="2362" Score="0" Text="is `y2 - y1` close to 0?" CreationDate="2016-04-27T10:53:42.743" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2826" PostId="2362" Score="0" Text="it is already checked that y2 != y1. One more issue may be to check how the algorithm behaves when both the line segments overlap. I checked that the algorithm does not filter it out. Should it be a intersection in this case ?" CreationDate="2016-04-27T11:28:13.740" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2827" PostId="2362" Score="1" Text="subtracting 2 nearly equal values and then dividing with it is generally bad for precision." CreationDate="2016-04-27T11:37:38.387" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2829" PostId="2362" Score="0" Text="Initially it just returns if the two line segments does intersect or not and the algorithm is edited. If there any intersection I proceed with next part of the algorithm to find the point where it intersects and then I use getXIntersection(..) function. I am afraid that the issue of two overlapped line segments are not identified in the initial part of the algorithm . What do you think ? Do you suggest any changes in the initial part ?" CreationDate="2016-04-27T12:07:49.303" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2834" PostId="2362" Score="0" Text="I tried with several point values with the help of geogebra and I get valid result as you asked." CreationDate="2016-04-27T12:22:56.343" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2839" PostId="2362" Score="0" Text="I've deleted my answer now that I understand that the initial step checks for segment intersection, not just line intersection. Sorry for the confusion. I've checked the pseudocode in the book and your code seems to match it. So your code for both steps looks correct to me. Since I can't see what is wrong, could you include the values you are testing with, showing the input values and returned result? The segment end points in the code appear to be identical (the same segment twice, rather than distinct segments): (5,1),(5,3) and (5,1),(5,3) again." CreationDate="2016-04-27T21:23:03.610" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2840" PostId="2362" Score="0" Text="The result shows that it has the intersection with those points you mentioned - in other words overlapping line segments intersects. In my case loverlapping line segments must not be decided as intersection and I , believe that I have to add some extra condition to it. Need some hint on those conditions. At last but not the least, once the initial test mentions that there is intersection between the line segments, Do I have test if the calculated intersection point reside on both the line segments ?" CreationDate="2016-04-27T21:34:02.133" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2841" PostId="2361" Score="0" Text="I wonder if there are any high performance libraries out there instead of stb? Would halide support doing something like this?" CreationDate="2016-04-27T21:52:01.653" UserId="1924" ContentLicense="CC BY-SA 3.0" />
  <row Id="2842" PostId="2362" Score="0" Text="Could you share the input values that result in your X-intersection value of -32768?" CreationDate="2016-04-27T22:25:30.363" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2843" PostId="2362" Score="0" Text="No, if the initial test confirms that the segments intersect, then the intersection point automatically lies on both segments." CreationDate="2016-04-27T22:26:07.070" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2844" PostId="2362" Score="0" Text="Lets name the line segments. Surrogate Segment - (0,1762)---(1057,1762) and Axis Aligned Segment - (1066,1762)----(1038,1762). As you can see both the segments overlap each other. The current algorithm does not deal with these type of scenarios" CreationDate="2016-04-27T22:48:44.470" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2845" PostId="2356" Score="2" Text="_&quot;Matlab is not the choice of language for this particular task&quot;_... nor mine. I'm not familiar with the language nor have access to Matlab.&#xA;&#xA;Having said that, I see you have the eigenvectors of the covariance matrix and are taking the dot product of each vertex against each of those vectors. The mins and maxs give the bounds of a box.&#xA;&#xA;However, I don't think this is guaranteed to give you a *good* bound. Imagine a dense central cluster of vertices with a few outliers.  The directions of your axes are going to governed by the dense cluster but your box really only depends on the outliers." CreationDate="2016-04-28T08:14:31.777" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2846" PostId="2362" Score="0" Text="Does your code work correctly for segments that do not overlap (that is, segments that do not share the same line)? If so, then the original question is solved and you can ask about detecting overlapping segments as a new question. If not, then overlapping segments is best left until the code is fixed - let's get it working before adding extra exceptions. Please don't change the aim of a question - feel free to ask a new question for a new problem." CreationDate="2016-04-28T08:50:19.533" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2847" PostId="2356" Score="1" Text="There seems to be a problem with matlab on this site, every time somebody puts up a question with matlab code it seems to rot forever. I have matlab installed but honestly would not care to fire it up for debugging" CreationDate="2016-04-28T08:54:03.000" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2848" PostId="2356" Score="0" Text="@SimonF Surely though, as a bounding box, it still has to include those outliers - unless of course you have multiple bounding boxes. However, I'm only attempting to model the mathematics and the shape in question is just one I quickly drew up while I had the time..." CreationDate="2016-04-28T12:24:01.427" UserId="2646" ContentLicense="CC BY-SA 3.0" />
  <row Id="2849" PostId="2356" Score="0" Text="Well, yes, it will still include the outliers, but one aim of OBBs over AABBs is to get a much smaller bounding volume. I just meant to point out that using PCA might not give a good result.  I must admit, though, I don't really have an efficient scheme for doing better :-|" CreationDate="2016-04-28T13:09:56.963" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2850" PostId="2361" Score="0" Text="@Decave I don't know off the top of my head, but I did find [this image-resizing sample](https://github.com/halide/Halide/blob/master/apps/resize/resize.cpp) in the Halide repository." CreationDate="2016-04-28T15:54:44.777" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2852" PostId="2151" Score="4" Text="The thesis is publicly available here: https://dspace.cvut.cz/bitstream/handle/10467/62770/F8-DP-2015-Chlumsky-Viktor-thesis.pdf" CreationDate="2016-04-28T22:00:30.783" UserId="3255" ContentLicense="CC BY-SA 3.0" />
  <row Id="2853" PostId="2370" Score="1" Text="Could you tell also where you found the &quot;mentions of using Interval Arithmetics&quot;?" CreationDate="2016-04-29T12:11:46.873" UserId="2074" ContentLicense="CC BY-SA 3.0" />
  <row Id="2854" PostId="2374" Score="0" Text="Yes! Thanks! This solved the problem. As an additional note, &quot;Deterministic  Monte Carlo&quot; is (now) known as &quot;Brute Force&quot; in Vray. I thought I tried this already, but suspect I may have failed to set it in both my primary and secondary GI engines" CreationDate="2016-04-29T16:38:53.410" UserId="3251" ContentLicense="CC BY-SA 3.0" />
  <row Id="2855" PostId="2370" Score="0" Text="@rych [In this paper.](http://www.cs.utah.edu/~knolla/dagstuhlHijazi.pdf)" CreationDate="2016-04-29T16:55:01.940" UserId="3233" ContentLicense="CC BY-SA 3.0" />
  <row Id="2856" PostId="2327" Score="2" Text="Thanks! You were right, but I should also make note for future readers that I had misunderstood the inverse of the matrix. What I got from that inverse matrix (variable called x) was actually 3 scalars, not a coordinate. Think about that and everything else will click once this is realized. The first value (x.x) is actually the scalar distance from the start vector along the direction vector where the intersection occurs!" CreationDate="2016-04-29T22:44:42.077" UserId="3153" ContentLicense="CC BY-SA 3.0" />
  <row Id="2857" PostId="2372" Score="1" Text="Two lines cannot partially overlap, but two line segments can." CreationDate="2016-04-30T00:42:17.940" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2858" PostId="2364" Score="0" Text="It would be helpful to see your code in this question to make the post self-contained. I'll add an answer that refers to your code but it will make more sense if the code is visible in the question." CreationDate="2016-04-30T00:52:51.550" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2859" PostId="2372" Score="0" Text="Ah, good point. I was thinking infinite lines. I'll update when I have a minute." CreationDate="2016-04-30T01:03:54.483" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="2860" PostId="2366" Score="1" Text="It seems you have the answer you need, but if you ever want to look up the mathematics for anything else, the 2d donut-like shape you describe is called an [annulus](https://en.wikipedia.org/wiki/Annulus_%28mathematics%29)" CreationDate="2016-04-30T01:21:28.413" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2861" PostId="2377" Score="1" Text="I've seen both, unfortunately. I'm not fond of ray per second as meaning exclusively primary rays and I'd suggest &quot;paths per second&quot; or  &quot;samples per second&quot; instead. &quot;Complete ray&quot; is not a name you'll find elsewhere: a ray is an unbounded line segment. &quot;Rays per second&quot; is ill specified for a path tracer: do shadow rays count, for instance? It's a useful metric for an acceleration framework (i.e. Embree or OptiX) but not a renderer." CreationDate="2016-04-30T21:20:15.810" UserId="3075" ContentLicense="CC BY-SA 3.0" />
  <row Id="2862" PostId="2377" Score="0" Text="Also, be aware that samples per second still isn't a great metric of actual performance since sample quality will vary wildly depending on implementation details. It's probably the best thing you can do starting out though, as the better solutions involve fairly complex variance estimates." CreationDate="2016-04-30T21:30:43.067" UserId="3075" ContentLicense="CC BY-SA 3.0" />
  <row Id="2863" PostId="2377" Score="0" Text="@KarlSchmidt I think you should post those comments as an answer ;)" CreationDate="2016-04-30T22:05:09.677" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2864" PostId="2377" Score="0" Text="Probably a good idea, yes. :)" CreationDate="2016-04-30T22:56:45.753" UserId="3075" ContentLicense="CC BY-SA 3.0" />
  <row Id="2865" PostId="2381" Score="0" Text="Can the pieces be of arbitrary shape, or do you require concave pieces to be decomposed into convex pieces?" CreationDate="2016-05-01T08:26:58.217" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2866" PostId="2381" Score="2" Text="When you say &quot;clipped&quot;, can we assume that the inside of the square is discarded, and we are counting the number of disconnected pieces left outside the square?" CreationDate="2016-05-01T08:28:36.027" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2867" PostId="2365" Score="0" Text="I can't find where it says there are two hardware queues in the programming guide. Could you post a quote from the document? Mention which chapter that says there are two queues? Is number of hardware queues queryable in runtime using OpenCL?" CreationDate="2016-05-01T12:06:03.263" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2868" PostId="2359" Score="0" Text="Can you be more precise about what your problem is (time, memory, logic)? What is stopping you from &quot;just do it&quot;?" CreationDate="2016-05-01T12:16:20.813" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="2869" PostId="2380" Score="0" Text="Great answer. To add to it, since it is hard to come up with one metric that is universally meaningful, I'd suggest you pick one that makes most sense / is most honest for your usage case, and make sure you explain what you mean by your terminology." CreationDate="2016-05-01T13:59:28.150" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2870" PostId="2365" Score="0" Text="I've updated my post. It does say *possible* execution, but if it can do a few why can't it do them all? Also the OpenCL runtime has no notion of hardware queue, so it's not something that you can query." CreationDate="2016-05-01T16:05:48.387" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="2872" PostId="2375" Score="1" Text="Thank you for the answer, also sorry for being so vague in my question. The goal is to create diverging lens using the CSG difference operator \ in the following manner: (C \ A) \ B. You've made it much more clearer to me on the general approach, but I'm still struggling to implement \. Can you please give an example for this operator as well?" CreationDate="2016-05-01T19:19:19.040" UserId="3233" ContentLicense="CC BY-SA 3.0" />
  <row Id="2873" PostId="2359" Score="1" Text="Hey @Andreas I've updated the question with how am I doing it right now. The main issues are listed. I was wondering if a &quot;canonical&quot; best way of solving this problem is known. Typically when I cannot find them in google is because i am searching for &quot;the wrong keywords&quot;." CreationDate="2016-05-02T09:03:18.050" UserId="3225" ContentLicense="CC BY-SA 3.0" />
  <row Id="2876" PostId="2387" Score="0" Text="A image might do wonders." CreationDate="2016-05-03T04:43:22.337" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2880" PostId="2375" Score="2" Text="(Hoping I've got this right) Would it be easier for if you constructed the difference operator from *Intersection* and *Not* i.e.  A \ B  =  A ^ ~B.  To generate ~B you just need the &quot;gaps&quot;. For example, if the intersection along a ray for B is  { [b1, b2] }, then for ~B it'd be { [-infinity, b1] , [b2, +inf] }." CreationDate="2016-05-03T10:50:38.390" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2881" PostId="2388" Score="0" Text="I'd upvote if there was also an example of a (possibly unrelated) soft shadow technique for contrast, as part of the question was what is the difference between them." CreationDate="2016-05-03T14:56:57.397" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2883" PostId="2388" Score="1" Text="Soft shadows with shadow volumes makes me think of Instant Radiosity - essentially approximate area light sources as multiple point light sources and accumulate the results. Many shadow (100+) passes are doable in real time on a modern GPU with stacked memory if the shadow casting geometry can be limited." CreationDate="2016-05-03T19:09:00.507" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2884" PostId="2311" Score="0" Text="@joojaa But have you ever tried looking at something that's been pushed through your eyeball? :-)" CreationDate="2016-05-04T08:00:46.650" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2885" PostId="2311" Score="0" Text="@SimonF Actually I have, although i tried to not concentrate on it.. But the projectile geometry actually pushes everything that crosses your view plane trough one point which reality does not ;)" CreationDate="2016-05-04T08:04:12.513" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2886" PostId="2367" Score="0" Text="Hey thanks a lot for sharing ur knowledge. I wrote a code but it seems to have some error ive edites the post with an image. I would appreciate if u cab give me help with that. Im constantly running into problems so i searched for ur contact number cuz i cant post image in a comment lol." CreationDate="2016-05-04T10:24:59.313" UserId="3248" ContentLicense="CC BY-SA 3.0" />
  <row Id="2887" PostId="2367" Score="1" Text="@quinnavery ...did you just copy/paste the shadertoy into your code? :D You're going to need a bit more plumbing than that. It's written in GLSL, not C++, which is where most of the errors are coming from. If you're not familiar with using shaders in Cocos2d-x, you should look up some tutorials on the subject first, such as [this one](https://www.raywenderlich.com/10862/how-to-create-cool-effects-with-custom-shaders-in-opengl-es-2-0-and-cocos2d-2-x)." CreationDate="2016-05-04T16:17:46.320" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2888" PostId="2392" Score="0" Text="AMD's GCN will execute graphics and compute concurrently even when both are issued on the graphics queue, but generally not across multiple command buffers (multiple draw calls might even be sketchy). The driver (or application - I think in DX12 or Vulkan) must check for data dependencies and block between draw (graphics) and dispatch (compute) if needed. Multiple command queues would probably be useful if you have compute that is truly asynchronous from graphics (like physics for the next frame), but I have no direct experience with this." CreationDate="2016-05-05T00:22:33.503" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2892" PostId="2397" Score="1" Text="You could also have white bars. You can also use some image synthesis function to fill the pixels you can overlay a picture with itself  one that is cropped and one that stretches for example. here simply is a infinite amounts of ways to deal with no data. I think most of us just assumed its going to be a nonuniform scale." CreationDate="2016-05-06T08:23:02.803" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2893" PostId="2394" Score="0" Text="Thanks for writing an answer! I'll take a look into BVH! Yes determining whether the voxels are inside/outside/intersected by the mesh is important for me. Could you elaborate a bit or point to a resource for using ray casting to determine inside/outsideness? From where to where do the rays go?" CreationDate="2016-05-06T11:27:18.987" UserId="3225" ContentLicense="CC BY-SA 3.0" />
  <row Id="2895" PostId="2397" Score="0" Text="Very true. I assumed the original poster specifically didn't want to fill with a solid color and was interested only in the scaling aspect, since that's what they asked about. But there are lots of other fill methods. There are also non-linear scaling methods, where the center maintains its aspect ratio and the edges are stretched out, for example." CreationDate="2016-05-06T15:44:30.103" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="2896" PostId="2394" Score="0" Text="@gnzlbg To test if a point P is inside the mesh, you can fire a ray starting at P in any direction, to infinity. If P is inside, it will hit the mesh an odd number of times; if P is outside, the ray will hit the mesh an even number of times (counting zero as an even number). [This Wikipedia article](https://en.wikipedia.org/wiki/Point_in_polygon#Ray_casting_algorithm) explains it for the 2D case, but it's the same idea in 3D." CreationDate="2016-05-06T19:43:02.843" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2897" PostId="2403" Score="0" Text="This is a quick answer over the phone, spelling corrections and tex formulas welcome." CreationDate="2016-05-06T20:15:31.890" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2900" PostId="2404" Score="0" Text="And the neat geometric interpretation is what I used. Personally I prefer to build the matrixes with a few number of atomic operations because then I only need to remember 3 rules of construction." CreationDate="2016-05-07T06:05:53.883" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2901" PostId="2395" Score="0" Text="This is a great and informative answer, but Daniel's answer talks more about the tesselation in a historical context, which is what my question was really about. But thanks your answer. :)" CreationDate="2016-05-07T10:07:40.883" UserId="88" ContentLicense="CC BY-SA 3.0" />
  <row Id="2905" PostId="2409" Score="0" Text="I'm not familiar with the framework, but wouldn't it be more efficient to create an acceleration structure for each model?" CreationDate="2016-05-07T12:39:30.493" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="2908" PostId="2402" Score="0" Text="explain your API. I would expect to see something like vertices A1,B1, A2,B2. Beside, what about the case they are not even aligned ? what kind of error threshold do you plan ?" CreationDate="2016-05-08T10:17:12.200" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="2909" PostId="2412" Score="1" Text="The lines in plane $y = y_0$ are not all parallel." CreationDate="2016-05-08T13:06:10.150" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2910" PostId="2412" Score="0" Text="Of course, I need a group of parallel lines in plane $y=y_0$ that their vanishing point is some point, say $(x_0,0,1)$ (the camera position and direction and the view plane are defined in the question)" CreationDate="2016-05-08T13:07:41.200" UserId="3305" ContentLicense="CC BY-SA 3.0" />
  <row Id="2911" PostId="2414" Score="0" Text="I have tough about that, so if my camera in $(0,0,0)$ and it's looking to $(0,0,1)$, the view plane is $z=1$ and lets say that the vanishing point is $(10,0,1)$. Then the line from the camera to the vanishing point is $(0,0,0)+t(10,0,1)$. So all the parallel lines that are going to that vanishing point will be $(a_1,a_2,a_3) + t(10,0,1)$? And then all the lines in plane $y=y_0$ that have that vanishing point will be $(a_1,y_0,a_3) + t(10,0,1)$?" CreationDate="2016-05-08T13:58:57.273" UserId="3305" ContentLicense="CC BY-SA 3.0" />
  <row Id="2912" PostId="2414" Score="1" Text="stop focusing on the $y = y_0$ plane, just having the vanishing point on the view plane and the camera position is enough." CreationDate="2016-05-08T14:08:22.040" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="2913" PostId="2407" Score="0" Text="What about the case where one corner of the square is inside the polygon and all others are outside?" CreationDate="2016-05-08T14:08:51.740" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2914" PostId="2414" Score="0" Text="OK, so $(a_1,a_2,a_3)+t(10,0,1)$ are all the lines that their vanishing point is $(10,0,1)$?" CreationDate="2016-05-08T14:09:50.400" UserId="3305" ContentLicense="CC BY-SA 3.0" />
  <row Id="2915" PostId="2407" Score="1" Text="@RichieSams its the same case as splits into one shard, it exits 2 times and results in one concave shard." CreationDate="2016-05-08T14:56:47.270" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2916" PostId="2367" Score="0" Text="hi, Im having some trouble with my developer.... he really doesnt know how to code (Just my opinion) I'm considering to hire somebody else to create the game for me. You really seem like you can code since u shared your knowledge in less than a day. I think it will be quite easy for you? ^^ I'm trying to create for android platform and dont mind what program u use cocos, unity...., Will you be interested? I do pay upfront LOL" CreationDate="2016-05-08T17:03:50.183" UserId="3248" ContentLicense="CC BY-SA 3.0" />
  <row Id="2917" PostId="2409" Score="0" Text="Okay. Yes, but then I have to switch out which mesh is the currently active one... I think that would be doable... switching just works by setting a variable in the graphics memory I think. I would have to try if this influences perfomance in a big way. Thank you for your suggestion." CreationDate="2016-05-08T17:10:12.533" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2918" PostId="2367" Score="0" Text="if u are interested, plz let me know and we can proceed to further negotiations via email or something. I can pay you via paypal just to let ya know~" CreationDate="2016-05-08T17:10:20.763" UserId="3248" ContentLicense="CC BY-SA 3.0" />
  <row Id="2919" PostId="2411" Score="1" Text="BTW, what is the &quot;*&quot; notation in this formula? I've never seen that in the rendering equation before." CreationDate="2016-05-08T18:32:01.207" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2920" PostId="2411" Score="0" Text="@NathanReed p* is just a point from another surface, L(p*,-wi) is just the radiance from point p in direction wi" CreationDate="2016-05-08T18:53:54.177" UserId="2359" ContentLicense="CC BY-SA 3.0" />
  <row Id="2923" PostId="2380" Score="0" Text="Yeah right, thank you for clearing my thoughts! Currently I choose the &quot;samples per second&quot; since it is easy to understand and hard to be misunderstood IMO. I'll eventually implement something that can calculate variance and measure rendering performance based on that." CreationDate="2016-05-08T19:42:39.173" UserId="3267" ContentLicense="CC BY-SA 3.0" />
  <row Id="2924" PostId="2407" Score="1" Text="@trichoplax right i can add that. There shouldn't be less,  but see i added the or less for your comment.  I am not sure i have proven there isn't a smaller case (i could), there shouldn't be but i haven't laid out the proof very well for that case, not rigorous enough. I have however proven that there are no more cases than 4 and since i can find a 4 it proves that is maximum which is enough to answer the question at hand." CreationDate="2016-05-08T20:11:39.083" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2925" PostId="2407" Score="0" Text="@joojaa fair point." CreationDate="2016-05-08T20:24:26.147" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2926" PostId="2398" Score="0" Text="The C version to the above code is in the link at description of question." CreationDate="2016-05-09T01:32:42.353" UserId="2383" ContentLicense="CC BY-SA 3.0" />
  <row Id="2928" PostId="2409" Score="0" Text="@Rotem I tried to just create everything before displaying, and then only switching it out... It turns out each time tell Optix to use another model this takes nearly half a second and my &quot;animation&quot; looks really unsmooth." CreationDate="2016-05-09T11:41:06.550" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="2929" PostId="2402" Score="0" Text="If integers are 32 bits, the multiplies in orientation could overflow if p1's coordinates are large negatives and p2, p3's coordinates are large positives. Other than that, this looks correct - cross products are zero, h's vertexes are within the bounding box of c." CreationDate="2016-05-09T13:40:06.563" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2931" PostId="2402" Score="0" Text="will it solve the issue if d1, d2 , d3 and d3 are stored as 64-bit integer type instead ?" CreationDate="2016-05-09T15:40:48.973" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2932" PostId="2402" Score="0" Text="&quot;orientation&quot; would have to compute with 64 bit integers too; the math there is currently promoted to integers, but they won't quite hold your worst case multiply result (if 32 bits). If you know your original data is limited (e.g. I think if you only had positive values - effectively positive 15 bit values, assuming 2's compliment integers), this may not be a problem. Start with a test that fails due to overflow, then work modify to fix. That is, prove my thinking isn't wrong before using the extra bits (or another strategy)... ;)" CreationDate="2016-05-09T17:46:49.560" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2934" PostId="2398" Score="0" Text="I have removed the &quot;//&quot; and C# implementation of above code is this much only" CreationDate="2016-05-10T00:05:01.570" UserId="2383" ContentLicense="CC BY-SA 3.0" />
  <row Id="2935" PostId="2406" Score="0" Text="`combination of a quite dark texture with very bright lighting or an extremely overexposed camera setting can show banding in the final frame` very good insight. But we may note that gamma encoding is here precisely to mitigate this point. If he has the problem why not try a superior gamma exponent ? that would inhibit usage of hardware sRGB samplers though." CreationDate="2016-05-10T00:50:33.420" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="2936" PostId="2419" Score="3" Text="The magic keyword to search for is &quot;polygon offsetting&quot;. The curves you're trying to generate are called offset curves. [Here is a StackOverflow question](http://stackoverflow.com/questions/1109536/an-algorithm-for-inflating-deflating-offsetting-buffering-polygons) with some good background and links to some libraries that can do it. You can also find plenty of material with a web search." CreationDate="2016-05-10T02:16:25.833" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2938" PostId="2406" Score="0" Text="Thank you for your input. But to clarify, I'm mostly interested in this from a photo realistic rendering point of view, with ray trace renderers (like mental ray, V-Ray, Arnold, etc.) with full light simulations and global illumination, rather then for real-time game engines." CreationDate="2016-05-10T07:32:58.720" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="2939" PostId="2420" Score="0" Text="Thank you for your input. But to clarify, I'm mostly interested in this from a photo realistic rendering point of view, with ray trace renderers (like mental ray, V-Ray, Arnold, etc.) with full light simulations and global illumination, rather then for real-time game engines. I've updated my original post." CreationDate="2016-05-10T07:33:27.423" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="2940" PostId="2406" Score="0" Text="@KristofferHelander Good to know, but I think what I wrote probably applies just as well to offline rendering. But I admit that I don't have much direct experience in that area." CreationDate="2016-05-10T07:39:04.373" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2941" PostId="2406" Score="0" Text="@NathanReed Yes, you most certainly made some really good points there :)" CreationDate="2016-05-10T07:54:49.367" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="2942" PostId="2401" Score="0" Text="This doesn't really seem to adress the actual question much at all." CreationDate="2016-05-10T09:51:32.520" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="2943" PostId="2401" Score="0" Text="I added it as a comment and the original questioner asked me to promote it to an answer. See questioner's comment below..." CreationDate="2016-05-10T14:28:51.530" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2944" PostId="2422" Score="2" Text="I didn't read your code because I saw this:&#xA;&#xA;*&quot;For example, when one of the vertices is behind the camera, the texture is stretched.&quot;*&#xA;&#xA;You *have* to clip your polygons to a Z=constant plane in front of the camera or else all sorts of chaos will ensue :-)" CreationDate="2016-05-10T15:47:29.307" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2945" PostId="2422" Score="0" Text="@SimonF I am already clipping on the Z axis, see _ZCLIP_ in the 3rd code." CreationDate="2016-05-10T16:32:41.590" UserId="3330" ContentLicense="CC BY-SA 3.0" />
  <row Id="2946" PostId="2422" Score="0" Text="My apologies! I saw that opening paragraph and it &quot;raised a red flag&quot;.    So given *&quot;I am guessing that it has something to do with the texture mapping not being perspective-correct.&quot;* .. I tried to understand your &quot;map&quot; function but got lost. For example, I don't understand why you need a square root.  IIRC there's an initial set up from triangle coords to create 9 params, a,b,c d e f  p q r, then @ pixel x,y,  U= (a x +by + c)/(px+qy+r)  and V = (d x +ey + f)/(px+qy+r).   To determine the abc..pqr requires adjoint of a simplified &quot;3x3&quot; matrix (i.e. inverse w/out det) and mat muls." CreationDate="2016-05-10T16:55:31.523" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2947" PostId="2416" Score="1" Text="Hi headbanger, image warping transformations are certainly possible but it's a bit unclear from your question what you're looking for. An example before/after screenshot or diagram would help." CreationDate="2016-05-10T17:08:59.257" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2948" PostId="2422" Score="0" Text="@SimonF My *map()* function is adapted from [this StackoverFlow answer](http://stackoverflow.com/questions/808441/inverse-bilinear-interpolation). I thought it was perspective-correct, but I guess it isn't. Does all perspective-correct texture mapping techniques require to break down the polygon into triangles? Is there any formula that is not in a matricial form? Thanks!" CreationDate="2016-05-10T17:49:48.373" UserId="3330" ContentLicense="CC BY-SA 3.0" />
  <row Id="2949" PostId="2423" Score="1" Text="Wow, thanks @LarryGritz! Considering you work for Sony Pictures Imageworks, I'll take you as a very reliable source! :)&#xA;&#xA;But how do you capture these textures? Most cameras only shoot in 14bit RAW files, do you have special cameras with 16bit linear sensors? Or do you take multiple exposures for textures, just like one does for HDRI imaged based lighting? Or do you simply capture with 14bit camera RAW and saves it as &quot;16bit&quot;? Ow, and what file format do you use, .tiff (.tx, .tex) or .exr?&#xA;Thanks again for your input!! :)" CreationDate="2016-05-10T19:27:15.013" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="2950" PostId="2401" Score="1" Text="My question was about why a seemingly underused feature like tesselation has gotten so much API attention. RichieSams' answer talks more about what tesselation is, rather than explaining how it got to be so important. The above answer is the best answer I've gotten so far." CreationDate="2016-05-11T04:52:39.783" UserId="88" ContentLicense="CC BY-SA 3.0" />
  <row Id="2951" PostId="2422" Score="0" Text="Why on earth do people want to avoid the matrix calculations. Triangles make it easy to transform the coordinates." CreationDate="2016-05-11T10:22:36.163" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2952" PostId="2425" Score="1" Text="But how can I use this with the Rendering Equation which involves radiance ?" CreationDate="2016-05-11T10:36:33.913" UserId="3339" ContentLicense="CC BY-SA 3.0" />
  <row Id="2953" PostId="2422" Score="0" Text="@joojaa My reason is that creating a matrix object, calling and updating members is going to take longer to the JVM than simply executing some bytecode. Okay, I understand. Does it work w/o breaking it down into triangles though?" CreationDate="2016-05-11T10:38:23.343" UserId="3330" ContentLicense="CC BY-SA 3.0" />
  <row Id="2954" PostId="2426" Score="0" Text="Hi @sajis997, it's hard to understand what you're talking about from words alone. Could you add a screenshot or diagram to show what you mean by inner and outer loops and tree depth?" CreationDate="2016-05-11T16:43:11.213" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2955" PostId="2401" Score="0" Text="Another aspect is that geometry shaders (flexible enough to implement tessellation, and available in DX10), are extremely hard to implement efficiently in HW. My understanding is the main issue is the in-order processing of the output from each input primitive means executing geometry shaders in parallel requires substantial buffering. A fixed function tessellator turned out to be far more efficient." CreationDate="2016-05-11T16:52:41.040" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="2956" PostId="2426" Score="0" Text="Hi @NathanReed, The initial question is edited with some image snapshot. I hope it wil be clear now." CreationDate="2016-05-11T19:24:10.857" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2957" PostId="2424" Score="0" Text="In short, the number of photos you'll encounter drops as you get farther from a point light source. You'll encounter 1/(distance^2) in fact (;" CreationDate="2016-05-12T03:28:08.710" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2959" PostId="2416" Score="0" Text="I think the question is pretty clear in intention. Just dont expect people to publish code for you as the question sounds more like do my work for me the way its presented." CreationDate="2016-05-12T08:38:12.673" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="2960" PostId="2422" Score="0" Text="*&quot;Does it work w/o breaking it down into triangles though&quot;*. If your surface is planar and the texturing, in world space, is linear, then &quot;yes&quot;. You just need 3 non-colinear points on that surface along with their matching UVs." CreationDate="2016-05-12T14:01:57.077" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="2961" PostId="2431" Score="0" Text="The code appears to return a point uniformly selected from the *interior volume* of the unit sphere. This will give different results from the surface of the sphere. Is this what you intend? I ask because you mention picking a point &quot;on&quot; the light, which sounds like a point on the surface." CreationDate="2016-05-12T17:04:48.740" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2963" PostId="2431" Score="0" Text="But.. _centerOfSphere + Vec3&lt;float&gt;(x, y, z) * radius;_ should give me a point on the surface of the sphere! right? cause I move from the center towards the surface of _radius_" CreationDate="2016-05-12T20:01:05.323" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="2966" PostId="2431" Score="1" Text="If the points are already on the surface of the sphere, then multiplying by radius will give the surface of a sphere of that radius. If the points are already in the interior volume of the sphere, then multiplying by radius will put them in the interior volume of a sphere of that radius. Multiplying by, for example, radius 2, will give a sphere twice as large. However, whether the points are on the surface or in the volume of this larger sphere depends on whether they were on the surface or in the volume of the original unit sphere" CreationDate="2016-05-12T22:18:24.223" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2968" PostId="2436" Score="0" Text="I believe you are correct. I will check my PBRT book when I get home and correct the code. IIRC, I need to add a square root." CreationDate="2016-05-12T23:09:45.017" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2969" PostId="2426" Score="0" Text="Do you have access to the tree shown in the image? If so my answer would be &quot;the outer loops are at even depths, the inner loops are at odd depths&quot;. Is your requirement to produce this tree, or to interpret its results?" CreationDate="2016-05-13T01:01:46.353" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2970" PostId="2436" Score="2" Text="Yes, @trichoplax is right, sampling both angles uniformly will lead to points bunching at the poles. The simplest way to fix it is sample theta and Y uniformly, then set the XZ radius = $\sqrt{r^2 - y^2}$. Which, BTW, is exactly what you're doing in the second code snippet when you sample `cosPhi` (which is proportional to Y) uniformly. See also [this MathWorld article](http://mathworld.wolfram.com/SpherePointPicking.html)." CreationDate="2016-05-13T02:01:09.303" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2971" PostId="2436" Score="0" Text="Fixed and updated." CreationDate="2016-05-13T02:36:22.383" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2972" PostId="2424" Score="0" Text="By photos I meant photons by the way" CreationDate="2016-05-13T04:21:23.637" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="2973" PostId="2426" Score="0" Text="After the slicing operation, I have closed loops . I want separate into separate layer parts when each layer part contain an outer loop, one/several loops and infill pattern. I want to use Tree data structure for this purpose as mentioned in the paper." CreationDate="2016-05-13T07:38:12.677" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="2975" PostId="2438" Score="1" Text="You could pass the color data in a constant buffer rather than vertex data. If they are all the same." CreationDate="2016-05-13T11:55:50.033" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="2979" PostId="2441" Score="0" Text="Do you require a neighbourhood that extends beyond immediate neighbours or would you be interested in approximations based on repeated application of a nearest neighbour approach?" CreationDate="2016-05-13T16:18:50.413" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="2980" PostId="2441" Score="0" Text="@trichoplax As long as the number of iterations required to reach a 10-hop standard deviation isn't going to be prohibitive. (I guess it wouldn't be, because at least in the 1D case a 10-hop deviation can be reached by mixing nearest neighbors 100 times.) I just wouldn't know how to weight the neighbors in this case." CreationDate="2016-05-13T17:11:29.177" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="2981" PostId="2438" Score="1" Text="If GL ES 3.0 is a possibility, you could also use geometry instancing, but it doesn't seem to be available in GL ES 2.0 as far as I can tell from some googling." CreationDate="2016-05-13T18:02:20.797" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="2985" PostId="2438" Score="0" Text="@Ratchet freak OK I re-strategised my plan! Thank you for letting me know.  If you are willing I made a new one because I am stuck on that one, I need to get good at vertex buffers somehow." CreationDate="2016-05-14T00:12:35.043" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="2986" PostId="2438" Score="0" Text="http://stackoverflow.com/questions/37221027/how-to-write-complicated-vertex-buffers" CreationDate="2016-05-14T00:12:37.637" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="2990" PostId="2441" Score="0" Text="@joojaa It is separable when applied on a flat surface/grid but is it still separable when you have edges going off in arbitrary directions from each vertex?" CreationDate="2016-05-14T11:56:59.197" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="2991" PostId="2443" Score="0" Text="Yes, MIS is an important production-verified technique, which helps a lot and I employ it in my solution (I guess, I should have stated that more clearly in the question).&#xA;&#xA;However, the overall performance of a MIS-based estimator depends on the quality of its partial sampling strategies. What I am trying to do here is to improve one of the the sub-strategies to improve the overall performance of the estimator. In my experience, it is usually more efficient to use less high-quality samples than may be more expensive to generate than more easily-generated low-quality ones." CreationDate="2016-05-14T12:28:28.730" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="2992" PostId="2444" Score="0" Text="Thank you. My mention of the 3D Euclidean distance may have done more harm than good. I only meant it as an expensive (and conditional) approximation that is simple to explain. I wouldn't pursue the approximation if it is less attainable than the ideal answer." CreationDate="2016-05-14T23:28:35.730" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="2993" PostId="2444" Score="0" Text="What I am ideally after is what you call &quot;using 2D distance&quot;. On a regular grid (with consistent local geometry) it is easy because the same symmetrical 1-hop-neighborhood kernel can be applied (repeatedly) throughout the grid. My real problem is determining the kernel-weights of 1-hop neighbors around *each* vertex *from the local geometry* of that 1-hop neighborhood." CreationDate="2016-05-14T23:28:54.640" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="2994" PostId="2444" Score="0" Text="Each vertex has valence 4 but edge lengths and directions vary. (Alternatively (if for some reason planar faces make the problem tractable) each quadrilateral face can be broken along its shortest diagonal into two (now planar) triangles, in which case vertex valence varies from 4 to 8, and edge lengths become even more varied.)" CreationDate="2016-05-14T23:29:34.767" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="2995" PostId="2444" Score="0" Text="Ultimately, I have a vertex with a ring of neighbors at different distances and angles, and I need to know the local kernel weights over these neighbors. Constraining the mean/centroid of the kernel to lie &quot;in the center&quot; (which in this case presumably means along the mean curvature normal's span from the vertex) already constrains 2DoF in the kernel's weights. Choosing some variance provides another constraint. But the system is still underdetermined since valence&gt;3. How to choose the weights?" CreationDate="2016-05-14T23:30:08.583" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="2998" PostId="2444" Score="0" Text="Yes, &quot;using 2D distance&quot; is a little vague. I guess we should distinguish between &quot;shortest edgewise distance&quot; and &quot;shortest distance within the surface&quot; (which won't necessarily stay on edges). Fortunately the difference becomes less relevant the more times you apply the blur (even a box filter will approximate a Gaussian blur if applied several times). If you can determine whether you need a Gaussian blur with a precise parameter or just a blur that is Gaussian with an adjustable parameter, then you'll be able to decide whether a repeated box-style filter will be sufficient." CreationDate="2016-05-15T00:44:22.903" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3003" PostId="2444" Score="0" Text="If the blur is to be performed by repeated convolution with a local (1-hop-neighborhood) kernel, that kernel (or &quot;box&quot;) would have to vary from vertex to vertex, because the valence and/or neighborhood geometry may vary from vertex to vertex. I think what matters most is that the (per-vertex) kernels' means/centroids and variances/moments of inertia must be consistent. These constraints would fully determine a valence-3 vertex's kernel's weights, but in cases of higher valence perhaps any kernel complying with these constraints will eventually converge to the effect of a Gaussian kernel." CreationDate="2016-05-15T07:42:21.683" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="3004" PostId="2446" Score="0" Text="1. Does glGetError return anything else than no error? 2. &quot;Blown up&quot; does not really mean anything to me. Maybe it's just me but I'm having hard time figuring out what the problem is and/or expected behavior." CreationDate="2016-05-15T11:30:47.090" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3005" PostId="2444" Score="0" Text="I see. Rereading the question I see I misread the start and thought the main focus was a 3D distance approach. I'll have another look and edit the answer in a while." CreationDate="2016-05-15T11:38:20.457" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3007" PostId="2446" Score="0" Text="As far as I can understand, the images each show some aspect working as it is intended. It isn't clear to me which one (if any) shows the problem you are having. Could you edit to either add an image showing the problem, or indicate which existing image already shows the problem?" CreationDate="2016-05-15T11:42:12.197" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3008" PostId="2441" Score="0" Text="@Museful Ok whereabout  iterative smaller blurs that are a bigger that way you can do it only over edge connections and let the iterations propagete the effect. This is used in FEM and fluid sims to high efficiency" CreationDate="2016-05-15T15:49:26.570" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3009" PostId="2446" Score="0" Text="@Andreas so sorry this wasn't clear! I have made some edits that hopefully make things a bit easier to understand.  The simulation I am doing is just a bunch of moving particles that are going to leave a trail behind them. Thankyou so much for looking into this." CreationDate="2016-05-15T16:18:39.363" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3010" PostId="2446" Score="0" Text="@trichoplax Same thing for you, I have edited it to make that bit more clear." CreationDate="2016-05-15T16:18:58.380" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3011" PostId="2446" Score="0" Text="@Andreas also glGetError does not return anything, except for one error every time I create a shader.  That has not proven to be significant.  Also this error happens on the code apple wrote for their template so It is probably a non-issue." CreationDate="2016-05-15T16:24:14.013" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3012" PostId="2446" Score="0" Text="This is a little clearer. If I understand correctly, you want the texture shown in image 1 to cover the image, but without being scaled up as in image 2. So you want to tile the image with the texture shown in image 1?" CreationDate="2016-05-15T17:35:22.787" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3013" PostId="2441" Score="0" Text="@joojaa Yes I think that is the best approach but how exactly does the small blur work, as the connections/edges of a mesh are not generally regular, like they are between, say, pixels in an image. The mesh geometry should somehow influence how &quot;conductive&quot; each edge is relative to other edges in its neighborhood. How?" CreationDate="2016-05-15T17:38:45.307" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="3014" PostId="2441" Score="1" Text="Im not sure, im not so deeply invested in the lore of FEM. All I know that they can solve the diffusion over irregular meshes and that is entirely analogous with gaussian blur." CreationDate="2016-05-15T18:04:02.593" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3015" PostId="2449" Score="0" Text="Thank you **very** much for your explanation. This is maybe off-topic, but I'll ask anyway; in some source codes that compute noise, people use vector vec3(1, 57, 113) to compute dot product with current coordinate (I suppose the aim is also to obtain a hash). Why this particular choice of constants (57 is approx. 1 radian in degrees, 133 = approx. 2*radian in degrees)? Is it because of periodicity in trig functions? I'm unable to google this." CreationDate="2016-05-15T18:55:16.090" UserId="3365" ContentLicense="CC BY-SA 3.0" />
  <row Id="3016" PostId="2449" Score="3" Text="@sarasvati I'm not really sure, but a guess is that 57 and 113 are chosen because they're prime-ish numbers. (113 is prime; 57 isn't, but it's 3*19, so still kinda primey...if that's a thing.) Multiplying or modding by a prime-ish number tends to jumble up the bits, so it's not an uncommon ingredient in hashes." CreationDate="2016-05-15T19:15:58.503" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3017" PostId="2446" Score="0" Text="@trichoplax No, I am thinking that the texture in the FBO is not big enough." CreationDate="2016-05-15T19:19:35.480" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3018" PostId="2449" Score="0" Text="Wow, I didn't think about it this way. Thanks." CreationDate="2016-05-15T19:20:40.767" UserId="3365" ContentLicense="CC BY-SA 3.0" />
  <row Id="3019" PostId="2446" Score="1" Text="As far as opengl errors go all indicate something did not have desired effect. It matters not who wrote the code. Fix it. Btw what does the FBO look like in the debugger? Anything unusual?" CreationDate="2016-05-15T19:55:22.667" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3020" PostId="2449" Score="0" Text="What are the magic numbers?" CreationDate="2016-05-15T20:41:06.323" UserId="3367" ContentLicense="CC BY-SA 3.0" />
  <row Id="3021" PostId="2449" Score="0" Text="@cat Not sure how Mikkel came up with them. They may just be random / arbitrarily chosen values." CreationDate="2016-05-15T21:04:08.753" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3022" PostId="2449" Score="0" Text="@NathanReed Doesn't GLSL have an RNG / PRNG? Why not use that or hardware entropy over hardcoded arbitrary constants?" CreationDate="2016-05-15T21:05:05.290" UserId="3367" ContentLicense="CC BY-SA 3.0" />
  <row Id="3023" PostId="2449" Score="0" Text="@cat Well any PRNG or hash function has hardcoded constants in its implementation. And anyway a PRNG is not what you want for this application; you want a hash function. You could use a PRNG as a hash by re-seeding it every time, but a lot of PRNGs don't give good results when used that way; they're primarily designed to give good results in the sequence generated from one seed." CreationDate="2016-05-15T21:10:23.537" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3024" PostId="2446" Score="0" Text="@Andreas Yeah for some reason the &quot;Depth Attachment&quot; appears to also have a renderbuffer." CreationDate="2016-05-15T21:21:42.247" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3025" PostId="2449" Score="1" Text="@cat I doubt GLSL has a PRNG, given that GLSL programs are deterministic." CreationDate="2016-05-15T22:20:40.880" UserId="2316" ContentLicense="CC BY-SA 3.0" />
  <row Id="3026" PostId="2449" Score="0" Text="@immibis Shows what I know, I thought GLSL was at least turing-complete." CreationDate="2016-05-15T22:32:31.670" UserId="3367" ContentLicense="CC BY-SA 3.0" />
  <row Id="3028" PostId="2449" Score="1" Text="Looks like there are several potential new questions in this comment thread..." CreationDate="2016-05-16T00:22:52.760" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3029" PostId="2449" Score="0" Text="@cat Being deterministic does not preclude being Turing-complete." CreationDate="2016-05-16T01:28:36.453" UserId="2316" ContentLicense="CC BY-SA 3.0" />
  <row Id="3030" PostId="2449" Score="0" Text="@immibis I think I misinterpreted your comment as &quot;it's decidable whether a given program will halt&quot; not &quot;deterministic side-effects&quot;, because I can read. I'll shut up now :P" CreationDate="2016-05-16T01:33:17.547" UserId="3367" ContentLicense="CC BY-SA 3.0" />
  <row Id="3031" PostId="2444" Score="0" Text="I have a feeling that [this discretization of the Laplace-Beltrami operator](http://computergraphics.stackexchange.com/a/1721/3294) (&quot;cotan formula&quot;) may give the right weights to attribute to the edges in the mesh. When applied to (ambient space) coordinates of the surface itself, the operator yields the mean curvature normal, but in general when applied to some function defined over the surface, the operator yields the (generalized) Laplacian, or divergence of the function's gradient on that surface, I think." CreationDate="2016-05-16T18:02:46.193" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="3033" PostId="2444" Score="0" Text="Once we have the Laplacian we can diffuse the initial function over the surface just like heat i.e. $\frac{\partial f}{\partial t}=-\Delta_S f$. Doing so with explicit Euler method may be tantamount to the local mixing we had in mind. Not sure what time-step to choose for good numerical properties. A large time-step means higher variance per iteration and therefore fewer iterations needed, but it would also mean we aren't staying true to the PDE, and I'm not sure at what step-size that begins to matter." CreationDate="2016-05-16T18:14:58.950" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="3034" PostId="2441" Score="0" Text="@joojaa Thanks for helping me realize it's just diffusion ala $\frac{\partial f}{\partial t}=-\nabla_S f$ with the Laplace-Beltrami operator. I'm thinking of running explicit Euler using [this discretization of the operator](http://computergraphics.stackexchange.com/a/1721/3294). Not sure what time-step to use." CreationDate="2016-05-16T18:26:40.580" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="3035" PostId="2451" Score="0" Text="Haha, when I posted the answer, SE asked me if I'm a human or a robot, the site wasn't sure :D I hope it is not because of the length of the answer, It got a little bit out of hand." CreationDate="2016-05-16T20:33:39.170" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="3036" PostId="2451" Score="0" Text="you want to make my brain melt, don't you. ;-) BTW: I already managed to read two of the papers/presentations so I'll hopefully extend the question or write a superficial answer at the end of this week. And now, GoT FTW!" CreationDate="2016-05-16T20:39:10.357" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3037" PostId="2449" Score="0" Text="The math speak here is: 57 and 113 are coprimes.  GCD(57,133)=1.  And 57 is square free." CreationDate="2016-05-17T09:11:15.923" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="3038" PostId="2436" Score="0" Text="Also in MathWorld article: Marsaglia (1972) - point in disk method should trump trig based." CreationDate="2016-05-17T09:18:09.863" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="3039" PostId="2453" Score="0" Text="Exactly how do you &quot;know&quot; that? Reference maybe?" CreationDate="2016-05-17T10:16:01.747" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3040" PostId="2453" Score="0" Text="@Andreas the linked article states this, although doesn't provide a reference. However, it does provide evidence that this affects performance, unless that can be explained in terms of some other cause." CreationDate="2016-05-17T10:44:45.463" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3041" PostId="2453" Score="0" Text="@trichoplax My bad. Did not read question properly." CreationDate="2016-05-17T10:47:51.343" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3042" PostId="2453" Score="0" Text="Also I saw several times that instead of fullscreen quad people use big triangle that covers the screen." CreationDate="2016-05-17T10:48:44.857" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="3044" PostId="2453" Score="0" Text="@trichoplax I saw this explanation several times. For example https://www.reddit.com/r/gamedev/comments/2j17wk/a_slightly_faster_bufferless_vertex_shader_trick/." CreationDate="2016-05-17T11:17:57.233" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="3045" PostId="2453" Score="1" Text="@nikitablack Oh I see - I thought you meant 2 triangles instead of a quad - but you mean 1 oversized triangle that encompasses the whole screen. That's interesting. I apologise - that is very much related to this question :)" CreationDate="2016-05-17T11:20:34.220" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3046" PostId="2436" Score="0" Text="I would need to do some profiling, but I'm not entirely convinced that rejection sampling would beat trig." CreationDate="2016-05-17T11:56:21.650" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3047" PostId="2454" Score="1" Text="Enabling GL_POLYGON_SMOOTH would cause multiple rasterization." CreationDate="2016-05-17T16:55:41.177" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3048" PostId="2456" Score="0" Text="A geometry shader may add more primitives than found in the original mesh. Does this do it for you?" CreationDate="2016-05-17T17:41:03.227" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3049" PostId="2450" Score="0" Text="I believe you need to define the &quot;edge&quot; more in detail. How does it differ from a simple wireframe? In cifz answer there is a good description of screen space post-process, but from your question it is difficult to determine if it is applicable." CreationDate="2016-05-17T19:07:58.727" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3050" PostId="2391" Score="0" Text="I have one question. Is the second picture generated only by sampling the EM? Or is it MISed version of sampling cosine and sampling EM? I really hope that it is the MISed version, because if so, then I might have a remedy for the high noise in the shadowy part." CreationDate="2016-05-17T20:42:02.350" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="3051" PostId="2391" Score="0" Text="No @tom, it uses sperical EM sampling only, ignoring both the (Lambert) BRDF and the cosine factor. 64 samples were used and no image-space filtering applied, just averaging over pixel area. When MIS is applied to combine the EM sampling with the cosine sampling, the noise in the shadow decreases a lot, but slightly increases in the sunlit part." CreationDate="2016-05-17T22:17:39.087" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3052" PostId="2450" Score="0" Text="Well, with edge I mean the &quot;creases&quot; and &quot;ridges&quot; that form the solids. A wireframe would display all triangle faces, which is not what I want." CreationDate="2016-05-18T07:27:05.180" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3053" PostId="2452" Score="0" Text="Very nice explanation cifz, but what if no gradient is visible in a certain situation? For example, there is no light source at the back of the cube and therefore, no gradient is visible. Then this image based edge detection process you describe wouldn't work, am I right?" CreationDate="2016-05-18T07:31:15.297" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3054" PostId="2436" Score="0" Text="super nice answer! thanks a lot.. not giving back my opinion yet about it because I still have to really check and implement your answer.. I will get back to you soon.. but in the meantime a big thanks!" CreationDate="2016-05-18T09:28:37.203" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="3055" PostId="2450" Score="0" Text="Ok, exactly what I asked for :-) I would generate a wireframe from those creases and ridges. The tricky part is still to determine what a crease/ridge is. Do you have any idea of how to do that?" CreationDate="2016-05-18T10:44:20.217" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3057" PostId="2460" Score="0" Text="How do area lights create sampling bias?" CreationDate="2016-05-18T14:32:42.377" UserId="3385" ContentLicense="CC BY-SA 3.0" />
  <row Id="3058" PostId="2436" Score="0" Text="Everything is more than clear, thanks a lot for your awesome explanation!" CreationDate="2016-05-18T14:36:19.800" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="3059" PostId="2436" Score="0" Text="If I may ask you, if you feel like of course, I have another question that might definitely solve my project.. Thanks in any case for your precious help so far:&#xA;http://computergraphics.stackexchange.com/questions/2461/resulting-probabilty-density-in-path-tracer-for-paths-using-next-event-estimatio" CreationDate="2016-05-18T14:38:39.343" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="3060" PostId="2460" Score="0" Text="@Jake: In the same way as the rest: you have to cast more than one ray, so you need to decide what distribution to use." CreationDate="2016-05-18T19:34:14.567" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="3061" PostId="2460" Score="0" Text="I suppose I thought area lights where directional so that light was only emitted along the normal of the light surface. Is that not how area lights work?" CreationDate="2016-05-18T22:27:16.833" UserId="3385" ContentLicense="CC BY-SA 3.0" />
  <row Id="3062" PostId="2460" Score="1" Text="@Jake No, area lights generally emit light into the entire hemisphere of directions at each point on their surface. You can see this if you imagine what a real-world area light looks like (such as a computer display showing solid white). You can see the emitted light from any direction." CreationDate="2016-05-18T23:24:53.520" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3063" PostId="2460" Score="3" Text="I'd add that [distribution ray tracing](https://en.wikipedia.org/wiki/Distributed_ray_tracing) is sort of a middle ground between recursive ray tracing and path tracing. It allows one to _somewhat_ have things like glossy reflection or depth of field in the recursive framework. The limitation is it's easy to run into problems with the branching factor: imagine if every ray that hits a surface spawns 10-100 secondary rays; the ray count would grow exponentially with each bounce! So path tracing is much more efficient if you want to do more than a very limited amount of stochastic phenomena." CreationDate="2016-05-18T23:42:16.707" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3064" PostId="2460" Score="1" Text="Yea the laptop example really clears things up. Of course it's hemispherical from each point now that you put it that way. Thanks so much!" CreationDate="2016-05-19T00:07:55.837" UserId="3385" ContentLicense="CC BY-SA 3.0" />
  <row Id="3065" PostId="2452" Score="0" Text="If you use  a deferred renderer you can do the same but on the normal buffer or again, if you have a prepass you could apply the algorithm to the depth. &#xA;Still you are right, a screen space approach might be not ideal in all cases :) What solution is viable depends a lot on what is your budget for this effect, how complex are your scene." CreationDate="2016-05-19T07:24:06.327" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="3066" PostId="2452" Score="0" Text="Potentially if this is really central for your game, a very easy, but potentially taxing, would be computing a gradient on your neighboring normals for each vertex (offline at load time) and pass this factor as additional vertex attribute." CreationDate="2016-05-19T07:40:19.280" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="3067" PostId="2462" Score="2" Text="The standard approach is to use a transformation $T$ that does the conversion form space to space. In this case Homogeneous coordinates could be used. The process is pretty well described [here](http://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix/building-basic-perspective-projection-matrix). Feel free to write a proper answer so that the question wont hang here forever." CreationDate="2016-05-19T08:12:02.633" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3068" PostId="2452" Score="0" Text="Thangs again cifz for your help.&#xA;Well, what I want to achieve is to develop a CAD/CAM solution that looks similar to this: https://www.youtube.com/watch?v=-qTJZtYUDB4&#xA;And I really would like to know how they managed it to render all the black edges, creases and ridges.&#xA;cifz, do you think as a graphics programming specialis that they achieved this look by using one of your screen space approaches? Or maybe by computing a gradient on neighboring normals? I really want to know how this can be done. Thanks again!" CreationDate="2016-05-19T09:01:21.427" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3069" PostId="2430" Score="0" Text="Thanks for this Joojaa.  It does sound like do my work for me doesn't it? But it was genuinely just as idle thought on the train.  You'll given me some cool clues to get started with (I've never fiddled with image manipulation like this before (which is why it sounds a bit 'DMWFM' - I didn't have a starting point.  Now, thanks to you, I do)).  I'll probably fiddle with this in idle moment - and I may be back with further questions (and my own source code!) later.  Thanks again!" CreationDate="2016-05-19T09:18:53.697" UserId="3319" ContentLicense="CC BY-SA 3.0" />
  <row Id="3070" PostId="2452" Score="0" Text="As a premise, I have didn't look that closely to the video as I am at work and super busy :) But judging from the cylindrical green shape at the beginning of the video, they are not using a screen space approach on the final image, as you don't see edges highlighted between that green shape and the white background. Not sure if that is a problem for you, as a first try I'd try a screen space edge detection combining information from depth and normals that should be quite fast to prototype, and if it is failing you go for the offline version or keep experimenting in SS :)" CreationDate="2016-05-19T09:48:16.243" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="3071" PostId="2452" Score="0" Text="Also, update your question to a reference image of what you are looking for :) Maybe it will attract someone with a precise answer. i'll try and think about it and have a better look at the video if I ever stop being so busy :(" CreationDate="2016-05-19T09:59:06.440" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="3072" PostId="2452" Score="0" Text="Thank's very much cifz for your hints. The thing is that I'm a total noob regarding OpenG, GLSL and Computer Graphics in general and don't know how to start. I know you haven't that much time but I would be very thankful if you could help me starting programming the shaders. Of course I would gladly pay you for your efforts." CreationDate="2016-05-19T15:16:49.877" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3074" PostId="2463" Score="0" Text="Thanks! I have been doing what you've said at the beginning, using geometry shaders to create geometry around the points and so enable to draw pixels around the point. It works, of course, but its a more cumbersome solution. I got particularly interested in the last bits of what you've said: if I understood you correctly, what you mention is one could render the whole scene and then use a screen shader aimed at the area of interest and then do a pass over the rendered frame in order to work on how many pixels one wants around the points in the center?" CreationDate="2016-05-20T00:06:26.130" UserId="3383" ContentLicense="CC BY-SA 3.0" />
  <row Id="3075" PostId="60" Score="0" Text="I seriously doubt the ROP adds any performance overhead if blending is disabled." CreationDate="2016-05-20T03:33:03.143" UserId="3386" ContentLicense="CC BY-SA 3.0" />
  <row Id="3076" PostId="60" Score="0" Text="@GroverManheim Depends on the architecture! The output merger/ROP step also has to deal with ordering guarantees even if blending is disabled. With a full-screen triangle there aren't any actual ordering hazards, but the hardware may not know that. There might be special fast paths in hardware, but knowing for certain that you qualify for them…" CreationDate="2016-05-20T06:05:40.787" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="3077" PostId="2465" Score="0" Text="There are also cases where a read-only buffer will do better than storing 4kb of read-only data in shared local memory. For example, storing it in local memory may mean that there is a unique copy of your data for every thread group. If the buffer fits in cache, it's quite possible that the cache performs better than local memory for read-only access patterns." CreationDate="2016-05-20T06:17:13.907" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="3078" PostId="2463" Score="0" Text="You could render the first pass in a texture. Then in the second pass on full screen, in each pixel you could inspect the texture content in a disk around the pixel and do what you want (e.g. color according to the closest filled pixel, or paint white only if distance is below some threshold). Of course for just drawing disks, this is far from being the most efficient way ;-)" CreationDate="2016-05-20T07:12:31.913" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="3079" PostId="2463" Score="0" Text="Sure, I just gave the simples example I could think of. But yes, I think what you say is in line with what I've been thinking, i.e. rendering to a texture in one pass and then later we have the information for, in other passes, alter pixels in relation to what lays in surrounding pixels. Many thanks." CreationDate="2016-05-20T09:04:10.657" UserId="3383" ContentLicense="CC BY-SA 3.0" />
  <row Id="3080" PostId="2452" Score="1" Text="Don't need to pay anyone, just start reading few tutorials online, buy some books and study hard :) It will be very rewarding at the end!" CreationDate="2016-05-20T13:41:32.590" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="3081" PostId="2450" Score="1" Text="The cad programs do not do this with a shader mostly. Instead they know the hard edges from the model and draw a line on top of the mesh form that info." CreationDate="2016-05-20T14:51:40.600" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3082" PostId="2469" Score="2" Text="Additionally, the code for `vSize()` does not return very precise results as the square root of an integer is not necessarily also an integer, but the method returns an `int`. Actually, in most cases the given method will return imprecise results." CreationDate="2016-05-20T15:36:14.207" UserId="127" ContentLicense="CC BY-SA 3.0" />
  <row Id="3083" PostId="2469" Score="0" Text="It might be used as a hash of some kind. Or maybe they are using a integer quantized field." CreationDate="2016-05-20T20:22:29.413" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3084" PostId="2469" Score="0" Text="Basically the integer is 64-bit. Will it make any difference in that case ?" CreationDate="2016-05-21T09:56:39.100" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="3085" PostId="2469" Score="0" Text="64 bit integers allow for a [larger maximum number](https://en.wikipedia.org/wiki/9223372036854775807) than 32 bit integers, but they can still only take whole number values. You could ask why integers are used as a separate question, but that would require access to more of the code and some context on what it is used for. Without that context it is difficult to judge whether this would be on topic for Computer Graphics." CreationDate="2016-05-21T12:48:31.720" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3086" PostId="2471" Score="0" Text="There are several different meanings for &quot;OSG&quot;. Please could you specify which one this question refers to?" CreationDate="2016-05-21T13:04:33.680" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3087" PostId="2471" Score="0" Text="I dont think there is a good equation for a prism as such." CreationDate="2016-05-21T13:54:29.767" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3088" PostId="2474" Score="1" Text="Thank you for your help, successfully created the prism. :)&#xA;&#xA;I think, set the 8 points manually and draw with triangles is easier solution in this case. I believed it could be solved with a similar parametric equation, like cylinder.&#xA;&#xA;Like you said, first create a cube, and scale it to the proper size solved the problem." CreationDate="2016-05-21T16:19:53.773" UserId="3408" ContentLicense="CC BY-SA 3.0" />
  <row Id="3089" PostId="2479" Score="0" Text="Many thanks for your answer! So, in the meanwhile I was able to come with the exact same solution for the cross-shape problem, but in a messier way. So, I am glad I was not thinking it wrong, but happy to see a cleaner solution. Thanks! About the performance, it's currently terrible: ~14ms now, which is of course unacceptable for real time simulations. I got interested in both your suggestions. Would care to elaborate a bit more? (just a PS: I am trying to implement the intensity of halos of point lights depending on the aggregation of how many point lights are close to each other)." CreationDate="2016-05-22T01:45:46.643" UserId="3410" ContentLicense="CC BY-SA 3.0" />
  <row Id="3090" PostId="2479" Score="0" Text="By down-sampling (with which I would be totally fine and would be even welcome) you mean something like this http://stackoverflow.com/questions/14366672/how-can-i-improve-this-webgl-glsl-image-downsampling-shader or something simpler? And I the spatial hierarchy was what I though in the first place, but I though there wasn't a way to save an array of arrays from one pass to the next that would hold the quad-tree. Isn't that the case? I don't see how I would populate the quad-tree in the first pass but then store the info for the second pass (unless using a compute shader)." CreationDate="2016-05-22T01:47:52.310" UserId="3410" ContentLicense="CC BY-SA 3.0" />
  <row Id="3091" PostId="2472" Score="0" Text="What do you mean by &quot;random sampling&quot;? Does it mean &quot;uniform random sampling&quot; or &quot;cosine-weighted random sampling&quot;? What does `createRandomReflect` do? How do you compute the PDF of the generated sample? How does your Monte Carlo estimator look like?" CreationDate="2016-05-22T13:05:05.427" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3093" PostId="2472" Score="0" Text="Ohh... Sorry,&#xA;&quot;random sampling&quot; means &quot;uniform random sampling&quot;. createRandomReflect creates sample on a hemisphere for uniform random sampling. As for the PDF, I did some integral(on paper) and google/wolfram|alpha search. I'm using a basic Monte Carlo estimator which could be found in any basic Path Tracer(eg. smallpt/smallpaint). It's a loop that throws samples at the scene and averages whatever I get back from the samples." CreationDate="2016-05-22T13:20:25.433" UserId="2448" ContentLicense="CC BY-SA 3.0" />
  <row Id="3094" PostId="2472" Score="0" Text="Of course, but in a naive path tracer you usually use a simple Monte Carlo estimation, which consists of picking the direction, evaluating the BRDF for that direction and dividing the result by probability density of generating the direction. You showed us the sampling and evaluation of the BRDF, but we cannot see how you compute the PDF and how you use the generated sample later on. We need more code to understand your implementation." CreationDate="2016-05-22T15:49:06.213" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3095" PostId="2479" Score="0" Text="@GiovannS For this I think you would start with a simple 2x box filter downsample. It can be done by just copying a texture into a 2x smaller render target, using bilinear filtering; that will average together 2x2 pixels in the source to 1 pixel in the destination. For the spatial hierarchy approach, if the points are generated on the GPU then you probably want to build the data structure on the GPU. It's going to be a more complicated endeavor with append buffers, compute shaders, etc. If you google &quot;build quadtree on GPU&quot; or similar, there's a number of articles and papers on it." CreationDate="2016-05-22T21:02:41.577" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3096" PostId="2481" Score="0" Text="WOW! That last link hit the spot! Things are looking great.  However the code they have makes no sense to me as I am a glsl programmer.  Thankfully I found two pass code somewhere." CreationDate="2016-05-23T06:44:48.920" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3097" PostId="2481" Score="0" Text="My confusion is the following, how exactly do you increase/decrease the size of the blur? Increasing how many texels away each sample is, or increasing the amount of texels sampled? And have you seen any good code for this working inside of a for loop based on glow radius?" CreationDate="2016-05-23T06:46:00.707" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3098" PostId="2481" Score="1" Text="_&quot;I would think rendering to a downsampled FBO would be much faster than generating mipmaps. glGenerateMipmap will generate the complete mipmap pyramid, which seems wasteful:&quot;_&#xA;Surely that might depend on whether there is custom hardware or not? Also, given that the total MIP map chain is a 33% overhead on the original image, but the 1st MIP map level is 25% of that, the storage for the remainder is only an additional 8%." CreationDate="2016-05-23T12:13:15.337" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="3099" PostId="2483" Score="1" Text="This is interesting but I'm confused by one point. Could you clarify what it means to &quot;divide the result for that direction by probability of picking the direction&quot;? If it is not a binary choice but a direction chosen from a continuous distribution, won't the probability be zero?" CreationDate="2016-05-23T15:00:26.530" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3100" PostId="2483" Score="0" Text="Thank you! This helps a lot. The method you describe will work for ideal Fresnel surfaces. And I just discovered Walter et. al. [2007] and Burley [2015], which can be used for rough BSDFs." CreationDate="2016-05-23T15:10:04.173" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3101" PostId="2483" Score="1" Text="@trichoplax: Yes it would, but in that paragraph I was describing the sampling technique just for a (dielectric) Fresnel BSDF - ideally smooth surface, which is a sum of two Dirac delta functions. In such case you are picking one of the directions with some discrete probability. In case of a non-delta (finite) BSDF, you generate directions according to a probability density function. Unfortunately, delta and non-delta cases have to be handled separately, which makes the code a little messy. More details on sampling microfacet BSDFs can be found, for example in the Walter et. al. [2007] paper." CreationDate="2016-05-23T16:37:09.823" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3102" PostId="2483" Score="2" Text="@RichieSams: Walter et. al. [2007] is basically still the state-of-the art for dielectric rough surfaces, but to make it work well you need a good sampling which was published just recently by Heitz and D'Eon the 2014 paper &quot;Importance Sampling Microfacet-Based BSDFs using the&#xA;Distribution of Visible Normals&quot;. And note that it is a single-scattering model which neglects inter-reflections between microfacets making it visibly dark for higher roughness values. See my question &quot;Compensation for energy loss in single-scattering microfacet BSDF models&quot; for more details." CreationDate="2016-05-23T16:53:04.740" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3104" PostId="2483" Score="6" Text="Just wanted to point out that if you choose probability = fresnel() as the question suggested, then when you divide by the probability, you cancel out the Fresnel factor that would normally be multiplied in. So (in the discrete, two-Dirac case) you end up with the ray contribution not including any Fresnel factor at all. It's standard importance-sampling theory, but I thought I'd point that out as a potentially confusing issue." CreationDate="2016-05-23T18:22:23.363" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3105" PostId="2485" Score="2" Text="As an example two edge sharing triangles may perfectly splitting the same pixel in two but an OpenGL implementation shall guarantee only one of the triangles cover the pixel center. Which one is implementation dependent." CreationDate="2016-05-23T18:27:22.187" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3106" PostId="2483" Score="0" Text="@Nathan, that's actually a very good point. I'm glad you mentioned it." CreationDate="2016-05-23T18:32:49.610" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3107" PostId="2212" Score="0" Text="@FabrizioDuroni, does it mean, that adjusting the power of your light source with the brightness parameter doesn't make the resulting picture bright enough? I think I am missing something and I'm afraid you are adding to much alchemy into it ;-) Anyway, I'm glad it helped at least partially." CreationDate="2016-05-23T20:04:32.153" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3108" PostId="2481" Score="0" Text="Good point, I spoke too broadly there.  It is possible that generating the chain is faster on some systems." CreationDate="2016-05-24T03:43:14.057" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="3109" PostId="2481" Score="0" Text="To increase the size of the blur you increase the amount of pixels sampled.  See here for a 7x7 blur: https://software.intel.com/en-us/blogs/2014/07/15/an-investigation-of-fast-real-time-gpu-based-image-blur-algorithms" CreationDate="2016-05-24T03:48:38.897" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="3110" PostId="2487" Score="0" Text="Oh awesome, I guess I should have dug a little deeper on the corporation websites.  I assumed that since apitrace didn't offer these details, there was some sort of reversing going on.  Thanks!" CreationDate="2016-05-24T06:06:11.660" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="3111" PostId="2483" Score="2" Text="@Nathan, I incorporated your notice into the answer." CreationDate="2016-05-24T11:27:33.883" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3112" PostId="2490" Score="0" Text="That first case sounds more like the  non-zero rule (https://en.wikipedia.org/wiki/Nonzero-rule) (except using  the normal rather than 2D edge direction)" CreationDate="2016-05-24T16:40:01.747" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="3114" PostId="2212" Score="0" Text="I was missing something too, and in fact i remove that strange &quot;alchemy&quot; that i though was the right thing to do (web is not always a good source :D) Just a brightness multipler is good. This parameter could be substituted with more physically correct values, like the light power (using a Rienmann sum to calculate the integrate the light SPD ) and its area. But for my thesis has been decided that the brightness parameter is enough." CreationDate="2016-05-24T20:06:11.123" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="3115" PostId="2212" Score="0" Text="This is an example of a final rendered image that will be included in the thesis https://drive.google.com/open?id=0BxeVnHLvT8-7TVBZZE44VTRuT1k Keep watching my github repo for the final result ;)" CreationDate="2016-05-24T20:06:16.093" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="3116" PostId="2212" Score="0" Text="@fabrizio, but that is almost exactly what I suggested you to do. ;-) And yes, having a more physically meaningful parameter (e.g. power) in a physically-based renderer would be definitely a nicer thing than some unitless brightness parameter. Especially when you have a spectral renderer (it is not clear how to do it in a colour-space renderer). The image looks pretty good to me. Good luck with the thesis!" CreationDate="2016-05-24T20:39:39.613" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3118" PostId="2493" Score="0" Text="Requests for off site resources are off topic, but by removing the final sentence this is now an on topic question. Feel free to override my edit if I've changed your intent too much." CreationDate="2016-05-25T10:26:30.260" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3119" PostId="2498" Score="0" Text="I don't understand, can you be more precise ?" CreationDate="2016-05-25T16:28:13.513" UserId="3339" ContentLicense="CC BY-SA 3.0" />
  <row Id="3120" PostId="2499" Score="0" Text="Does that mean that the geometry term cancels the cosine term ?" CreationDate="2016-05-25T17:35:20.943" UserId="3339" ContentLicense="CC BY-SA 3.0" />
  <row Id="3121" PostId="2499" Score="0" Text="No. The geometry term is a normalization factor that accounts for the fact that the microfacets will shadow and mask each other. The 'visibility' term I mention in answer is the integral itself. The integral adds up all *visible* incoming light." CreationDate="2016-05-25T17:45:59.950" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3122" PostId="2499" Score="0" Text="However, in the book Real Time Rendering I read that for the Blinn-Phong model, the geometry term is (n.v)(n.l) which cancels the denominator." CreationDate="2016-05-25T20:06:31.837" UserId="3339" ContentLicense="CC BY-SA 3.0" />
  <row Id="3123" PostId="2500" Score="0" Text="&quot;As you can see this causes an issue because of the segments.&quot;. Could you describe what problem you are seeing? What about the picture should be different?" CreationDate="2016-05-25T21:28:56.693" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3124" PostId="2500" Score="0" Text="@Trichoplax the line has clear segments inside of it. The line should be smooth." CreationDate="2016-05-25T21:30:22.467" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3125" PostId="2502" Score="0" Text="What does n mean? Why 0.95?" CreationDate="2016-05-25T21:31:44.270" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3126" PostId="2500" Score="0" Text="Do you mean that there should be no visible colour difference between the segments?" CreationDate="2016-05-25T21:50:14.087" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3127" PostId="2499" Score="1" Text="@Livetrack: The part (n.v)(n.l) is not usually called &quot;geometry factor&quot;, that is the G in the formula. The only part which gets cancelled when this BRDF is placed into the rendering equation is (n.l)." CreationDate="2016-05-25T21:54:58.507" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3128" PostId="2500" Score="0" Text="@trichoplax I just want the color to be smooth. I want the new segment to math nicely with the previous faded one.  Just so that everything looks continuous in both shape and color." CreationDate="2016-05-25T22:22:25.927" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3130" PostId="2495" Score="0" Text="Oh I had totally missed that this was already a result in the paper. That certainly clears it. :) I'll have to reread it to get a better grasp of how it fits within the BRDF though." CreationDate="2016-05-26T09:00:09.833" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3131" PostId="2503" Score="0" Text="could you apply your thinking to my example as stated above? I did not get the grasp of your method." CreationDate="2016-05-26T09:00:55.963" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3133" PostId="2507" Score="0" Text="The second method is Phong shading model ? And could you elaborate more on how you conclude that in Gouraud Shading we cannot have brighter points and in the second method we can have brighter points ?" CreationDate="2016-05-26T09:17:50.953" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3134" PostId="2507" Score="0" Text="@johnjohn gouraud shading is taking a weighted average of the colors of the vertices. While with per pixel you can have a point light near the middle of a large surface. The vertices won't get much light but the middle should have a lot of light." CreationDate="2016-05-26T10:08:00.483" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3135" PostId="2509" Score="0" Text="Thanks. And how big are these groups?" CreationDate="2016-05-26T12:10:58.630" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="3136" PostId="2509" Score="1" Text="@nikitablack depends on the hardware and is kinda impossible to extract naively from any marketing information." CreationDate="2016-05-26T12:13:00.820" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3137" PostId="2509" Score="0" Text="And also a question. Lets's say one thread in a group reads from some buffer in a branch based on that thread's id. Lets's say the buffer size is 1, so only the first thread can read, all other threads will cause out of bounds read. Is it safe? And one more thing - the read from the system memory is very slow, but all threads will read from it, even if they don't need, right?" CreationDate="2016-05-26T12:16:15.593" UserId="386" ContentLicense="CC BY-SA 3.0" />
  <row Id="3138" PostId="2509" Score="1" Text="@nikitablack that's going to depend on a few things; 1) if the driver is smart enough to forward the constant in the test into the then clause, 2) if the hardware can do a &quot;masked read&quot;, 3) what exactly happens on out of bounds reads" CreationDate="2016-05-26T13:12:26.167" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3139" PostId="2507" Score="0" Text="@johnjohn just to add to ratchet's comment, WRT Gouraud, just consider two points on a 2D graph where height represents brightness and connect them by a straight line segment. No point along the segment can be higher (i.e. brighter) than the higher end point.  If you consider the edges of your triangle it shows that no point on an edge can be brighter than the end vertices.  Now for any point inside the triangle just draw, say, a line through the point from edge to edge - the point inside can't be brighter.&#xA;&#xA;With per-pixel shading, however, you don't have linear interpolation." CreationDate="2016-05-26T13:53:55.863" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="3140" PostId="2503" Score="0" Text="If fact I'm not sure what you meanhere by &quot;interpolate&quot;. If what you aim at is a mesh, your formula directly give the *position* where the isoval cross an edge. Then you obtain the mesh by connecting these points." CreationDate="2016-05-26T15:50:39.080" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="3141" PostId="2502" Score="0" Text="opacity 0.05 means transparency 0.95, i.e. the intensity scaling. n is the number of passes (i.e. number of times you apply a 0.05 opacity)." CreationDate="2016-05-26T15:52:19.933" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="3142" PostId="2498" Score="0" Text="The other answer state the same differently in its last paragraph :-)" CreationDate="2016-05-26T15:54:02.783" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="3143" PostId="2509" Score="2" Text="@nikitablack The lockstep group size is 32 for NVIDIA and 64 for AMD (they call them &quot;warps&quot; and &quot;wavefronts&quot; respectively). For Intel the size is variable from 8 to 32, determined by the shader compiler. Also, out-of-bounds reads are defined to just return zero in DirectX, and I'm pretty sure it's the same for GL and the other APIs." CreationDate="2016-05-26T17:13:09.697" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3144" PostId="2492" Score="0" Text="The diagrams are nice, but you might want to label the points (p0, p1, p2) to make them easier to understand. :)" CreationDate="2016-05-26T17:16:53.060" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3145" PostId="2492" Score="1" Text="There are just sketches @NathanReed I threw away the sources, but yes i can do that." CreationDate="2016-05-26T18:29:57.653" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3156" PostId="2509" Score="1" Text="For the reason Nathan just described, you should always make your work group size a multiple of 64 to avoid wasting shader invocations in the last warp / wavefront of each group. If you don't need to use shared variables, exactly 64 is usually a good size." CreationDate="2016-05-27T07:57:30.677" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="3157" PostId="2503" Score="0" Text="What I do not understand, is where you put what in the formula. I know that in V1 and in V2 I should replace the values from the vertices but in P1 and P2 I have no clue what I should put there? Can you give me a hint how it works in the current example?" CreationDate="2016-05-27T08:08:52.733" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3158" PostId="2512" Score="1" Text="I do not understand what you mean though in the last paragraph. Could you elaborate more on that ? For example, what do you mean by 'negative' and 'positive' end ?" CreationDate="2016-05-27T09:02:37.070" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3159" PostId="2510" Score="1" Text="It looks to me that it's just finding runs of monotonic edges. Given a defined winding order (in this case anticlockwise), then you can identify P6 through P0 as a decreasing run, as is P2 through P4.  Since the *left most* vertex, P8, is on a decreasing run, the *decreasing* runs define left boundaries (and therefore increasing runs, right boundaries)" CreationDate="2016-05-27T09:59:49.447" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="3160" PostId="2503" Score="0" Text="P1 and P2 are the coordinates of the segment extremities." CreationDate="2016-05-27T11:18:50.863" UserId="1810" ContentLicense="CC BY-SA 3.0" />
  <row Id="3161" PostId="2507" Score="0" Text="@SimonF So, to recap there are three methods of finding the brightness:&#xA;1) Firstly, calculate the dot product of normals and light vector for each vertex, then interpolate over triangle.&#xA;2) Interpolate normals then calculate the dot product with light vector for each point.&#xA;3) Interpolate gradient vectors then calculate the dot product with light vector.&#xA;&#xA;From these methods, which can give brightness bigger than the vertices?" CreationDate="2016-05-27T18:50:58.750" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3162" PostId="2514" Score="0" Text="what if we assume a directional&#xA;light, whose direction does not depend on the position on the surface. What happens then ?" CreationDate="2016-05-27T19:47:59.517" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3163" PostId="2514" Score="0" Text="@johnjohn same thing, the view direction is still affected by the surface." CreationDate="2016-05-27T20:35:11.993" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3165" PostId="2498" Score="0" Text="I don't understand : Imagine that I have a single point source, then I can simplify the equation like this : $$ L = f(\omega_o, \omega_i) E(\omega_i) $$. Why don't I need to bother about the cosine term ?  PS : J'ai vu que vous êtes chercheur à l'Inria. Je ne veux pas trop vous déranger mais vous pourriez faire l'explication en français ? Merci." CreationDate="2016-05-27T21:02:36.193" UserId="3339" ContentLicense="CC BY-SA 3.0" />
  <row Id="3166" PostId="2499" Score="0" Text="I don't understand how the integral (which is about incoming lights) can cancel the cosine term." CreationDate="2016-05-27T21:05:33.693" UserId="3339" ContentLicense="CC BY-SA 3.0" />
  <row Id="3167" PostId="2515" Score="0" Text="So there are three methods which you can find the brightness .&#xA;1) Firstly, calculate the dot product of normals and light vector for each vertex, then interpolate over triangle. 2) Interpolate normals then calculate the dot product with light vector for each point. 3) Interpolate gradient vectors then calculate the dot product with light vector. &#xA;&#xA;From these methods, which can give brightness bigger than the vertices?" CreationDate="2016-05-28T08:00:48.460" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3168" PostId="2515" Score="1" Text="@johnjohn 2 and 3. 1 is Gouraud shading that is hardly used anywhere anymore because it does not look correct." CreationDate="2016-05-28T08:03:34.127" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3169" PostId="2515" Score="0" Text="So I assume from what you said that the **order** of interpolation (comes first in Gouraud Shading or comes second in the other methods) can lead to better shading results, only for the last two." CreationDate="2016-05-28T08:07:32.857" UserId="2214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3170" PostId="2515" Score="1" Text="@johnjohn Gouraud shading is a trick to compensate for the fact that hardware was not fast enough for the job. Nobody even back whan it was conceived thought it was any good. (well obviously if you use Gouraud on a mesh that has one triangle per pixel theres no difference) Since the lightning is heavily dependent on where the local normal points it means that any method that is locally per pixel calculated can have hot spots in other than corners." CreationDate="2016-05-28T08:11:42.950" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3171" PostId="1721" Score="3" Text="What Meyer et al. 2003 did not (perhaps explicitly) mention was how to compute the curvature for border vertices. Since they have taken an angle deficit approach, the Gaussian curvature for border vertices should read $K = (\pi - \sum_j{\theta_j})/A_{mixed}$." CreationDate="2016-05-28T09:33:15.520" UserId="3444" ContentLicense="CC BY-SA 3.0" />
  <row Id="3173" PostId="2510" Score="1" Text="Not sure if it will help you but [Clipper](http://www.angusj.com/delphi/clipper.php) uses Vatti's algorithm. The [docs](http://www.angusj.com/delphi/clipper/documentation/Docs/Overview/_Body.htm) mention: &quot;A section in 'Computer graphics and geometric modeling: implementation and algorithms' by By Max K. Agoston (Springer, 2005) discussing Vatti Polygon Clipping was also helpful in creating the initial Clipper implementation.&quot;" CreationDate="2016-05-28T13:56:46.263" UserId="141" ContentLicense="CC BY-SA 3.0" />
  <row Id="3175" PostId="2516" Score="0" Text="Does orthogonal in this context mean using only vertical and horizontal edges? Are you looking for a way to represent more than 4 edges meeting at a single vertex?" CreationDate="2016-05-28T14:49:28.780" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3178" PostId="2522" Score="0" Text="So what should be the correct implementation ?. If i am not wrong i used the variable as defined in this video on the algorithim.  https://www.youtube.com/watch?v=TRbwu17oAYY&#xA;&#xA;Any idea how to implement a correct Bresenham's line drawing algorithm for slope &lt; 1." CreationDate="2016-05-28T22:50:00.063" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3179" PostId="2522" Score="0" Text="Although the algorithm is often stated using fractions, it can be implemented in integers only, by using the horizontal and vertical measures of the line to keep track of when to move horizontally and when to move diagonally." CreationDate="2016-05-29T02:06:55.973" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3180" PostId="2524" Score="1" Text="Thanks that was on spot. also got the reason why it was wrong. excellent answer with great explanation." CreationDate="2016-05-29T05:18:06.737" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3181" PostId="2522" Score="1" Text="Interesting, I'll delete the lastparagraph. Nice explanation BTW." CreationDate="2016-05-29T07:12:11.497" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3183" PostId="2519" Score="0" Text="Thank you for your reply. I've implemented the formula you've supplied and I got identical results as with my own (when using the macrosurface normal). So it seems it's just a different form (I got it from: http://graphicrants.blogspot.nl/2013/08/specular-brdf-reference.html)&#xA;&#xA;I was confused about the half vector because the SIGGRAPH 2015 PBS math course specifically state the geometry function dependant on the view, light and half vectors. So this is an error in the slides?" CreationDate="2016-05-29T08:26:00.767" UserId="3424" ContentLicense="CC BY-SA 3.0" />
  <row Id="3184" PostId="2519" Score="0" Text="@Erwin, now that you provided also the formula itself, it is much clearer. Next time do it right at the beginning, it helps. Yes, both version (mine and yours) are equivalent, but neither of them uses halfway vector for computing sine or tangent function. It uses $n\cdot v$ rather than $h\cdot v$ as you did in your implementation - that seems to be the mistake. I suspect you did the same mistake with the new implementation as well." CreationDate="2016-05-29T10:01:42.137" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3185" PostId="2526" Score="0" Text="There are a lot of code snippets on the linked page, but none of them appear to have the character &quot;`#`&quot;. Could you show a minimal example of code that you know gives this error message?" CreationDate="2016-05-29T14:26:59.780" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3186" PostId="2526" Score="0" Text="@trichoplax my bad. I should have explained that there is an executable provided in order for you to run it. I will update the question with a minimal example." CreationDate="2016-05-29T14:50:16.080" UserId="3452" ContentLicense="CC BY-SA 3.0" />
  <row Id="3188" PostId="2527" Score="1" Text="Yet as you can see by yourself Chapman's unedited algorithm is given in this exact form, and I do guarantee that it could run perfectly on this but also another computer (both having ATI graphics cards). I think it could have something to do with Vulkan and a more strict set of rules, but I have not been able to verify it yet." CreationDate="2016-05-29T20:56:59.567" UserId="3452" ContentLicense="CC BY-SA 3.0" />
  <row Id="3189" PostId="2527" Score="0" Text="@green_leaf oh yes I'm not doubting that it did work - I just don't have an explanation for why. I guess the change was a &quot;correction&quot; even though it hasn't turned out to be helpful for you..." CreationDate="2016-05-29T22:10:04.893" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3192" PostId="2527" Score="0" Text="that's very classic with ATI" CreationDate="2016-05-30T01:04:12.653" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3194" PostId="2487" Score="0" Text="oh I thought he wanted to write another perfkit. which the answer would be that it's not possible unless you make your own hardware." CreationDate="2016-05-30T01:30:37.823" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3196" PostId="2482" Score="0" Text="russian roulette" CreationDate="2016-05-30T01:46:15.387" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3197" PostId="2483" Score="0" Text="I got it to work! http://i.imgur.com/m1fYRdr.png" CreationDate="2016-05-30T02:09:01.033" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3200" PostId="2512" Score="0" Text="Say your cube is at position 0 on each axis in your volume, then it will fill the space from position 0 to position 1. So the left side of your cube will be at X=0 and the right side will be at X=1, same for the other two axes. So you use P1 and V1 for the edge end with the lower coordinate value and P2 and V2 for the end with the higher value." CreationDate="2016-05-30T07:30:24.077" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="3201" PostId="2518" Score="0" Text="The 'rotation by area mapping' algorithm described at http://www.leptonica.com/rotation.html looks like a good place to start" CreationDate="2016-05-30T07:39:46.800" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="3202" PostId="2465" Score="0" Text="Thanks for the feedback guys. I've finished the project I was using this for now, and wound up just using a r8ui readonly buffer texture, which worked pretty nicely :)" CreationDate="2016-05-30T08:01:59.140" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="3205" PostId="2483" Score="0" Text="@RichieSams: Cool, but does your glass attenuate light? If it doesn't, the object should not be visible under constant illumination I'm afraid." CreationDate="2016-05-30T12:34:54.060" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3206" PostId="2519" Score="0" Text="I did use N dot V in my new implementation, that gave me identical results to the second image I've posted. But I'm still not clear on why the PBS course slides state that the halfway vector should be used (See: http://blog.selfshadow.com/publications/s2015-shading-course/hoffman/s2015_pbs_physics_math_slides.pdf, Slide 88)." CreationDate="2016-05-30T15:24:04.623" UserId="3424" ContentLicense="CC BY-SA 3.0" />
  <row Id="3207" PostId="2483" Score="0" Text="Currently, the BTDF just returns the surface Albedo, (which is float3(0.95) in this render). If I set it to float3(1.0), yes, it will disappear. Implementing Beer-Lambert is my next task." CreationDate="2016-05-30T15:50:47.180" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3209" PostId="2423" Score="1" Text="@KristofferHelander :  Converting from 14 bit capture to a 16 bit representation of the 0-1 range is easily achieved by multiplication. But most of our textures are painted, not photographed -- sometimes they are painted directly in a 16 bit format, sometimes they are painted in sRGB and then converted to 16 bit when &quot;linearized&quot; for use as a texture. There's no need for HDR for albedo textures." CreationDate="2016-05-30T17:31:00.883" UserId="1781" ContentLicense="CC BY-SA 3.0" />
  <row Id="3211" PostId="2423" Score="1" Text="@KristofferHelander : For albedo textures, we tend to use TIFF with 16 bit integer data (what we call .tx is just TIFF format but tiled and with MIP-map multiresolution stored as multiple subimages within the TIFF file). For true HDR data, like environment captures, we use OpenEXR. Renderer output also tends to be OpenEXR." CreationDate="2016-05-30T17:34:53.267" UserId="1781" ContentLicense="CC BY-SA 3.0" />
  <row Id="3213" PostId="2531" Score="0" Text="What i understand is that you want to make User interface that will not be used in a game. right ?" CreationDate="2016-05-30T17:43:53.450" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3214" PostId="2527" Score="0" Text="@v.oddou indeed now that you mention it. It seems that they do that quite often." CreationDate="2016-05-30T18:19:32.347" UserId="3452" ContentLicense="CC BY-SA 3.0" />
  <row Id="3215" PostId="2531" Score="0" Text="Yes, My current goal is to create my own pomodoro time management application, so I want to create a simple User Interface that can be used for that purpose." CreationDate="2016-05-30T18:57:16.693" UserId="3459" ContentLicense="CC BY-SA 3.0" />
  <row Id="3216" PostId="2531" Score="0" Text="Check the answer i posted just now" CreationDate="2016-05-30T19:55:42.357" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3219" PostId="2519" Score="0" Text="Do I understand it correctly that using $h\cdot v$ instead of $n\cdot v$ was THE problem? Regarding the use of halfway vector in $G_1$: In fact it is used in both of the versions I posted (I made a mistake when constructing the LaTeX formula and wrote geomeotric normal into the first one, I'll fix it soon), but the point is that the halfway vector is not used to compute the cosine value (i.e. there is no $h\cdot v$ used)." CreationDate="2016-05-30T21:28:30.040" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3220" PostId="2533" Score="0" Text="@ArdenRasmussen So what are you using to make UI ?" CreationDate="2016-05-30T21:49:57.927" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3221" PostId="2534" Score="1" Text="I am unsure what you are asking here. What you posted as quote from the book is just the definition of a line, your title sounds unrelated. Are you asking how to find point on a line of which you have an implicit form like the one you posted?" CreationDate="2016-05-30T21:59:39.150" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="3222" PostId="2534" Score="0" Text="The midpoint algorithm uses the implicit equation of the line and I am looking forward to some proofs of the three cases mentioned in my initial post." CreationDate="2016-05-30T23:51:27.607" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="3223" PostId="2534" Score="0" Text="This question should be on Math StackExchange." CreationDate="2016-05-31T04:51:05.573" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3226" PostId="2538" Score="0" Text="Are you doing coding by deferring to stackexhange, perchance?" CreationDate="2016-05-31T12:38:14.947" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3228" PostId="2516" Score="0" Text="@trichoplax Exactly, and I do appreciate any help." CreationDate="2016-05-31T12:52:09.830" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="3229" PostId="2538" Score="0" Text="I totally missed your last question ?" CreationDate="2016-05-31T12:52:12.900" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="3231" PostId="2538" Score="0" Text="if `xMax` and `xMin` are `float`s, why is `xDist` an `int`?" CreationDate="2016-05-31T14:32:39.117" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="3232" PostId="2538" Score="0" Text="All the declared variable are in short int . The only floating point issue may arise with xDist/scanSpacingStep." CreationDate="2016-05-31T14:47:44.910" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="3233" PostId="2540" Score="0" Text="No. Constant buffers are just a chunk of memory. The DX spec and hlsl spec define the different types." CreationDate="2016-05-31T23:04:07.060" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3234" PostId="2542" Score="0" Text="Will you always have a fixed number of floats to store per cell? If so, would using one texture for each float work? For example, using 3 textures to store 9 floats per cell (3 per texture using the R, G, and B channels)." CreationDate="2016-06-01T00:12:22.283" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3235" PostId="2538" Score="0" Text="I'm not quite sure what you are asking. Are xMax and xMin pixel locations? If so are both inclusive? If xMin=2 and xMax=4, does that cover 2 columns of pixels or 3?" CreationDate="2016-06-01T00:22:07.583" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3236" PostId="2543" Score="0" Text="I love the code review part. I'd add that in addition to renaming `startHueInRadians` the poster should also change it from an array to a `struct`. None of hue, saturation, or brightness represent the same type of value, so you shouldn't treat them as such!" CreationDate="2016-06-01T05:05:30.140" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="3237" PostId="2543" Score="0" Text="@user1118321 Sadly there is nothing like a c++ struct in java, i need to create a special class for it." CreationDate="2016-06-01T06:02:35.160" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3239" PostId="2538" Score="0" Text="yes , it can be associated with pixel locations. (xMin,yMin) and (xMax,yMax) are inclusive. If xMin = 2 and xMax = 4 , and if both of them are inclusive , it should cover three columns, is not it ?" CreationDate="2016-06-01T11:04:27.273" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="3240" PostId="2544" Score="0" Text="Can show some pictures and code ?" CreationDate="2016-06-01T17:22:46.613" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3241" PostId="2544" Score="0" Text="@ritwiksinha Done!" CreationDate="2016-06-01T17:24:52.747" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3242" PostId="2544" Score="0" Text="Are you using $sine/cosine$ as the oscillating function ?" CreationDate="2016-06-01T17:30:14.887" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3243" PostId="2544" Score="0" Text="Yes technically. However as you can see by the value v I am going to map it differently." CreationDate="2016-06-01T17:37:01.890" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3244" PostId="2516" Score="2" Text="Is the graph guaranteed to be planar so that, forgetting the orthogonal edges for a moment, the graph can be drawn without edges overlapping?" CreationDate="2016-06-01T18:36:59.830" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="3245" PostId="2542" Score="0" Text="@trichoplax Thanks for your comment! No, I won't have a fixed number of floats, unfortunately. But even if I could fix it for those purposes, that solution wouldn't works since I will have high number of floats - which would result in great memory overhead due to the high number of textures to be used in order to implement your suggestion" CreationDate="2016-06-01T20:00:51.870" UserId="3410" ContentLicense="CC BY-SA 3.0" />
  <row Id="3246" PostId="2542" Score="0" Text="This is very relevant information - I'd recommend editing the question to include this." CreationDate="2016-06-01T21:19:55.183" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3247" PostId="2544" Score="1" Text="If you solved your own problem, you can also post an answer for anyone who sees the same problem in future. This is [actively encouraged](http://computergraphics.stackexchange.com/help/self-answer)." CreationDate="2016-06-01T21:28:31.180" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3248" PostId="1978" Score="0" Text="Is it correct to think of this model-matrix as a model-to-world matrix? in case I want to transform a point to the coordinate system of the model, then will I have to use the inverse of the model-matrix?" CreationDate="2016-06-02T05:24:49.527" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3249" PostId="1978" Score="0" Text="Regarding the camera-matrix (built with cam position, lookat point and up vector), can I call it a camera-to-world matrix? Then, if I want to go from world to camera space, will I have to use the inverse of that camera-matrix?" CreationDate="2016-06-02T05:27:06.040" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3250" PostId="2549" Score="0" Text="Yes, you are completely right, the direction vector from the camera has nothing to do with the world z-axis, my mistake. What confused me, I think it is silly but still, is [this image](http://www.scratchapixel.com/images/upload/perspective-matrix/camera2.png?) where the +z-axis enters the lens and I think you mean the +z-axis exits the lens ... what are the implications of using one or the other? This is what I cannot seem to understand" CreationDate="2016-06-02T06:02:47.743" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3251" PostId="2549" Score="0" Text="@BRabbit27 your image is interesting, it does mention the **default** word which is important. I'm curious as to why the cam is pointing to `-z`, maybe a row matrix vs column matrix thing again ?" CreationDate="2016-06-02T06:07:12.227" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3252" PostId="2549" Score="0" Text="@BRabbit27 oh right, we can think of it this way: an identity camera matrix is the same as no view matrix. therefore we end up with the default basis of the API. In OpenGL we definitely see towards `-z` by default. In DirectX historically I've used LeftHanded systems I guess which made me reverse z." CreationDate="2016-06-02T06:09:42.617" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3253" PostId="2549" Score="0" Text="In [Scratchapixel](http://www.scratchapixel.com/index.php) the authors use row-major matrix and right-hand system. So actually when you say &quot;camera looking along&quot; it means the direction the lens points to? Maybe a mistake in the image? I think, if they say &quot;looking along -z&quot;, the direction vector is as you just said `lookat - campos` ?" CreationDate="2016-06-02T06:14:16.007" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3254" PostId="2549" Score="0" Text="@BRabbit27 Yes that what it means, I think there is no mistake in their image. That just happens to be configurable by a `-1` in the projection matrix later on. So any image would still be right." CreationDate="2016-06-02T06:16:24.417" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3255" PostId="2549" Score="0" Text="Ok, now I think I am getting somewhere. So if I choose to have the direction as `camPos-lookat` to agree with the image, that's why the projection matrix has the `-1` in the (2,3) entry to reverse this, right?" CreationDate="2016-06-02T06:18:28.343" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3256" PostId="2549" Score="0" Text="The other thing I still don't get, is how can I convince myself that the view-matrix = camera-to-world? What kind of exercise can I do in order to visualize it?" CreationDate="2016-06-02T06:22:52.727" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3257" PostId="2549" Score="1" Text="@BRabbit27 it's world-to-camera because &quot;the view matrix changes space from world points to view points&quot;. If a point is far away, but your camera is also far away (close to the point) passing the point into the view matrix will make it close to zero. that is: a point visible in your view space. Another way to see it, is use the fact &quot;view matrix = inverse of world matrix for the camera&quot;. The wold matrix for the camera would be the matrix to move it from zero in world, to its place in world (away from origin)." CreationDate="2016-06-02T06:28:26.767" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3258" PostId="2516" Score="1" Text="General graph layout algorithms are anoying to program and implement. Even just finding proper ones from literature is pain. You should describe a bit more what you expect, draw a picture." CreationDate="2016-06-02T07:57:35.683" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3259" PostId="2551" Score="3" Text="&quot;fails to compile&quot; is kinda useless without the error messages, check the `shaderInfoLog` and/or the `programInfoLog` after attempting the compile and link." CreationDate="2016-06-02T08:42:53.983" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3260" PostId="2516" Score="0" Text="@DanielMGessel It is an electricity network and I have the real world map(coordinates) in hand, The graph is planar and maybe the edges cross each other, so its not a problem if the orthogonal edges cross each other, because in some case I believe there is no way to draw the graph without overlapping edges." CreationDate="2016-06-02T09:01:18.467" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="3261" PostId="2516" Score="0" Text="@joojaa I have GIS data of an electricity network and just like any electricity computation software(cyme, digsilent, pti,...) I just need to draw an arc-node structure in an orthogonal manner. As I described above for Daniel. I have extracted the hypernodes and their connections so the ortho graph could be drawn from sketch using these data. I will attach some picture to the question to increase the clearity, thanks for suggesting." CreationDate="2016-06-02T09:10:01.800" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="3262" PostId="2552" Score="3" Text="More annoyingly, this fails on only some drivers/cards, which makes it hard to debug in the wild. As best practice, never rely on implicit casts to float in GLSL, always specify the decimal dot." CreationDate="2016-06-02T12:33:37.630" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="3263" PostId="1978" Score="1" Text="The model-matrix is indeed a model-to-world matrix. If you can call the camera-matrix or often view-matrix a &quot;camera-to-world&quot; or &quot;world-to-camera&quot; matrix depends on how you build it i think. I aways use a approach of &quot;world-to-camera&quot; in which the matrix is composed with the coordinate system and the _negative_ position of the camera. Then multiplying by this takes a world coordinate into camera/view space. http://www.opengl-tutorial.org/beginners-tutorials/tutorial-3-matrices/ explains this in a much more detailed fashion and in the way I use the terms and matrices." CreationDate="2016-06-02T15:55:16.567" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3264" PostId="2542" Score="1" Text="@trichoplax Thanks! I did it, in order to better clarify that particular detail" CreationDate="2016-06-03T00:20:28.447" UserId="3410" ContentLicense="CC BY-SA 3.0" />
  <row Id="3265" PostId="2533" Score="0" Text="I am going to take a look at QT, as that uses c++ which I know best, and seems like a good multi platform utility." CreationDate="2016-06-03T03:48:22.027" UserId="3459" ContentLicense="CC BY-SA 3.0" />
  <row Id="3266" PostId="2549" Score="0" Text="To complement the answer, check [this video](https://www.youtube.com/watch?v=mpTl003EXCY&amp;list=PL_w_qWAQZtAZhtzPI5pkAtcUVgmzdAP8g&amp;index=5)" CreationDate="2016-06-03T04:45:42.690" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3267" PostId="2556" Score="3" Text="This is actually a very interesting question. I really hope this isn't considered off topic here." CreationDate="2016-06-03T08:25:39.840" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3268" PostId="2516" Score="1" Text="The vertexes already have a given position in 2D (or on a sphere), so it's more of an edge routing problem? The trivial solution is to route each edge first along the x, then along the y axis (or the other way 'round). If this is right, explaining the problems with this trivial solution might be helpful. I imagine a minimization problem by assigning penalties for the problems..." CreationDate="2016-06-03T16:54:26.387" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="3269" PostId="2559" Score="0" Text="Thanks for your answer! Yes you are right, and I know them: they were introduced with the Compute Shaders and indeed they solve part of the problem because they allow saving values from one shader pass to the next. However, I don't think they solve the main problems I described: the array-of-arrays or array-of-lists issue" CreationDate="2016-06-03T20:05:16.150" UserId="3410" ContentLicense="CC BY-SA 3.0" />
  <row Id="3270" PostId="2562" Score="2" Text="The things that stand out to me about the model are its symmetrical features. Not all of these are mismatched on all people, but most people have *some* asymmetry. This model has ears positioned at the same height. The wrinkles in general are symmetrical, particularly the frown lines on the left and right of the bridge of the nose. In addition to these permanent features, the expression is also very symmetrical. None of these things are impossible on a human face, but they invite closer inspection, making it more difficult to miss other subtle clues that the face is artificial." CreationDate="2016-06-04T00:38:34.417" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3271" PostId="2562" Score="2" Text="The reason why i would not want to go there is that i know that the fact that it IS artificial makes you more sceptical and start to see things that may or may not be critical. Yes i agree its a archetypical character. But again since its not a blind test i would refrain from few of the points." CreationDate="2016-06-04T06:00:12.370" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3273" PostId="2516" Score="0" Text="@DanielMGessel I have added some pictures and explanation, hope it clear the ambiguities." CreationDate="2016-06-04T21:41:29.730" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="3274" PostId="2516" Score="0" Text="@joojaa Please take a look at the added picture.Thanks" CreationDate="2016-06-04T21:42:15.963" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="3275" PostId="2558" Score="0" Text="Thank you, Dragonseel! I actually suspected that all went down to collision detection. Do you think AABB is enough?" CreationDate="2016-06-05T15:47:59.580" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3276" PostId="2558" Score="1" Text="A General BB should be fine but the axis alignment of the boxes prohibits you from calculation correct rotation of the vehicles. For the easier just up-down movment you could maybe make them work. But I think for the more general solution you need rotating bounding volumes." CreationDate="2016-06-05T23:42:31.653" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3277" PostId="2559" Score="0" Text="Doh you have the same number of floats for all pixel? I mean just in a single frame the number could change between frames if necessary. If you have you could unpack the 2D data structure into a 1D array by index shifting. This could then be written as a buffer. If the amount of data varies you could write some raw data Ald just bytes and then an additional index structure which pixel has what amount of data... That would be just one not per pixel overhead." CreationDate="2016-06-05T23:46:16.750" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3278" PostId="2554" Score="0" Text="I can't test this until I get home so can't fix your main problem right now, but there's a fair bit of unnecessary maths in your shader :" CreationDate="2016-06-06T06:15:01.990" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="3279" PostId="2554" Score="2" Text="the W divide of vertexPositionHomogenous is pointless as only the projection matrix alters W ; you're normalizing lightDirection twice, once in the declaration and once in the calculation of cosTheta ; and the halfway vector calculation is cheaper as (lightDirection + viewDirection) * 0.5 .... could you post the fragment shader you're using with this vertex shader as well? Because other than that the maths look right." CreationDate="2016-06-06T06:27:01.367" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="3280" PostId="2519" Score="0" Text="Yes, that was the problem. But my main question was: What IS the half vector used for, since it appears in the function definition. As far as I understand now it is only used in the check if H dot V is positive. Thank you for taking time to write the answers." CreationDate="2016-06-06T08:00:54.563" UserId="3424" ContentLicense="CC BY-SA 3.0" />
  <row Id="3284" PostId="2519" Score="0" Text="That is my understanding as well. Glad to help." CreationDate="2016-06-06T09:28:04.990" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3288" PostId="2556" Score="0" Text="An oft-overlooked feature is how well (or poorly) the scene data was mapped to the display. It's common for high-dynamic-range scene data to be arbitrarily &quot;squashed&quot; into an sRGB range without careful mapping. This can create a lot of VERY subtle problems that cue the brain to be skeptical. A great (generic) discussion was started here: http://blender.stackexchange.com/questions/46825/render-with-a-wider-dynamic-range-in-cycles-to-produce-photorealistic-looking-im" CreationDate="2016-06-06T19:59:42.570" UserId="4494" ContentLicense="CC BY-SA 3.0" />
  <row Id="3289" PostId="3569" Score="0" Text="This looks like a homework problem. Is this a homework problem?" CreationDate="2016-06-06T20:00:49.890" UserId="4494" ContentLicense="CC BY-SA 3.0" />
  <row Id="3290" PostId="3569" Score="2" Text="@mHurley Question from an exam paper" CreationDate="2016-06-06T20:20:10.177" UserId="1971" ContentLicense="CC BY-SA 3.0" />
  <row Id="3291" PostId="3569" Score="0" Text="What exactly don't you understand? You have n and you have VUP. And you have the formula for u. Just plug in the values. The x stands for the cross product which you can lookup if that makes you trouble. And the | | means length. / should be clear as division." CreationDate="2016-06-07T00:29:30.770" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3292" PostId="3569" Score="0" Text="Should this be on Math.stackexchange ?" CreationDate="2016-06-07T03:36:12.037" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3293" PostId="3572" Score="0" Text="Thanks, i thought i need a translation matrix but i didn't know that i messed the order of operation." CreationDate="2016-06-07T06:32:58.537" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3294" PostId="2558" Score="0" Text="The collision check has to be done with all objects in the scene, correct? Or at least, the ones within a certain distance.&#xA;I wonder if calculating the vector between the center of the car and the center of the obstacle and adding it to the car's speed would suffice." CreationDate="2016-06-07T08:33:52.040" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3295" PostId="3569" Score="2" Text="@ritwiksinha questions can be on topic on more than one site." CreationDate="2016-06-07T08:47:23.357" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3296" PostId="2558" Score="1" Text="The collision would have to be checked against all nearby objects that you want to be able to colide with. In general using that distance would not work. Think of a long object. It's center of mass would be far away from the object and the force you add would be much too large. What is physically correct to use is the penetration depth aka how deep is the object inside the obstacle (using the maximum depth). But I cannot say that in your special case it is impossible to find some plausible simplification. I just don't know enough about your project and goals." CreationDate="2016-06-07T08:53:46.657" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3297" PostId="2538" Score="0" Text="Could you explain what &quot;tool-path&quot; and &quot;scanSpacingStep&quot; mean in this context?" CreationDate="2016-06-07T09:03:19.353" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3298" PostId="2538" Score="0" Text="The tool-path is basically the path that follows a series of coordinate points over a grid cell. And the scanSpacingStep is the distance between two adjacent lines. Check the link http://imgur.com/NTtPYoB The fractal path in the image could be a tool path that is defined by series of coordinates and the in-between distance of each and every fractal line segments is the scanSpacingStep." CreationDate="2016-06-07T09:09:47.980" UserId="2712" ContentLicense="CC BY-SA 3.0" />
  <row Id="3300" PostId="3570" Score="0" Text="How would you sample distances for non-monochromatic scattering/absorption coefficients? Randomly choose a channel, then divide by 1/3 (in the case of RGB or XYZ)?" CreationDate="2016-06-07T17:25:55.173" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3301" PostId="3570" Score="1" Text="@RichieSams The recommendation I've seen for that case is to assign each ray a single wavelength or color channel. So you basically calculate the scattering for each channel separately. For instance, in atmospheric scattering, blue light scatters much more strongly than red and therefore needs a lot more scattering events, and the blue photons will follow much more convoluted paths than the red ones. So it makes some sense to simulate them separately—much like dispersion due to refraction. I've never really tried this myself though." CreationDate="2016-06-07T17:46:09.757" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3302" PostId="3570" Score="0" Text="Ahh, that makes sense. Though, performance will suffer... No wonder everyone wants to estimate Monte-Carlo Participating media. Thanks for all the info!" CreationDate="2016-06-07T17:51:53.400" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3303" PostId="2568" Score="0" Text="As to your last comment - it's not a requirement of course for CG to look like a photo, but the goal of a lot of animation is to look as life-like as possible. Maybe I picked a less-than-perfect example, but I've always been interested in how real-time rendering could be made more realistic, especially in games." CreationDate="2016-06-07T18:29:52.393" UserId="3477" ContentLicense="CC BY-SA 3.0" />
  <row Id="3304" PostId="2568" Score="0" Text="Of course, in this case I was talking about the information provided in the question, so I was discussing a still image. For extra realistic look, I don't know what software you're using, but if it's Blender, there's a new LUT that can be replaced (not official yet) which makes Cycles rendering very close to real life filming quality, concerning material's reaction to light, lighting is a major aspect, specially when discussing photo real." CreationDate="2016-06-07T18:36:48.823" UserId="3487" ContentLicense="CC BY-SA 3.0" />
  <row Id="3305" PostId="2568" Score="0" Text="So do you think it's often the case that while something is almost realistic, the CG is &quot;designed&quot; (if you will) with the idea that it shouldn't be picture-perfect?" CreationDate="2016-06-07T21:09:22.370" UserId="3477" ContentLicense="CC BY-SA 3.0" />
  <row Id="3306" PostId="2568" Score="0" Text="I wouldn't use &quot;Should not&quot; or &quot;must not&quot; with anything related to an artistic matter, there is no should nor must in art, everything is subjective, what I meant is that (as I believe) a glance of non realism, even in realistic artworks, keeps a small window open to the personality of the artist, compared to stylized artworks which for this example would be no roof at all. It all depends on were you like to stand." CreationDate="2016-06-07T21:43:00.577" UserId="3487" ContentLicense="CC BY-SA 3.0" />
  <row Id="3307" PostId="3575" Score="1" Text="If you can do something in one API, you can do it any other API, just the way of doing it will depend on the API." CreationDate="2016-06-07T21:46:02.013" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3309" PostId="3577" Score="0" Text="I would bet that if OSX would ever support Vulkan it will support only a small subset of it, and that would also become the Next Generation graphics api for web browsers, OpenGL is still ways simpler to use (to a certain degree of course) than Vulkan, what vulkan gain in simplicity over rendering pipeline it lose it in explicit handling of much other stuff" CreationDate="2016-06-08T08:04:14.783" UserId="4503" ContentLicense="CC BY-SA 3.0" />
  <row Id="3310" PostId="3577" Score="1" Text="@DarioOO OpenGL immediate mode is way simpler to use than whatever-you-call-the-thing-that's-not-immediate-mode, yet it's not recommended." CreationDate="2016-06-08T08:56:08.637" UserId="2316" ContentLicense="CC BY-SA 3.0" />
  <row Id="3312" PostId="3580" Score="0" Text="Excellent question! I've been wondering about this myself. It's not that the checkerboard pattern has a yellow tint... it's the grays that are incorrect. When I compare what I see on my laptop to my phone, where the checkerboard pattern seems to have roughly the same color as the grays, the gradient on my laptop appears distinctly more blue. Perhaps a better question would be to ask why do shades of gray look blue on most LCD computer screens." CreationDate="2016-06-08T14:11:16.993" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="3313" PostId="3580" Score="0" Text="@Quinchilion Interesting perspective. I'm so used to how gray looks on my screen that I might not notice if there's some blue to it, though it looks really metallic to me. It might be relevant that blue is the complementary color of yellow." CreationDate="2016-06-08T14:28:41.600" UserId="4490" ContentLicense="CC BY-SA 3.0" />
  <row Id="3314" PostId="3576" Score="2" Text="Good answer, reminded me of this: http://www.elise.com/quotes/heinlein_-_specialization_is_for_insects" CreationDate="2016-06-08T16:52:32.307" UserId="3473" ContentLicense="CC BY-SA 3.0" />
  <row Id="3316" PostId="3586" Score="2" Text="I'm not aware of any distinction between &quot;GPU instancing&quot; and &quot;standard instancing&quot;. As far as I'm aware, there's only one kind of instancing and it is a GPU feature. Whatever this is, it's probably something specific to Unity (and perhaps a term made up by Unity marketing), and you might have to track down a Unity engineer to get an explanation of what the new feature really is." CreationDate="2016-06-09T07:50:32.520" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3317" PostId="3586" Score="0" Text="By 'standard instancing', could they be referring to just having one copy of the mesh in system memory? That sounds rather elementary to any game engine. It also seems odd that they didn't have GPU instancing until now." CreationDate="2016-06-09T07:55:16.000" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="3318" PostId="3581" Score="0" Text="Vulkan is stable right now. Your GL 1.0 *vs* 2.0 analogy is apt. Starting with GL now would be like starting learning GL 1.0 six months after GL 2.0 comes out." CreationDate="2016-06-09T08:15:01.433" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3319" PostId="3585" Score="1" Text="re `new`, it looks more like c#, not c++, though OP has not explicitly specified." CreationDate="2016-06-09T08:17:30.277" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="3320" PostId="3585" Score="0" Text="Actually looks more like Java." CreationDate="2016-06-09T08:27:45.267" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="3321" PostId="3584" Score="0" Text="Are you writing a rasterizer in Java?" CreationDate="2016-06-09T08:48:18.887" UserId="3331" ContentLicense="CC BY-SA 3.0" />
  <row Id="3322" PostId="3585" Score="0" Text="@Rotem I don't think there is any way of knowing the language except when i tell that it is Java." CreationDate="2016-06-09T09:06:21.463" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3323" PostId="3584" Score="0" Text="@Syntac_ Yes the language is Java." CreationDate="2016-06-09T09:06:54.213" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3324" PostId="3585" Score="0" Text="@ritwik In c# the `Math` methods are capitalized, e.g. `Round()`, not `round()`." CreationDate="2016-06-09T09:08:49.103" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="3325" PostId="3585" Score="0" Text="@Rotem ok that's legit." CreationDate="2016-06-09T09:09:53.553" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3326" PostId="3585" Score="0" Text="@Nathan reed i used the good old school maths first but not the way that you suggested. Thanks i will try your method." CreationDate="2016-06-09T09:12:20.440" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3327" PostId="3580" Score="1" Text="Monitors are frequently very poorly calibrated. That's the most likely cause of issues with images not looking grey.&#xA;&#xA;Further, assuming the display is meant to be sRGB, then (and this is from memory so please take with a grain of salt) the RGB values that should correspond to a B&amp;W chequer board are about 186.&#xA;&#xA;What you may find helpful is to repeat your experiment separately with each of R,G &amp; B using a Primary/Black pattern VS Black-&gt;Primary blend. The crossover points are ideally meant to be at the same position but frequently aren't." CreationDate="2016-06-09T10:34:52.397" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="3328" PostId="3586" Score="0" Text="@Rotem Those were exactly my thoughts and the origin of my question. See here in their roadmap what they say about it: https://unity3d.com/pt/unity/beta/unity5.4.0b1 And yes, the only thing I can think of is that their &quot;non-GPU&quot; instancing is a copy of mesh in RAM, which **is** quite a ppor way of doing that. As per the link I sent, it seems that now they are firstly introducing the possibility of instancing using shaders, i.e. directly in the GPU" CreationDate="2016-06-09T12:22:30.517" UserId="2061" ContentLicense="CC BY-SA 3.0" />
  <row Id="3329" PostId="2567" Score="1" Text="What's an orthographic cuboid? &quot;Orthographic&quot; is a property of the projection, it's not a shape. Perhaps you could explain what effect you're hoping this projection will have." CreationDate="2016-06-09T13:04:08.070" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3330" PostId="3588" Score="0" Text="If you coded the calculation wouldn't it be easier to query the data before it gets turned into colours?" CreationDate="2016-06-09T13:14:31.240" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3331" PostId="3576" Score="8" Text="This C++ analogy is inappropriate.  Vulkan is a new next generation API developed by the creators of OpenGL.  C++ is an established, mostly backwards-compatible competitor to C." CreationDate="2016-06-09T13:33:16.703" UserId="4531" ContentLicense="CC BY-SA 3.0" />
  <row Id="3332" PostId="3588" Score="0" Text="I did not code it it is not an open source component I can not acces to surce code of 3d sun exposure component. But I want to convert result to data. Is it possible? Are there any component for this purpose? Or Do I have to code myself all workflows of analysis to achieve this?" CreationDate="2016-06-09T13:36:26.720" UserId="4513" ContentLicense="CC BY-SA 3.0" />
  <row Id="3333" PostId="3576" Score="0" Text="Those specifics are irrelevant to the point of the analogy." CreationDate="2016-06-09T13:59:02.013" UserId="4494" ContentLicense="CC BY-SA 3.0" />
  <row Id="3334" PostId="3583" Score="0" Text="&quot;*Another problem I have is with Vulkan's claims is how it claims to be better.*&quot; Vulkan does not claim to be anything. It's just an API; it cannot make claims." CreationDate="2016-06-09T15:18:32.767" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3335" PostId="3587" Score="0" Text="Please include a reference image with your question. Googling 'logic studio environment editor' for images gives me nothing useful." CreationDate="2016-06-09T15:20:50.717" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="3336" PostId="3581" Score="1" Text="Vulkan might be &quot;stable&quot; it doesn't mean things wont change, this is the nature of languages, which I presented 2 recent changes to languages within the last few years, so how do we know this wouldn't happen to Vulkan? You're basically saying that learning OpenGL is a waste, and that's false.  Making it seem like OpenGL is dead, and wont be used any more... Also false.  Besides, I'm not the only one who believes that Vulkan could have stability issues.  It might not, but with any new language, that's what you look for." CreationDate="2016-06-09T16:07:02.913" UserId="3420" ContentLicense="CC BY-SA 3.0" />
  <row Id="3337" PostId="3581" Score="1" Text="Of course things will change, and the working group is already working towards new features for a next spec version (ssh, you didn't hear that from me). *Stable* means that those things will add to the spec, and the things you learn about it today will still be valid two years from now. OTOH, the things you learn about GL today are already a poor match for how GPUs work: just like learning about fixed-function pipelines in a programmable-shader world. If you read my answer, you see I don't think learning GL now is a waste. But &quot;Vulkan is too new&quot; is not a good reason to discount it." CreationDate="2016-06-09T16:13:32.113" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3338" PostId="3581" Score="0" Text="Libraries change with addons, but I'm talking about architecture changes, which is what happened in FX, Play!, OpenGL, and others.  How can we say in 2 years it will be the same?   The Fixed-Pipeline comment is what the issue with GL 1-2 was, so yes, why learn it if soon it could change, and we wouldn't even know?   Vulkan is too new isn't the full reason.  It's that it's subject to change, not many companies will adapt to using it right away, and how many people really want to change from OpenGL to Vulkan who are hardcore GL coders?  I think you should look at Vulkan though." CreationDate="2016-06-09T16:18:43.730" UserId="3420" ContentLicense="CC BY-SA 3.0" />
  <row Id="3339" PostId="3590" Score="1" Text="Yup same exact result, i even went as far as calibrate the one not calibrated showing hue and poof gone." CreationDate="2016-06-09T16:30:50.033" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3341" PostId="3581" Score="1" Text="@Lasagna: &quot;*It's that it's subject to change, not many companies will adapt to using it right away*&quot; Valve. Epic. Unity. All of them are heavily invested in making their engines work on Vulkan. CryTech and Id aren't exactly ignoring it either. So your statement is not commensurate with the facts." CreationDate="2016-06-09T17:37:22.600" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3342" PostId="3577" Score="1" Text="@Gavin: It should be noted that OpenGL versions 4.2 or greater aren't supported on OSX either. So if you want to use anything *recent* from OpenGL on OSX, you can't. And Apple is highly unlikely to adopt later OpenGL versions. The Apple platform is all-in on Metal, so cross-platform is a bust either way." CreationDate="2016-06-09T17:40:28.680" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3343" PostId="3576" Score="6" Text="Claiming specialization is inefficient or unmarketable is incredibly naive. A translator that knows all of five words in every single spoken language is useless, people will hire translators that mastered (a.k.a specialized in) a small number of languages. Now *over*-specializing *is* problematic. A translator that completely mastered one language and only knows that language is also not a useful translator. And devs (indie or otherwise) need to be careful not to spend all their time learning new tools. Eventually they need to actually make something to sell, lest they find themselves bankrupt" CreationDate="2016-06-09T18:32:26.773" UserId="4535" ContentLicense="CC BY-SA 3.0" />
  <row Id="3344" PostId="3576" Score="0" Text="Just to be clear, I don't necessarily disagree with you in regards to OpenGL and Vulkan, This quite possibly is a case where learning both instead of just Vulkan is the better choice." CreationDate="2016-06-09T18:34:30.713" UserId="4535" ContentLicense="CC BY-SA 3.0" />
  <row Id="3345" PostId="3576" Score="0" Text="I'm pretty sure that's what I implied by using the examples I did ;-) maybe not..." CreationDate="2016-06-09T18:38:41.940" UserId="4494" ContentLicense="CC BY-SA 3.0" />
  <row Id="3346" PostId="3581" Score="0" Text="Just because it's being adapted into 3D Engines, doesn't mean that companies are investing their projects into it, especially projects that are mid-way.  You might get a brand new project in Vulkan, but again, are you willing to hire new employees, train new ones, and be the one to test out a new platform?  I've done testing on new platforms, and it's a PITA sometimes working around bugs, reporting bugs, etc, when you're trying to just put out a product." CreationDate="2016-06-09T19:38:40.367" UserId="3420" ContentLicense="CC BY-SA 3.0" />
  <row Id="3347" PostId="3586" Score="1" Text="If the mesh moves and needs to be drawn in many places then one could instantiate it by reusing the draw calls and not redo mesh deformations. Used to be done in ages past ages." CreationDate="2016-06-09T20:23:37.800" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3348" PostId="3581" Score="2" Text="@Lasagna: &quot;*Just because it's being adapted into 3D Engines, doesn't mean that companies are investing their projects into it*&quot; ... So 3D engines don't qualify as &quot;projects&quot;? Does DOTA2 count as a &quot;project&quot;? Because it's got support for Vulkan *right now*. Does Samsung's line of smartphones and tablets count as a &quot;project&quot;? Because they're looking at incorporating it into the UI. The reality is that *lots* of people are willing to &quot;be the one to test out a new platform&quot;, if that platform gets them what they need." CreationDate="2016-06-09T22:30:15.220" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3349" PostId="3587" Score="0" Text="https://macprovid.vo.llnwd.net/o43/hub/media/1074/8250/Arp_Env_Pic.png" CreationDate="2016-06-10T02:13:57.483" UserId="4528" ContentLicense="CC BY-SA 3.0" />
  <row Id="3350" PostId="3587" Score="0" Text="http://media.soundonsound.com/sos/jan03/images/staudiomedia712.l.gif" CreationDate="2016-06-10T02:16:06.510" UserId="4528" ContentLicense="CC BY-SA 3.0" />
  <row Id="3351" PostId="3587" Score="0" Text="http://www.pcrecording.com/extlinkexp.jpg" CreationDate="2016-06-10T02:16:39.340" UserId="4528" ContentLicense="CC BY-SA 3.0" />
  <row Id="3352" PostId="3587" Score="0" Text="http://musicplayers.com/reviews/recording/2015/images/Eventide_patchbay.jpg" CreationDate="2016-06-10T02:16:58.610" UserId="4528" ContentLicense="CC BY-SA 3.0" />
  <row Id="3353" PostId="3587" Score="0" Text="http://i40.tinypic.com/2dhwqp1.jpg" CreationDate="2016-06-10T02:17:44.490" UserId="4528" ContentLicense="CC BY-SA 3.0" />
  <row Id="3355" PostId="3579" Score="2" Text="I somewhat disagree. Vulkan (and DX12)has proven to be very difficult to implement, for *experienced* devs. With all the power that Vulkan gives you, it's very very easy to shoot yourself in the foot. In a addition, Vulkan requires a lot more boilerplate code. For someone just learning about GPU programming, I tend to think that Vulkan will be very overwhelming." CreationDate="2016-06-10T12:09:34.540" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3356" PostId="3593" Score="0" Text="Is there any reason why you use the old style openGL" CreationDate="2016-06-10T14:06:58.027" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3357" PostId="3595" Score="0" Text="Looks like simple trigonometry to me, if you consider that the two triangles are mirror images of each other. The triangle (v1, v2, middle) is isosceles and you know both the length of v1-v2 and the angle defined by (v1, middle, v2), so you should be able to calculate both v1-middle and alpha." CreationDate="2016-06-10T14:45:32.137" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="3358" PostId="3595" Score="0" Text="I don't know where middle is, though. I only have the vertices of the cube and a degree by which I want it bend." CreationDate="2016-06-10T15:00:36.320" UserId="4543" ContentLicense="CC BY-SA 3.0" />
  <row Id="3359" PostId="3588" Score="0" Text="I have closed the [older question](http://computergraphics.stackexchange.com/questions/3582/reporting-results-of-3d-sun-exposure-analysis) as a duplicate of this one, since this one is more specific about what is required." CreationDate="2016-06-10T15:17:06.007" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3360" PostId="3595" Score="0" Text="If you know where v1 and v2 are, you can calculate where 'middle' is from the requested angle. If you don't, can you clarify the problem? I see no cube in the picture." CreationDate="2016-06-10T15:45:36.743" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="3361" PostId="3595" Score="0" Text="http://prntscr.com/ber3a7 that is input and result (with a 45° angle). Those are Screenshots from Cinema4D's bending tool. As you can see the 45° is the angle of the topmost edge of the cube. The bending process strectches the sides but not the 'spine' of the object. I'm trying to emulate the tool in code with a given Mesh." CreationDate="2016-06-10T15:55:16.443" UserId="4543" ContentLicense="CC BY-SA 3.0" />
  <row Id="3362" PostId="3579" Score="2" Text="@RichieSams I don't think you meant &quot;implement&quot;. Only the GPU vendor has to implement Vulkan, and it's a lot easier than implementing OpenGL. (Believe me, I've done it!) But assuming you meant it's difficult to integrate with or program against, I've added a paragraph with some information I learned from the Vulkan WG." CreationDate="2016-06-10T16:01:46.880" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3363" PostId="3579" Score="0" Text="Correct. Implement is perhaps a poor word choice. I meant 'to use' in your program. But I like your edits. Well put" CreationDate="2016-06-10T16:06:08.887" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3364" PostId="3589" Score="2" Text="I'm glad you posted this, because it makes very clear some ideas that I was hesitating to express in my answer: that learning the concepts is most important. I think where we differ is that you encourage GL because it's easier to get started, while I think that ease of getting started brings a risk of doing things in legacy ways. It's quite hard to know in GL what the &quot;modern GL idiom&quot; is, compared with programming in a legacy style. If nobody tells you to use VAOs instead of a separate draw call per primitive, it's a lot harder to unlearn that mistake later." CreationDate="2016-06-10T16:08:50.723" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3365" PostId="3589" Score="1" Text="@DanHulme: It's all a matter of using the right learning materials. There are plenty of such materials online that use modern idioms. Nobody should ever try learning graphics just by reading a reference manual or specification." CreationDate="2016-06-10T16:28:17.220" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3366" PostId="3597" Score="0" Text="how can I entegrate it to my software. I am thinking to use unity3d as a developing platform. Can I entegrate Nvidia Optix to it?" CreationDate="2016-06-10T17:16:25.420" UserId="4513" ContentLicense="CC BY-SA 3.0" />
  <row Id="3367" PostId="3597" Score="0" Text="Depends. OptiX has a C/C++ API so if you use Unity with Cpp you should be fine. Just install CUDA and link against the OptiX library. The tricky part will be to get the Nvcc to do its thing. I use CMake for that but I don't know about Unity + CMake. Then just ist unity to visualize the output of the OptiX simulation." CreationDate="2016-06-10T17:20:06.220" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3370" PostId="3580" Score="0" Text="@SimonF 186 corresponds roughly to the middle of the bottom gradient, and that place also seems to match the intensity of the top rectangle." CreationDate="2016-06-10T23:30:52.850" UserId="4490" ContentLicense="CC BY-SA 3.0" />
  <row Id="3371" PostId="3583" Score="0" Text="You do realise that Vulkan and GL are made by mostly the same people?" CreationDate="2016-06-11T00:03:25.313" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3372" PostId="3583" Score="0" Text="Yes I do. I still prefer GL" CreationDate="2016-06-11T00:04:18.247" UserId="4522" ContentLicense="CC BY-SA 3.0" />
  <row Id="3373" PostId="3589" Score="0" Text="You make it sound really easy! Even so, I still see programmers getting started in graphics - some of them very competent in other fields - and using legacy features and learning nothing about the performance characteristics. It's really hard to get that wrong in Vulkan, because it makes expensive things *look* expensive. Anyway, we don't need to argue. I agree with your main point: learning the concepts is most important, and you can do that without Vulkan or GL." CreationDate="2016-06-11T00:08:47.103" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3374" PostId="3589" Score="0" Text="@DanHulme: &quot;*It's really hard to get that wrong in Vulkan*&quot; Because you're too busy getting *everything else* wrong ;) Also, it's still pretty easy to get performance wrong in Vulkan. Unnecessary synchonization. Using only the &quot;general&quot; image layout. Not taking advantage of subpasses. Frequent pipeline changes. And so forth. Not to mention, different vendors don't even agree on how best to use Vulkan." CreationDate="2016-06-11T00:23:03.147" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3375" PostId="3589" Score="3" Text="@DanHulme: As for how easy it is to use modern OpenGL, I [do my best](https://www.opengl.org/wiki/Main_Page) to [make it easy](http://alfonse.bitbucket.org/oldtut/). If people insist on learning from garbage sites like NeHe, or by reading random documentation, that's not something anyone can help. You can lead horses to water, but you can't make them drink." CreationDate="2016-06-11T00:25:05.303" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3376" PostId="2347" Score="1" Text="Right is not defined, if it works it works" CreationDate="2016-06-11T10:48:57.437" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3377" PostId="3593" Score="0" Text="It is a computer graphics project in OpenGL and SDL." CreationDate="2016-06-11T14:58:30.990" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3378" PostId="3599" Score="0" Text="Thank you! I might as well try to reimplement some basic reader - but in the sample code that was given as example the calls didn't mess up things like this. I'm going to look at it again!" CreationDate="2016-06-11T15:00:05.277" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3379" PostId="3593" Score="0" Text="that's not a reason its a circular definition." CreationDate="2016-06-11T15:07:44.933" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3380" PostId="3599" Score="0" Text="I see you're using the SDL surface's pixel buffer directly. You could get the debug version of the SDL library and the source, and inspect what's happening inside SDL. The core of the problem is that OpenGL expects the pixels to be laid out in memory differently than what is actually in the buffer." CreationDate="2016-06-11T16:29:33.877" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="3381" PostId="3602" Score="1" Text="Could you also explain how this differs from standard instancing?" CreationDate="2016-06-11T16:55:58.717" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3382" PostId="3593" Score="0" Text="Yeah, you're right, I didn't explain, sorry. I'm doing this for a computer graphics class which mainly focuses on OpenGL, SDL and many basic concepts of cg." CreationDate="2016-06-12T20:19:48.513" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3383" PostId="3593" Score="1" Text="Yes but why are you using obsolete and deprecated openGL calls?" CreationDate="2016-06-12T21:57:10.490" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3384" PostId="3606" Score="0" Text="Alright, thank you for the hint to gl_vertextID. That's what I looked for. Regarding the number of blendshapes: I am writing a renderer for facial animation research and in this context the possibility to use more than 10 shapes should at least exist. Another possibility I just stumbled upon is the use of transform feedback to process blocks of shapes iteratively. Do you have a take on that? Again, thank you!" CreationDate="2016-06-13T09:52:01.777" UserId="4556" ContentLicense="CC BY-SA 3.0" />
  <row Id="3385" PostId="3608" Score="3" Text="[Same question on Math.SE.](http://math.stackexchange.com/q/184121/50421)" CreationDate="2016-06-14T07:00:06.440" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="3386" PostId="3593" Score="0" Text="Because the class taught this &quot;dialect&quot;, mainly... Are there better ways to use OpenGL?" CreationDate="2016-06-14T07:51:27.473" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3387" PostId="3593" Score="1" Text="Yes  that's what [deprecated means](http://stackoverflow.com/questions/8111774/deprecated-meaning), any new code should **not** use this method if possible (here its possible). Read its not very performant [why-do-vertex-buffer-objects-improve-performance](http://computergraphics.stackexchange.com/questions/32/why-do-vertex-buffer-objects-improve-performance). Nobody should be teaching you this stuff anymore." CreationDate="2016-06-14T08:03:27.687" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3388" PostId="3593" Score="0" Text="also read http://stackoverflow.com/questions/14300569/opengl-glbegin-glend" CreationDate="2016-06-14T08:13:06.377" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3389" PostId="3612" Score="7" Text="Dividing normals by w is not useless, because it affects the interpolation. The values are weighed differently. Even if you normalize after the interpolation, it gives different results than if you didn't divide by w. Perspective correct interpolation for normals is the same as for any other surface attribute." CreationDate="2016-06-14T17:48:19.440" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="3390" PostId="3613" Score="0" Text="Thanks, works great! I'm still not quite understand the purpose of the float. Is it possible to have the object's speed static?" CreationDate="2016-06-14T19:13:02.223" UserDisplayName="user4570" ContentLicense="CC BY-SA 3.0" />
  <row Id="3391" PostId="3613" Score="0" Text="yes, the part (X.x - Y.x) is the first component of the direction vector. If you normalize it, you could multiply it with your speed and delta time. Regarding a former question that you posted on stackexchange, you should gather more information on linear algebra. See [this](http://blog.wolfire.com/2009/07/linear-algebra-for-game-developers-part-1/) for example." CreationDate="2016-06-14T20:01:49.413" UserId="4558" ContentLicense="CC BY-SA 3.0" />
  <row Id="3393" PostId="3615" Score="0" Text="Most 3d apps can do this." CreationDate="2016-06-15T09:26:46.287" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3395" PostId="3620" Score="0" Text="Yeah i was just looking for that answer" CreationDate="2016-06-15T11:00:46.270" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3396" PostId="3617" Score="0" Text="Normals, gotcha." CreationDate="2016-06-15T12:33:07.117" UserId="361" ContentLicense="CC BY-SA 3.0" />
  <row Id="3397" PostId="3615" Score="0" Text="Could you add some details?If you could explain the general process in Blender (or any other app) that would be great." CreationDate="2016-06-15T13:28:24.457" UserId="4574" ContentLicense="CC BY-SA 3.0" />
  <row Id="3398" PostId="3620" Score="1" Text="Relevant passage from the Black Book: http://www.jagregory.com/abrash-black-book/#how-do-you-fit-polygons-together" CreationDate="2016-06-15T15:36:39.593" UserId="4582" ContentLicense="CC BY-SA 3.0" />
  <row Id="3399" PostId="3620" Score="0" Text="Very many thanks! It sorta kinda makes sense now, and that black book looks like a great resource too" CreationDate="2016-06-15T16:10:58.480" UserId="4577" ContentLicense="CC BY-SA 3.0" />
  <row Id="3401" PostId="3627" Score="0" Text="This is going to delve into the core (proprietary) design of the gpu. Also the ALU isn't the magic component (it only does integral math) but the FPU is." CreationDate="2016-06-15T20:39:33.493" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3402" PostId="3627" Score="1" Text="Im trying to build a simple gpu on an fpga that only joins multiple points in 3d space and diplays it on a screen. Im using fixed point operations only." CreationDate="2016-06-15T20:48:07.153" UserId="4583" ContentLicense="CC BY-SA 3.0" />
  <row Id="3404" PostId="416" Score="0" Text="I'm afraid resource requests are off topic. Ironically I only spotted this was off topic because of the recent edit... This could potentially be edited to ask specifically for details of the benefits of this data format, but after the time since posting and the existing answers, such a significant change is probably better posted as a brand new question. Feel free to pop into [chat] if you have any queries (or anyway...)." CreationDate="2016-06-15T22:02:58.727" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3405" PostId="3631" Score="0" Text="Will this work with perspective projection ?" CreationDate="2016-06-16T06:09:18.900" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3406" PostId="3615" Score="1" Text="@MichaelLitvin I think it would be off-topic if you ask &quot;How to do in a specific software&quot; on this site." CreationDate="2016-06-16T06:10:23.703" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3407" PostId="3631" Score="2" Text="This is a solution for a single image or frame, not the while video. To use this, the texture maps for many frames will need to be stitched somehow... This is the main problem here." CreationDate="2016-06-16T08:00:35.683" UserId="4574" ContentLicense="CC BY-SA 3.0" />
  <row Id="3408" PostId="3615" Score="0" Text="I'm not asking about just Blender, though a suggestion for how to do this in Blender would be nice." CreationDate="2016-06-16T08:01:23.600" UserId="4574" ContentLicense="CC BY-SA 3.0" />
  <row Id="3409" PostId="416" Score="1" Text="Fair enough. I was looking for a mininal working example anyways and solved that via [this thread](http://computergraphics.stackexchange.com/q/1934/361)." CreationDate="2016-06-16T09:53:49.033" UserId="361" ContentLicense="CC BY-SA 3.0" />
  <row Id="3411" PostId="2346" Score="1" Text="Can you tell how you are calling the draw function ? and what functionality does Vertex class perform. I remember i once experienced the same problem when i was overwriting two VBOs. Maybe you are doing the same, I can't tell without seeing how you are drawing" CreationDate="2016-06-16T12:22:41.590" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3412" PostId="3631" Score="1" Text="unknown @MichaelLitwin they dont need to be stitched, just masked. Its pretty easy all you do is you calculateuseful  how much the pixel area is facing the camera tha when the facing drops under a certain treshold mask it out. Then median filter the overlap to remove noise. Then either bake to 2D or just use the projection as is. Any projection works just as long as you can track the projector location in 3D" CreationDate="2016-06-16T13:08:57.653" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3413" PostId="405" Score="0" Text="What about using GeoGebra for graphs, 2D/3D geometric figures." CreationDate="2016-06-16T21:58:13.753" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3414" PostId="3599" Score="0" Text="I &quot;surprisingly&quot; solved by letting the code load a jpg rather than a png. I guess SDL likes MPEG codecs more than PNG." CreationDate="2016-06-17T11:55:14.220" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3416" PostId="2558" Score="0" Text="I actually solved by creating a list of points, checking continually the vehicle's position and direction with respect to the list and updating its speed and angles to match the expected result." CreationDate="2016-06-17T11:57:02.197" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3417" PostId="3638" Score="0" Text="`OES_element_index_uint` extension is available but it's not rendering the buffers. Strange." CreationDate="2016-06-17T13:00:36.523" UserId="361" ContentLicense="CC BY-SA 3.0" />
  <row Id="3418" PostId="3631" Score="0" Text="I don't understand - I'm using several images for texture. These images will meet somewhere, and edge lines won't necessarily meet at exactly the same place. How do I ensure they do?" CreationDate="2016-06-17T13:08:54.047" UserId="4574" ContentLicense="CC BY-SA 3.0" />
  <row Id="3419" PostId="3638" Score="0" Text="have you changed the index buffer to store ints?" CreationDate="2016-06-17T13:10:03.973" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3420" PostId="3640" Score="0" Text="Hi and thank you :) sorry, it is a typo: the cycle is correct (otherwise i would be out of scope). I'm adding the necessary braces." CreationDate="2016-06-17T13:29:43.373" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3427" PostId="3645" Score="8" Text="17 milliseconds sounds more like the synchronisation to screen refresh rate than a actual rendering speed since that would be 1/60 (or well 16.666ms) of a second. So most likely your render is faster than that is just that displaying is restricted to one cycle. You can disable screen sync or render to a offscreen buffer for a more representative rendertime." CreationDate="2016-06-19T07:57:28.473" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3429" PostId="3647" Score="0" Text="Nice information, thanks Julien. One more question: How could I use the depth or normal information in my fragment shader? I'm still quite new with GLSL and therefore have no idea how to include the values in the Sobel algorithm." CreationDate="2016-06-19T14:25:37.927" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3430" PostId="3646" Score="0" Text="I wonder if you might get the results you want by utilizing OpenGL's [edge flag](https://www.opengl.org/sdk/docs/man2/xhtml/glEdgeFlagPointer.xml) in some way. It looks like only edges between vertices with the edge flag on are drawn when in line mode. There's [a description here](http://www.songho.ca/opengl/gl_vertexarray.html). (I've not used it myself or I'd post an example.)" CreationDate="2016-06-19T21:44:11.327" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="3431" PostId="3645" Score="2" Text="sounds indeed like a vsync, turn it off and try again or render it several times to see when you get any slowdown." CreationDate="2016-06-19T22:48:34.840" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3432" PostId="3639" Score="0" Text="What's your `GL_TEXTURE_MATRIX` set to? Perhaps it has some scaling in it? (Also, what's the matrix mode for this code? You push the current matrix, but don't appear to be setting the mode before doing so.)" CreationDate="2016-06-19T23:54:04.283" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="3433" PostId="3647" Score="2" Text="When you create the framebuffer for your rendering pass, you can attach more than one texture to it. If you attach a depth texture, it will be used automatically for depth. It you attach another color texture you can write into it separately (see https://www.opengl.org/sdk/docs/man/html/glDrawBuffers.xhtml), a feature called &quot;Multi Render Target&quot; (MRT)." CreationDate="2016-06-20T00:12:16.130" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3434" PostId="3648" Score="0" Text="What was the original wording of what you read?" CreationDate="2016-06-20T01:27:20.107" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3435" PostId="3599" Score="1" Text="PNG supports alpha, JPEG doesn't. Have you tried uploading your PNG texture as GL_RGBA, not GL_RGB? Because you haven't solved your problem, you just sidestepped it." CreationDate="2016-06-20T07:26:51.490" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="3436" PostId="3638" Score="0" Text="Yeah, forgot about that. Now it works. Thanks." CreationDate="2016-06-20T14:14:09.760" UserId="361" ContentLicense="CC BY-SA 3.0" />
  <row Id="3437" PostId="3650" Score="0" Text="Tried that and didn't work. I think it's not the case of Direct3D geometry, because I'm on Ubuntu - isn't Direct3D a Microsoft exclusive?" CreationDate="2016-06-20T19:38:34.540" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3438" PostId="3639" Score="0" Text="Before the bind code:&#xA;`glMatrixMode(GL_PROJECTION ); glLoadIdentity(); gluPerspective(70, 3.0/4, 0.2, 1000 ); glMatrixMode(GL_MODELVIEW); glLoadIdentity();`&#xA;No scaling of sort (actually there was, but when you mentioned it I removed it and it didn't change the result by much)." CreationDate="2016-06-20T19:44:46.640" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3439" PostId="3631" Score="1" Text="@MichaelLitvin you dont need to you can simply let them overlap." CreationDate="2016-06-20T20:02:12.593" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3440" PostId="3655" Score="0" Text="How bilateral filtering specifically works is what I am trying to get information about. Can you elaborate on that?" CreationDate="2016-06-20T21:01:51.810" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="3441" PostId="3655" Score="0" Text="One can lend images for the explanations in the beginning from [here](http://computergraphics.stackexchange.com/questions/1862/how-does-a-computer-upscale-1024x768-resolution-to-1920x1080/1870#1870)" CreationDate="2016-06-20T21:10:43.230" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3443" PostId="98" Score="0" Text="you can get brutal shutdowns after the security threshold is passed. eletronics do weaken, condensers particularly. which diminishes power supply quality and have a variety of bad consequences. For ASICs electromigration is an issue, but usually after 30 years." CreationDate="2016-06-21T01:33:47.413" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3444" PostId="1755" Score="0" Text="From the caption text, it seems like Prusinkiewicz's own implementation." CreationDate="2016-06-21T02:37:09.173" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="3445" PostId="3647" Score="0" Text="@enne87: Happy to help. :)" CreationDate="2016-06-21T02:52:35.430" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3446" PostId="3655" Score="0" Text="@AlanWolfe I assumed you were asking after broader context, since implementing basic bilateral filtering is highly Google-able and rather simple. As I wrote, the basic idea is to make values factor into the weight, not just distance." CreationDate="2016-06-21T03:49:44.197" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="3447" PostId="3655" Score="0" Text="Also @joolaa's answer is quite nice and may be useful as a second explanation of my overview." CreationDate="2016-06-21T03:49:58.150" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="3449" PostId="3655" Score="1" Text="Now that I have the term &quot;bilateral filter&quot; I am having more luck on Google. Thanks for that." CreationDate="2016-06-21T03:53:37.333" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="3450" PostId="1755" Score="0" Text="[ref 152](https://team.inria.fr/virtualplants/files/2013/09/treesviennot.pdf), [ref 70](http://www.geos.ed.ac.uk/homes/s0451705/horton_1945.pdf)" CreationDate="2016-06-21T05:18:29.297" UserId="482" ContentLicense="CC BY-SA 3.0" />
  <row Id="3453" PostId="3660" Score="0" Text="the alignment of your left border of the toroidal slice with  the rectangle axis is purely coincidental ?" CreationDate="2016-06-22T01:42:17.693" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3454" PostId="3661" Score="0" Text="Commenting all the code related to the normal results in a correct rendered mesh with colors (or white)" CreationDate="2016-06-22T04:56:57.343" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3455" PostId="3660" Score="1" Text="Is this a coverage calculation? Why would you choose to implement a very ineficient box filter?" CreationDate="2016-06-22T05:44:05.863" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3456" PostId="3660" Score="0" Text="@joojaa Agree but if it makes it easier, an approx, say, Gaussian could be built from a few of these coverage calculations." CreationDate="2016-06-22T08:33:15.223" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="3463" PostId="3647" Score="0" Text="@trichoplax: thanks for the suggestion; done." CreationDate="2016-06-22T12:27:37.463" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3464" PostId="3661" Score="1" Text="In one statement in the vertex shader you do a left multiply with the vertex data and in the other you do a right multiply. One of those is probably wrong." CreationDate="2016-06-22T14:00:45.187" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3465" PostId="3661" Score="0" Text="Good catch but still not rendering. I am starting to think that the normals are not being passed to the shader ... somehow." CreationDate="2016-06-22T14:06:56.720" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3466" PostId="3660" Score="1" Text="@trichoplax: points well taken!  I'll take down the question on Maths.SE - thank you for that guidance." CreationDate="2016-06-22T14:52:59.153" UserId="4625" ContentLicense="CC BY-SA 3.0" />
  <row Id="3467" PostId="3660" Score="0" Text="@joojaa: yes, this is exactly a coverage calculation.  Does your comment mean that there's an efficient box filter to calculate this?" CreationDate="2016-06-22T15:00:48.383" UserId="4625" ContentLicense="CC BY-SA 3.0" />
  <row Id="3468" PostId="3660" Score="2" Text="Something like that. Box filtering in general is the worst filter you could choose. Pixels are **not** really squares but point samples (read a [pixel is not a square x 3](http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf)) By using a standard function reconstruction you can do the same with a finite sampling which results in a better image than a box filter would with less work since your box filtering algorithm is so incredibly convoluted and expensive." CreationDate="2016-06-22T15:08:19.370" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3469" PostId="3660" Score="0" Text="@joojaa: duly noted.  in fact, your comment motivated me to entirely reword  the question.  now: how do I design a sensible filter for this?  :)" CreationDate="2016-06-22T15:23:53.630" UserId="4625" ContentLicense="CC BY-SA 3.0" />
  <row Id="3470" PostId="3660" Score="0" Text="Dont delete answer your own question to let somebody else answer with this info and then ask a new one. Its useful for others to see that people before them also made mistakes. Now that we know your scope isnt intentional we can start to answer the question at hand. But you could ass a note saying that you do not really require this sort of filtering just something thats good for the problem at hand." CreationDate="2016-06-22T15:32:55.153" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3471" PostId="3660" Score="0" Text="True -- that's why I kept the original question un-edited." CreationDate="2016-06-22T15:42:28.927" UserId="4625" ContentLicense="CC BY-SA 3.0" />
  <row Id="3472" PostId="3655" Score="0" Text="While this answer doesn't answer the intent of my question, it does answer the letter of the question, so I feel I ought to accept it as an answer hehe.  It's good info on upsampling, so is a very good answer.  I should have asked &quot;what is bilinear filtering&quot;, but that could be another question.  I have a lot more info about that now so am not going to be asking it, perhaps someone in the future will :P" CreationDate="2016-06-22T17:09:55.717" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="3473" PostId="3659" Score="0" Text="Keep in mind that most stencil buffers only have 8-bits allotted to them, which limits the number of unique IDs to 256. A similar strategy that allows for a much greater number of items is color picking. With it you render each object using a basic flat shader and a unique RGB color value. You then check your render target for the color under the mouse click. [Here is an example](http://www.opengl-tutorial.org/miscellaneous/clicking-on-objects/picking-with-an-opengl-hack/) (have not done more than a cursory look over the article)." CreationDate="2016-06-22T19:38:01.543" UserId="4630" ContentLicense="CC BY-SA 3.0" />
  <row Id="3474" PostId="3659" Score="0" Text="@ssell Thank you for your comment. I have also seen that article when I started working on object selection. As I am a newbie in OpenGL, I resorted to the stencil buffer approach, which I think is pretty straightforward, and also fits my need as I have only 10 objects to pick. I have found a (buggy) workaround to this problem, I will share the details soon (by weekend I guess:P)." CreationDate="2016-06-22T19:46:20.773" UserId="3098" ContentLicense="CC BY-SA 3.0" />
  <row Id="3475" PostId="3659" Score="0" Text="My OpenGL is rusty, but everything seems to be set up OK. Keep in mind that an index of `0` indicates your background (an area where no triangle was drawn) as that is the default stencil clear value when calling `glClear`. Do you happen to have an image of the scene you are drawing? An accompanying render of the stencil buffer would also be useful. Also keep in mind that you may need to invert your `y` value when performing the actual picking if your window coordinate system does not match your texture coordinate system (OpenGL origin is in lower-left corner, Windows origin is in upper-left)." CreationDate="2016-06-22T19:59:39.347" UserId="4630" ContentLicense="CC BY-SA 3.0" />
  <row Id="3476" PostId="3660" Score="0" Text="very interesting article about sources pixels to consider for filtering. by Nathan http://www.reedbeta.com/blog/2014/11/15/antialiasing-to-splat-or-not/" CreationDate="2016-06-23T05:44:26.993" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3477" PostId="3661" Score="0" Text="I notice you are specifying your vertices as vec3s in the CPP side but use vec4s in the vertex shader. Change &quot;in vec4 position&quot; to &quot;in vec3 position&quot;, you may need to convert to vec4 with something like vec4(position, 1); . Also I would perform colour calculations in the pixel shader using a vec3 otherwise you will get lighting in your alpha channel." CreationDate="2016-06-23T05:52:34.133" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3482" PostId="3659" Score="0" Text="Are you actually making these triangles visible? If not, could you draw them? The fact that quads are returning a non-zero stencil value just makes me think you should check you haven't got a winding order problem with the second triangle of each pair." CreationDate="2016-06-24T09:02:46.860" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="3483" PostId="3661" Score="0" Text="I changed that, but still not able to see anything. Am I binding the index buffer correctly to the positions and normals of the vertices?" CreationDate="2016-06-24T16:30:53.587" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3484" PostId="3672" Score="1" Text="Ill give you a hint, use cross product." CreationDate="2016-06-25T07:01:42.247" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3487" PostId="3673" Score="0" Text="OK will try narrowing the question, thanks." CreationDate="2016-06-25T20:54:30.980" UserId="4638" ContentLicense="CC BY-SA 3.0" />
  <row Id="3488" PostId="3671" Score="0" Text="@trichoplax ok just wanted to err on the side of caution regarding spam, will upload screenshots. re what i'm asking, questions #1 and #2 aren't specific enough?" CreationDate="2016-06-25T20:55:24.463" UserId="4638" ContentLicense="CC BY-SA 3.0" />
  <row Id="3491" PostId="3671" Score="0" Text="Incidentally, thank you for erring on the side of caution regarding spam - this is much appreciated. I think it's quite clear this is a genuine question rather than advertising - so you need not worry." CreationDate="2016-06-25T22:21:53.440" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3492" PostId="3671" Score="0" Text="@trichoplax ok thanks will upload within 15-20 minutes. thanks again for the help!" CreationDate="2016-06-25T22:26:03.283" UserId="4638" ContentLicense="CC BY-SA 3.0" />
  <row Id="3493" PostId="3671" Score="0" Text="@trichoplax uploaded with screenshots from our app. thanks!" CreationDate="2016-06-25T22:37:48.127" UserId="4638" ContentLicense="CC BY-SA 3.0" />
  <row Id="3494" PostId="3674" Score="0" Text="Could you expand on how this can be used to determine the angle of the tube? Sorry if I'm missing something obvious..." CreationDate="2016-06-26T00:09:20.400" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3495" PostId="3674" Score="2" Text="@trichoplax you don't even need the angle. You rarely, if ever, do for setting up geometry. You can nearly always skip getting the angle explicitly because 99% of the time you will be feeding it into a sin or cos right after you get it from a asin or acos." CreationDate="2016-06-26T00:20:49.083" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3496" PostId="3673" Score="2" Text="Looks like you are more up to speed on scene kit than I thought. Trichoplax has the answer, its about the lighting/shadowing." CreationDate="2016-06-26T02:06:46.713" UserId="4495" ContentLicense="CC BY-SA 3.0" />
  <row Id="3499" PostId="3677" Score="2" Text="You can not be helped until you describe what you interpolate. A keyframe is just the frame where you have no interpolation. But what is in thw keyframe. The underlying mathematical model is important to the interpolation. No model no interpolation. So how do you plan to track the pixels. Vague waving of hands does not help." CreationDate="2016-06-26T09:13:08.343" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3501" PostId="3678" Score="0" Text="[Related](http://computergraphics.stackexchange.com/questions/2381/finding-the-maximum-number-of-disconnected-fragments) (though not a duplicate)" CreationDate="2016-06-26T11:17:25.157" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3502" PostId="3678" Score="0" Text="Does &quot;clipping&quot; in this context mean &quot;clipping against a rectangle&quot;?" CreationDate="2016-06-26T11:18:20.427" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3503" PostId="3678" Score="0" Text="Yes, that is exactly what I mean :)" CreationDate="2016-06-26T11:57:34.617" UserId="4646" ContentLicense="CC BY-SA 3.0" />
  <row Id="3504" PostId="3678" Score="0" Text="Sorry - I just realised you did mention a rectangle in your question..." CreationDate="2016-06-26T12:02:37.173" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3505" PostId="2521" Score="0" Text="What is DDA (in this context)? [Digital differential analyzer](https://en.wikipedia.org/wiki/Digital_differential_analyzer_(graphics_algorithm))?" CreationDate="2016-06-26T13:23:00.840" UserId="3304" ContentLicense="CC BY-SA 3.0" />
  <row Id="3507" PostId="3681" Score="1" Text="Normalizing is almost certainly faster than doing anything with trigonometry: a square root and a few adds and multiplies should be cheaper to evaluate than a sequence of trigonometric functions. I'm surprised to hear that normalizing is too expensive, as this should be quite fast on modern CPUs. Can you say more about the context of this problem? How have you determined that the normalization is causing a performance issue?" CreationDate="2016-06-26T19:41:12.927" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3508" PostId="3681" Score="0" Text="The problem is their are tons of points that this needs to be done for. Anywhere from 400-1500 points have this done about every frame" CreationDate="2016-06-26T19:45:40.577" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3509" PostId="3681" Score="1" Text="OK. 400-1500 points is a fair number, but not _that_ many; it should only take microseconds to normalize them if it's coded efficiently in a native-compiling language like C++. What language/framework/engine/etc are you using? How are you implementing the normalization process?" CreationDate="2016-06-26T20:07:23.617" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3510" PostId="3681" Score="0" Text="Is the radius a variable or a constant?" CreationDate="2016-06-26T20:09:11.737" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3511" PostId="3681" Score="0" Text="I added some details about the simulation above." CreationDate="2016-06-26T20:20:08.580" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3512" PostId="3675" Score="1" Text="Take a look at the [SCNTechnique Class Reference...](https://developer.apple.com/library/ios/documentation/SceneKit/Reference/SCNTechnique_Class/)" CreationDate="2016-06-26T20:20:10.517" UserId="4495" ContentLicense="CC BY-SA 3.0" />
  <row Id="3513" PostId="3681" Score="0" Text="The radius is variable. Right now I first check if it's in the circle using the squared distance. Then after that I subtract the center from the point and decide that by the sqrt of the distance squared. That is then multiplied by the radius." CreationDate="2016-06-26T20:21:56.127" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3514" PostId="3681" Score="0" Text="If you have existing code then it might help to edit it into the question so we can see it. There may be potential optimisations that are not obvious from the question wording." CreationDate="2016-06-26T21:37:08.937" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3515" PostId="3681" Score="0" Text="For example, from your last comment I would suggest storing the radius squared and comparing that to the distance squared if many points are compared against the same radius, but I wouldn't have thought to mention that just from the question wording." CreationDate="2016-06-26T21:39:15.400" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3516" PostId="3681" Score="0" Text="It would also help to know how often the point is inside the circle. If there are enough points that are outside the circle then it may be worth including a fast test that excludes points outside a bounding square." CreationDate="2016-06-26T21:40:53.640" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3517" PostId="3677" Score="1" Text="Possible duplicate of [How to do keyframe interpolation on matlab? Any toolboxes present?](http://computergraphics.stackexchange.com/questions/3644/how-to-do-keyframe-interpolation-on-matlab-any-toolboxes-present)" CreationDate="2016-06-26T22:19:37.397" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="3518" PostId="3681" Score="0" Text="Does your current implementation use an acceleration structure such as a grid? If not that would be your answer (for a huge speed up). Let me know and I'll write something up if that's the case." CreationDate="2016-06-26T22:27:36.783" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3519" PostId="3682" Score="0" Text="Is that even faster then doing a quick broad phase AABB check?" CreationDate="2016-06-26T23:00:37.050" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3520" PostId="3681" Score="1" Text="BTW, although an acceleration structure may be helpful, I still think that even brute-force testing 1500 points against each of 10 circles (for instance) is not a huge amount of calculation, even on a mobile device. I would look into making sure it's coded efficiently, i.e. not having unnecessary function calls or pointer indirections, and possibly using SIMD for the calculations, before going to the trouble of implementing an acceleration structure." CreationDate="2016-06-26T23:04:47.543" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3521" PostId="3682" Score="0" Text="If you do a bounding box check on each point, then that fast check is still being done on all 1000 points, for each circle. With a grid acceleration structure, if a circle only overlaps grid cells containing 30 points, none of the points outside those grid cells get checked at all, as they are not relevant." CreationDate="2016-06-26T23:10:09.330" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3522" PostId="3682" Score="0" Text="You will see the best speed up from a grid if the points are evenly spread over the screen. If the circles force all the points into a small area of the screen then the speed up will be less significant, and then you might need to look for other optimisations." CreationDate="2016-06-26T23:12:47.687" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3523" PostId="3682" Score="0" Text="A rough guide to whether this is the right answer is to observe the change of speed in your current implementation. Does it slow down (1) in proportion to the total number of points, or (2) in proportion to the number of points needing to be moved? (1) suggests you need an acceleration structure (such as a grid), while (2) suggests you need to optimise the movement of the points to the circumference. This answer is a best guess without seeing the code or the app running, so it may not be what you need." CreationDate="2016-06-26T23:16:39.597" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3524" PostId="3682" Score="0" Text="Also see Nathan Reed's comment on the question. I trust Nathan's intuition far more than my own." CreationDate="2016-06-26T23:19:37.483" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3525" PostId="3681" Score="0" Text="@NathanReed so as of now I am using a static math library of my own design that has functions like add subtract and multiply. It creates a new point struct and returns it. Would it be faster to now use the static class?" CreationDate="2016-06-26T23:30:57.657" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3526" PostId="3681" Score="1" Text="@J.Doe Ideally you would look at the generated code (assembly language) to see how it's coming out. Compilers can often optimize away small function calls by inlining, and can keep small structs in registers, etc. I don't develop for Swift or iOS so I can't tell you in detail about that platform, but a quick google did turn up [this article on viewing disassembly in Swift](https://medium.com/swift-programming/secret-of-swift-performance-fcc5d2a437a8#.6e33qpynu)." CreationDate="2016-06-26T23:36:51.247" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3527" PostId="3682" Score="2" Text="Ok I tried out broad phase, and grid phase.  Turns out the best one was doing a AABB check before anything else. Reading all of your comments (thankyou so much) has helped me make sense of why. The radius is growing, and points group up quickly. Thank much!" CreationDate="2016-06-26T23:42:51.710" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3528" PostId="3681" Score="0" Text="@NathanReed Looking at assembly code is a bit beyond my knowledge so far, I think I will just try and replace the static methods before the final build. Thanks!" CreationDate="2016-06-26T23:43:25.710" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3530" PostId="3683" Score="2" Text="I don't think that is going to measure what you think it will. Not to mention fragment shaders run (at least) in a 2x2 block for each fragment." CreationDate="2016-06-27T10:00:28.337" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3531" PostId="3683" Score="0" Text="@ratchetfreak 2x2 block per fragment even without multisampling? Why?" CreationDate="2016-06-27T10:12:39.550" UserId="4647" ContentLicense="CC BY-SA 3.0" />
  <row Id="3532" PostId="3683" Score="3" Text="@Ruslan: fragments are treated as 2x2 blocs so the derivatives can be evaluated." CreationDate="2016-06-27T11:09:12.183" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3533" PostId="3688" Score="0" Text="If you set the `w` component in the vertex output to 0 the triangle it is part of will not get pushed down the rasterizer. Though that may not help if the bottle neck is vertex processing." CreationDate="2016-06-27T15:20:15.583" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3534" PostId="3688" Score="0" Text="Thanks for your answer. But if I understand you correctly I would need to render every position with every LOD and then check the distance to camera for every vertex and set w to 0 if it belongs to the wrong LOD for this position. If this is correct, I don't think that it will help because doing this test for all vertices of the highest LOD at every position leads to an insane number of tests. Even if I don't send most of this vertices to the rasterizer, I don't think that this will work, but if I don't find a better solution I will try it anyways and comment my result here." CreationDate="2016-06-27T15:32:18.177" UserId="4654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3535" PostId="3693" Score="0" Text="I didn´t see the error! Thank you so much!" CreationDate="2016-06-27T20:36:41.917" UserId="4653" ContentLicense="CC BY-SA 3.0" />
  <row Id="3536" PostId="3693" Score="1" Text="@Maribel Common mistake. I do it all the time." CreationDate="2016-06-27T20:38:31.777" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3537" PostId="3697" Score="4" Text="Curves are mathematical entities they do not have centers as such. However you might be looking for bouding box center, minimum bounding box center, center of gravity of the enclosed area underlying the curve when endpoints are connected, center of gravity of closed polybeziers, curve midpoint, control point average etc." CreationDate="2016-06-27T21:41:51.703" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3538" PostId="3694" Score="0" Text="Thanks for your answer and the extensive code sample. I'm not familiar with DX, but it pointed me in the right direction." CreationDate="2016-06-28T00:27:08.420" UserId="4654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3539" PostId="3694" Score="1" Text="And your assumption that the pcie bus is the bottleneck seems to be indeed right (on my hardware I can now render about 300k bushes and I get the same framerate for the middle and the coarse LOD, therefore the number of vertices is not the limiting factor). For anyone who has the same problem: The use of instanced arrays did the trick for me (very nice explanation here: http://learnopengl.com/#!Advanced-OpenGL/Instancing). Therefore you don't run into the limit of uniforms, which was the problem of my 2nd approach." CreationDate="2016-06-28T00:33:45.973" UserId="4654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3540" PostId="3685" Score="0" Text="Nailed it. Of course, doing anything on a single fragment is massively wasteful anyway since you will have 31 or 63 other threads idling while it completes. If i was gonna  run a test like this to get at least a ballpark figure, i'd probably draw a full-window quad at say 1024^2, load up a bunch of different textures then sample each of them in turn until the framerate dropped, making sure to actually use the results like you say." CreationDate="2016-06-28T05:51:25.017" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="3541" PostId="3697" Score="1" Text="@joojaa there are definitely 2 natural centers I can think of, one is the `t=0.5` point. and two is the geometric midpoint regarding travel cartesian distance along the line." CreationDate="2016-06-28T06:12:10.427" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3542" PostId="3697" Score="1" Text="@v.oddou yes in fact there are many more thats why the question needs clarification. All vector applications that i have used (Illustrator, xara, corel, sketch, etc..) Use the local bounding box center to rotate objects, so its rare to see the other center definitions used at all." CreationDate="2016-06-28T06:44:21.133" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3544" PostId="3698" Score="0" Text="maybe this is better as a comment..." CreationDate="2016-06-28T09:15:26.723" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3545" PostId="3698" Score="2" Text="Could you expend the question a bit so that its not just a link only answer," CreationDate="2016-06-28T12:37:00.660" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3546" PostId="3699" Score="3" Text="I wouldn't call this a comment. I'd call it an excellent answer that addresses the asker's current level of knowledge, explains fully why the question is broader than expected, and opens the way for new questions." CreationDate="2016-06-28T14:00:40.993" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3547" PostId="3699" Score="0" Text="@trichoplax though could be summarized as &quot;define center&quot; by the less polite." CreationDate="2016-06-28T15:24:46.537" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3548" PostId="3699" Score="0" Text="@ratchetfreak I prefer answers that try to identify the knowledge gap of the asker rather than expecting them to fully understand the topic they are asking about." CreationDate="2016-06-28T17:56:16.807" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3549" PostId="3700" Score="0" Text="Are you looking for something different from [this](http://computergraphics.stackexchange.com/questions/295/does-it-matter-whether-i-learn-opengl-or-direct3d)?" CreationDate="2016-06-28T19:23:24.583" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3550" PostId="3700" Score="0" Text="@trichoplax Yes, I have read all of the answers before asking this question. Is this a duplicate, Or broad ?" CreationDate="2016-06-28T19:25:34.833" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3551" PostId="3700" Score="0" Text="I don't think this is a duplicate, but the title looks like one, so I'm just trying to think of a way to word it" CreationDate="2016-06-28T19:45:39.533" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3553" PostId="3700" Score="2" Text="BTW, I answered [a similar question on GameDev.SE](http://gamedev.stackexchange.com/a/49038/9894) a couple years ago." CreationDate="2016-06-28T21:10:34.987" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3554" PostId="3700" Score="1" Text="here's a good lesson of history that sorta answer your question: http://programmers.stackexchange.com/questions/60544/why-do-game-developers-prefer-windows" CreationDate="2016-06-29T01:01:08.207" UserId="4665" ContentLicense="CC BY-SA 3.0" />
  <row Id="3555" PostId="3700" Score="0" Text="@darius especially Nicol Bolas's answer." CreationDate="2016-06-29T01:53:55.793" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="3556" PostId="3700" Score="1" Text="@NathanReed *&quot;So, it's possible we will see a swing back toward multi-platform developers using OpenGL on Windows.&quot;* And now, 3 years later, would you say this has happened?" CreationDate="2016-06-29T04:08:32.790" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="3557" PostId="1941" Score="0" Text="This is a great idea, and probably the closest you can get to single-stepping shader code.  I wonder if running through a software renderer (Mesa?) would have similar benefits?" CreationDate="2016-06-29T06:03:38.983" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="3558" PostId="3700" Score="0" Text="You should check these two blogs out:&#xA;[Why you should use OpenGL and not DirectX](http://blog.wolfire.com/2010/01/Why-you-should-use-OpenGL-and-not-DirectX) and &#xA;[DirectX vs. OpenGL revisited](http://blog.wolfire.com/2010/01/DirectX-vs-OpenGL-revisited).&#xA;&#xA;They both helped me forumate an opinion on which to use. Although from a technical standpoint both are going in very similiar directions." CreationDate="2016-06-29T08:58:21.233" UserId="214" ContentLicense="CC BY-SA 3.0" />
  <row Id="3559" PostId="1941" Score="0" Text="@racarate: I thought about that as well but did not have the time to try yet. I am no expert on mesa but I think it might be hard to debug the shader as the shader debug information has to somehow reach the debugger. Then again, maybe the folks at mesa already have an interface for that to debug mesa itself :)" CreationDate="2016-06-29T10:21:35.590" UserId="2521" ContentLicense="CC BY-SA 3.0" />
  <row Id="3560" PostId="3700" Score="2" Text="@Rotem I don't think so, no. OGL (and now Vulkan) today is better at maintaining feature parity with D3D than it was, but AFAICT, the GPU vendors still prioritize D3D drivers for bugfixes and perf improvements, and most games/engines still use D3D by default on Windows. A notable exception is the [idTech engines](https://en.wikipedia.org/wiki/Id_Tech), which all use OGL on Windows." CreationDate="2016-06-29T18:50:46.827" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3565" PostId="3685" Score="0" Text="Thank you for this, giving up on that quest helped me realize how stupid it was to think I could take the amount of samples I do and somehow calculate a downsample size, and radius for a gaussian blur shader. Turns out it will work much better to just have predefined combinations of varying intensity and see which ones run at the desired fps." CreationDate="2016-06-30T07:34:04.937" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3566" PostId="3685" Score="0" Text="Are GPU compilers more innovative then CPU ones? I mean I have heard stuff about a gpu compiler recompiling because a uniform triggers a different part of an if statement and you speak of pulling out of the loop because the calculation is the same. I haven't heard of CPU's doing that." CreationDate="2016-06-30T07:35:16.233" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3567" PostId="3577" Score="0" Text="Apple will never support Vulkan unless they're forced to, and the economics of it are pretty simple. By going all in on Metal, they hope to lock in mobile game developers that need more than GLES2 can offer but don't have the resources to write their games for both Vulkan and Metal. They're prepared to sacrifice Mac's tiny gaming ecosystem for this, especially if iOS devs also release on the Mac App Store." CreationDate="2016-06-30T07:52:16.723" UserId="4671" ContentLicense="CC BY-SA 3.0" />
  <row Id="3568" PostId="3685" Score="0" Text="@J.Doe optimizers (for gpu and cpu) try to use every trick in the book however GPU optimizers know more about the program they are trying to optimize and what hardware it will be running on. The optimizations you mention are called [partial evaluation](https://en.wikipedia.org/wiki/Partial_evaluation) (with the uniforms as additional constant data) and [Loop-invariant code motion](https://en.wikipedia.org/wiki/Loop-invariant_code_motion)" CreationDate="2016-06-30T08:02:38.230" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3569" PostId="3702" Score="0" Text="So there is no performance reasons to prefer Direct3D ?" CreationDate="2016-06-30T10:07:51.453" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3570" PostId="3707" Score="0" Text="No the power is 2.2 usually (read about [gamma correction](https://en.wikipedia.org/wiki/Gamma_correction))" CreationDate="2016-06-30T11:14:48.513" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3573" PostId="3702" Score="0" Text="@ritwik sinha DirectX 11 has better support for multi threading rendering than OpenGL 4, but to my knowledge that has not resulted in any realt performance difference. On the other side, comparing Windows with DirectX 11 and Linux with OpenGL 4, the winner was Linux with OpenGL. Regarding DirectX 12 vs Vulkan, I think it is too early to know, but i don't know." CreationDate="2016-06-30T20:46:43.773" UserId="4665" ContentLicense="CC BY-SA 3.0" />
  <row Id="3574" PostId="3688" Score="0" Text="Have you considered an imperfect solution spreading LOD update of the 10000 bushes across multiple cycles? I doubt your view changes alot anyways. And if it changes a huge amount, a little lag is usually not noticable." CreationDate="2016-06-30T20:50:29.330" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="3575" PostId="3702" Score="0" Text="i never worked with Direct3D, so i don't understand what you mean by multi threading. As far as i know different threads are handled by programmers for different tasks. Does Direct3D does this under the hood  ? Anyway nice answer." CreationDate="2016-06-30T21:09:44.243" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3576" PostId="3702" Score="1" Text="@ritwik sinha an application of multi threading to real time graphics is that you may use diffrent threads to allocate resoruces in the background (texture streaming, vertex buffers), wile a main thread does the draw calls. This is also possible in OpenGL, but very very tricky. In DirectX 11 you can also make draw calls, from many threads, which should improve performance, but that's not a common practice. I think that the last release of UnrealEngine4 does it. However, with DirectX 12 and Vulkan multi threading rendering is going to be a real deal." CreationDate="2016-06-30T23:59:26.960" UserId="4665" ContentLicense="CC BY-SA 3.0" />
  <row Id="3577" PostId="3707" Score="0" Text="So wait add the sum of your pixels with each one raised to the 2.2 power. And then take that sum by the 1/2.2 power?" CreationDate="2016-07-01T05:09:22.967" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3578" PostId="3707" Score="0" Text="Depends on how pedantic you want to be, a gamma correction of 2.2 certainly is close to srgb but if you really want to be pedantically correct google dor linear to srgb" CreationDate="2016-07-01T05:27:04.647" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3580" PostId="3710" Score="0" Text="Beautifully answered.  And rendered.  Thanks." CreationDate="2016-07-01T23:24:17.753" UserId="4625" ContentLicense="CC BY-SA 3.0" />
  <row Id="3581" PostId="3710" Score="0" Text="Use a indented block for code in future please. Nice answer" CreationDate="2016-07-02T07:14:18.663" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3582" PostId="3709" Score="0" Text="@ritwiksinha Everything written in black is known.The central points, the radii, and the arc angle." CreationDate="2016-07-02T11:53:18.863" UserId="4620" ContentLicense="CC BY-SA 3.0" />
  <row Id="3584" PostId="3712" Score="2" Text="This is a [duplicate question](http://computergraphics.stackexchange.com/questions/1669/why-for-perfect-reflections-a-surface-must-have-g2-continuity) Basically its because you want the reflective properties of the surface to continue the same way over the gap. Otherwise humans will interpret a sudden change as a crease" CreationDate="2016-07-02T17:40:24.233" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3585" PostId="3712" Score="0" Text="Although this question mentions two additional properties in addition to the previous question, it is not clear what is being asked. If a term has multiple conflicting definitions, then each one is probably for a specific purpose. Without knowing your intention it is difficult to judge what you need to know." CreationDate="2016-07-02T18:30:09.230" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3586" PostId="3712" Score="0" Text="@trichoplax actually all the 3 definitions seem to say the same thing. Curvature must not suddenly change. 1 specifies how deep your continuity must be 2 that it may not abruptly change 3 that the change must not be too fast. All of those have been discussed n the duplicate, i would agree. But yes the question is not so clear. Also note G2 curvature does not apply to curves as such for silhouettes it could be less than G2 is its not too abrupt change. However CAD deals with surfaces a curve must have same properties as a surface it models or it will destroy the surfaces continuity requirement." CreationDate="2016-07-02T18:56:37.067" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3587" PostId="3712" Score="0" Text="@joojaa I don't have the knowledge to judge how close to being a duplicate this is - thanks for the extra information. If this is closed as a duplicate then people searching for the terms in this question will still be redirected to the previous question, so we just need to judge whether anything new would be present in answers to this new question." CreationDate="2016-07-02T19:22:00.157" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3588" PostId="3713" Score="0" Text="I don't know if it would be practical to implement this, but would it help to have the precomputed sine values spaced unevenly, with more closely clustered values where the slope of the sine curve is steepest, and fewer values where it levels out and doesn't change as much? Would this allow higher accuracy where it is needed, without needing to store a large number of values?" CreationDate="2016-07-02T19:26:16.430" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3589" PostId="3713" Score="0" Text="Can you just render to a texture that is 1/2 or 1/4 the size of the final target?" CreationDate="2016-07-02T21:59:49.260" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="3590" PostId="3713" Score="5" Text="Regarding #2, the built-in [`mod` function](http://docs.gl/sl4/mod) is what you want. You would write `mod(angle, 360.0)`." CreationDate="2016-07-03T01:10:42.147" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3591" PostId="3713" Score="1" Text="@trichoplax brilliant idea but I don't know how you would be able to look up values on the table then. Let's say we put them in an array with some of them more concentrared. How could we find the right index?" CreationDate="2016-07-03T01:35:25.583" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3592" PostId="3713" Score="0" Text="@racarate actually will probably do that. Feel free to post that as the answer." CreationDate="2016-07-03T01:35:56.930" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3593" PostId="3712" Score="0" Text="They aren't 3 definitions. It is an unique definition; all 3 conditions must hold." CreationDate="2016-07-03T10:03:59.617" UserId="1636" ContentLicense="CC BY-SA 3.0" />
  <row Id="3594" PostId="3688" Score="0" Text="I have not tried it, since the performace that I gained by using instanced arrays (see my comment under the accepted answer) was enough in my case. But it is something that could be combined with the use of instanced arrays if even more performace would be needed." CreationDate="2016-07-03T17:09:28.237" UserId="4654" ContentLicense="CC BY-SA 3.0" />
  <row Id="3596" PostId="3713" Score="6" Text="How about putting your values into a 3-channel 1D texture? That way you can get sin, cos and tan out for the price of a single texture lookup. If you map 0 - 2pi angle to 0 - 1 UV and use repeat texture mode you don't even need the mod call, it will 'wrap' automatically, and you can also get linear-filtered approximations in between your stored values rather than snapping to the nearest one." CreationDate="2016-07-04T05:31:04.567" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="3597" PostId="3712" Score="0" Text="@Valerio yes and if you read the linked post you would know why. Basically eyes structure will see up to second derivate. For obvious reasons this means inflection will be seen and uneven variation will be seen by a human because of the nature of the system." CreationDate="2016-07-04T08:18:30.957" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3598" PostId="3713" Score="3" Text="A lot of the time you can eliminate trig functions when used for geometry by not using the angle but start and end with the sin/cos pair and use trig identities for half angles and such." CreationDate="2016-07-04T08:27:06.563" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="3599" PostId="3717" Score="0" Text="This sounds promising. Could you edit to add some detail of what this provides?" CreationDate="2016-07-04T12:24:45.830" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3600" PostId="3711" Score="0" Text="Thank you. Looking at the solutions, I think knowing the quadrants where the circles touch is sufficient, if I also know the direction of curvature." CreationDate="2016-07-04T13:57:32.307" UserId="4620" ContentLicense="CC BY-SA 3.0" />
  <row Id="3602" PostId="3711" Score="0" Text="Could you please elaborate on what you would do with the given data and radius $x$ to obtain the tangential points and directions?" CreationDate="2016-07-04T15:51:19.163" UserId="4620" ContentLicense="CC BY-SA 3.0" />
  <row Id="3605" PostId="3711" Score="0" Text="@Rikki-Tikki-Tavi, What part do you not know how to compute: The other angles of a triangle  (use [law of sines](https://en.wikipedia.org/wiki/Law_of_sines)) or [vector rotation](https://en.wikipedia.org/wiki/Rotation_matrix)? If you look at the image you'll see that 2 of the solutions are in same quadrant. I would Instead use whether or not its the inside or outside tangent, or then you need to check all of them. (by the way it feels like I'm teaching you your high school trig again as i have taught you [Law of Cosines](https://en.wikipedia.org/wiki/Law_of_cosines) already)" CreationDate="2016-07-04T17:20:30.423" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3606" PostId="3717" Score="1" Text="It would also be useful to have an overview of how each algorithm works, so that they can be compared without needing to leave the answer." CreationDate="2016-07-05T01:21:35.790" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3607" PostId="2011" Score="0" Text="Perhaps you are missing enabling the attrin pointers? Take a look at glEnableVertexAttribArray()." CreationDate="2016-07-05T05:05:54.453" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3608" PostId="2011" Score="0" Text="Another thing that comes to mind is trying to set the location in the shaders. The first parameter in glVertexAttribPointer is the index of the vertex attribute. Therefore, trying something in your shader like `layout(location = 0) in vec3 posición;` and `layout(location = 3) in vec4 i_color;` might help." CreationDate="2016-07-05T05:09:18.037" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="3609" PostId="3723" Score="1" Text="Just to confirm, you want the output vertices to be a subset of the input vertices? It isn't sufficient for the output vertices to be on the surface defined by the input mesh?" CreationDate="2016-07-05T09:32:38.003" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3611" PostId="3692" Score="1" Text="Now I understand, thanks. It seems most people use projected area instead of projected solid angle in the definition for radiance. I guess it doesn't make any difference in practice. I can see the math checks out using projected solid angle, but intuitively it doesn't make much sense to me (using projected area seems like &quot;the right&quot; way to me). Can you please describe/show the reasoning for using projected solid angle visually/geometrically?" CreationDate="2016-07-05T17:32:27.700" UserId="4578" ContentLicense="CC BY-SA 3.0" />
  <row Id="3612" PostId="3724" Score="1" Text="Many animation tools have such functions, such as mayas curve deformer. But thenagain your question is a bit vague in terms of what you want as you may in fact want to know about spline patches such as nurbs surfaces that indeed have a quite wide mathematical research behind them." CreationDate="2016-07-06T06:50:04.203" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3614" PostId="3713" Score="0" Text="Russ has, I think, the best solution here.  If you only need sine, then a single channel half precision will work best, as it'll be important for performance that the 1d texture to fit in cache (less precision than half float will likely not reproduce the sine function very well)." CreationDate="2016-07-06T19:15:29.220" UserId="3386" ContentLicense="CC BY-SA 3.0" />
  <row Id="3615" PostId="3729" Score="0" Text="Could you clarify what difference you want to see each frame? Do you want the total &quot;mass&quot; to stay constant, and have it spread a small distance each frame until it is evenly distributed? Do you want it to spread evenly in all directions, giving a circular spread?" CreationDate="2016-07-06T19:19:03.353" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3616" PostId="3729" Score="0" Text="I think it would be most appropriate to have it spread outwards each frame at a constant rate, but since the initial mass for each pixel is random, all pixels will stop spreading at different times. If the &quot;mass&quot; is constant across all pixels, the effect would just be an even circular spread. I think a major issue I'm having is figuring out how to pass that mass value along." CreationDate="2016-07-06T19:22:53.910" UserId="4708" ContentLicense="CC BY-SA 3.0" />
  <row Id="3617" PostId="3731" Score="0" Text="Thank you very much, I hadn't heard of the Kronecker delta before." CreationDate="2016-07-06T19:59:49.547" UserId="4705" ContentLicense="CC BY-SA 3.0" />
  <row Id="3618" PostId="3713" Score="0" Text="That is brilliant! Though filling up a texture with values would be a new technique for me. I can write to a texture but I can't ensure that I'm writing exact pixels rather then say .5 pixels" CreationDate="2016-07-06T20:11:04.593" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3619" PostId="3713" Score="0" Text="And I don't quite understand how I would look up values on the texture for a given radian value." CreationDate="2016-07-06T20:11:33.710" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3620" PostId="3729" Score="0" Text="Maybe try to start with just describing the overall effect you want to give, rather than the implementation details. Do you want multiple drops of ink that all spread out independently and eventually overlap, or are the multiple pixels you mention part of modeling a single ink drop?" CreationDate="2016-07-06T22:39:12.627" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3621" PostId="3729" Score="0" Text="Thank you all for the help. The effect I want is as if a heavy drop of ink was dropped on paper, and the ink is spreading outwards. With that it spreads to different degrees, and maybe even has some sort of capillary element." CreationDate="2016-07-06T22:45:58.847" UserId="4708" ContentLicense="CC BY-SA 3.0" />
  <row Id="3622" PostId="3729" Score="1" Text="This is much clearer. Could you edit the question to reflect this more specific requirement? Comments aren't guaranteed to last forever." CreationDate="2016-07-06T22:50:49.060" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3623" PostId="3735" Score="0" Text="There is a less understood trick to handle the flip of the spline direction, it happens because you use a naive local approach to the problem. But it is quite easy to find all the locations where the flip will occour than its just a matter of saying okay this segment is reversed." CreationDate="2016-07-07T09:08:06.480" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3624" PostId="3735" Score="0" Text="Interesting. In my case the flip was never a problem though. I had to handle the case when the spline was backtracking, but that was easy to detect and then just a matter of switching a sign." CreationDate="2016-07-07T09:25:53.757" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3625" PostId="3734" Score="0" Text="You're right! I just tried to delete those lines and the first example now behaves like the second one. At this point I think the second example has just a &quot;rotated texture&quot; issue, I'll try and let you know." CreationDate="2016-07-07T11:33:37.720" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3626" PostId="3734" Score="0" Text="Yep, I confirm, that did the trick. Thank you immensely!" CreationDate="2016-07-07T12:20:48.917" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3627" PostId="3677" Score="0" Text="@flawr yes it was me. My previous question got closed as mentioned." CreationDate="2016-07-07T13:41:23.377" UserId="4608" ContentLicense="CC BY-SA 3.0" />
  <row Id="3628" PostId="3677" Score="0" Text="@joojaa	&#xA;I'm interpolating image frames. Literal image frame pixel values. Imagine this, there are some pixel values I select, then I call those pixel values features and decide to track those pixel values against the frames. Those pixel values must have some sort of trajectory. I want to interpolate pixel values at various points of a trajectory such that the trajectory approximates a smooth motion between the pixel values." CreationDate="2016-07-07T13:42:24.893" UserId="4608" ContentLicense="CC BY-SA 3.0" />
  <row Id="3629" PostId="3677" Score="0" Text="@user6334139 yes but finding the trajectory is another thing and has nothing to do with interpolation. Interpolation only comes into play once you know where the trajectory is going (quite literally you have billions of options on where the pixel went). Look into optical flow tracking. https://www.youtube.com/watch?v=KoMTYnlNNnc" CreationDate="2016-07-07T13:46:03.407" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3630" PostId="3677" Score="0" Text="@joojaa Okay fine I know the per pixel affine transform matrices between frames, the warps they perform on any single pixel in the image give rise to displacement in the pixel in a frame-time plot and end up being a trajectory. I smooth this thing somehow and have another set of matrices now describing the same thing but smoother. So I know where the trajectory is going, now what?" CreationDate="2016-07-07T13:51:40.533" UserId="4608" ContentLicense="CC BY-SA 3.0" />
  <row Id="3631" PostId="3677" Score="1" Text="Yes time to rewrite your question. Your not looking to interpolate pixels but interpolation of matrices. Is it 4 by 4 matrix? Please [EDIT](http://computergraphics.stackexchange.com/posts/3677/edit) your question" CreationDate="2016-07-07T13:53:22.243" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3632" PostId="3677" Score="0" Text="Matrices are only a representation of things needed to get a trajectory. The end result is still interpolation of the pixel values in the trajectory." CreationDate="2016-07-07T13:56:59.437" UserId="4608" ContentLicense="CC BY-SA 3.0" />
  <row Id="3634" PostId="3677" Score="0" Text="Not being wiser friend, I've articulated it as it is, thats the problem statement, thats what I couldnt solve, thats why I posted here" CreationDate="2016-07-07T14:11:03.093" UserId="4608" ContentLicense="CC BY-SA 3.0" />
  <row Id="3636" PostId="3737" Score="0" Text="Also if you use subdivision meshes then all quads makes it easier for you to anticipate how the mesh flows. Which can be important to the artist" CreationDate="2016-07-07T16:40:27.647" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3637" PostId="3736" Score="0" Text="Well on hardware side but in software renders its actually pretty common. After all a quad representation is more compact." CreationDate="2016-07-07T16:41:19.307" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3638" PostId="3677" Score="0" Text="Rather than trying to keep everything abstract, if you have some example data this may make it easier to understand your intention. A series of images or the location data of the points you are working with, perhaps." CreationDate="2016-07-07T19:25:02.637" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3639" PostId="3741" Score="0" Text="How much higher resolution should I upscale it" CreationDate="2016-07-07T20:22:03.073" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3640" PostId="3741" Score="0" Text="Usually 4 is fine but read this first https://www.opengl.org/wiki/Multisampling you dont actually need to store those extra pixels" CreationDate="2016-07-07T20:24:11.140" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3641" PostId="3740" Score="1" Text="Well you probably could solve the render equation for a point analytically if youd have a stupidly simple scene. And possibly you could get an analytical solution for the entire projected image if your scene is even simpler than that. But that would be useless... :)" CreationDate="2016-07-07T20:33:31.707" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3642" PostId="3740" Score="3" Text="The scene would have to be really simple like only a single flat object. Since there is the integration about the whole sphere even if there are any two points which can see each other the rendering equation gets infinite. Each point includes the other in the integration domain. so Only a single reflector that cannot reflect on itself. then you could solve it. Then there would simply no global lighting effects so is comes down to local lighting. And that is solvable." CreationDate="2016-07-07T20:37:11.353" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3643" PostId="3740" Score="0" Text="Yes that would be stupidly simple." CreationDate="2016-07-07T20:38:44.160" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3644" PostId="3741" Score="0" Text="I hope that is compatible with opengl es 2.0. It seems almost nothing is. My fear is that my app uses an accumulation buffer so I never clear the buffer that is drawn on, i simply fade it then draw a new frame. Oviously a bigger FBO means slower loading time. Hopefully the extension is supported making that a non issue." CreationDate="2016-07-07T23:36:25.307" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3645" PostId="3738" Score="5" Text="Good question, no research effort." CreationDate="2016-07-08T05:43:47.840" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="3646" PostId="3736" Score="0" Text="&quot;_After all a quad representation is more compact_&quot;. Surely not when compared to a triangle strip/fan." CreationDate="2016-07-08T11:05:57.873" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="3647" PostId="3742" Score="1" Text="If you google for corner pin you get a few implementations of this" CreationDate="2016-07-08T15:42:21.993" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3649" PostId="3677" Score="0" Text="Not keeping anything abstract, I've uploaded my source. It has the problem in it" CreationDate="2016-07-09T08:08:44.407" UserId="4608" ContentLicense="CC BY-SA 3.0" />
  <row Id="3650" PostId="3738" Score="0" Text="How much do you know about integral equations in general? Are you asking about an analytical solution to it?" CreationDate="2016-07-10T01:38:40.420" UserId="3477" ContentLicense="CC BY-SA 3.0" />
  <row Id="3651" PostId="3740" Score="2" Text="@joojaa To my understanding, it's not that the rendering equation is impossible to solve in all cases, but rather that for any time it is solvable, it's of no practical use" CreationDate="2016-07-10T02:48:54.103" UserId="3477" ContentLicense="CC BY-SA 3.0" />
  <row Id="3652" PostId="3740" Score="0" Text="@jaska it is solvable whenever there is no global effect in the lighting calculation. But we want to solve it because of the global lighting. So yes, whenever it is solvable it is pretty useless." CreationDate="2016-07-10T10:50:08.813" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3655" PostId="3740" Score="4" Text="FYI the Mathjax syntax works on this StackExchange, so if you put $ signs around your identifiers they'll look all math-y." CreationDate="2016-07-11T02:08:31.183" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3656" PostId="3597" Score="0" Text="@Ziya6161 I don't know but Unity3D may have what you need." CreationDate="2016-07-11T09:33:13.603" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3657" PostId="3725" Score="1" Text="Does the array length vary per-pixel?" CreationDate="2016-07-11T13:42:22.097" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="3658" PostId="3746" Score="0" Text="You need only 4 vertices for all cases. `1:-`  You could easily use nested `for loops` to draw the `case 0`. You could get 4 end points by simple maths and then draw lines and triangles between them for `case 1`, `case 2` respectively. `2:-` Or you can have the end points and then divide them up for `case 0`. I would probably do the latter." CreationDate="2016-07-11T19:08:10.910" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3659" PostId="3746" Score="2" Text="Your data is off. You do not account for winding rules." CreationDate="2016-07-11T20:00:48.447" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3660" PostId="3746" Score="0" Text="@ritwiksinha Yeah the thing is I want it to be that many vertices." CreationDate="2016-07-12T07:19:59.593" UserId="2485" ContentLicense="CC BY-SA 3.0" />
  <row Id="3661" PostId="3746" Score="0" Text="@joojaa What do you mean by &quot;data is off&quot;? I know some of the points are not in the right order I changed that, It does not fix the issue." CreationDate="2016-07-12T07:21:42.937" UserId="2485" ContentLicense="CC BY-SA 3.0" />
  <row Id="3662" PostId="3718" Score="0" Text="@Nathan_Reed I applied most of your tips, except instead of doing MSAA which oddly enough did the trick with not to much of a hit at 2x. If I had a question about adding more highlights to to the current effect should I edit the question or make a new one?" CreationDate="2016-07-12T07:31:10.410" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="3663" PostId="3718" Score="0" Text="@J.Doe Great, glad to hear it's working out! I'd say it's better to ask a new question about tweaking the effect if you have a specific look you're going for." CreationDate="2016-07-12T07:36:54.207" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3664" PostId="3746" Score="0" Text="@Käsebrot Why don't you use section formula to calculate the points in between the 4 vertices, namely $(1, 1), (-1, 1), (-1, -1), (1, -1)$" CreationDate="2016-07-12T09:00:10.267" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="3665" PostId="3725" Score="1" Text="How many bits are the integers? 32? 64?" CreationDate="2016-07-12T22:04:20.717" UserId="1781" ContentLicense="CC BY-SA 3.0" />
  <row Id="3666" PostId="3726" Score="0" Text="Since the OP said &quot;array of integers&quot; without qualifying the number of bits, it perhaps seems safe to assume they mean a C-style 'int' (at least 32 bits). Does DDS support 32 bit integer channels?" CreationDate="2016-07-12T22:05:36.857" UserId="1781" ContentLicense="CC BY-SA 3.0" />
  <row Id="3667" PostId="3726" Score="0" Text="Also note that since the OP did not specify that they were Windows-only, DDS may be a bit of a problem on other platforms, where few tools will properly read or write them." CreationDate="2016-07-12T22:06:42.153" UserId="1781" ContentLicense="CC BY-SA 3.0" />
  <row Id="3668" PostId="3750" Score="0" Text="Thank you for your answer. I tried with disabling the depth test, it produces the same result. However, I would like to understand what do you mean by &quot;...drawing with a non-opaque alpha channel.&quot; For opacity I am relying on alpha channel of the texture." CreationDate="2016-07-13T05:00:10.230" UserId="3098" ContentLicense="CC BY-SA 3.0" />
  <row Id="3669" PostId="3750" Score="0" Text="In setting the color of the texture in `transformValueToColor()`, you set the alpha value to `255 * (1 - tValue)`, which means that as long as `tValue` is not 1.0, will be somewhere between 0 and 255. An alpha value that is not 255 will produce a non-opaque (or semi-transparent) pixel - one you can see through. My point was that if the front-most geometry is semi-transparent, you must draw it after the farther away geometry or the depth test will cause the farther away geometry not to ever draw." CreationDate="2016-07-13T05:07:22.190" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="3670" PostId="3750" Score="0" Text="I think I am drawing the furthest quad first and the nearest one last: `model = glm::translate(model, glm::vec3(0.0f, 0.0f, -index*0.003f));`. I think that should have done?" CreationDate="2016-07-13T05:25:50.370" UserId="3098" ContentLicense="CC BY-SA 3.0" />
  <row Id="3671" PostId="3746" Score="0" Text="For doing the solid part, you're better off using something like TRIANGLE_STRIP. You will need topology information in the form of an indice buffer which tells the GPU how to interpret your list of points as a strip. Vertices will need be reused for TRIANGLE_STRIP so the indice buffer is necessary." CreationDate="2016-07-13T06:06:32.553" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3672" PostId="3726" Score="0" Text="According to `MSDN` DDS Format Overview:&#xA;&#xA;`Note that the DDS format supports any valid DXGI_FORMAT value...`&#xA;&#xA;so it support 32 bit integer.&#xA;But yes, DDS may be problem on other platforms." CreationDate="2016-07-13T11:27:12.340" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="3673" PostId="3750" Score="0" Text="It depends on the other matrices, too, though. How are your view and projection matrices set?" CreationDate="2016-07-13T15:57:55.053" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="3674" PostId="3750" Score="0" Text="I have added the details of `projection` and `view` matrix in the question above." CreationDate="2016-07-13T16:31:58.570" UserId="3098" ContentLicense="CC BY-SA 3.0" />
  <row Id="3675" PostId="3750" Score="0" Text="I've added an edit with some additional information to my answer." CreationDate="2016-07-14T04:35:59.063" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="3676" PostId="3725" Score="0" Text="array length vary per-pixel, 32 bit integers." CreationDate="2016-07-14T08:37:29.837" UserId="4700" ContentLicense="CC BY-SA 3.0" />
  <row Id="3677" PostId="3714" Score="1" Text="You can use glGetAttribLocation(&quot;vposition&quot;); also to get the location without needing assignment in the shader. Although be careful with that, if your attributes are never referenced in your shader the entire attribute may be optimised away and you won't be able to get its location." CreationDate="2016-07-14T09:13:19.600" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3679" PostId="3755" Score="0" Text="Does that estimate the distance of the real world object?" CreationDate="2016-07-14T14:13:43.323" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3680" PostId="3755" Score="0" Text="I'm not sure. I was trying to interpret the equation you put up. But I'll give it a try with a ruler when I get home tonight and post the results here." CreationDate="2016-07-14T15:55:02.413" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="3681" PostId="3755" Score="0" Text="This was exactly what I wanted, a conversion system. A straight forward one from the sensor size is not good enough because cameras usually crop part of the image they receive.&#xA;Thankfully though most cameras will have something called pixel size (in microns usually) which is exactly the conversion measure needed for this!" CreationDate="2016-07-14T19:15:28.217" UserId="4735" ContentLicense="CC BY-SA 3.0" />
  <row Id="3682" PostId="3751" Score="0" Text="For your last comment, do you mean that moving average should be used for the simulated accumulation buffer?  Aren't you missing a step where you draw a fullscreen black quad to &quot;erase&quot; the buffer lest it blow out to white?" CreationDate="2016-07-15T06:16:49.030" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="3684" PostId="3751" Score="1" Text="Actually, that should be taken care of by the moving average. If new pixel's value is:&#xA;&#xA;`gl_FragColor = weight * current + (1.0 - weight) * history`&#xA;&#xA;Then note that the contribution to the final image of a particular frame will simply approach zero over time by principle of diminishing weight, thus fading away.&#xA;&#xA;There is no need to clear the RT, then, just overwrite it with each ping-pong." CreationDate="2016-07-15T08:03:35.323" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="3685" PostId="3758" Score="1" Text="You have no doubt read [this](http://stackoverflow.com/questions/540014/compute-the-area-of-intersection-between-a-circle-and-a-triangle) already? Would you be open to a montecarlo of say 16 samples?" CreationDate="2016-07-15T12:21:18.653" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3686" PostId="3759" Score="0" Text="I betcha the size of the arc size can be computed with closest point on line that would make this very neat." CreationDate="2016-07-15T13:36:53.297" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3687" PostId="3757" Score="1" Text="I would write an answer if I had time, but in a concise way: because of definition. Roughly speaking radiance measure OUTGOING light in a certain direction (or better: radiant flux per solid angle). Irradiance is INCOMING light from a certain direction (or better radiant flux received per unit area. BRDF is describing the ratio of *outgoing* light to *incoming* light" CreationDate="2016-07-15T21:06:23.883" UserId="100" ContentLicense="CC BY-SA 3.0" />
  <row Id="3688" PostId="3760" Score="0" Text="Can you expand on what you mean by &quot;complex 3D objects&quot;? How are you representing the objects currently?" CreationDate="2016-07-16T05:26:07.157" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3689" PostId="3759" Score="0" Text="I love the elegance of this approach. Does it need a special case approach when one of the vertices is inside the triangle? It looks like that could count part of the area twice." CreationDate="2016-07-16T12:25:00.307" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3693" PostId="3760" Score="0" Text="Are you dealing only with closed surfaces with holes (such as a bagel or doughnut) or also with holes that lead to the interior of the surface (such as a sphere with a hole leading inside)?" CreationDate="2016-07-17T10:35:59.927" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3694" PostId="3760" Score="0" Text="A given collection of points can be connected into a wide variety of different shape surfaces. Could you explain how these points are produced or how they relate to the intended surface?" CreationDate="2016-07-17T10:51:29.487" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3697" PostId="3759" Score="0" Text="@joojaa This can indeed be computed using the sagitta, and there even is an approximate formular for this (Harris and Stocker 1998). See http://mathworld.wolfram.com/CircularSegment.html" CreationDate="2016-07-17T15:33:36.233" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="3698" PostId="3759" Score="0" Text="@trichoplax Indeed this method would fail if at least one vertex is inside the circle, and I can't guarantee it won't. Still looking for a nice solution there..." CreationDate="2016-07-17T15:34:33.203" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="3699" PostId="3766" Score="0" Text="It prserves vertical lines, that is true. But I am working wit Bézier surface (16 points in a plane) and I believe it must be doable just by moving these 16 points, without computing points on bézier curves etc. Here you can see, how Bézier surface works: http://philipandrews.org/sandbox/BezierSurface/bin/BezierSurface.swf" CreationDate="2016-07-19T15:02:23.283" UserId="4761" ContentLicense="CC BY-SA 3.0" />
  <row Id="3700" PostId="3766" Score="0" Text="I think I found solution to that problem if I understood is correctly. You are asking how to deform bezier surface A to bezier surface B or C to D with d parameter being 0.8, is it correct?" CreationDate="2016-07-19T15:22:10.283" UserId="4524" ContentLicense="CC BY-SA 3.0" />
  <row Id="3701" PostId="3766" Score="0" Text="Well it seems that is not the exact formula but quite close. I will ponder on this a little more. Formula is correct at least for on-the-curve-points." CreationDate="2016-07-19T15:32:48.593" UserId="4524" ContentLicense="CC BY-SA 3.0" />
  <row Id="3702" PostId="3766" Score="0" Text="You are getting closer :) But for the second example, the Y coordinate changes, too.  As I mentioned, all points move along the lines, so it is enough to find the new position for d = 1 for each point, then I can interpolate linearly." CreationDate="2016-07-19T15:33:24.697" UserId="4761" ContentLicense="CC BY-SA 3.0" />
  <row Id="3703" PostId="3766" Score="0" Text="I have added another picture, it may help you." CreationDate="2016-07-19T15:46:00.767" UserId="4761" ContentLicense="CC BY-SA 3.0" />
  <row Id="3706" PostId="3767" Score="0" Text="See code above. We are not multiplying by world first. Even if you have a matrix stack it's always projection first if you were to put it in the stack. Maybe in your mind we are but in code we are not. If you were to write `a = b + c + d` no one would describe that as adding `d` first" CreationDate="2016-07-20T03:43:47.190" UserId="4766" ContentLicense="CC BY-SA 3.0" />
  <row Id="3707" PostId="3767" Score="3" Text="Personally I would prefer more explicit naming conventions. For example:   &#xA;`worldToScreen = worldToView * viewToScreen` , or   &#xA;`screenPosition = worldToScreen * worldPosition` ..." CreationDate="2016-07-20T05:44:40.087" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="3709" PostId="3768" Score="0" Text="To add, this is usually seen when the light direction is near parallel to the surface (like a sunset scene). When the light direction is perpendicular the specular highlight becomes much more circular. I would check your normal calculation in color(..), it looks like it is producing a normal that isn't perpendicular to your plane. Something like {0,0,-1} should be correct and facing towards your {0,0,1} light, not {pos.x,pos.y,1} like it currently is" CreationDate="2016-07-20T07:01:38.030" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3711" PostId="3766" Score="0" Text="Oh, you basically solved it, as soon as I got time I will formulate it for you." CreationDate="2016-07-20T07:55:41.473" UserId="4524" ContentLicense="CC BY-SA 3.0" />
  <row Id="3712" PostId="3767" Score="2" Text="Can we just blame this on the fact we multiply right-to-left while reading from left-to-right ?" CreationDate="2016-07-20T07:58:47.347" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3713" PostId="3766" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/42753/discussion-between-ivan-kuckir-and-cem-kalyoncu)." CreationDate="2016-07-20T09:22:11.523" UserId="4761" ContentLicense="CC BY-SA 3.0" />
  <row Id="3714" PostId="3773" Score="0" Text="Thanks for your reply. I try this but I only have the ambiant color" CreationDate="2016-07-20T09:52:53.937" UserId="3328" ContentLicense="CC BY-SA 3.0" />
  <row Id="3715" PostId="3773" Score="0" Text="Maybe flip the sign. I'm not sure what your intention for the normals is, but for a plane every pixel should have the same normal." CreationDate="2016-07-20T09:55:40.323" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3716" PostId="3773" Score="0" Text="Yes I try -1 and then 1. My purpose is to have a circular specular on the surface plane" CreationDate="2016-07-20T09:57:18.407" UserId="3328" ContentLicense="CC BY-SA 3.0" />
  <row Id="3717" PostId="3773" Score="0" Text="The half-vector takes care of this normally, this will vary across your image. It looks like you are close to what you are intending, just something is on the wrong axis, at a guess. I encountered this problem and had similar results several times, was almost always down to incorrectly working out the eye-space/half-vector." CreationDate="2016-07-20T10:06:16.520" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3718" PostId="3771" Score="2" Text="Its called frustum culling. You can accelerate this with spatial querries like bspor occtrees for example." CreationDate="2016-07-20T10:13:01.630" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3719" PostId="3773" Score="0" Text="Looking a bit more at your code example, the L vector calculation looks wrong. Should that not just be the world-space light vector?(Lum). Your lambert calculation would be something like dot( N={0,0,-1}, L={0,0,1} ) for the whole surface, which should return an intensity of 1 for the diffuse component." CreationDate="2016-07-20T10:17:42.097" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3721" PostId="3775" Score="1" Text="@trichoplax I removed the pic and added a GIF. Thanks." CreationDate="2016-07-20T11:30:25.487" UserId="4770" ContentLicense="CC BY-SA 3.0" />
  <row Id="3729" PostId="3766" Score="0" Text="To your new formlas: so p(k) is always zero? You suggest moving along X the  same distance, as moving along Y, it can not work. You also suggest treating control points the same way, as corner points, which is wrong. It seems like you are just throwing at me what comes into your head. I doubt you can come up with the right formula without acutally implementing it and &quot;seeing the thing&quot;." CreationDate="2016-07-20T22:55:30.110" UserId="4761" ContentLicense="CC BY-SA 3.0" />
  <row Id="3733" PostId="3780" Score="0" Text="Normally you would use YUV semi-planar for this, which is a seperate single channel Y texture and a packed UV channel.  You could also unpack inside a fragment shader by testing if the Y texel you are reading is on a odd or even column. So say you read Y0,U for the first pixel (Assuming a 2 channel texture), you test if Y0 is on a even-column, which means you need to read the texel to right to get the missing V. For odd-columns you would read the texel to the left to get the missing U." CreationDate="2016-07-21T04:36:26.513" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3734" PostId="3650" Score="0" Text="Update: it actually had something to do with that, although it was a minor fault." CreationDate="2016-07-21T07:28:24.947" UserId="3472" ContentLicense="CC BY-SA 3.0" />
  <row Id="3735" PostId="3780" Score="0" Text="Just did a little googling, you want NV12 pixel format in Microsoft Media Foundation if you want the semi-planar format. Should be the fastest format for doing colour space conversion on the GPU." CreationDate="2016-07-21T09:04:57.653" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3743" PostId="3775" Score="0" Text="Looks like a palette cycling effect. The image itself is static." CreationDate="2016-07-20T13:19:57.157" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3745" PostId="3751" Score="1" Text="Isn't this the same as Alpha blending?" CreationDate="2016-07-22T06:38:39.143" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3747" PostId="3751" Score="0" Text="As long as you don't need to accumulate alpha, it is so indeed, since you can put weight in the alpha channel." CreationDate="2016-07-22T12:30:30.760" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="3748" PostId="3780" Score="0" Text="@PaulHK The camera only supports YUY2 and MJPG formats. I tried unpacking in the shader - the results were close to correct but there were vertical lines. I assume this is because I am incorrectly fetching the next pixel: `texture(tex, vec2(Texcoord.x + 1, Texcoord.y));` I'm guessing this is due to scaling." CreationDate="2016-07-22T19:57:25.430" UserId="4497" ContentLicense="CC BY-SA 3.0" />
  <row Id="3749" PostId="3757" Score="0" Text="The short answer is: _&quot;because then it wouldn't be bidirectional&quot;_. It's been a while, but I believe my [alternate formulation](http://geometrian.com/programming/tutorials/clearer_re/index.php) of the rendering equation works out to be using a 1:1 reflectance function." CreationDate="2016-07-22T21:13:54.303" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="3754" PostId="3755" Score="0" Text="Focal length is also affected by your camera focus. so in the course of shooting the focal length varies slightly because that is how the focus is achieved, by moving the elements. This is why you use lots of measurements and a ransac on the image to estimae the actual values better. So not only is your effective value different from spec, its diffenet in each camera and in each shot. Also your ruler may not be perfectly aligned to camera and the camera.lense not being a perfect pinhole etc." CreationDate="2016-07-24T19:44:46.350" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3756" PostId="3780" Score="0" Text="It might be simpler to deinterleave it into 2 buffers on the CPU ( Y &amp; CrCb ), then the GPU can do CSC." CreationDate="2016-07-25T02:25:10.113" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3758" PostId="3652" Score="1" Text="The original paper on bilateral filtering, _&quot;Bilateral Filtering for Gray and Color Images&quot;_, by Tomasi and Manduchi: https://users.cs.duke.edu/~tomasi/papers/tomasi/tomasiIccv98.pdf" CreationDate="2016-07-26T05:30:40.837" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3759" PostId="3791" Score="1" Text="For something more interesting you can animate the origin of radial shapes too. -&gt; float d = sin(length(uv - vec2(0.5)) * 35.0) + sin(length(uv - vec2(0.2+sin(iGlobalTime),0.3)) * 45.0);" CreationDate="2016-07-26T08:21:40.353" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3760" PostId="3785" Score="0" Text="I am not sure I understand it correctly, your idea is to compute screen space positions via vertex shader, save it to some texture, and then read the texture and make computations on CPU or via GPGPU if possible? computing everything on GPGPU and sending it to OpenGL would be better?  Currently, I am trying to figure out the architecture and frameworks to use, not the algorithm to use, but the part with SIMD approach is interesting, thank you for that." CreationDate="2016-07-26T08:39:52.783" UserId="4765" ContentLicense="CC BY-SA 3.0" />
  <row Id="3761" PostId="3755" Score="0" Text="Maybe you can perform several experiments on your ruler to calibrate the parameters of your camera. Or just use samples to estimate parameters directly by regression." CreationDate="2016-07-26T16:29:52.933" UserId="120" ContentLicense="CC BY-SA 3.0" />
  <row Id="3766" PostId="3786" Score="1" Text="Yes, I've decided it's best to unpack the data on the CPU (a task that lends itself well to multi-threading), then do color conversion in a shader." CreationDate="2016-07-27T14:20:20.693" UserId="4497" ContentLicense="CC BY-SA 3.0" />
  <row Id="3767" PostId="3785" Score="0" Text="The example alg I gave could be performed entirely within a single OpenCL kernel or OpenGL compute shader. OpenCL can directly share data with OpenGL via OpenGL/OpenCL interop, while OpenGL can share data among its shaders. (Data = buffer or texture.) The CPU need never be involved, though you may find using OpenCL on CPU faster if you have too few objects on screen to overcome bridge overhead." CreationDate="2016-07-27T19:22:14.983" UserId="58" ContentLicense="CC BY-SA 3.0" />
  <row Id="3768" PostId="3785" Score="0" Text="Updated my answer with a graphics pipeline example." CreationDate="2016-07-27T20:24:27.957" UserId="58" ContentLicense="CC BY-SA 3.0" />
  <row Id="3769" PostId="3795" Score="2" Text="Why you want to use GPGPU for drawing million cubes in first place?" CreationDate="2016-07-28T06:39:57.177" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="3770" PostId="3773" Score="0" Text="I edited my post. I change the way to compute L and E vectors and I get some inverse specular lobe" CreationDate="2016-07-28T10:12:28.170" UserId="3328" ContentLicense="CC BY-SA 3.0" />
  <row Id="3771" PostId="3795" Score="3" Text="Compute shaders are used for compute workloads, not rendering.&#xA;&#xA;The rule of thumb is that if you need rasterization (i.e. processing of triangulated geometry into pixels), you should be using the rendering pipeline; if you simply need to process a large piece of data, you should be using compute.&#xA;&#xA;I'm also interested in sound arguments for and against compute shaders and CUDA/OpenCL (with graphics API interop). One that I've heard of is better queueing of compute workloads with the compute-specific API, but I'd like to know more (i.e. how does async compute come into the picture)." CreationDate="2016-07-28T10:20:26.327" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="3772" PostId="3795" Score="0" Text="@Derag Just trying to feed my cube fetish as fast as possible" CreationDate="2016-07-28T13:44:09.133" UserId="4820" ContentLicense="CC BY-SA 3.0" />
  <row Id="3774" PostId="3768" Score="0" Text="It appears that the new images that have replaced the old ones show a new and unrelated problem. Would this be better posted as a new question, so that the old problem is still here for future readers? There is some [discussion on meta](http://meta.computergraphics.stackexchange.com/a/182/231) about this general point." CreationDate="2016-07-29T10:48:23.123" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3776" PostId="3799" Score="0" Text="For reference: Previously asked 2 hours earlier [on an external site](http://www.gamedev.net/topic/680892-does-anyone-know-what-el-means-in-the-context-of-shading-equations-from-the-book-real-time-rendering/)" CreationDate="2016-07-29T13:20:42.510" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3777" PostId="3799" Score="0" Text="I've edited the title to include MathJax to show the subscript. If anyone finds this causes a problem please contribute to the [discussion on meta](http://meta.computergraphics.stackexchange.com/questions/238/should-we-use-mathjax-latex-in-question-titles)." CreationDate="2016-07-29T13:33:51.560" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3778" PostId="3797" Score="0" Text="It can bypass the OS / driver" CreationDate="2016-07-29T15:36:48.320" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="3779" PostId="3797" Score="0" Text="@RichieSams: Interesting; does that mean in principle someone could achieve much the same result by accessing a dedicated PCIe SSD over the system bus?  (I don't know to what extent main system PCIe traffic needs OS support for devices to do any communication, or whether they can do it more autonomously once set up)." CreationDate="2016-07-29T15:55:55.073" UserId="4824" ContentLicense="CC BY-SA 3.0" />
  <row Id="3780" PostId="3801" Score="1" Text="Should the colors be equally spaced in terms of t? Or do you need some more general solution for other distributions also?" CreationDate="2016-07-29T23:07:52.117" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="3781" PostId="3801" Score="0" Text="Do you require the same colour value for t=0 and t=1? That is, must the colour change wrap smoothly at the end points?" CreationDate="2016-07-30T13:50:41.707" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3782" PostId="3804" Score="1" Text="The final image appears to be using subpixel rendering (note the coloured fringes to the left and right are different colours). Do you want to use this approach (which requires knowing the pixel geometry of the particular screen you are rendering to) or do you just want the best approach for pixel rendering, ignoring subpixels (which can be displayed on any screen)?" CreationDate="2016-07-30T21:48:56.747" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3783" PostId="3804" Score="1" Text="@trichoplax I think for right now I'd be happy with the best approach for pixel rendering, ignoring subpixels (which sounds more complicated). One step at a time. :)" CreationDate="2016-07-30T22:11:29.727" UserId="2507" ContentLicense="CC BY-SA 3.0" />
  <row Id="3784" PostId="3804" Score="0" Text="That sounds like a good approach - I just wanted to have it stated explicitly so answerers don't get distracted into explaining the last image." CreationDate="2016-07-30T22:30:51.197" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3785" PostId="3803" Score="0" Text="[Here is a link to the project in github](https://github.com/amdreallyfast/render_particles_2D_basic_GPU/).  It includes all necessary 3rd party libraries and header files, so it should simply build and run after downloading. &#xA;&#xA;Note: This was built in VS2015 express, but I built it such that it should not need any project-specific setup.  All linking is performed in the source file (should never be done for release code, but this is a barebones example program), so a non-VS2015 setup shouldn't be difficult." CreationDate="2016-07-30T22:38:05.777" UserId="4838" ContentLicense="CC BY-SA 3.0" />
  <row Id="3786" PostId="3804" Score="0" Text="Can you explain what you mean by bilinear filtering in this context? I don't understand how that concept applies to rasterizing the glyph shapes. Also, when you tried &quot;setting the alpha value at the edges according to the pixel coverage of the outline&quot;, how did you accomplish that? Supersampled rasterization?" CreationDate="2016-07-30T22:39:33.053" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3787" PostId="3804" Score="1" Text="@NathanReed Sure. For bilinear filtering, I sweep over the raw bitmap and for each pixel, look at the three pixels surrounding it (the one to the right, the one below, and the one diagonally down and to the right) and average them together to come up with the final intensity value. Pixels off the edge of the bitmap are treated as 0." CreationDate="2016-07-31T00:07:49.193" UserId="2507" ContentLicense="CC BY-SA 3.0" />
  <row Id="3788" PostId="3804" Score="0" Text="@NathanReed For the pixel coverage, I use the intersection point and the pixel coordinate to determine how much of the pixel is &quot;covered&quot;. i.e. Let's say we're at pixel position (4, 2). The intersection at this location with the curve or line is (4.6, 2.3). `xf = 4.6 - 4`, `yf = 2.3 - 2`, so pixel coverage is then: `coverage = (xf + yf) / 2.0;` This might be a naive approach, but it was worth a try. :)" CreationDate="2016-07-31T00:12:09.983" UserId="2507" ContentLicense="CC BY-SA 3.0" />
  <row Id="3789" PostId="3803" Score="0" Text="is uDeltaTimeSec none-zero? If you are drawing something then we know you are outputting _position correctly. So I would suspect some how the delta is 0" CreationDate="2016-07-31T10:25:02.683" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3790" PostId="3805" Score="0" Text="Why would scaling the billboard quad not mimic the appearance of scaling the asteroid mesh? If they're far away enough to use billboards in the first place, that should work fine, unless I'm missing something." CreationDate="2016-07-31T16:37:09.927" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="3791" PostId="3804" Score="0" Text="Every os has its own way of dealing with this. Windows uses hinting while OSX does not for example." CreationDate="2016-07-31T18:05:05.643" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3792" PostId="3804" Score="0" Text="@joojaa Hinting has to do with adjusting the glyph outlines when scaled down to a small size to best maintain the font's characteristics while ensuring readability. Hinting doesn't have anything to do with anti-aliasing or font smoothing directly does it? Because that's handled during scan conversion." CreationDate="2016-07-31T19:36:09.697" UserId="2507" ContentLicense="CC BY-SA 3.0" />
  <row Id="3793" PostId="3805" Score="0" Text="Are you concerned about scaling up a billboard to the point that it requires a different level of detail?" CreationDate="2016-07-31T19:56:34.817" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3794" PostId="3806" Score="1" Text="This is great, thank you! I have looked at the source of stb_truetype, but my knowledge in computer graphics isn't thorough enough to really know what was happening on a theoretical level. I was not aware of the article he posted on it though, so that's a great help and will help clarify the process he uses, I'm sure. I'm glad to hear that maybe I was on the right track with the pixel coverage method, but I just wasn't taking into account the vertical part. :)" CreationDate="2016-08-01T03:38:26.250" UserId="2507" ContentLicense="CC BY-SA 3.0" />
  <row Id="3795" PostId="3773" Score="0" Text="Getting close, maybe negate the half vector ?" CreationDate="2016-08-01T06:41:57.193" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3796" PostId="3808" Score="1" Text="Could you explain the difference in what you are looking for here, compared to your [previous question](http://computergraphics.stackexchange.com/q/2271/231)? Is this a different approach?" CreationDate="2016-08-01T13:27:21.677" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3797" PostId="3810" Score="0" Text="Would sampling half pixels give equivalent results to generating an image at double resolution, producing lines 2 pixels wide, and then scaling the image down to the intended size?" CreationDate="2016-08-01T16:01:03.243" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3798" PostId="3810" Score="1" Text="Couldn't I just use the sign of the dot-product (i.e. gx or gy) in any way? For example, when sampling a horizontal edge, i get -4 for gx on one side of the edge and 4 on the other side." CreationDate="2016-08-01T18:49:32.863" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3799" PostId="3801" Score="0" Text="@Dragonseel.. I think i can make them to be equally spaced out.. but I would definitely be interested in hearing about a more general solution as well.." CreationDate="2016-08-01T18:58:23.330" UserId="4830" ContentLicense="CC BY-SA 3.0" />
  <row Id="3800" PostId="3801" Score="0" Text="@trichoplax.. i don't really need them to be wrapped around.. would it be different than having the same colors at the end points of the array?" CreationDate="2016-08-01T18:59:38.477" UserId="4830" ContentLicense="CC BY-SA 3.0" />
  <row Id="3801" PostId="3801" Score="0" Text="@lokstok good point - I think that covers it so it shouldn't make a difference to the solution." CreationDate="2016-08-01T19:08:31.807" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3803" PostId="3803" Score="0" Text="@PaulHK I am sending a non-zero value by glUniform1fv, so it should be." CreationDate="2016-08-01T23:42:00.530" UserId="4838" ContentLicense="CC BY-SA 3.0" />
  <row Id="3804" PostId="3814" Score="0" Text="Isn't it a bit restrictive? I mean, IMO, the texture is not the best-suited data storage for passing data in general (but I am new to computer graphics and maybe I have to get used to it)." CreationDate="2016-08-02T06:28:33.867" UserId="4765" ContentLicense="CC BY-SA 3.0" />
  <row Id="3805" PostId="3814" Score="2" Text="A texture is just a normal buffer, although in the fragment shader we use fancy sampler2D to read it with filtering/etc. You can use a compute shader to read it more in its raw form and skip the texture sampler stage." CreationDate="2016-08-02T06:30:21.073" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3806" PostId="3797" Score="0" Text="@timday partially but there would still be overhead as you would need to wait for your turn form the graphic contriollers" CreationDate="2016-08-02T07:45:03.430" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3807" PostId="3794" Score="0" Text="I've made some minor edits, and edited the code to match. Hopefully this leaves the answer as you intended it but feel free to revert anything that doesn't." CreationDate="2016-08-02T12:47:41.790" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="3809" PostId="3810" Score="0" Text="Just one more question: Would morphological erosion (http://homepages.inf.ed.ac.uk/rbf/HIPR2/erode.htm) or thinnening (http://homepages.inf.ed.ac.uk/rbf/HIPR2/thin.htm) be an alternative?" CreationDate="2016-08-02T22:46:13.750" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3810" PostId="3810" Score="0" Text="@enne87 why not skip edge detection alltogether and use edge geometry to draw the lines?" CreationDate="2016-08-03T01:35:57.573" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3811" PostId="2271" Score="0" Text="I'm not sure Moore's law applies to GPU, you can keep adding cores and using wider buses." CreationDate="2016-08-03T04:43:36.190" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="3813" PostId="3808" Score="0" Text="@trichoplax, thanks for your comment. It is same. I'm trying to implement VR in Cloud rendering. But I have to decide which way. Rendering in single server would be easier than distributed parallel rendering, but there are still 2 ways to achieve that: rely on SLI driver or use equalizer. It isn't flexible to do all things in one server, in terms of resource sharing and hardware sourcing. If we can achieve distributed parallel rendering, maybe we can choose PowerVR or Mali GPU, not NVIDIA/AMD. I know concerns on network latency, which we have solutions to handle. So here I don't discuss that." CreationDate="2016-08-03T06:10:46.907" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="3814" PostId="3810" Score="0" Text="@enne87: yes I think erosion could be a possibility, but I didn't suggest it because I'm not familiar enough with it so I don't know how well suited it is." CreationDate="2016-08-03T06:14:15.107" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="3815" PostId="3811" Score="0" Text="thanks for your answer. I have read this article before. Do you mean distributed parallel rendering will introduce much latency? I just want to lower down the rendering latency." CreationDate="2016-08-03T06:18:12.910" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="3816" PostId="3810" Score="0" Text="Thank you both! @joojaa: I thought about it but how would I draw the outline of a sphere with geometrical data?" CreationDate="2016-08-03T08:55:50.733" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3817" PostId="3810" Score="0" Text="@enne87 you draw the edges on the culling treshold and hard edges slightly offset towards the camera. A sphere is in fact one of the easier shapes." CreationDate="2016-08-03T09:10:08.330" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3818" PostId="3811" Score="0" Text="Yes. Distributed rendering has the inherent cost of integrating results from all the distributed nodes. In this case, I imagine serializing a scene, sending it out to computation nodes and then collecting parts of the resulting image (frame buffer tiles probably?). It seems unlikely to me that at current I/O speeds you can fit all of that + the rendering itself under ~10 ms. What I would expect from such a scenario is that you could probably get silky-smooth, high-performance rendering, but lagging a consistent dozen frames behind the player input, therefore somewhat defeating the purpose." CreationDate="2016-08-03T10:29:38.290" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="3819" PostId="3810" Score="0" Text="@joojaa: Thanks for your suggestion. Just to make clear that I understand your idea: &#xA;&#xA;1. For the silhouette I render the edges of the backfaces and move the faces in screen Z forward so that only the edges of the backfacing triangles are visible, right?&#xA;&#xA;2. Sorry but I don't understand how I would draw the inner edges (creases and ridges) ?" CreationDate="2016-08-03T10:30:47.260" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="3820" PostId="3810" Score="0" Text="@enne87 edges on the front backface boundary prefeerably. You draw them as gl lines." CreationDate="2016-08-03T11:01:01.980" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="3822" PostId="3815" Score="0" Text="Yes, normal at witness point P must have same direction as vector S-P (where S is slave point, or red one on the picture above), but surface can have several such points (If it's folded, and point is inside), and we will have to choose closest from ones with &quot;good&quot; normal. Anyway thanks for answer." CreationDate="2016-08-03T13:33:50.740" UserId="2644" ContentLicense="CC BY-SA 3.0" />
  <row Id="3823" PostId="3811" Score="0" Text="Sort-last may have this issue as what you said, but how about sort-first? Use 2d of sort-first, it seems that few interactions are introduced  between rendering worker. According to Equalizer evaluation result, it consumes little network bandwidth: http://eyescale.github.io/equalizergraphics.com/scalability/2D.html. Thanks" CreationDate="2016-08-04T00:29:23.813" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="4823" PostId="2271" Score="0" Text="@PaulHK, the main issue is power and thermal. It's very hard to make a very big chip, especially for mobile system." CreationDate="2016-08-04T03:14:42.310" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="4824" PostId="3811" Score="0" Text="But I never even mentioned interaction between workers. :) It's entirely possible to eliminate it altogether. By synchronisation I meant dispatching the workload to nodes and then compositing the results together. Also, &quot;little&quot; network bandwidth is a pretty imprecise and relative term – I expect it not to be &quot;little&quot; on the scale that VR applications operate in. It's quite simple, really – if you can make a round trip within 10 ms, you're fine. Experience simply tells me that you can't. You are welcome to prove me wrong. :)" CreationDate="2016-08-04T07:19:26.580" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="4837" PostId="3808" Score="0" Text="I get the feeling what you want/need is an open minded discussion, not a definite answer." CreationDate="2016-08-05T17:13:59.430" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="4838" PostId="3814" Score="0" Text="There's a few options. Render-to-texture is probably the most straightforward but also kind of wasteful unless you need the results of your calculations in screen-space (like in deferred rendering), since you're rasterizing geometry before you can do any calculation. There's transform feedback, where you pull the output of a vertex or geometry shader before it hits the rasterizer and write it back to a buffer instead, might be good for the use case you mentioned but I haven't tried it myself. Otherwise compute shaders are the way to go, they're not that scary once you get used to them ;-)" CreationDate="2016-08-05T17:38:24.513" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="4839" PostId="3795" Score="0" Text="If you want to draw a million cubes, use ray-marching :)" CreationDate="2016-08-05T17:53:23.497" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="4840" PostId="3808" Score="0" Text="Keep in mind that transfer of information between computers isn't free.  For instance, even getting data from RAM to the GPU is a pretty costly operation, let alone from another machine's RAM, even on the local network.  Transferring data between GPUs is also not free.  VR requires very low latency frames (not just high frame rate), and what you are proposing sounds like it would add quite a bit of latency and may not be practical due to that." CreationDate="2016-08-05T18:09:58.597" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="4841" PostId="3823" Score="1" Text="I'm not sure, but if you scan a document multiple times with slightly different tiny offsets, resulting in slightly different scans of the same document, you should be able to use that information to do Monte Carlo style sampling and get a better result of your scans.&#xA;&#xA;If you don't get any good answers here, another venue that is likely to be able to help you is the digital signal processing stack exchange: http://dsp.stackexchange.com/" CreationDate="2016-08-05T18:12:32.563" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="4842" PostId="3823" Score="1" Text="Could you add more detail about the characteristics of the documents being scanned? At present it's difficult to judge whether this is on topic. For example, if the documents have arbitrary sized fonts and images, the task will be different than if the documents have a single small font size with no images." CreationDate="2016-08-05T20:04:50.620" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4843" PostId="3825" Score="0" Text="Your question header seems to ask a different question than the body of your question. So which is it? How to sample points or how many do you need to sample?" CreationDate="2016-08-06T16:13:07.167" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4844" PostId="3760" Score="0" Text="Would using some algorithm like [marching cubes](https://en.wikipedia.org/wiki/Marching_cubes) over a density field work for you? You could populate a density field with contributions from the points and then sample from that." CreationDate="2016-08-06T22:10:58.320" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="4845" PostId="165" Score="0" Text="I personally would guess that the garish result is taking the subpixel samples further apart than the nice result, which means that there's a more dramatic difference in coverage and therefore brightness." CreationDate="2016-08-06T22:23:25.930" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="4846" PostId="3825" Score="0" Text="@joojaa Sorry for my unclear description. My question originates from this [question](http://mathematica.stackexchange.com/questions/122056/specialized-sampling-techniques-of-built-in-bsplinesurface). For example, in Mathematica, I could use `ParametricPlot3D[f[x, y], {x, 0, 1}, {y, 0, 1}]` to draw a B-spline surface." CreationDate="2016-08-07T01:51:08.737" UserId="1796" ContentLicense="CC BY-SA 3.0" />
  <row Id="4847" PostId="1720" Score="0" Text="What software did you use in the screenshot? THX:)" CreationDate="2016-08-07T02:50:31.993" UserId="1796" ContentLicense="CC BY-SA 3.0" />
  <row Id="4848" PostId="3827" Score="0" Text="Are you talking specifically about OpenGL? Specifically about WebGL? Is instancing available for you?" CreationDate="2016-08-07T08:06:15.780" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="4849" PostId="3823" Score="1" Text="Can you even add an example of such artifact so we can have a look at it?" CreationDate="2016-08-07T12:06:21.213" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="4850" PostId="3827" Score="1" Text="Good question, let me clarify that. I'm not particularly interested in a specific graphics API, although I do want the work done on the GPU." CreationDate="2016-08-07T14:26:07.770" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="4851" PostId="3825" Score="0" Text="As joojaa points out, the question in your title is different from the rest of your question. Are you looking to only sample uniformly, and adjust only the size of the sample, or are you also interested in sampling in different ways, that may reduce the number of points required?" CreationDate="2016-08-07T16:11:39.040" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4853" PostId="3808" Score="0" Text="Ok, get it. We still want to try. At last, we have to try multi gpus in one server. Thanks." CreationDate="2016-08-08T00:41:02.663" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="4854" PostId="3825" Score="0" Text="@trichoplax Obviously, the *uniformly sample* is **not** a good method. My confusion is whether exist a **specialized algorithm** about sampling 3D-points and visualization for the B-spline surface. Please see this [screenshots](http://i.stack.imgur.com/BmCqn.png)" CreationDate="2016-08-08T01:15:01.013" UserId="1796" ContentLicense="CC BY-SA 3.0" />
  <row Id="4855" PostId="3829" Score="0" Text="Could point sprites be used in place of 2x triangle vertices? The instance data could be a vec2 to define both the orientation and the size. Although you might need another float in there for rectangular sprites." CreationDate="2016-08-08T03:00:39.677" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4856" PostId="3829" Score="1" Text="@PaulHK Point sprites are very limited—they sometimes have a small maximum size depending on GPU, and they don't support rotation or any non-square shapes. Also, they get clipped based on their center point, so they pop out too soon when they move off the edge of the screen. I would not bother with them." CreationDate="2016-08-08T03:32:17.280" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="4857" PostId="3829" Score="0" Text="Yeah I thought they would be a little inflexible, you could rotate in the frag shader but then you need to over-project the size to fit the diagonal.." CreationDate="2016-08-08T03:51:36.223" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4858" PostId="3824" Score="1" Text="Your ASCII diagram is cool" CreationDate="2016-08-08T04:23:50.967" UserId="3386" ContentLicense="CC BY-SA 3.0" />
  <row Id="4859" PostId="3831" Score="2" Text="Most software define chord height as a subdivision criteria along with angle error (which is what you describe... ) . Reason being  that usually in engineering solutions the tolerance is usually for deflection from true surface not how well the curvature is preserved. But there is many ways to skim the fish depenzing on what your needs are. Simple UV division can be bad for multipatched /  trimmed surfaces." CreationDate="2016-08-08T04:29:27.650" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4860" PostId="3832" Score="0" Text="I've [updated the diagram](http://geometrian.com/programming/tutorials/gltextureterm/index.php); can you check it? Also, perhaps you could elaborate on the &quot;context&quot; for &quot;mipmap chain&quot;?" CreationDate="2016-08-08T05:56:02.177" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="4861" PostId="3823" Score="0" Text="The problem is that i cannot access the physical document, I only have access to the output of the scan. &#xA;Most of the documents are text or ID Card.&#xA;I cannot had an example because all of the documents that I have access are confidencial." CreationDate="2016-08-08T07:48:40.930" UserId="4866" ContentLicense="CC BY-SA 3.0" />
  <row Id="4862" PostId="3829" Score="0" Text="I've seen it commented before that instancing can be slow if you're dealing with very small meshes (like a single pair of triangles). Can you comment on that at all?" CreationDate="2016-08-08T19:14:12.373" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="4863" PostId="3829" Score="0" Text="@porglezomp Hmm, I haven't heard that before, and in fact my experience is the opposite: instancing is a _more_ efficient way to draw small meshes, as it allows the GPU to pack verts from different instances together in each warp/wave. AFAIK, verts from different draws can't be packed together that way." CreationDate="2016-08-08T19:19:12.977" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="4864" PostId="3834" Score="0" Text="Did you try the smooth conductor?" CreationDate="2016-08-08T19:46:47.540" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4865" PostId="3829" Score="0" Text="@NathanReed I didn't mean as opposed to individual draw calls, just in contrast with larger meshes. If you've had success using instancing on such small meshes, then it seems like it'd be worth trying since it's a really clean solution." CreationDate="2016-08-08T20:53:01.457" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="4866" PostId="3832" Score="0" Text="Yeah, so by context, i mean that the 'mipmap chain' could refer just to the image pyramid for a single layer-face, or for all faces in a layer, or for the whole thing.  Which is meant will depend on what is being discussed.&#xA;&#xA;So, for the purposes of your ASCII chart, where you have &quot;mipmap / mipmap chain&quot; grouped, I'd qualify that as the &quot;layer-face mipmap chain&quot;" CreationDate="2016-08-08T22:29:54.770" UserId="3386" ContentLicense="CC BY-SA 3.0" />
  <row Id="4867" PostId="3835" Score="0" Text="Already tried that case unfortunately.. it's the sphere on the image you can find here: http://imgur.com/a/2unOE . Which seems quite metal-ish but not really a mirror unfortunately" CreationDate="2016-08-08T23:02:33.933" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="4868" PostId="3835" Score="0" Text="Hm. Weird. What kind of integrator are you using to render this image? Maybe share your hole scene xml file so I can try to reproduce it?" CreationDate="2016-08-08T23:22:30.820" UserId="1930" ContentLicense="CC BY-SA 3.0" />
  <row Id="4869" PostId="3835" Score="0" Text="I am using &lt;integrator type=&quot;path&quot;/&gt; . I downloaded a scene from the website of the Mitsuba renderer itself, implementing a simple cornell box. I just added there my simple sphere. You can find the whole XML here (https://www.dropbox.com/s/qzjuzyg3bont7od/cbox.xml?dl=0) and at the end of the file my simple added sphere.. thanks in advance!" CreationDate="2016-08-08T23:29:55.183" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="4870" PostId="3835" Score="0" Text="Here is a rendering using your scene: https://www.dropbox.com/s/k1s825tkus2s4yk/tarta.png?dl=0&#xA;Looks good to me.&#xA;Did you render the scene? Your image looks a lot like the OpenGL preview at the start..." CreationDate="2016-08-08T23:41:32.327" UserId="1930" ContentLicense="CC BY-SA 3.0" />
  <row Id="4871" PostId="3835" Score="0" Text="I am ashamed..you are totally right. I thought it was already the render itself, while it was just a preview. I am sorry.." CreationDate="2016-08-08T23:44:47.603" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="4872" PostId="3835" Score="0" Text="No worries. Glad to help :)" CreationDate="2016-08-08T23:45:26.873" UserId="1930" ContentLicense="CC BY-SA 3.0" />
  <row Id="4873" PostId="3835" Score="0" Text="thanks once mroe! One last question: I don't know if you are familiar with the GDPT algorithm that Mitsuba renders.. but I was trying to understand if it is realtime and I was basing my judgement on the preview.. thinking wrongly that it was real-time. Then you made me realise that I have actually to render the image, and the real-time thing seems going away. Do you know by any chance anything about this?" CreationDate="2016-08-08T23:47:38.850" UserId="3069" ContentLicense="CC BY-SA 3.0" />
  <row Id="4874" PostId="3835" Score="0" Text="No sorry, I'm not familiar with GDPT at all.. didn't even know this was implemented in Mitsuba." CreationDate="2016-08-09T00:11:34.623" UserId="1930" ContentLicense="CC BY-SA 3.0" />
  <row Id="4878" PostId="3841" Score="0" Text="Thanks Nathan!  I'm attempting to simulate a LIDAR scanner, so that's the reason for the fan beam and the equal angle spacing.  The particular scanner I'm modeling my simulation after has its lasers in the fan beam configuration external to the housing, but as the returns pass in they are diverted from a fan beam into parallel beams (by way of a lens).  I'm trying to find a way to maintain this model so as to make the simulation as accurate as possible.  Any thoughts as to how I could do this?" CreationDate="2016-08-10T15:19:47.513" UserId="4886" ContentLicense="CC BY-SA 3.0" />
  <row Id="4879" PostId="3845" Score="0" Text="Here is a YouTube video also explaining that the stored value in an image is the 1/2.2 (but he simplifies it and just say that it's the square root, there is a disclaimer of that in text.)&#xA;Computer Color is Broken - MinutePhysics &#xA;[link](https://www.youtube.com/watch?v=LKnqECcg6Gw)" CreationDate="2016-08-10T15:22:55.063" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="4880" PostId="3845" Score="0" Text="@KristofferHelander Yes, but he says it's only the camera that stores the values as square roots and only the display that squares it back. Image editing software like Photoshop then just open and save the image with no conversions, which results in artifacts shown in the video when you try to blend colors." CreationDate="2016-08-10T15:33:50.350" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="4882" PostId="3841" Score="0" Text="@Irongrave Sorry, I don't know much about LIDAR scanners. Do you have a diagram or something of the setup with the lens you mention? I'm having a hard time visualizing it." CreationDate="2016-08-10T20:54:14.940" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="4883" PostId="3846" Score="0" Text="How do you tell Photoshop to save an image with the gamma baked in as you said above: _&quot;And so does Photoshop if you tell it so.&quot;_" CreationDate="2016-08-10T21:05:59.033" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="4884" PostId="3841" Score="0" Text="Here's a picture from the manual:  (http://imgur.com/a/czhpt)" CreationDate="2016-08-11T01:16:47.347" UserId="4886" ContentLicense="CC BY-SA 3.0" />
  <row Id="4886" PostId="3845" Score="0" Text="Correct me if I'm wrong on this. When I draw a gradient ramp in Photoshop that looks linear to me on the screen the values in that ramp actually are linear. But Photoshop adds a viewing gamma of 1/2.2 since the monitor adds a gamma of 2.2, thus the images pixel values are linear and they also look linear. But this 1/2.2 gamma is only a viewing gamma that Photoshop use for displaying the image, it is not baked into to image, and therefore no gamma correction to the image need to be done in the 3D software, (use gamma 1.0) since the values already are linear?" CreationDate="2016-08-11T13:31:03.027" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="4887" PostId="3845" Score="0" Text="@KristofferHelander https://goo.gl/yqkeUP The bottom gradient represents physical brightness. If Photoshop applied a gamma of 1/2.2 for either drawing the gradient or displaying the image, you would see the bottom gradient. Instead, you see the top one, which coincidentally appears to be linear, but isn't. It also wouldn't work for viewing photos. The camera applied 1/2.2 when saving the photo, then Photoshop would apply another 1/2.2, but the display only adds gamma of 2.2 once. That wouldn't look right." CreationDate="2016-08-11T13:58:48.603" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="4888" PostId="3845" Score="0" Text="If I fill a document in Photoshop half side red (255,0,0) and the other half with green (0,255,0) and then use a Gaussian blur on that, the resulting gradient becomes brownish rather then yellow in the middle as shown in that video I posted. Why is that? If the pixels in the image are linear then why does the math get screwed up? I don't understand how a displacement map painted in Photoshop is linear and good to go for rendering but at the same time the blurring of pixels gets screwed up with the wrong gamma and colors become to dark and brownish?" CreationDate="2016-08-11T14:19:24.253" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="4889" PostId="3845" Score="0" Text="@KristofferHelander It's a confusing subject, I understand. As I was trying to say, Photoshop does no conversions. So if you load an image that is stored with gamma, it stays with gamma applied and because of that blurring is wrong. If you load a displacement map that is treated as linear data, it is loaded as linear and saved as linear because no conversions are applied by Photoshop by default. There is no special treatment based on the color model. The only difference between the two is that the displacement map is displayed wrong - the display still applies a gamma of 2.2 to it." CreationDate="2016-08-11T16:11:49.957" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="4890" PostId="3845" Score="0" Text="If I create a gradient, let say 256 pixels wide, and I make sure those pixels are linear going from 0, 1, 2, 3, ... to 255. Then I have created a linear ramp, and if I use that as a displacement in 3D without any gamma correction in the 3D software I get a linear displacement effect. But does that same gradient look linear on the screen? And if I wanted to use that same gradient as a diffuse texture, I would have to apply the gamma correction on it, right? So when piping the map into Displacement you keep the pixels as is, but for Diffuse you have to linearize the texture by gamma correcting?" CreationDate="2016-08-12T09:58:51.210" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="4891" PostId="3845" Score="0" Text="You got it. The gradient will only look linear on the screen if you apply gamma correction to it. That, however, makes the gradient itself non-linear in terms of the raw values and thus unsuitable for use as a displacement map." CreationDate="2016-08-12T12:07:05.883" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="4892" PostId="3850" Score="0" Text="You could use a 16 bit image format, but then paint dot net wont cut it. Though quite many other editors would work fine." CreationDate="2016-08-13T08:19:17.027" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4893" PostId="3850" Score="0" Text="@joojaa and how well do GPUs cope with 16 bit textures? This game needs to run on mobile too." CreationDate="2016-08-13T09:25:06.283" UserId="2437" ContentLicense="CC BY-SA 3.0" />
  <row Id="4896" PostId="3850" Score="0" Text="What are you trying to accomplish with a 16-bit or 32-bit alpha channel? The human eye can barely distinguish 64 levels of any given color, so it's unlikely that more bits for your alpha channel will be noticeable." CreationDate="2016-08-14T01:11:24.507" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="4897" PostId="3850" Score="0" Text="To answer your question above, modern desktop and laptop GPUs should have no problem using 16-bit half float textures. Mobile will be another story. Some may support it but most probably won't at this point in time." CreationDate="2016-08-14T01:12:08.880" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="4898" PostId="3830" Score="0" Text="Supersampling would work albeit not very practical for limiting platforms." CreationDate="2016-08-14T10:47:02.380" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="4905" PostId="3846" Score="0" Text="This doesn't explain the &quot;why&quot; at all. It just reiterates the &quot;what&quot;." CreationDate="2016-08-15T03:05:57.087" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="4906" PostId="3845" Score="0" Text="This answer is factually incorrect. You tell Photoshop in what color space to manipulate data, and many images have information about what colorspace they are stored in. Photoshop will convert from the image's color space to its working space (which you can change), and can save color space info in the images it creates. If there is no color space info stored in the image, it makes a decision about which color space it assumes. But it is always doing this unless you specifically set it not to, and that is not the default." CreationDate="2016-08-15T03:09:19.413" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="4907" PostId="3823" Score="0" Text="When you say &quot;black and white&quot; do you literally mean 1-bit per pixel images where each pixel is either fully black or fully white? Or do you actually mean grayscale (where there are shades between fully black and fully white)? What type of compression is used?" CreationDate="2016-08-15T03:18:59.837" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="4909" PostId="3849" Score="1" Text="Is this in 2D or 3D? In 2D what do you do if both offsets intersect?" CreationDate="2016-08-15T07:02:52.420" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4910" PostId="3846" Score="0" Text="@user1118321: I believe it does. I added emphasis in an attempt to make it clearer." CreationDate="2016-08-15T07:42:23.793" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="4911" PostId="3846" Score="0" Text="Well, true but it still does not really explain why. I admit I have the same problem i just mention the lack of benefit or harm done by such compression. There is no need but the why is that the data of images is peculiar due to human senses whereas our spatial reasoning is more linear in nature (as far as the 3D app is concerned)" CreationDate="2016-08-15T07:58:38.783" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4912" PostId="3844" Score="0" Text="This is especially true for a normal map were you expect vector lengths to be 1." CreationDate="2016-08-15T09:01:25.493" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4915" PostId="3859" Score="1" Text="Yup, that's the one. Thanks. :-) I've been doing indexed Marching Cubes on pretty large volumes and was curious about how they were being rendered, as vertex locality is pretty bad between slices. Makes sense to cache the vertices somewhere (I guess just in standard L1 / 2 cache?). My meshes are probably out-of order enough to be re-processing a lot of vertices, so I'll look into those algorithms." CreationDate="2016-08-16T09:06:03.783" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="4916" PostId="3829" Score="1" Text="In my own experiments drawing sprites, rendering one mesh with 6k vertices (ie one mesh containing all sprites) was faster than rendering a mesh with 6 vertices and 1k instances. Note that in my instancing I did not use glVertexAttribDivisor but instead read from a buffer object. Using glVertexAttribDivisor may be faster since the gpu could push the data to the shader, while reading a buffer object means the shader has to pull its data. But I didn't benchmark this, so dunno if it matters. I still like instancing, because its a simple clean solution." CreationDate="2016-08-16T10:17:13.393" UserDisplayName="user4925" ContentLicense="CC BY-SA 3.0" />
  <row Id="4917" PostId="3861" Score="0" Text="IS taking the real star position data and pojecting it on your map considered realistic enough?" CreationDate="2016-08-16T12:51:54.893" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4918" PostId="3861" Score="1" Text="You can find data here: http://www.astronexus.com/hyg" CreationDate="2016-08-16T12:57:55.317" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4919" PostId="3862" Score="1" Text="Have you called `glGetError()` to see if there are any errors?" CreationDate="2016-08-16T14:40:11.073" UserId="67" ContentLicense="CC BY-SA 3.0" />
  <row Id="4920" PostId="3862" Score="0" Text="Gives me 1281 - Invalid Value" CreationDate="2016-08-16T14:56:28.573" UserId="4543" ContentLicense="CC BY-SA 3.0" />
  <row Id="4921" PostId="3862" Score="0" Text="So I get GL_INVALID_ENUM on the first run of the render loop after glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, m_IndexBuffer);&#xA;glBindBuffer(GL_ARRAY_BUFFER, m_VertexBuffer); and GL_INVALID_VALUE after the first shaderProgram.activate() and everytime thereafter." CreationDate="2016-08-16T15:06:22.370" UserId="4543" ContentLicense="CC BY-SA 3.0" />
  <row Id="4923" PostId="3861" Score="0" Text="Maybe calculate random 3D positions for the stars and then project them onto a sphere. Their brightness depends on the distance from the sphere center." CreationDate="2016-08-16T19:06:09.997" UserId="2670" ContentLicense="CC BY-SA 3.0" />
  <row Id="4924" PostId="3861" Score="4" Text="Maybe you could add a screenshot of your current results versus what you're looking for (e.g. a photo of a real starry sky that you'd like to imitate)? Otherwise, we're just guessing what &quot;unnatural-looking&quot; means here." CreationDate="2016-08-16T19:52:34.583" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="4925" PostId="3864" Score="0" Text="I don't know if any renderers implement something like this, but one thing that springs to mind is this would only be useful when objects have a single solid color, or just a few discrete colors. It wouldn't scale when textures are used, for instance (although I suppose you could apply a global RGB tint to the texture with this method)." CreationDate="2016-08-17T05:43:20.467" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="4926" PostId="3864" Score="0" Text="@NathanReed no thats one of the first things people made these things do." CreationDate="2016-08-17T09:55:34.360" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4927" PostId="3865" Score="0" Text="Thanks! Do you know where I can read more about this? Maybe the documentation page of a freely available renderer?" CreationDate="2016-08-17T16:52:14.343" UserId="4933" ContentLicense="CC BY-SA 3.0" />
  <row Id="4928" PostId="3865" Score="1" Text="@DaanMichiels Pixar used to have a video showing their relight and continious render capabilities go search for renderman" CreationDate="2016-08-17T17:11:28.703" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4929" PostId="3863" Score="3" Text="As it stands this is more of a comment than an answer. Could you expand to explain the bug and how it affected your code?" CreationDate="2016-08-17T21:53:29.477" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4930" PostId="3861" Score="0" Text="Maybe you could look into poisson sampling, you can define some density function and transform that into distributed points of fairly convincing random distribution. Check here http://devmag.org.za/2009/05/03/poisson-disk-sampling/" CreationDate="2016-08-18T06:07:11.843" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4931" PostId="3871" Score="2" Text="Not sure that is strictly true for diffuse, although I may be wrong. Low diffuse does not imply high specular, what about strongly absorbing materials like carbon or charcoal ?" CreationDate="2016-08-19T09:31:07.670" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4932" PostId="3867" Score="0" Text="Usually to avoid getting uniform patterns everywhere, if you are using equally spaced uniform samples then you will get moire pattern effects as some pixels hit and miss features at a regular interval. Ideally some kind of importance sampling should be used to bias sampling directions." CreationDate="2016-08-19T09:33:22.333" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4933" PostId="3872" Score="0" Text="Yes but mostly it can be summed as this for a unknown problem, ramdom is the best strategy. since contribution  is unknown..." CreationDate="2016-08-19T13:10:24.670" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4934" PostId="3872" Score="0" Text="@joojaa: oh, I realize you and I have read the question differently. Yes, Monte Carlo is a strategy that works when the analytical solution is unknown (although a math person would probably have a much more precise way of putting it)." CreationDate="2016-08-19T14:34:15.447" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="4935" PostId="3872" Score="0" Text="Said differently: even if the solution is unknown, Monte Carlo is proven to converge toward it." CreationDate="2016-08-19T14:35:59.760" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="4937" PostId="3871" Score="0" Text="@PaulHK. charcoal have low albedo because low proportion of its incident light is reflected away from a surface." CreationDate="2016-08-20T00:36:40.890" UserId="4946" ContentLicense="CC BY-SA 3.0" />
  <row Id="4938" PostId="3877" Score="0" Text="Ah.  And blue noise prevents the clumping by having less low frequency data in the sampling pattern." CreationDate="2016-08-20T12:35:43.813" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="4941" PostId="3861" Score="0" Text="Is the starfield to be a fixed background? That is, do your spaceships move slowly enough that the stars do not need to move?" CreationDate="2016-08-20T13:00:50.700" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4942" PostId="3878" Score="0" Text="I think the Signal Processing SE (http://dsp.stackexchange.com/) would be more suited for this type of question." CreationDate="2016-08-20T14:54:45.953" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="4945" PostId="3878" Score="2" Text="I like your idea of reversing the darkened band so the inpainting only needs to be applied to the text and small face. The darkening is likely to be a fairly simple function such as halving the colour values of each pixel. You could experiment with different values to see if there's an obvious fit." CreationDate="2016-08-20T17:17:36.523" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4946" PostId="3878" Score="0" Text="Bear in mind that the darkening may have been applied to linear or gamma corrected pixel values, so that's another thing to experiment with." CreationDate="2016-08-20T17:18:26.420" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4947" PostId="3878" Score="0" Text="@trichoplax Thanks! Alright I'll try that then. Also, is there any way to reach you by email? I'd love some guidance and I can't find anyone" CreationDate="2016-08-20T17:22:44.603" UserId="4954" ContentLicense="CC BY-SA 3.0" />
  <row Id="4948" PostId="3878" Score="0" Text="There was an [inpainting programming competition](http://codegolf.stackexchange.com/questions/70483/patch-the-image) over on Programming Puzzles and Code Golf Stack Exchange, which may give you some inspiration and different approaches to try. The top approach is quite in depth, and there are also a number of different simpler ideas lower down." CreationDate="2016-08-20T17:30:37.880" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4949" PostId="3878" Score="1" Text="If you start work on your own approach and run into a problem, you can ask another question here, explaining what you have tried so far. If you have any doubts over what you can ask about, you can contact me in [chat] using @trichoplax" CreationDate="2016-08-20T17:32:07.867" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4950" PostId="3878" Score="0" Text="@tricoplax: but I should go for the impainting approach only after I've gotten rid of the darkened strips yes?" CreationDate="2016-08-20T17:33:42.150" UserId="4954" ContentLicense="CC BY-SA 3.0" />
  <row Id="4951" PostId="3879" Score="0" Text="Ok, I'll add one." CreationDate="2016-08-20T18:14:20.377" UserId="4956" ContentLicense="CC BY-SA 3.0" />
  <row Id="4953" PostId="3879" Score="0" Text="Yes, I'd need something that could work with different widths, I'l change the picture once again, because it's slightly different than what I achieved in d3.js." CreationDate="2016-08-20T19:10:43.440" UserId="4956" ContentLicense="CC BY-SA 3.0" />
  <row Id="4954" PostId="3881" Score="1" Text="Can you be more specific about what you do and don't get? Right now we're just guessing about what kind of explanation would help you." CreationDate="2016-08-20T19:25:07.627" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="4955" PostId="3879" Score="0" Text="I notice in your updated second image there are some that overlap. For example, the mid grey strip near the top of the circle overlaps the green strip that starts next to it (even though both the grey end points are inside the range of the green end points). If that is permitted in the solution then it will be easier to solve." CreationDate="2016-08-20T19:31:47.617" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4956" PostId="3879" Score="0" Text="If you have existing code in d3.js that already gives the desired result, it would be helpful to see it." CreationDate="2016-08-20T19:32:24.837" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4957" PostId="3879" Score="0" Text="I think I'll have to live with some of the curves overlapping. Yes, I can paste here d3.js code, but it does not deal with drawing the chords. It's already in the library, I think that it's here in ribbon function: https://github.com/d3/d3-chord/blob/master/src/ribbon.js." CreationDate="2016-08-20T19:39:38.397" UserId="4956" ContentLicense="CC BY-SA 3.0" />
  <row Id="4958" PostId="3881" Score="0" Text="Well, it is an old computer graphics exam question, perhaps I should have started with this and I decided to work on it a bit to prepare myself for future examinations." CreationDate="2016-08-20T19:58:11.477" UserId="4959" ContentLicense="CC BY-SA 3.0" />
  <row Id="4959" PostId="3881" Score="0" Text="The question goes as stated &quot;How does an analogue image is sampled and then stored in a ture-colour frame buffer (e.g. in a camera) and how do you prevent noise when you do so?&quot;" CreationDate="2016-08-20T19:58:58.737" UserId="4959" ContentLicense="CC BY-SA 3.0" />
  <row Id="4960" PostId="3883" Score="0" Text="Thanks. Just managed to get it working (option 2). Transformation while raymarching to sample and inverse transformation while injecting. But I'm still raymarching to worldpos as usual, maybe i can be done simpler - iterating like you said but it would still require a loop." CreationDate="2016-08-21T11:45:11.353" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="4961" PostId="3883" Score="0" Text="I mean not using transformations but iterate through z." CreationDate="2016-08-21T11:56:41.673" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="4962" PostId="3890" Score="0" Text="Would it help to replace the edge texture with one that uses pure black and several levels of alpha, rather than using several levels of grey?" CreationDate="2016-08-21T23:11:00.450" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4963" PostId="3890" Score="0" Text="@trichoplax: I'd say yes, since I could use the glsl mix funtion. But I'm not sure how I'd have to change my fxaa-shader to use alpha instead of grey values. I added the fxaa shader code abvoe." CreationDate="2016-08-21T23:23:23.957" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="4964" PostId="3890" Score="0" Text="I haven't tested this so I'll leave it to someone else to post an answer if they are confident it works, but I'd imagine it working something like this: For each pixel in the texture, change the alpha value to the opposite of the grey value, and set the colour to black (0, 0, 0). Black pixels become fully opaque, white pixels become fully transparent, so the final colour can simply be set to black for all pixels, since the ones you want to appear white will be fully transparent preventing the black from showing up." CreationDate="2016-08-21T23:28:40.397" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4965" PostId="3833" Score="0" Text="@Julien Guertault, Thank you for refining the format. I don't know how to do that:)" CreationDate="2016-08-22T00:34:50.647" UserId="3058" ContentLicense="CC BY-SA 3.0" />
  <row Id="4967" PostId="3891" Score="1" Text="Could you [edit] to expand on exactly what you require, and what you have tried? Is there a particular approach to clustered lights that you want to implement? What effect are you looking to achieve?" CreationDate="2016-08-22T01:11:42.393" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4968" PostId="3884" Score="1" Text="This isn't enough info to be a full answer obviously but the scattering function of the surface (aka BSDF) is what describes how light acts on a surface.  The rendering equation includes the BSDF and shows how this stuff all fits together.  I recommend checking out graphicscodex.com. It is 13 chapters long, where each chapter is only a few pages, but after you read through it (shouldn't take long) you'll have a deep understanding of this stuff.  A deeper read would be anything on &quot;physically based rendering&quot; specifically the book at pbrt.com." CreationDate="2016-08-22T01:25:20.833" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="4969" PostId="3884" Score="0" Text="Apologies, pbrt.org is the deeper read I mean." CreationDate="2016-08-22T01:37:08.940" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="4970" PostId="3887" Score="0" Text="It does look weird. Could you put some of your actual code for us to see? Pseudo code shows your intent, which looks decent, but the problem might be in some subtle issue of the math or a typo (:" CreationDate="2016-08-22T01:59:10.037" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="4971" PostId="3871" Score="0" Text="Ah yes I get what you mean now" CreationDate="2016-08-22T02:11:59.467" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4972" PostId="3833" Score="0" Text="At your service. :) Here's an overview if you're interested: http://stackoverflow.com/editing-help" CreationDate="2016-08-22T02:16:58.200" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="4973" PostId="3890" Score="2" Text="Something like: gl_FragColor = mix(colourRGB, vec3(0), 1 - edgeRGB.r); Would blend in your edge outlines as black" CreationDate="2016-08-22T02:18:47.630" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4974" PostId="3890" Score="2" Text="I would suggest something very similar to @PaulHK's snippet too: `combinedColor = mix(sceneColor, edgeColor, edgeOpacity * (1. - edgeRGB.r));`." CreationDate="2016-08-22T03:17:30.913" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="4975" PostId="3881" Score="1" Text="The question is a bit ambiguous, are we talking about the A-to-D process that goes on in the sensor? Or are we talking about the bayer conversion camera sensor arrays need to apply to convert sensor data into RGB ?" CreationDate="2016-08-22T04:19:16.473" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4976" PostId="3729" Score="0" Text="Are you wanting something like diffuse-reaction? There is a great example of this on shadertoy (https://www.shadertoy.com/view/4dcGW2)" CreationDate="2016-08-22T04:22:07.523" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4977" PostId="3887" Score="0" Text="@AlanWolfe updated my question, added some real code." CreationDate="2016-08-22T04:25:42.377" UserId="4966" ContentLicense="CC BY-SA 3.0" />
  <row Id="4978" PostId="3892" Score="0" Text="Done it. It looks like the problem is really with hamma. But I still has a question. There are to results of my ray tracing. In the first one I" CreationDate="2016-08-22T04:27:47.463" UserId="4966" ContentLicense="CC BY-SA 3.0" />
  <row Id="4979" PostId="3892" Score="0" Text="Sorry for kind of spamming. Didn't use to sending commentary with Enter button. There is continuation. In the left one I make at first gamma to linear conversion and then multiplying it on the max(0, scalar multiplication of light and normal) and after that summing results in pixel. And convert results into gamma space again. At the right one I convert colour into linear space only after multiplying it on on the max(0, scalar multiplication of light and normal). I don't quite get the reason why  should I multiply in the gamma space. ![img](http://i.imgur.com/7WRCFb0.png)" CreationDate="2016-08-22T04:40:27.343" UserId="4966" ContentLicense="CC BY-SA 3.0" />
  <row Id="4980" PostId="3892" Score="0" Text="The most important thing is the output is converted to gamma (meaning your light accumulation is linear). Because your inputs are constant colours it doesn't affect shading as much as the output conversion." CreationDate="2016-08-22T04:50:26.380" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4981" PostId="3892" Score="0" Text="I'd be interested to see your new image output if you can post an update" CreationDate="2016-08-22T04:51:01.107" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4982" PostId="3892" Score="0" Text="Don't forget, you picked the orange/green colours by eye (presumably) using a gamma-space monitor, so likely these are the correct/intended colours." CreationDate="2016-08-22T04:54:22.533" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4983" PostId="3892" Score="0" Text="I understand it. Don't have any complaints about color. Just about dark parts at the left part of the second image in the updated post. The right one is good without them. But why should I make transformation to the linear space after multiplication." CreationDate="2016-08-22T05:05:02.593" UserId="4966" ContentLicense="CC BY-SA 3.0" />
  <row Id="4984" PostId="3892" Score="0" Text="I think the left image is the correct one looking at the function, although you may need to adjust your light colours to something a bit darker." CreationDate="2016-08-22T07:43:07.343" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="4985" PostId="3890" Score="0" Text="Thank you both, but just to make clear: How would I retrieve the edgeOpacity?" CreationDate="2016-08-22T08:32:19.067" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="4987" PostId="3890" Score="0" Text="@enne87 edge opacity in this case is edge color (so 1 - color) because edge is black" CreationDate="2016-08-22T11:44:48.440" UserId="4503" ContentLicense="CC BY-SA 3.0" />
  <row Id="4988" PostId="3890" Score="0" Text="@enne87: `edgeOpacity` is meant to be a parameter between 0 (no edges at all) and 1 (opaque edges) that you choose to your liking." CreationDate="2016-08-22T14:08:39.733" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="4989" PostId="3891" Score="3" Text="WebGL doesn't have compute shaders and I'm not sure if clustered lighting would be fast enough without them." CreationDate="2016-08-22T15:02:55.103" UserId="67" ContentLicense="CC BY-SA 3.0" />
  <row Id="4990" PostId="3893" Score="0" Text="Do the buildings have any properties that would help distinguish them? Are they disconnected items, or is the city a single connected mesh with the buildings rising from a plane?" CreationDate="2016-08-22T15:35:21.550" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4991" PostId="3893" Score="0" Text="yes absolutely second one a single connected mesh" CreationDate="2016-08-22T17:21:08.150" UserId="4513" ContentLicense="CC BY-SA 3.0" />
  <row Id="4992" PostId="3893" Score="1" Text="It's worth editing to include that in your description." CreationDate="2016-08-22T17:55:02.320" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="4993" PostId="3878" Score="0" Text="Is this your original image? Or did you just dump a jpeg compression on it?" CreationDate="2016-08-22T18:12:12.430" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4994" PostId="3878" Score="1" Text="@trichoplax it is a simple function. If it were for the horrible compression artifacts in he image Id have deleted the line by now, [here is the test](http://i.stack.imgur.com/cDE1A.png) again i wouldn't bother to do this with these kinds of jepg qualities. The color cast could be easily deleted if the jpeg artifacts wouldn't have ruined the area." CreationDate="2016-08-22T18:29:49.930" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="4996" PostId="3878" Score="0" Text="@jooja Actually the original image is of better quality. This one o downloaded off facebook so its compressed. How exactly did you transform the image? I mean remove the darkened part? What's the function?" CreationDate="2016-08-22T21:59:17.777" UserId="4954" ContentLicense="CC BY-SA 3.0" />
  <row Id="4997" PostId="3892" Score="0" Text="Could your colors be overflowing?" CreationDate="2016-08-23T02:44:07.993" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="4998" PostId="3892" Score="0" Text="Nope, I clamp to 1.0f every component when it becomes more than 1.0f. I thought about this case that's why I specially check it in my tests." CreationDate="2016-08-23T05:18:39.280" UserId="4966" ContentLicense="CC BY-SA 3.0" />
  <row Id="4999" PostId="3893" Score="4" Text="Then it really depends on the buildings. Are all your buildings just cubes? Or at least convex? And is the ground plane really just a plane or has it terrain?" CreationDate="2016-08-23T10:32:44.027" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="5000" PostId="3878" Score="0" Text="@joojaa What was the technique you used to remove the darkened pixels?" CreationDate="2016-08-23T19:17:57.980" UserId="4954" ContentLicense="CC BY-SA 3.0" />
  <row Id="5001" PostId="3897" Score="1" Text="If R2 is a vector, it _can't_ be equal to NL which is a scalar." CreationDate="2016-08-24T14:51:07.393" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5005" PostId="3901" Score="1" Text="Hi and welcome to the site. Can you edit the question and explain a bit more about the context, what the algorithm is and what you're trying to accomplish? Currently I can't make heads or tails of it. :)" CreationDate="2016-08-25T02:10:45.730" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5007" PostId="3898" Score="0" Text="The original code with its location param set to 3 for the colour attribs looks like the source of the problem." CreationDate="2016-08-25T07:45:46.513" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5012" PostId="3904" Score="0" Text="Updated question for clarification" CreationDate="2016-08-25T13:16:41.583" UserId="3331" ContentLicense="CC BY-SA 3.0" />
  <row Id="5014" PostId="3903" Score="0" Text="Right. Basically I wrote the module instead of the vector. Thank you" CreationDate="2016-08-25T20:41:08.780" UserId="4981" ContentLicense="CC BY-SA 3.0" />
  <row Id="5015" PostId="3907" Score="0" Text="I clarified a lot of things and I really appreciate your response. It remains a doubt, you correct me if I'm wrong or confirm:     if  **t&lt;0** the intersection point (plan and line) is surely behind the origin of light source but it can happen that the object is still in front but surely the object isn't crossed from the ray (because the intersection is behind) and not being illuminated  can be eliminated.          I'm a beginner in CG, forgive me if I say stupid things" CreationDate="2016-08-25T23:50:55.727" UserId="4981" ContentLicense="CC BY-SA 3.0" />
  <row Id="5016" PostId="3907" Score="1" Text="@Umbert That's right. A plane and line (or ray) have at most a single point of intersection, and if that point is behind the ray origin, then no object in the plane can intersect the ray." CreationDate="2016-08-26T00:08:03.360" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5017" PostId="3910" Score="0" Text="&quot;the normal direction of the arc is not always tangential to the spine&quot;. I'm not quite sure what this means in 3d. Is it equivalent to &quot;the plane in which the arc lies is not always perpendicular to the closed curve&quot;?" CreationDate="2016-08-26T12:17:13.977" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5018" PostId="3910" Score="0" Text="Is &quot;spine&quot; a specific term here, or should this say &quot;[spline](https://en.wikipedia.org/wiki/Spline_(mathematics))&quot;?" CreationDate="2016-08-26T12:20:01.677" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5019" PostId="3905" Score="0" Text="I'd be interested to see an explanation of how your solution works, and it might be useful for future visitors too..." CreationDate="2016-08-26T12:38:52.143" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5020" PostId="3894" Score="0" Text="I have questions. **1.** In the &quot;putting it all together&quot; part you have a condition on whether an object is opaque or transparent. How do I determine this? Do I just add an `opacity` (or `transparency`) boolean value to my materials? **2.** If `ks` is for point lights and `reflectivity` for reflected rays, how do I deduct one from the other as you suggest? There seems to be an equation taking refractive indices and calculating ks from that. (I'm still having a hard time really understand your complete answer. Bear with me, please. Thank you. :)" CreationDate="2016-08-26T13:33:37.437" UserId="4962" ContentLicense="CC BY-SA 3.0" />
  <row Id="5021" PostId="3909" Score="0" Text="Unity can export to WebGL." CreationDate="2016-08-26T14:34:29.973" UserId="67" ContentLicense="CC BY-SA 3.0" />
  <row Id="5023" PostId="3911" Score="1" Text="Seems to me that $|AC|$ is just shorthand for length of vector A to C which is $|C-A|$ or $|A-C|$ which is your second question. If so then this does work. For you to uderstand why you would need to draw the images of the vectors but im not able to do that on mobile phone." CreationDate="2016-08-27T05:49:59.210" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5024" PostId="3905" Score="1" Text="I've added a simple explanation..." CreationDate="2016-08-27T06:38:11.690" UserId="4956" ContentLicense="CC BY-SA 3.0" />
  <row Id="5025" PostId="3894" Score="1" Text="@kleinfreund 1. Right, I'm suggesting a simplified material model where everything is either opaque (diffuse) or translucent, so you could use a boolean flag for this. 2. I'm saying `ks` and `reflectivity` are equal—literally the same value." CreationDate="2016-08-27T07:20:57.477" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5026" PostId="3914" Score="0" Text="I was thinking about the mirror row and it is possible that we are seeing a slightly malformed version of the original question. In which case it could be correct." CreationDate="2016-08-27T08:46:44.857" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5027" PostId="3914" Score="0" Text="@joojaa Why do you think the question is a malformed version?" CreationDate="2016-08-27T10:06:08.440" UserId="4993" ContentLicense="CC BY-SA 3.0" />
  <row Id="5028" PostId="3914" Score="0" Text="@Nathan, Thanks alot. Although I'm left with several questions:&#xA;1. Can you clarify &quot;You could fix the answer either way.. fully orthonormalizing u and v.&quot;) ? How do I fully orthonormalize u and v? That is also a question regarding bullet #3 of your answer. Would leaving $u$ as it is and setting $v=u\times|B-A|$ satisfies the requirement? If not, can you please show me explicitly?&#xA;2. So for a point I can use anything, even $(1,0,0),(0,1,0),(1,0,0)$?&#xA;3. Regarding the normalization with $|AC|$, so is it correct? or it should be $|C-A|$? Because I'm not sure the first one is a shorthand." CreationDate="2016-08-27T10:16:20.233" UserId="4993" ContentLicense="CC BY-SA 3.0" />
  <row Id="5029" PostId="3914" Score="1" Text="@jiang because its a trivial error and depends on exact wording of the problem it is eady to construct a sentence where the 2 solutions would ony differ by a two letter preposition. $|AC|$ is perfectly normal shorthand in literature." CreationDate="2016-08-27T11:15:51.780" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5030" PostId="3914" Score="1" Text="@Jjang For orthonormalization, look at the [Gram–Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). Briefly, you'd start with $v = B - A$, then set $v' = v - (u \cdot v)u$. This subtracts off the component of $v$ that's parallel to $u$, leaving only the perpendicular component. Then you normalize $v'$. And joojaa is right, $|AC|$ is common shorthand for the distance between $A$ and $C$." CreationDate="2016-08-27T17:45:28.297" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5031" PostId="3914" Score="0" Text="Ok, and just to clarify, $u=\dfrac{B-A}{|AB|}$, $v'=u-((C-A)*u)(C-A))$ and then $v=\dfrac{v'}{|v|}$, $w=u\times v$? I'm just verifying this because I've never seen this way before (the way you created $v'$)." CreationDate="2016-08-27T19:03:18.123" UserId="4993" ContentLicense="CC BY-SA 3.0" />
  <row Id="5032" PostId="3910" Score="0" Text="Indeed, it is equivalent. The &quot;spine&quot; is not necessarily a spline, it can be an array of points connected with straight lines. Some software calls the spine curve trajectory." CreationDate="2016-08-27T19:22:42.767" UserId="4992" ContentLicense="CC BY-SA 3.0" />
  <row Id="5034" PostId="3914" Score="0" Text="By the way, I've just seen this solution (to finding $u,v,w$), what do you say? $u=\dfrac{AB}{|AB|}$, $v=u\times w$, $w=\dfrac{AB\times AC}{|AB\times AC|}$? Is it correct? So there are 2 questions here: 1. Was I right in the previous answer? 2. Is the answer given here correct?" CreationDate="2016-08-27T21:41:38.830" UserId="4993" ContentLicense="CC BY-SA 3.0" />
  <row Id="5035" PostId="3909" Score="0" Text="Why are JavaScript-based physics engines &quot;mostly suitable for 2D use cases&quot;? Also you said &quot;Keep in mind that Game Engines may be a viable way for 2D applications too and can make your life a whole lot easier&quot;, which is why I'm making http://github.com/infamous/infamous (renaming it soon and adding docs/demos). It's a 3D API that currently renders to DOM (CSS3D with my DOMMatrix polyfill), and I'd like to add WebGL soon. Here's a couple small demos: http://trusktr.io (hover on the left edge for the 3D menu) and http://mightydevs.com (letters moving into formation, needs word wrapping)." CreationDate="2016-08-28T00:53:17.450" UserId="4991" ContentLicense="CC BY-SA 3.0" />
  <row Id="5036" PostId="3905" Score="0" Text="Can you post a working example? (one way to do it could be with http://jsfiddle.net)" CreationDate="2016-08-28T01:00:11.153" UserId="4991" ContentLicense="CC BY-SA 3.0" />
  <row Id="5037" PostId="3914" Score="0" Text="@Jjang I added more to the answer that will hopefully clarify it. Yes, your second approach is fine too and gives the same results as the Gram–Schmidt process I outlined." CreationDate="2016-08-28T01:22:23.257" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5039" PostId="3920" Score="0" Text="Excellent answer, thanks! I just didn't get the last part: using a `half` wouldn't incur in the very same problems that you mentioned before, due to the lack of precision? I though that would be particularly important at the GPU, due to the many color calculations that are frequently done in shaders" CreationDate="2016-08-28T05:09:13.383" UserId="3383" ContentLicense="CC BY-SA 3.0" />
  <row Id="5040" PostId="3920" Score="1" Text="@AndrewSteer A `half` is 16 bits, including 11 effective bits of mantissa precision, so while not as precise as `float`, it's still sufficient for most color operations. (Maybe not _quite_ sufficient if you're outputting to a high-gamut HDR display; I'm not sure.)" CreationDate="2016-08-28T05:10:51.280" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5042" PostId="3922" Score="2" Text="Well you could live without the matrices but it would be hideously complex and wasteful. Many people who did not understand matrices attempt this regularily in stackoverflow but either end up reimplementing matrices without knowing it or ending with huge problems." CreationDate="2016-08-28T06:55:39.897" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5043" PostId="3909" Score="0" Text="@trusktr Because, or at least that's what I thought, there is no JS 3D physics engine that comes close to PhysX in terms of features. Then again I just found out that someone [ported Bullet to JS](https://github.com/kripken/ammo.js). YMMV." CreationDate="2016-08-28T15:47:28.807" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="5044" PostId="3922" Score="0" Text="I heard of an alternative algebra before that does not use linear transformations, but some other theory. I can't remember what it is or any of the keywords so I don't know what to search for, so I am hoping someone might mention it here. The things you mentioned all seem to build on top of linear algebra, but I believe there are other techniques to build from." CreationDate="2016-08-28T18:49:26.360" UserId="4991" ContentLicense="CC BY-SA 3.0" />
  <row Id="5045" PostId="3922" Score="0" Text="@tru There are additional algebras such as [geometric algebra](https://en.wikipedia.org/wiki/Geometric_algebra) that supplement linear algebra with additional concepts, but I'm not aware of anything that _replaces_ linear algebra and fulfills the same purposes." CreationDate="2016-08-28T19:03:25.800" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5046" PostId="3922" Score="0" Text="@NathanReed Thanks for the link! That might possibly be what it was. I really dislike all that symbolic mathematical jargon though. I'd much love to see it expressed with code, so I'll search for that. Thanks!" CreationDate="2016-08-28T20:01:05.637" UserId="4991" ContentLicense="CC BY-SA 3.0" />
  <row Id="5048" PostId="187" Score="0" Text="@ratchetfreak but not all uses of 3D formats end up on the screen." CreationDate="2016-08-28T20:59:45.067" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5049" PostId="3927" Score="0" Text="Can you please give a full technical answer with the vectors exact calculations and normalizations? I'm finding it difficult to follow the logic. 1. Why is it the same as rotation about an axis? With rotation around axis, you HAVE to use the axis as one of the 3 orthonormal basis you create (here you don't use the normal N).&#xA;2. Why do you take a &quot;vector thats not parallel to normal&quot; and not the normal?" CreationDate="2016-08-28T21:44:06.077" UserId="4993" ContentLicense="CC BY-SA 3.0" />
  <row Id="5050" PostId="3927" Score="0" Text="I'd expect something like: $w=\dfrac{n}{|n|}$, $v=\dfrac{n\times (0,0,1)^T}{|n\times (0,0,1)^T|}$, $u=v\times w$" CreationDate="2016-08-28T21:46:30.633" UserId="4993" ContentLicense="CC BY-SA 3.0" />
  <row Id="5051" PostId="1541" Score="0" Text="While this is a good answer I don't think it answers the OPs question and kind of suggests that homogenous coords are used because the hardware is optimised for it, rather homogenous coords are more useful and hardware was eventually developed around that. Another argument for vec4s is they are 128bit aligned which makes it more efficient to read on wide memory buses (GPUs)" CreationDate="2016-08-29T01:53:56.173" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5052" PostId="3922" Score="0" Text="Monte carlo / Path tracing should be added to your list. Great list btw." CreationDate="2016-08-29T02:49:51.747" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5053" PostId="3768" Score="1" Text="Yeah, my comments make no sense now the question has been updated." CreationDate="2016-08-29T02:53:07.970" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5054" PostId="3922" Score="1" Text="@PaulHK I mentioned that under raytracing" CreationDate="2016-08-29T03:17:59.397" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5055" PostId="3922" Score="0" Text="So you did, more coffee needed." CreationDate="2016-08-29T03:29:29.080" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5056" PostId="3927" Score="0" Text="@jiang you need to stop thinking in formulas and drawing the vectors. Once you do it gets easier to understand the only reason you do not understand is because you look at the formulas. Ys there was a paste error in the procedure." CreationDate="2016-08-29T05:15:19.470" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5057" PostId="3927" Score="0" Text="But note you do not have to orthonormalize the matrix, you do not have to normalize the vectors, you could use the sizing in the reflection vectors. You dont need the vectors to be calculated this way. Your secodary directions could easily be  n+(0,1,0) and n+(0,0,1) but its just easier if you have a rotation matrix. But as i said draw the matrix vector on a piece of paper or screen @jiang then you will never need to remember the formulas as it becomes evident once you draw enough images." CreationDate="2016-08-29T05:30:53.837" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5058" PostId="3917" Score="0" Text="So those values needs to be computed on a case basis? and are not real constants?" CreationDate="2016-08-29T07:42:11.683" UserId="4503" ContentLicense="CC BY-SA 3.0" />
  <row Id="5059" PostId="3917" Score="0" Text="ok, seems Those are the actual constants used in every existing shader." CreationDate="2016-08-29T08:20:58.427" UserId="4503" ContentLicense="CC BY-SA 3.0" />
  <row Id="5060" PostId="3923" Score="0" Text="One of the reasons I want to do this outside of cad softwares is that this feature is not that simple to create with them. a) On your image above the shape of the cross-section is changing, but the plane of the sketch is perpendicular to the local trajectory. b) The surface can self-intersect, what is often a problem" CreationDate="2016-08-29T09:51:16.847" UserId="4992" ContentLicense="CC BY-SA 3.0" />
  <row Id="5061" PostId="3923" Score="0" Text="@Tawhiri a) No the cross section is always the same its just oriented to the sweep path. The plane pf the sketch is not allways perpendicular it is changing with the funclion 60cos(4t)+110. b) is hard to avoid if you put arbitrary functions in place. So the function either selfintersects or does not. But yes that can be counteracted by making a rail sweep along and not thinking in terms of across." CreationDate="2016-08-29T10:05:06.743" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5062" PostId="3917" Score="0" Text="Yes, for diffuse lighting. For other forms of lighting you need to compute other constants." CreationDate="2016-08-29T11:39:15.993" UserId="5002" ContentLicense="CC BY-SA 3.0" />
  <row Id="5063" PostId="3923" Score="0" Text="@Tawhiri oh and what CAD are you using? What your proposing is actually very simple in Creo which is what i use mostly. Admittedly the tool used is not one that many users know how to use but still very easy to do." CreationDate="2016-08-29T12:27:22.437" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5064" PostId="3923" Score="0" Text="Right now Catia V5R24 and Creo 2.0 is available. What is your method? I have a cuple of years experience with both, but I cant figure it out how to vary the angle of the sketch plane." CreationDate="2016-08-29T13:06:38.953" UserId="4992" ContentLicense="CC BY-SA 3.0" />
  <row Id="5065" PostId="3923" Score="0" Text="When you use sweep you  must enable variable sections then inside the sketch add a relation to your dimension. In this relation you can write a function along curve by using the variable `trajpar` which is from 0 to 1 along the swept curve. Likewise you can make curves out fo equations with the **datum (additional dropdown) -&gt; curve -&gt; curve from equation** command. Also if you want to adjust the top down trajectory direction then  you can define an additional curve for the sweep. Its all there but very few people ever use these tools." CreationDate="2016-08-29T13:13:11.567" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5066" PostId="3923" Score="0" Text="@Tawhiri is this what your after? http://i.stack.imgur.com/bNJgi.png" CreationDate="2016-08-29T13:27:29.973" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5067" PostId="3923" Score="0" Text="Yes it is. Actually I've tried this before, but the feature keeps crashing with &quot;design intent is unclear&quot;, so I assumed this is not the proper way to go. Probably the problem is that there is a corner with a small radius in the path. And the mathematical derivation of the second curve is quite challenging. Anyway, I will keep trying, thank you so much for the help!" CreationDate="2016-08-29T13:54:52.177" UserId="4992" ContentLicense="CC BY-SA 3.0" />
  <row Id="5068" PostId="3923" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/44627/discussion-between-joojaa-and-tawhiri)." CreationDate="2016-08-29T14:04:27.227" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5069" PostId="1576" Score="1" Text="These are sometimes referred to as &quot;explicit normals&quot; (3ds max and maya terminology)." CreationDate="2016-08-29T14:06:57.273" UserId="5005" ContentLicense="CC BY-SA 3.0" />
  <row Id="5070" PostId="3848" Score="0" Text="Thank you for your input - wow! :) I'll have a look at this when I get more spare time. I'll get back to you, thanks!" CreationDate="2016-08-29T14:31:50.973" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="5071" PostId="3910" Score="1" Text="Even tough i have explained how to do this in comments does not mean that this is a forum for telling you how to operate your CAD application." CreationDate="2016-08-29T16:27:33.680" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5072" PostId="3930" Score="0" Text="Thanks, but that does not work for me. Here's the problem. Let's say you are 5 seconds into the countdown (percentCompleted = 0.5 or 50%). Then &quot;Update Now&quot; is pressed and `totalTime` is changed to 1. Now, when onUpdate is fired, percent complete will be 5 or 500%!! and there is no more animation as we have just flown way past 100%&#xA;&#xA;Or am I missing something?" CreationDate="2016-08-29T20:30:06.777" UserId="5003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5073" PostId="3930" Score="2" Text="no because at that point percent completed would remain at 0.5 % (the variables outside functions are the state kept across frames) and the next update would add a different delta based on the new totaltime as if the total time was always the new value." CreationDate="2016-08-29T20:41:48.473" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5074" PostId="3910" Score="0" Text="Obviously not. I guess I got the answer, though not the one I was hoping for. Again, thanks for the help." CreationDate="2016-08-30T04:05:03.443" UserId="4992" ContentLicense="CC BY-SA 3.0" />
  <row Id="5075" PostId="3930" Score="0" Text="Ah, my mistake. I just didn't read it correctly the first time.  Very good, just what I needed." CreationDate="2016-08-30T14:03:25.317" UserId="5003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5076" PostId="3894" Score="0" Text="I see. A `isTransparent` boolean attribute was added to our materials and I'm starting to replace the use of `reflectivity` with `ks`. The fresnel factor based on the light vector is calculated just as the one that is based on the view vector, correct? Instead of `fresnel_reflectance(v, n, rI1, rI2)` where `rI` are the refractive indices, I would do `fresnel_reflectance(l, n, rI1, rI2)` with the first parameter being the light vector instead of the view vector assuming the function is implemented correctly? (based on [this paper, page 6](http://stanford.io/1zco8xk))" CreationDate="2016-08-30T15:25:04.603" UserId="4962" ContentLicense="CC BY-SA 3.0" />
  <row Id="5077" PostId="3894" Score="0" Text="Ah! For opaque objects which I don't have a *real* refractive index for, I can determine that by using `ks` a.k.a. `reflectivity` as the solution for `fresnel_reflectance(l, n, rI1, rI2)` to get `rI1`. As for your section *For reflecting the rest of the scene* I would just use `ks` instead of calculating the Fresnel factor based on the view vector?" CreationDate="2016-08-30T16:17:38.717" UserId="4962" ContentLicense="CC BY-SA 3.0" />
  <row Id="5078" PostId="3894" Score="0" Text="Sorry for the confusion. I my last comment, instead of *For reflecting the rest of the scene* I meant *For each point light* and also the question concerns the light vector, not the view vector." CreationDate="2016-08-30T16:35:07.997" UserId="4962" ContentLicense="CC BY-SA 3.0" />
  <row Id="5079" PostId="3894" Score="0" Text="I will ask my questions in more detail in a [chat room](http://chat.stackexchange.com/rooms/44693/re-controlling-reflection-and-refraction-with-material-properties-in-ray-tracing) I created. If you want, you can join me there and help me out. This is not the right place for the comments." CreationDate="2016-08-30T17:48:46.767" UserId="4962" ContentLicense="CC BY-SA 3.0" />
  <row Id="5080" PostId="3932" Score="2" Text="Yes but due to numerical instability it will eventually skew. I should look up my lecture notes, havent revisited this after I stopped teaching." CreationDate="2016-08-31T05:40:06.783" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5081" PostId="3910" Score="0" Text="Tawhiri well your really trying to substitute a somewhat easy CAD problem with a horribly complicated programming problem. So we are talking 10 hours of getting intimate with advanced CAD components versus 1000 hours of calculus, programming and model fitting experience! I could do this for you but i doubt you can afford my services. Polygons are easy by comparasion." CreationDate="2016-08-31T07:30:34.683" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5082" PostId="1759" Score="2" Text="As [pointed out on meta](http://meta.computergraphics.stackexchange.com/questions/239/can-we-do-something-about-this-good-question-left-unanswered) this question is deserving of a good answer. I don't have one myself, but I'm happy to give some of my reputation to someone who does." CreationDate="2016-08-31T13:05:18.403" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5083" PostId="3932" Score="0" Text="Will it skew for any type of transformation, or only ones that are too large?" CreationDate="2016-08-31T14:42:41.197" UserId="4705" ContentLicense="CC BY-SA 3.0" />
  <row Id="5084" PostId="3931" Score="0" Text="Have you tried setting stride to 0in your glVertexAttribPointer calls? Having skim read your code it looks mostly ok :)" CreationDate="2016-09-01T06:23:03.643" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5085" PostId="3931" Score="0" Text="That worked for some reason. Unfortunately the rectangle only fills half the screen nomatter the scale factor." CreationDate="2016-09-01T06:37:00.323" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5086" PostId="3931" Score="0" Text="Your stride should be 0, usually that is only used when you have interleaved buffers (Pos&amp;Tex in the same VBO). For your half-screen thing I would need to see your orthographic matrix." CreationDate="2016-09-01T06:46:17.393" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5087" PostId="3931" Score="0" Text="@PaulHK Added to post, I use apples function for that and then put it into a float array. The width and height values are the same ones passed into the initialization of the ripple simulation." CreationDate="2016-09-01T06:48:31.220" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5088" PostId="3931" Score="0" Text="Can't see anywhere which would cause only half the screen to be visible, it could be a bad glViewPort setup ?" CreationDate="2016-09-01T06:56:53.743" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5089" PostId="3931" Score="0" Text="@paulhk I think it is that the triangles don't exist or aren't being drawn after the half way point. Sometimes it will have like triangles jetting out at the edge of the rectangle" CreationDate="2016-09-01T06:58:53.817" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5090" PostId="3931" Score="0" Text="@PaulHK I added a picture" CreationDate="2016-09-01T07:07:29.923" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5091" PostId="3931" Score="0" Text="I would suspect something is wrong with your indice buffer. It looks like there is a giant single triangle spanning your whole image (i can see a slanting near horizontal edge along the top amongst the saw tooth pattern). Your indice count for a grid of triangle strips should be ((W*2) + 1) * (H-1) * 2 indices. You only need 1 degenerate vertice per triangle strip emitted. Maybe enabling wireframe rendering will better help visualise this." CreationDate="2016-09-01T07:17:42.433" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5092" PostId="3934" Score="0" Text="Your code causes the same issue however it is significantly more efficent so thanks!, I have updated the post with some new diagrams that might help." CreationDate="2016-09-01T08:44:53.087" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5093" PostId="3934" Score="0" Text="It kind of looks like its being drawn as a triangle fan, but not quite. Is it possible you can make a small grid (say 9 points, so rSimXXX = 3) and then manually inspect/dump the index buffer ?" CreationDate="2016-09-01T08:59:55.597" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5094" PostId="3934" Score="0" Text="Sure, i added a section to the bottom of the post. Let me know if you need/want more data." CreationDate="2016-09-01T09:40:23.603" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5095" PostId="3934" Score="0" Text="Ok, things are starting to make sense, it looks like you have 0's interleaved for every other element, which explains all the spiky triangles going to one corner. I suspect your Indicie(..) objects are 32bits, but in the glDrawElements call you tell opengl the index buffer is 16bits (unsigned shorts). You can either change the glDrawElements to accept 32bit integers (change GL_UNSIGNED_SHORT -&gt; GL_UNSIGNED_INT) or change your Indicie object to be 16bit (I guess int-&gt;short but I am not familiar with swift)." CreationDate="2016-09-01T09:42:10.400" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5096" PostId="3934" Score="0" Text="I also editted my example above to remove the redundant end-of-row if(...)" CreationDate="2016-09-01T09:44:17.437" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5097" PostId="3934" Score="0" Text="OK so if I use glshorts then my draw call should have Glenum(gl_unsigned_short) however if I am using Ints  (32 I think) then it's glenum (gl_unsigned_int)?" CreationDate="2016-09-01T09:50:38.590" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5098" PostId="3934" Score="0" Text="Yes, I'm almost certain from your last image this is the cause of the problem. If you interpret a buffer full of 32bit ints as 16bit shorts, you will read the upper/lower halves of the 32bit indexes per 16bit index, which in your case will be index 0 every odd index." CreationDate="2016-09-01T09:57:20.047" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5099" PostId="3934" Score="0" Text="Yes you are most certainly right! It is working quite well now! If you could please add the bit about matching up the indicie data type with the gldraw call then I will award you the bounty when SO allows me to. Thankyou so much!" CreationDate="2016-09-01T09:58:45.507" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5100" PostId="3934" Score="0" Text="Brilliant, happy to help" CreationDate="2016-09-01T10:02:37.163" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5101" PostId="1759" Score="0" Text="@trichoplax the problem is that its done in matlab." CreationDate="2016-09-01T15:24:03.160" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5102" PostId="1759" Score="0" Text="@joojaa ah good point. If no matlab experts step in during the bounty period, I'll consider learning [Octave](https://en.wikipedia.org/wiki/GNU_Octave) to see if that's close enough to find a solution." CreationDate="2016-09-01T20:17:27.367" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5103" PostId="3932" Score="2" Text="A transformation matrix only captures the end result of a transformation, not &quot;how it got there&quot;. The matrix for 180° rotation in one direction, the matrix for 180° rotation in the other and the matrix for flipping the grid, that is scaling by (-1, -1), all look the same." CreationDate="2016-09-01T21:08:45.287" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="5104" PostId="1759" Score="0" Text="I understand what the matlab code is supposed to do. I don't understand some of the used terms. &quot;in camera coordinates&quot; means what? The camera is supposed to be at $(-1,1,5)$ and the plot does not have this position nor do the axis suggest the image is taken from that position." CreationDate="2016-09-02T08:28:57.793" UserId="273" ContentLicense="CC BY-SA 3.0" />
  <row Id="5105" PostId="1759" Score="1" Text="It's not very clear to me what the first image is supposed to mean. The second one is from the camera's point of view, and after a back of the envelope estimation I think it looks correct." CreationDate="2016-09-02T09:58:40.027" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5106" PostId="3938" Score="0" Text="You can not reliably work with polygon data in CAD applications. Why dont you use VBA to make the n discrete points directly in creo sketch and then sweep that? Note this is not what you asked. PS why dont you share a dummy formula and i can add instructions int the chat how to properly do this shape. PS this is not within scope of this forum." CreationDate="2016-09-02T10:28:15.520" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5107" PostId="3934" Score="0" Text="@J.Doe it's entirely your choice, but if you avoid awarding the bounty until nearer the end of the bounty period, then your question will continue to be promoted on the front page, which means more people will see it. This may mean more voting on both your question and on PaulHK's answer..." CreationDate="2016-09-02T12:35:13.583" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5108" PostId="3934" Score="0" Text="I've just been waiting for the 24 hours before bounty to expire. But now that you say that I'll hold off a bit so that perhaps he can get more votes." CreationDate="2016-09-02T16:34:35.657" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5109" PostId="3768" Score="0" Text="I did not figure out how to resolve it." CreationDate="2016-09-02T19:59:30.123" UserId="3328" ContentLicense="CC BY-SA 3.0" />
  <row Id="5110" PostId="3939" Score="1" Text="Can you expand the question a bit? Clarify what you mean by &quot;not shared by any other neighbour&quot;, and discuss how your current algorithm works? Looking at your other question it sounds like you have two clouds of points and you want to put them into 1:1 correspondence, so each point in cloud A is matched to exactly one in cloud B and vice versa. And you want the matched pairs of points to be close to each other. Is that right?" CreationDate="2016-09-02T21:53:15.507" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5111" PostId="3944" Score="0" Text="Thanks Aces, but I'm still a bit confused. You multiply each fragment by the view Matrix after the usual normal mapping, pass the x and y to the deferred shader, and then multiply by inverse the view Matrix during the deferred shader?" CreationDate="2016-09-03T03:37:28.513" UserId="5026" ContentLicense="CC BY-SA 3.0" />
  <row Id="5112" PostId="3943" Score="0" Text="To clarify, is the dot product you are referring to &quot;len&quot;?" CreationDate="2016-09-03T03:41:28.150" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5113" PostId="3943" Score="0" Text="Yes, I have used dot(delta, delta) before in place of dist * dist. I just did the dot product that way to simplify things." CreationDate="2016-09-03T03:43:57.580" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5115" PostId="3943" Score="0" Text="What is the average speed of the particle? Is it normal to be the same size as that blue diagram at the bottom? I'm just trying to get a good idea of the use cases." CreationDate="2016-09-03T04:02:03.603" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5116" PostId="3944" Score="0" Text="Yes, you can do that, but you can also just calculate the vertex normals through the vertex shader and apply the normal map from there. That way, you don't need to multiply each fragment by the View matrix." CreationDate="2016-09-03T04:05:47.103" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5117" PostId="3943" Score="0" Text="The simulation has many modes. Sometimes they move with herd mentality meaning they might all be longer or they all might be shorter. Sometimes they are vary individualistic with some being long and others short. I have thought about it quite a bit and thought the simulation the particles will be shorter and longer equally.  So unfortunately we can optimize due to use cases." CreationDate="2016-09-03T04:07:48.937" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5118" PostId="3943" Score="1" Text="@J.Doe: The only way you could avoid the dot product would be... by using a conditional branch around it. Which could wind up being slower than just doing the dot product. And let's face facts: a 2D vector dot-product is *trivially* fast on GPUs these days." CreationDate="2016-09-04T03:49:50.160" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="5119" PostId="3939" Score="0" Text="Yes, please check edit." CreationDate="2016-09-04T10:13:16.240" UserId="5021" ContentLicense="CC BY-SA 3.0" />
  <row Id="5120" PostId="312" Score="0" Text="note that the subscript is not a 0 (zero), but an O (oh).  it reads like... &quot;Light out at the out angle equations light emitted towards the out angle plus ... &quot;.  o and i are complements, meaning out and in respectively (:" CreationDate="2016-09-04T21:57:01.943" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5121" PostId="3939" Score="0" Text="The link you give to the code on Code Review.SE appears to be a question asking the same thing. Are you looking for the same thing from Computer Graphics.SE (a review of how to optimise for speed) or are you asking whether there is a different algorithm that gives different but acceptable results?" CreationDate="2016-09-04T22:13:32.713" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5122" PostId="3939" Score="0" Text="Do you just need a 1 to 1 pairing of points from set A with points from set B, or do you require the pairing that has the shortest possible total length?" CreationDate="2016-09-04T22:21:45.593" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5125" PostId="3943" Score="1" Text="This shader looks like it should be super fast already, can't think of any further optimizations really. If / else branches will only really hurt you in a shader if you have a fair bit of code in each branch, here you're not even doing any arithmetic in the branches, just returning values, so the performance hit should be negligible. You could always replace it with a ternary operator, as in gl_FragColor = (condition) ? true_value : false_value. This compiles to a conditional move rather than a branch, so might get you a small speed boost." CreationDate="2016-09-05T03:14:05.163" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="5126" PostId="3939" Score="1" Text="@trichoplax: I'm looking for another algorithm to make it faster. I need 1:1 pairing with shortest possible distance. My code is already working but it's too slow." CreationDate="2016-09-05T04:59:54.327" UserId="5021" ContentLicense="CC BY-SA 3.0" />
  <row Id="5127" PostId="3951" Score="0" Text="Thanks for the answer. I tried blurring the output but as you say I do just get a blurred edge which seems hard to get back without bringing the noise back in. &quot;applying an edge to a color buffer&quot; - think outline around geometry in games. I will try a bilateral blur instead of a Gaussian. Going to accept this as answer for now." CreationDate="2016-09-05T10:50:39.893" UserId="3331" ContentLicense="CC BY-SA 3.0" />
  <row Id="5128" PostId="3952" Score="0" Text="You don't project it.  You clip the polygon it against a front clipping plane (or more generally against the view frustum) and then project those results.&#xA;&#xA;I had a quick search to try to find a good page explaining the details but nothing really gave a simple/clear explanation. I could suggest a textbook if that'd help." CreationDate="2016-09-05T12:18:31.030" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5129" PostId="3952" Score="0" Text="@SimonF I would appreciate a textbook suggestion. Is the 'clip' approach then suitable for rendering? Moving around in a model a made, it does not look unnatural." CreationDate="2016-09-05T12:25:04.627" UserId="5043" ContentLicense="CC BY-SA 3.0" />
  <row Id="5130" PostId="3952" Score="0" Text="Actually it is possible to avoid near clipping by using homogeneous rendering but that has some other drawbacks.&#xA;Possible texts _include_  Watt &amp; Watt's &quot;Advanced Animation and Rendering Techniques&quot;  Foley, Van Dam et al's &quot;Computer Graphics: Principles and Practice&quot; - It's old but clipping hasn't changed.  &#xA;&#xA;I'm surprised nothing simple/clear shows up in my web searches (for example, [Wikipedia's](https://en.wikipedia.org/wiki/Clipping_(computer_graphics)#Clipping_in_3D_graphics) entry doesn't seem to contain much), but you could try searching for Weiler-Atherton or Cohen–Sutherland." CreationDate="2016-09-05T12:41:38.633" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5131" PostId="3951" Score="1" Text="Oh, I see. I would have answered differently if I knew what you were trying to achieve, as what I said was mainly for pixel shading on colors (I am more a computer vision person than graphics...). Have you seen this related question? http://answers.unity3d.com/questions/60155/is-there-a-shader-to-only-add-an-outline.html (First answer has a link to a code example)" CreationDate="2016-09-05T17:21:48.097" UserId="5039" ContentLicense="CC BY-SA 3.0" />
  <row Id="5132" PostId="3960" Score="0" Text="Were did you get the original equations from? I'm curious about the final part : return NL*(Spec + max(Diffuse, AmbientColor))*lightModifier;  Is there a reason to multiply by N.L? The Diffuse and Spec calculations already include this term." CreationDate="2016-09-06T08:21:17.597" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5133" PostId="3960" Score="0" Text="I think this might well be a point-lighting issue actually. This problem comes up in Unity, when using the standard shader, setting smoothness to 100% causes highlights to disappear completely. This is physically accurate, since the lights are being modelled as a single point, but obviously not what you want in a game." CreationDate="2016-09-06T08:36:06.157" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="5134" PostId="3960" Score="0" Text="@PaulHK Sorry, that was an oversight, but removing it didn't help. I reviewed multiple sources and found my favorite equations, including Unreal's paper, Frostbite's paper, and a couple tutorials, one by codinglabs, and one by trentreed." CreationDate="2016-09-06T09:07:31.357" UserId="5026" ContentLicense="CC BY-SA 3.0" />
  <row Id="5135" PostId="3960" Score="0" Text="@russ But isn't there a way around this? Point Lights still need to show...something. EDIT: Aside from giving it a size." CreationDate="2016-09-06T09:11:56.123" UserId="5026" ContentLicense="CC BY-SA 3.0" />
  <row Id="5136" PostId="3932" Score="0" Text="The first thought that comes to my mind is you could factor the matrix into a Möbius transformation and then lerp the quantities. https://en.wikipedia.org/wiki/M%C3%B6bius_transformation#Decomposition_and_elementary_properties" CreationDate="2016-09-06T09:22:29.000" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="5137" PostId="3960" Score="0" Text="@russ - Would using swapping to 1-Smoothness^4 and 0.16*Specular^2 help? This was suggested in the Frostbite papers, but it might be specific to them. I've tried it, and it just lowers the reflections on the nonmetals a bit, plus widens the reflection on the metals. I'm not sure if this is physically accurate. Also, I retract what I said, without NL, it's bright all over." CreationDate="2016-09-06T09:45:08.723" UserId="5026" ContentLicense="CC BY-SA 3.0" />
  <row Id="5138" PostId="3928" Score="0" Text="Same thing different way on xforms: converting a complex number to matrix M(2,R) as above is an isomorphism.  Addition-&gt; addition, product-&gt;product, conjugate-&gt;transpose.  All complex ops can be expressed in terms of these. https://en.wikipedia.org/wiki/2_%C3%97_2_real_matrices" CreationDate="2016-09-06T09:46:46.523" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="5139" PostId="3925" Score="1" Text="The problem with quaternions is the literature.  Most standard operations are complex numbers wearing a mustache." CreationDate="2016-09-06T09:49:49.813" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="5140" PostId="3925" Score="0" Text="@MBReynolds well that is to be expected, having 3 separate imaginary axes certainly has that effect. But then multiplicative transforns have their share of problems." CreationDate="2016-09-06T10:27:45.700" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5141" PostId="3925" Score="0" Text="Anything specific you thinking of?" CreationDate="2016-09-06T11:49:43.297" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="5142" PostId="3932" Score="0" Text="I hadn't heard of the Möbius transformation before, thank you!" CreationDate="2016-09-06T14:19:59.963" UserId="4705" ContentLicense="CC BY-SA 3.0" />
  <row Id="5143" PostId="3960" Score="0" Text="@Karim, I'm not that knowledgeable about PBR, just at the stage of digging into Unity's shaders and trying to understand them myself. But vanishing highlights at full smoothness are not 'wrong' from a physical perspective, just an artifact of using infinitely small light sources. Maybe someone on the forum will have a workaround for it but not me unfortunately. Don't worry too much about being physically 'accurate' though, it's still realtime CG after all. Cook-Torrance and the likes are not accurate, just plausible." CreationDate="2016-09-06T17:32:23.470" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="5144" PostId="3867" Score="0" Text="I recently read that both stratified sampling and low discrepency sequences (like halton, sobol, etc) are a middle ground between random sampling (which creates noise) and uniform sampling (which creates aliasing)." CreationDate="2016-09-06T21:04:58.757" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5147" PostId="3954" Score="0" Text="Just so you know, [self answers are very welcome here](http://computergraphics.stackexchange.com/help/self-answer), whether you know the answer already or find it after posting the question." CreationDate="2016-09-07T10:32:24.703" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5148" PostId="3964" Score="0" Text="When you say &quot;gradients&quot;, do you mean visible discontinuities in the colour? I'm assuming you don't want flat colour, but a smooth gradient with no bands visible?" CreationDate="2016-09-07T20:53:47.633" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5149" PostId="3964" Score="0" Text="That is exactly what I want." CreationDate="2016-09-07T20:59:10.497" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="5151" PostId="3965" Score="0" Text="Nice trick. I noticed you're adding .5 to pixel coordinates in the Shadertoy, is this to hit pixel centres in the texture rather than lerped samples?" CreationDate="2016-09-08T06:23:01.810" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="5152" PostId="3965" Score="2" Text="@russ Hmm, I actually copy-pasted that bit from another Shadertoy. :D On a second look, I think that .5 is actually not needed; `fragCoord` should be on pixel centers already." CreationDate="2016-09-08T06:28:14.263" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5155" PostId="144" Score="0" Text="Funny, and on Intel Haswell GPU ARBfp's `DP3` instruction appears implemented as 3 multiplies and 2 additions, see `INTEL_DEBUG=fs` output from Intel Linux driver: http://paste.ubuntu.com/23150494/ . Not sure whether it's just poor driver or the HW really doesn't have special vector mul instructions." CreationDate="2016-09-08T15:14:00.117" UserId="4647" ContentLicense="CC BY-SA 3.0" />
  <row Id="5159" PostId="144" Score="0" Text="@Ruslan Very likely that hardware just doesn't have special vector mul instructions. More accurately, though, they _do_, but they are vectorized across the SIMD width of the architecture (the lanes), not vectorized across the vec3/vec4 dimension." CreationDate="2016-09-08T18:17:01.540" UserId="196" ContentLicense="CC BY-SA 3.0" />
  <row Id="5160" PostId="3967" Score="0" Text="http://gizmodo.com/17-amazing-renders-that-youll-swear-are-photographs-1399998593" CreationDate="2016-09-09T03:37:42.483" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5161" PostId="3968" Score="1" Text="One of the property of all these images which betrays them is that they show ideally clean surfaces. In real world the surfaces are rarely so clean, even in jewelry shops." CreationDate="2016-09-09T09:41:06.030" UserId="4647" ContentLicense="CC BY-SA 3.0" />
  <row Id="5162" PostId="3970" Score="1" Text="Your monitor probably needs calibrating/adjusting if it looks like it's linear. To get a good range of colour brightness without obvious banding for the (non-linear) human visual system when you only have a very limited, 8 bits per channel, you need to use a non-linear image encoding as sRGB.  Typically monitors should support this but it's not uncommon for them to be poorly calibrated especially if someone's been fiddling with the contrast/brightness controls.  Your monitor might need adjusting. To summarise: render in linear with *more* than 8bpc (e.g 12 or half-float) and map to sRGB." CreationDate="2016-09-09T11:21:56.950" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5163" PostId="3971" Score="0" Text="Could you explain what you mean by mass in this context? Does it depend on the colour values of the hexagonal cells? Are you trying to find the centre of the light pixels/dark pixels/a particular specified colour? This is likely to affect the answer so it's worth giving as much detail as you have." CreationDate="2016-09-09T11:50:24.430" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5164" PostId="3970" Score="0" Text="@SimonF I don't think so. See [the linked question](http://computergraphics.stackexchange.com/q/1587/4647), it says that what I observe is as expected — superlinear change of brightness with input data. Also my monitor shows [this image](https://en.wikipedia.org/wiki/SRGB#/media/File:Srgbnonlinearity.png) as said in its description, suggesting that it's calibrated satisfactorily." CreationDate="2016-09-09T12:18:15.037" UserId="4647" ContentLicense="CC BY-SA 3.0" />
  <row Id="5165" PostId="3970" Score="1" Text="Sorry, caffeine deficiency!&#xA;RE _&quot; When you're going to present this on the screen, should the values be transformed as Clinear→CsRGB as given here&quot;_   It's that one, (not the reverse). What you should expect is that (and this is from memory so I may be off by a few values) approx &quot;128/255&quot; in linear should map to about &quot;187/255&quot; in sRGB.   Since the eye can perceive smaller deltas in brightness when the colours are darker, you want more displayable shades in the darker end of 'the palette' than at the brighter end. Also, at the start of the linked Q, subst sRGB for RGB to make correct." CreationDate="2016-09-09T13:25:44.833" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5166" PostId="3971" Score="0" Text="Good question. Yes colour value is good enough. Actually I'll be working with B&amp;W, so its really the cog of the ink." CreationDate="2016-09-09T14:32:52.820" UserId="5087" ContentLicense="CC BY-SA 3.0" />
  <row Id="5167" PostId="3970" Score="0" Text="@SimonF thanks, I seem to have realized where I had a mistake. Made an answer to my own question now." CreationDate="2016-09-09T14:34:44.117" UserId="4647" ContentLicense="CC BY-SA 3.0" />
  <row Id="5168" PostId="3972" Score="2" Text="[WebGL Path Tracing demo](http://madebyevan.com/webgl-path-tracing/) is also not very fast converging for diffuse reflectors. So yours is at least not the only one with such a property." CreationDate="2016-09-09T14:41:11.993" UserId="4647" ContentLicense="CC BY-SA 3.0" />
  <row Id="5169" PostId="3965" Score="3" Text="For anyone interested, [here](http://www.anisopteragames.com/how-to-fix-color-banding-with-dithering/) is also a well described article how to implement dithering to avoid color banding. Happy coding!" CreationDate="2016-09-09T14:38:59.200" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="5170" PostId="3973" Score="2" Text="Shouldn't the angle be `atan(c/a)`? (Or better `atan2(c, a)`, if your language supports that.) Also it's worth noting this assumes the matrix doesn't include shear. If it does, you could try extracting that part and interpolating it separately as well." CreationDate="2016-09-09T17:45:22.420" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5171" PostId="3964" Score="0" Text="Are you passing lightDir is in viewspace in this case?" CreationDate="2016-09-10T00:49:21.630" UserId="5095" ContentLicense="CC BY-SA 3.0" />
  <row Id="5172" PostId="3977" Score="0" Text="Could you clarify what you mean? It may help to include your motivation for wanting this effect, and to use simpler example images using only 2 or 3 objects." CreationDate="2016-09-10T05:33:32.203" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5173" PostId="3964" Score="0" Text="Yes, I do lighting in view-space." CreationDate="2016-09-10T10:03:47.850" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="5175" PostId="3973" Score="1" Text="In a pure rotation matrix, _c_ = sin(_t_), and both _a_ and _d_ equal cos(_t_). So maybe both your answers work?" CreationDate="2016-09-10T15:53:10.723" UserId="4705" ContentLicense="CC BY-SA 3.0" />
  <row Id="5176" PostId="3980" Score="0" Text="I'm so sorry, I'm an idiot. I thought it was there.." CreationDate="2016-09-10T16:47:19.610" UserId="5087" ContentLicense="CC BY-SA 3.0" />
  <row Id="5179" PostId="3980" Score="0" Text="OK. What's special about the areas removed is one of: There is a straight edged border (at least one). There is a border that is the arc of a circle - an interior or exterior border. The items left have irregular borders, with no straight edges and no arcs." CreationDate="2016-09-10T17:59:10.450" UserId="5087" ContentLicense="CC BY-SA 3.0" />
  <row Id="5180" PostId="3973" Score="1" Text="Thanks @NathanReed, for correcting the angle! And indeed, this does not take shear into account. I'm used to working in 3D were we can usually get away with ignoring shear, but in 2D transforms I guess shear is used much more frequently." CreationDate="2016-09-10T18:16:20.873" UserId="5088" ContentLicense="CC BY-SA 3.0" />
  <row Id="5181" PostId="3973" Score="0" Text="@Vermillion: indeed, for a pure rotation matrix they are the same, but for a scaled matrix the scale is different per column (so if you want to use element d in stead, you need to calculate scale first and do something like atan((sx / sy) * c / d)." CreationDate="2016-09-10T18:16:27.990" UserId="5088" ContentLicense="CC BY-SA 3.0" />
  <row Id="5183" PostId="3980" Score="0" Text="OK, thank you, that's a fair comment. Would it help the straight edge, or arc, will be at least x pixels or cm long? This will be the case, what I'm wanting to keep will never have a straight border, or a proper arc, more than .5 cm long. Does that make it easier?" CreationDate="2016-09-10T19:10:11.877" UserId="5087" ContentLicense="CC BY-SA 3.0" />
  <row Id="5185" PostId="3980" Score="0" Text="I am searching medical  images that have things like clips that I need to eliminate, so I can concentrate on the anatomy - our anatomy doesn't have straight lines, or, at the level I'm working at, arcs." CreationDate="2016-09-10T20:51:10.810" UserId="5087" ContentLicense="CC BY-SA 3.0" />
  <row Id="5186" PostId="3980" Score="0" Text="Ah I see - you have a practical real world problem rather than a computer graphics problem. This sounds like it will be off topic. Image processing is on topic here, but this sounds like it would need a sophisticated computer vision/machine learning approach, which goes outside our scope." CreationDate="2016-09-10T22:23:32.250" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5188" PostId="3980" Score="0" Text="It's difficult to judge what is simple image processing and what is far more complex. [This xkcd](http://xkcd.com/1425/) sums it up well." CreationDate="2016-09-10T22:26:05.573" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5189" PostId="3980" Score="0" Text="LOL! Yes, xkcd nails it (as it does so many things) brilliantly. I do understand. I'll look for a good representative image." CreationDate="2016-09-11T01:54:08.837" UserId="5087" ContentLicense="CC BY-SA 3.0" />
  <row Id="5190" PostId="3980" Score="0" Text="I've added a representative image - it doesn't show any clips, but the straight-edged areas it show are probably better, if I could remove them, then a rectangle, or perfect circle / ellipse, should be easy." CreationDate="2016-09-11T02:09:53.470" UserId="5087" ContentLicense="CC BY-SA 3.0" />
  <row Id="5191" PostId="3980" Score="0" Text="It's worth reading about the [XY problem](http://meta.stackexchange.com/a/66378/258647)." CreationDate="2016-09-11T15:24:44.723" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5192" PostId="3980" Score="2" Text="I suspect that the overall problem of finding dense masses will require a sophisticated machine learning approach, which may render the initial removal of smooth edges redundant." CreationDate="2016-09-11T15:25:50.683" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5193" PostId="3980" Score="0" Text="To summarise, I suspect that the only part of the solution that will be on topic here may turn out to be unnecessary. It may be worth having a look at [Data Science Stack Exchange](http://datascience.stackexchange.com/help/on-topic), where machine learning questions are on topic." CreationDate="2016-09-11T15:38:40.540" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5194" PostId="3982" Score="2" Text="For sparks, try a Google for &quot;particle systems&quot;" CreationDate="2016-09-11T15:55:13.157" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5195" PostId="3988" Score="0" Text="Altough it could be used to other things" CreationDate="2016-09-11T20:09:51.170" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5198" PostId="3990" Score="1" Text="Optimized for what? Memory use, speed etc. ? How often does your data update  and so on." CreationDate="2016-09-12T07:32:48.470" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5199" PostId="3990" Score="0" Text="Optimized for speed" CreationDate="2016-09-12T07:36:12.413" UserId="5089" ContentLicense="CC BY-SA 3.0" />
  <row Id="5204" PostId="3995" Score="0" Text="If you don't get good answers, graphicscodex.com talks about these things." CreationDate="2016-09-12T15:17:37.053" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5205" PostId="3996" Score="2" Text="This also works if you keep the same depth buffer for the two render passes without clearing it between them." CreationDate="2016-09-12T16:52:09.167" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5207" PostId="3998" Score="1" Text="The title refers to `Image2D` and the code and body refer to `image3D`. Does one of these need to be edited? Which one are you asking about?" CreationDate="2016-09-12T18:16:56.613" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5208" PostId="3959" Score="0" Text="Hi @Nathan, thank you for your detailed answer. I know this is a huge topic (and a big subject of study). The biggest problem is that the documentation online is fragmented, and it is difficult to find a good approch. My project is this http://goo.gl/Fgo21x: a BRDF viewer (inspired by WDAS viewer) to show the most common empirical and physically based BRDF models and that supports the colours calculation using Spectral data -  tristimulus values. This is an educational project to study OpenGL." CreationDate="2016-09-12T23:50:28.343" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="5209" PostId="3959" Score="0" Text="I think that a good first approach could be to use the common extension you mentioned: SH or small cubemap + environmental cube map (for reflection and refraction). What do you think about it? (I'm developing this application after work during my sleepless nights :)). Thank you again for the collection of sources you linked above (I have a lot of material to study now)." CreationDate="2016-09-12T23:53:34.133" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="5210" PostId="4004" Score="2" Text="The normal way to draw a static background if to draw a static textured full-screen quad." CreationDate="2016-09-13T13:25:42.010" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5211" PostId="3997" Score="0" Text="Just to make sure: If I had a double between two floats, would the two floats also be automatically expanded?" CreationDate="2016-09-13T14:00:53.110" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="5212" PostId="3997" Score="0" Text="@enne87 depends on the surrounding values and the precise rules of the padding algorithm." CreationDate="2016-09-13T14:05:52.753" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5213" PostId="4003" Score="2" Text="Just to add some more details about how the right answer can be situational, if your application is CPU bound, that means your GPU is doing less work and is waiting on your CPU between frames.  In this case, it would be less expensive to do it on the GPU since the CPU is already working the entire time, but the GPU isn't.  If you are GPU bound, the reverse is true.  Also, passing tangent and bitangent increases memory usage. If you are memory bound, calculating in the shader can be a good move." CreationDate="2016-09-13T14:17:57.563" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5214" PostId="3997" Score="0" Text="Aha, I thought this is standardized in every OpenGL implementation. Thanks anyway." CreationDate="2016-09-13T18:04:52.723" UserId="3377" ContentLicense="CC BY-SA 3.0" />
  <row Id="5215" PostId="3997" Score="1" Text="Well this layout is by the C# marshaller which is independent of OpenGL" CreationDate="2016-09-13T18:13:54.533" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5216" PostId="3959" Score="0" Text="@FabrizioDuroni Yep! For a BRDF viewer, a simple directional ambient plus an environmental cubemap should be great." CreationDate="2016-09-13T20:29:39.210" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5217" PostId="4000" Score="1" Text="Is there a typo in the third bullet point? I'm guessing it should say &quot;they do _not_ need to be part of framebuffer color attachments to write into them&quot;?" CreationDate="2016-09-14T00:46:37.490" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5218" PostId="4005" Score="0" Text="[Relevant](http://computergraphics.stackexchange.com/questions/2482/choosing-reflection-or-refraction-in-path-tracing)" CreationDate="2016-09-14T03:59:45.393" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5219" PostId="4004" Score="0" Text="@ratchetfreak please make that comment an answer, so we can vote it up :)" CreationDate="2016-09-14T06:08:37.170" UserId="5088" ContentLicense="CC BY-SA 3.0" />
  <row Id="5220" PostId="4000" Score="0" Text="Yes you are right. Hmm strange, I meant the opposite - edited." CreationDate="2016-09-14T09:05:20.380" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5222" PostId="4008" Score="0" Text="Assuming this is from an exercise question, does the question specify whether the height of the cylinder is the same as the distance between the specified points?" CreationDate="2016-09-14T12:24:32.390" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5223" PostId="4008" Score="0" Text="We need to calculate the height based on the distance between the two points." CreationDate="2016-09-14T12:26:11.993" UserId="5121" ContentLicense="CC BY-SA 3.0" />
  <row Id="5224" PostId="4008" Score="0" Text="And the result is wrong because it doesn't join the two points. I drawn the points(as small spheres) and the cylinder but the cylinder is attached only the point I have translated it to." CreationDate="2016-09-14T12:32:31.017" UserId="5121" ContentLicense="CC BY-SA 3.0" />
  <row Id="5226" PostId="4005" Score="0" Text="The proportions of light that refract and reflect are dependent on incident angle. Are you looking to take this into account? For example, the [Fresnel equations](https://en.wikipedia.org/wiki/Fresnel_equations) or [Schlick's approximation](https://en.wikipedia.org/wiki/Schlick%27s_approximation) if you want to ignore polarisation." CreationDate="2016-09-14T13:25:26.517" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5227" PostId="3979" Score="0" Text="I don't quite get what you mean. If you compute a value per vertex it will get interpolated over the surrounding triangles. Do you want each triangle to get a different stencil value, computed from its vertex normals somehow? In any case, outputting to the stencil buffer from a shader is not widely supported (though it can be done with [this extension](https://www.opengl.org/registry/specs/ARB/shader_stencil_export.txt)) , so you probably want to use a regular offscreen render target instead." CreationDate="2016-09-14T16:09:22.993" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5228" PostId="4009" Score="2" Text="As an addendum: The answer to this other question has some sample code for russian roulette. http://computergraphics.stackexchange.com/questions/2316/is-russian-roulette-really-the-answer/2325#2325" CreationDate="2016-09-14T19:23:27.573" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="5229" PostId="4008" Score="0" Text="A picture would definitely be helpful, you should also note that the cross product of two parallel vectors leads to the zero vector, which can happen if the vector p points in the same direction as the vector n. Are the 2 points in 3D lying on the xy plane as well?" CreationDate="2016-09-14T21:28:10.253" UserId="2351" ContentLicense="CC BY-SA 3.0" />
  <row Id="5230" PostId="4007" Score="0" Text="I tried, but no result..." CreationDate="2016-09-15T04:19:29.677" UserId="5117" ContentLicense="CC BY-SA 3.0" />
  <row Id="5231" PostId="4004" Score="0" Text="Without access to the code, it is difficult to guess what might have gone wrong. The link does not work for me (perhaps it requires you to be logged into your storage account). Would you be happy to paste the code here? If so you can [edit] and the question can be reopened. Until then I'm putting the question on hold so that there will be no further answers until it is clarified, to avoid further confusion." CreationDate="2016-09-15T08:58:45.180" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5233" PostId="3823" Score="0" Text="I can't find a really good solution but the closer I could get was to use Aforge library to help me detect blobs, not really a good solution but works. &#xA;&#xA;For the project where I work we change the operation so now people decide if the document has quality or not. &#xA;&#xA;For compressing I divide TIFF in pages then convert to JPEG reduce the quality and then add it to a pdf file, sometimes achieving around 80% less space occupied." CreationDate="2016-09-15T09:10:03.727" UserId="4866" ContentLicense="CC BY-SA 3.0" />
  <row Id="5234" PostId="4004" Score="0" Text="@trichoplax what do you think about github or mail, becouse the code is long and i think that if you look the all code will be easier what i wrong. Thank you so much!" CreationDate="2016-09-15T13:31:34.940" UserId="5117" ContentLicense="CC BY-SA 3.0" />
  <row Id="5235" PostId="4004" Score="0" Text="I recommend separating out just the part of the code that places the background, and seeing if it works on its own. If it doesn't, you can post just that part knowing that it contains a problem. If it does work, add in the simplest extra code that causes the problem, then post that slightly longer code knowing that it contains a problem." CreationDate="2016-09-15T18:13:48.977" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5236" PostId="4013" Score="0" Text="I'm finding some answers here: http://groups.csail.mit.edu/graphics/classes/6.837/F04/lectures/14_Sampling-used6.pdf" CreationDate="2016-09-16T18:04:09.997" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5237" PostId="4015" Score="0" Text="Thanks for the great link! the last link on that page seems to dive right into this.  In practical terms, do you happen to know if doing anything above a box blur really gives much visual difference? Or would it give any boost to convergence?" CreationDate="2016-09-16T18:47:58.780" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5238" PostId="4015" Score="2" Text="Its the best primer to 3D graphics rendering concepts that i have ever seen (Tough its not so much about physical based or tracing stuff but anyway). A sinc filtered image is much sharper than the box filter gives a very blurred look and feel. Would it converge faster, I doubt it." CreationDate="2016-09-16T19:02:08.410" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5239" PostId="4012" Score="0" Text="you can't do 3d translation with a 3x3 matrix, you need a 4x4.  The rest I totally agree with :P" CreationDate="2016-09-16T20:48:07.340" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5240" PostId="4012" Score="1" Text="Sorry if it was unclear. I was trying to say that you can do translation of 2D objects. I'll update to make that more clear." CreationDate="2016-09-16T20:49:38.973" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5241" PostId="4007" Score="0" Text="Or you simply put the square behind everything and render it same way as everything else." CreationDate="2016-09-16T21:08:18.380" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5244" PostId="4016" Score="0" Text="Could we see an image of how it looks?  You said it looks the same with tube lights vs not so if you don't feel like posting both images that's ok, but an image alone might shed some light on things (:" CreationDate="2016-09-16T23:54:53.233" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5245" PostId="4016" Score="0" Text="cant post images as i need 10 rep to do more then 2 links" CreationDate="2016-09-17T00:11:49.867" UserId="5136" ContentLicense="CC BY-SA 3.0" />
  <row Id="5246" PostId="4016" Score="0" Text="Annoying. Someone please upvote this question more :p" CreationDate="2016-09-17T00:18:24.220" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5247" PostId="4016" Score="0" Text="Could try this [http://imgur.com/a/IxhTd](http://imgur.com/a/IxhTd)" CreationDate="2016-09-17T00:21:00.593" UserId="5136" ContentLicense="CC BY-SA 3.0" />
  <row Id="5248" PostId="4016" Score="0" Text="I added the link.  For whatever it's worth, is the top or bottom image supposed to be the tube light?" CreationDate="2016-09-17T00:23:04.243" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5249" PostId="4016" Score="0" Text="yea the botton one is tube lights, the sizes are currently set through defines as its easier to debug. the values are  #define LightRad 0.4&#xA;    #define LightLengh 0.5" CreationDate="2016-09-17T00:24:22.033" UserId="5136" ContentLicense="CC BY-SA 3.0" />
  <row Id="5253" PostId="4016" Score="0" Text="I've edited to clarify which paper in the link is the Epic one - please edit if I've misinterpreted your intention. You should be able to edit in the link to the website you mentioned now too, with your increased reputation." CreationDate="2016-09-17T08:32:52.263" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5254" PostId="4016" Score="0" Text="thanks for cleaning it up a little, i changed the image to the tube lighting one as i felt it was more relevant :)" CreationDate="2016-09-17T08:48:28.493" UserId="5136" ContentLicense="CC BY-SA 3.0" />
  <row Id="5255" PostId="4004" Score="0" Text="I did that. Do you have any ideas? @trichoplax" CreationDate="2016-09-17T10:18:16.717" UserId="5117" ContentLicense="CC BY-SA 3.0" />
  <row Id="5256" PostId="4007" Score="0" Text="@joojaa Can you look my code and say me where i am wrong. I tried everything, but maybe there ware mistakes in my code." CreationDate="2016-09-17T12:11:56.597" UserId="5117" ContentLicense="CC BY-SA 3.0" />
  <row Id="5257" PostId="4007" Score="0" Text="@I.To I made an update, tl;dr you draw, clear and draw again move the first draw to after the clear." CreationDate="2016-09-17T12:31:16.757" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5259" PostId="4019" Score="0" Text="Your second question isn't something anyone else can answer. It's like asking, &quot;I want to plot 'sin(ωx);', what should 'ω' be?&quot; It depends on what you're trying to accomplish. You can set it to 1 to see what it looks like, and then try other values. Or, if you have a specific use-case in mind, explain that, and then maybe we can help you." CreationDate="2016-09-17T16:28:08.563" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5260" PostId="4019" Score="0" Text="@user1118321, Thank you very much for your response. OK. here is my use-case:  ...  ...  I need to implement Gabor Filter using both spatial and frequency domain equations. In the link ...  http://www.cs.utah.edu/~arul/report/node13.html  ..., you can see that there are several equations. (14) is the equation of Gabor Filter in spatial domain. (15) is the equation of Gabor Filter in frequency domain. Hence, my question." CreationDate="2016-09-17T16:41:35.307" UserDisplayName="user464" ContentLicense="CC BY-SA 3.0" />
  <row Id="5261" PostId="4020" Score="0" Text="Just a beginner question, so sorry in advance. Is what you explained equivalent to taking real component as x-axis and imaginary as y-axis ? if not, then can we also do that to represent a complex function ?" CreationDate="2016-09-17T17:47:46.303" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="5262" PostId="4020" Score="0" Text="@A---B The input to the function is the position on screen, with x = real, y = imaginary. The output is the color, with red = real, green = imaginary." CreationDate="2016-09-17T20:52:00.397" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5263" PostId="4016" Score="0" Text="Added a little more detail on multiplying in the light position into L01, it creates a closer to tube look but is not right as the tube rotates." CreationDate="2016-09-17T21:46:18.580" UserId="5136" ContentLicense="CC BY-SA 3.0" />
  <row Id="5265" PostId="3982" Score="0" Text="As for the second part of your question, I would start with Processing, or maybe OpenFrameworks if you know some C++ already. These both use OpenGL under the hood but don't force you to write 100 lines of boilerplate just to get a window open..." CreationDate="2016-09-18T05:37:32.630" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="5267" PostId="3841" Score="1" Text="It turned out that I had incorrectly believed that physical LIDAR units had some algorithmic way of &quot;straightening&quot; out the curves, which is not the case.  Your post helped me reach a final conclusion, so thank you, and I selected it as an answer as you were correct." CreationDate="2016-09-19T01:51:17.410" UserId="4886" ContentLicense="CC BY-SA 3.0" />
  <row Id="5268" PostId="4012" Score="0" Text="You can use 3x3 matrices for 3D animation though if the bones are connected at their end points and do not exhibit scaling (e.g., for human-shaped skeletons where all you need is the rotation matrix and bone length), and this can significantly reduce memory/bus-bandwidth." CreationDate="2016-09-19T03:20:21.600" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5272" PostId="4024" Score="0" Text="@justanothercoder While an ok answer. It does nothing for dispelling the myth that pixels are squares. You are dealing with point samples (that are not squares). And that resampling is in fact consisting of 2 distinct phases rolled into one, making the datafield continious and then sampling said continious function." CreationDate="2016-09-19T14:57:51.080" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5278" PostId="4018" Score="0" Text="Excellent answer, as always." CreationDate="2016-09-20T10:38:59.547" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="5282" PostId="4025" Score="0" Text="Nice looking results!" CreationDate="2016-09-20T16:37:24.520" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5283" PostId="4028" Score="0" Text="thanks, still both Diffuse and Specular should be colored" CreationDate="2016-09-20T17:15:14.033" UserId="2069" ContentLicense="CC BY-SA 3.0" />
  <row Id="5288" PostId="4030" Score="0" Text="Since comments were cleared theres nolonger a link." CreationDate="2016-09-22T18:42:19.030" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5289" PostId="4029" Score="0" Text="[Related](http://computergraphics.stackexchange.com/questions/2310/could-we-dispense-the-near-clipping-plane)" CreationDate="2016-09-22T22:15:20.343" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5290" PostId="4030" Score="0" Text="@joojaa even after the comment was auto-deleted, the linked question still shows in the &quot;linked&quot; section at the top right of this page. Since that's easy to miss I've added a comment with the link too." CreationDate="2016-09-22T22:16:47.903" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5291" PostId="4030" Score="0" Text="I still don't know how I can deal with internal register overflow problem. For example, we adopt a world window whose x coordinate from 0~1023, that's a 10 digits binary. While scanning, a line whose x coordinates exceeds the 1023 will have more than 10 digits, then the extra part will be ignored,  right? So this line will be display in 0~1023 area. How can I deal with this ?" CreationDate="2016-09-23T04:00:45.753" UserId="5105" ContentLicense="CC BY-SA 3.0" />
  <row Id="5292" PostId="4030" Score="0" Text="@zfb Just as an aside, typically  HW rasterisers will have, say, an additional 4 to 8 bits of sub-pixel precision to allow smoother animation.  As for raster scanning, you'd only start/end on pixels inside the window. Out of curiosity are you writing a software rasteriser?" CreationDate="2016-09-23T08:11:59.063" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5293" PostId="4030" Score="0" Text="I am still studying this, no creation..." CreationDate="2016-09-23T13:20:25.837" UserId="5105" ContentLicense="CC BY-SA 3.0" />
  <row Id="5294" PostId="4033" Score="0" Text="I wonder if it's the texture read(s?) that are slow.  If you don't use the value that results from the texture read, it probably isn't even doing the texture read.  You could try doing dummer math with those values to see if it's still slow.  like maybe return shadowValue + shadowTexZ;" CreationDate="2016-09-23T23:22:27.310" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5295" PostId="4033" Score="0" Text="I also think the compiler may be optimizing out some parts of your code if you comment this line. Please check the lower level generated code to be sure. Moreover, you should check the step function documentation for optimizing out this particular branch." CreationDate="2016-09-24T04:42:17.613" UserId="4768" ContentLicense="CC BY-SA 3.0" />
  <row Id="5296" PostId="4032" Score="0" Text="I'll give this a go asap and return with the results" CreationDate="2016-09-24T11:36:58.767" UserId="5171" ContentLicense="CC BY-SA 3.0" />
  <row Id="5298" PostId="4034" Score="2" Text="Could you share you reasons for settling on these approaches? The logic behind it or profiling evidence perhaps?" CreationDate="2016-09-25T11:40:28.387" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5299" PostId="4032" Score="0" Text="It worked Thanks a tonn man, it worked exactly I wanted it to" CreationDate="2016-09-25T12:20:40.937" UserId="5171" ContentLicense="CC BY-SA 3.0" />
  <row Id="5300" PostId="4032" Score="0" Text="see the result here BEFORE https://i.gyazo.com/a314d1f7a0509788c272f83279c5077b.png   and AFTER https://i.gyazo.com/b879db7294d82a72b86cc4eaf0132a13.png" CreationDate="2016-09-25T12:23:22.877" UserId="5171" ContentLicense="CC BY-SA 3.0" />
  <row Id="5301" PostId="4034" Score="0" Text="The first one to me seems like the most straightforward approach to obtain the desired result and the second one is something the GPU potentially might be better suited for than comparisons and type casting. Maybe I'm completely wrong, but it's worth a try at least." CreationDate="2016-09-25T14:30:10.763" UserId="2811" ContentLicense="CC BY-SA 3.0" />
  <row Id="5302" PostId="4032" Score="0" Text="@Allahjane: glad to hear it worked fine. :)" CreationDate="2016-09-25T16:07:43.850" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5303" PostId="4034" Score="2" Text="These seem like points worth expanding your answer with." CreationDate="2016-09-25T18:55:57.403" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5305" PostId="4037" Score="2" Text="Thanks for your guides. for generate a cube map with 32x32x6, we only use 32x32x6 texels from 64x64x6 ( i mean we only compute 32x32x6 texels), so why we have to multiply 32x32x6 to 64x64x6 again ? (i think my question would be answered if i understand why diffuse equation combines all texels of the environment maps. and also i can't understand the relation of diffuse equation and cube map). since i have learned computer graphics by myself, some stupid questions arise when i'm facing to bigger problems. sorry about that :D" CreationDate="2016-09-26T08:56:04.340" UserId="5182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5307" PostId="4021" Score="0" Text="This is for deferred lighting pipeline, right? Are you using tiled implementation to reduce bandwidth? Also, have you considered tiled deferred shading or tiled forward shading instead?" CreationDate="2016-09-26T14:50:33.917" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5311" PostId="2472" Score="0" Text="Are you implementing the original Phong IS or modified (cos weighted) Phong IS? The multiplier (pow+1)/(pow+2) indicates the latter but you seem to be missing the cos weighting and it should be (pow+2)/(pow+1). Didn't check if the rotation of the IS to reflection vector is correct but I assume so." CreationDate="2016-09-27T13:32:36.983" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5312" PostId="4045" Score="0" Text="Thank you, that was helpful. I will be checking out the conversions. I am using OpenGL." CreationDate="2016-09-28T03:24:27.023" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="5313" PostId="4045" Score="0" Text="For OpenGL, converting to and from YCbCr should just be a matrix multiply of your color value, so just a single instruction in a glsl shader." CreationDate="2016-09-28T03:40:33.747" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5314" PostId="4042" Score="2" Text="Also just for your information you are learning [legacy OpenGL](https://www.opengl.org/wiki/Legacy_OpenGL) which is obsolete since 2008. Since your spending effort in learning atleast go and learn relevant OpenGL. No you do not nedd matrices but the alternative is more painful." CreationDate="2016-09-28T14:48:12.493" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5316" PostId="4049" Score="3" Text="You could look into stencil buffers. You can draw the masking triangle into the stencil buffer only. Then the rest of the circle can be drawn using the stencil buffer as a mask." CreationDate="2016-09-29T03:23:48.760" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5318" PostId="4042" Score="0" Text="I didn't know that but anyway the legacy OpenGL is a manadatory course for me. But I will most certainly learn the new one." CreationDate="2016-09-29T06:03:09.253" UserId="5193" ContentLicense="CC BY-SA 3.0" />
  <row Id="5319" PostId="4042" Score="0" Text="Sounds weird, there are lot of devices that will never support legacy openGL. Like your mobile phone, and even desktops will see a gradual phasing out of legacy OpenGL. So no new code should be made with legacy openGL for any reason, including teaching." CreationDate="2016-09-29T06:26:45.230" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5320" PostId="4042" Score="1" Text="Like I said I've been learning opengl for a few weeks now and since it is a one year course I definately believe I will get to opengl4.5  later . And It is not weird at all because University education also  focus on teaching older stuff along with the newer stuff." CreationDate="2016-09-29T07:09:23.670" UserId="5193" ContentLicense="CC BY-SA 3.0" />
  <row Id="5321" PostId="4042" Score="1" Text="No, it's definitely not ok. There is no concepts that the old way does that the new way does not do. Theres absolutely no need for you to learn how to send data and use fixed pipeline programming, it does not even change all that much. In fact if you were not using the fixed pipeline you wouldn't have this question. All your doing is learning an API that nolonger should be used, i mean ist fine if you learned how to blit colors on a c64 also in the course bit other than that no benefit, other than perhaps knowing the difference." CreationDate="2016-09-29T07:21:06.540" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5322" PostId="4042" Score="1" Text="you know what I'm gonna leave you to  argue that with the education board." CreationDate="2016-09-29T07:26:51.037" UserId="5193" ContentLicense="CC BY-SA 3.0" />
  <row Id="5323" PostId="4042" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/46057/discussion-between-joojaa-and-neolardo-va-dinci)." CreationDate="2016-09-29T07:27:03.957" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5326" PostId="4051" Score="0" Text="Great! Do you have any advice on how to begin changing the level of detail?" CreationDate="2016-09-29T16:17:22.883" UserId="4708" ContentLicense="CC BY-SA 3.0" />
  <row Id="5327" PostId="4043" Score="0" Text="Can you give some more details? Are you rendering a 3d scene? What is happening right now to cause your luminance not to be preserved?  Are you just trying to render without lighting? It might also be useful info to know what you are using to render: OpenGL, DirectX, other?" CreationDate="2016-09-29T21:12:19.173" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5328" PostId="4051" Score="0" Text="@fluffological_studies Yeah, you do it by mixing multiple octaves of noise together. I added something to the answer about that." CreationDate="2016-09-30T01:43:10.010" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5329" PostId="4051" Score="1" Text="You can animate the FBM noise also by scrolling the octave textures without the need for extra texture dimension." CreationDate="2016-09-30T04:27:03.880" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5330" PostId="4054" Score="0" Text="Are you shearing the left and right halves of the [trapezoid](https://en.wikipedia.org/wiki/Isosceles_trapezoid) independently? Could you explain the underlying problem you are trying to solve?" CreationDate="2016-09-30T11:17:14.117" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5331" PostId="4054" Score="0" Text="@trichoplax    yes independently, but I am not convinced of the correctness of the exposed solution.      The underlyng problem is transform the view frustum , a truncated pyramid, in a parallepiped  (in 3-D) but isn't important for now." CreationDate="2016-09-30T12:11:01.070" UserId="4981" ContentLicense="CC BY-SA 3.0" />
  <row Id="5332" PostId="4054" Score="0" Text="I'd recommend describing the background of what you're trying to do because it can affect which solution is most useful. For example, if your objective is to draw a parallelepiped, a simple answer may be &quot;start with a cube instead of a truncated pyramid&quot;. If your objective is to convert a 3D texture then the use of a shear may be essential to the end result you require. In other cases there may be an easier approach. Without knowing the reasons behind it, it's hard to know what simplifying assumptions can be made." CreationDate="2016-09-30T12:36:22.783" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5333" PostId="4054" Score="0" Text="thing And this is one transformation when you have perspective divide but not possible with one 3x3 matrix without one." CreationDate="2016-09-30T13:30:39.337" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5334" PostId="4053" Score="0" Text="I made some edit" CreationDate="2016-09-30T14:11:04.027" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5335" PostId="4043" Score="0" Text="Rendering a 3D scene with OpenGL, ambient lighting only. I wanted blue to be as perceptually bright (if that's the correct term) as green." CreationDate="2016-10-01T01:26:21.590" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="5336" PostId="4043" Score="0" Text="I likely need to apply a conversion as the answer below suggests." CreationDate="2016-10-01T01:29:10.133" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="5338" PostId="4026" Score="0" Text="What are the walls that don't seem correct? How do they look? How do their N and T look?" CreationDate="2016-10-01T02:39:50.120" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5339" PostId="392" Score="1" Text="It might also be worth mentioning that rigid body transforms are a subset of affine transforms, and affine transforms are a subset of perspective transforms." CreationDate="2016-10-02T04:29:48.287" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5340" PostId="4059" Score="0" Text="What is it doing instead? Is it rotating along a different axis?" CreationDate="2016-10-02T11:35:54.200" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5341" PostId="4062" Score="1" Text="&quot;Why is a graphics processing unit not at universal processing unit&quot;, is that you question?" CreationDate="2016-10-02T14:02:44.700" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5342" PostId="4062" Score="1" Text="@Andreas No, my question is as it's stated in the post. Why are rasterizers still a hardware part, when they could be done in software (in fact they already could be done with OpenCL, or compute shaders). Question is why it is not common... maybe it's simply performance, I don't know, that's why I'm asking..." CreationDate="2016-10-02T14:09:54.537" UserId="1764" ContentLicense="CC BY-SA 3.0" />
  <row Id="5344" PostId="4058" Score="2" Text="This isn't really the type of question anyone else can answer. It's going to depend entirely on how your fluid sim and rendering are written and the hardware it's running on. Maybe you should put together a prototype and profile it. That could give you data to push you in one direction or the other." CreationDate="2016-10-02T14:52:09.190" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5345" PostId="4058" Score="1" Text="I would suggest to provide more information to make this question specific. For example, how do you simulate your fluid? Do you use a particle based method such as SPH? Do you use a grid base method(the benchmark you mentioned is a grid based method)? How is your fluid coupled with other objects? All of these will impact the stability, convergence rate, and performance." CreationDate="2016-10-02T15:57:11.243" UserId="120" ContentLicense="CC BY-SA 3.0" />
  <row Id="5346" PostId="4058" Score="0" Text="Well you did answer my question. The fact that you can't give me a (specific) answer tells me that there, in fact, are scenarios where GPU powered sim is viable and/or desirable. Look at the bold text." CreationDate="2016-10-02T18:10:29.233" UserId="5215" ContentLicense="CC BY-SA 3.0" />
  <row Id="5347" PostId="4063" Score="0" Text="I dont think you need to store the subpixel samples if box filttering is enough then you can just do running averages." CreationDate="2016-10-02T19:41:53.920" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5348" PostId="4063" Score="2" Text="I suspect texture sampling and cache are probably also areas where hardware implementations allows performance otherwise impossible to achieve with software implementations." CreationDate="2016-10-03T02:37:43.943" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5349" PostId="4064" Score="1" Text="Is the dot notation correct or even common for matrix multiplication? I've only seen it as a cross ($A \times B$) or omitted ($AB$)." CreationDate="2016-10-03T02:42:41.747" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5350" PostId="4058" Score="3" Text="_What's the use of rendering the graphics at 60FPS if fluid simulation runs at 10 steps per second,_ Even if the simulation is slower, 60fps gives a better interactivity (think of rotating the scene). You could also for example interpolate between simulation steps." CreationDate="2016-10-03T02:47:14.500" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5351" PostId="4064" Score="0" Text="Oops, yeah I think you're right. I just get used to using dots for multiplication. :)" CreationDate="2016-10-03T03:03:46.910" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5352" PostId="4065" Score="0" Text="Do you want to use specific features that are only available using a given graphics card? Or is it sufficient simply to know how fast the code can run? This will determine whether you need to determine which graphics card (if any) is being used, or whether you can use an approach that starts up in the lowest quality setting and detects frame rate, using that to decide whether to increase the quality." CreationDate="2016-10-03T07:10:47.163" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5353" PostId="4061" Score="0" Text="Do you know if this solves the problem?" CreationDate="2016-10-03T07:17:19.373" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5354" PostId="4060" Score="1" Text="[Relevant](http://computergraphics.stackexchange.com/questions/3964/opengl-specular-shading-gradient-banding-issues) - even 24 bit colour (16M colours) is insufficient to ensure smooth gradients. The human eye cannot distinguish this many colours, but it can spot the boundary between two adjacent colours." CreationDate="2016-10-03T07:22:13.860" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5355" PostId="4065" Score="0" Text="I think it would be sufficient to just know how fast it can run" CreationDate="2016-10-03T08:03:04.393" UserId="5220" ContentLicense="CC BY-SA 3.0" />
  <row Id="5357" PostId="4063" Score="3" Text="@aces One other item worth mention is power. Dedicated hardware will do the same job typically with a fraction of the power budget and, with power throttling already an issue, especially on mobile devices, going fully programmable would make it vastly worse." CreationDate="2016-10-03T12:02:19.677" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5358" PostId="4062" Score="0" Text="You can bypass the rasterization and implement your own rasterizer with compute units on modern GPU's and I in fact know people doing this for specific purposes." CreationDate="2016-10-03T13:18:26.700" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5359" PostId="4067" Score="0" Text="Do you by chance have a screenshot of what you are trying to achieve? (:" CreationDate="2016-10-03T14:05:19.983" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5360" PostId="4067" Score="0" Text="Unfortunately not but here is a little diagram. It's easy to do a simple directional light `dot(worldNormal, lightDir)` but how to limit it." CreationDate="2016-10-03T14:11:13.550" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5361" PostId="4061" Score="0" Text="I believe the problem was not rotating along its own axis so doing it this way should solve it. Maybe there are other solutions" CreationDate="2016-10-03T15:06:21.257" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5362" PostId="4067" Score="3" Text="This isn't a full answer but essentially you want to test if a point is in the oriented box defined by the light. You may want to soften the edges as it gets farther away from the light source but that is more challenging." CreationDate="2016-10-03T15:27:39.650" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5363" PostId="4060" Score="1" Text="@trichoplax good find. I´ll rephrase that part of the question." CreationDate="2016-10-03T16:18:54.840" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5364" PostId="4060" Score="1" Text="I think you have a error in assuming that dithering does not work on macro scales it does all you need to do is fool the visual systems sharpening and noise cancellation algorithm. Which with perfect dithering may be spectacularily big. Dithering works well into the range where you can see individual pixels at 8bits per channel color depth." CreationDate="2016-10-03T17:38:00.740" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5366" PostId="4068" Score="0" Text="I used to write a Mathematica script to infer the projection matrix(here: http://blog.linearconstraints.net/2016/02/03/inferring-projective-mappings.html) except that 8 vertices are used to solve a linear system at that time. I think 6 non coplanar vertices should be adequate. I will try later." CreationDate="2016-10-03T20:22:21.220" UserId="120" ContentLicense="CC BY-SA 3.0" />
  <row Id="5367" PostId="4063" Score="0" Text="@JulienGuertault Fair point, but I would think this is mostly applicable to MSAA. There test results seem to indicate that this isn't a huge issue when everything fits on on-chip memory (although this could have some performance impact regardless)." CreationDate="2016-10-04T03:47:06.850" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5368" PostId="4063" Score="0" Text="@SimonF I think that's a great point as well. I included it in the answer." CreationDate="2016-10-04T03:49:46.283" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5371" PostId="4071" Score="0" Text="Perhaps even using pixel doubling. By the way I get a consistent 60 fps no matter what." CreationDate="2016-10-04T13:18:19.993" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5372" PostId="4071" Score="0" Text="Sounds like I need a newer computer..." CreationDate="2016-10-04T14:06:26.470" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5373" PostId="4074" Score="0" Text="Got it, thanks, very confusing indeed but good article. Just one last thing, does this matrix pre-multiply $\vec{v'} = M\vec{v}$ or post-multiply $\vec{v'} = \vec{v}M$ ? Is just that I always confuse which is first, translation or rotation haha" CreationDate="2016-10-04T14:11:43.963" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="5375" PostId="4074" Score="1" Text="@BRabbit27 If $\vec{v}$ is a column vector it is always $\vec{v}'=\mathbf{M}\vec{v}$. There is no $\vec{v}\mathbf{M}$." CreationDate="2016-10-04T15:20:10.320" UserId="120" ContentLicense="CC BY-SA 3.0" />
  <row Id="5376" PostId="4060" Score="0" Text="@joojaa I´m not sure what you are implying... Asuming &quot;macro scale&quot; is &quot;large enough pixels to be seen with naked eye&quot;, then yes dithering may still fool the brain to blend them together. If you take an RGB8 image and convert it to RGB1 that would require crazy pixel density and an awesome dithering pattern! At some point it would work though. My question is how to derive the point at which it would work. If you or anyone else could try clarify or rewrite the post please do so. Because I don´t see where we disagree :-)" CreationDate="2016-10-04T16:58:09.980" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5377" PostId="4060" Score="0" Text="@Andreas i am implying that applying perfect dithering technique may in fact be infinitely good, we dont know. The brain might with right fooling react very favorably even in the 1 bit case. In fact we need nowehere near the typical 300 dpi to make color look smooth just that trying to do the other things we are trying to do is in fact the limiting factor." CreationDate="2016-10-04T17:04:29.480" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5378" PostId="4060" Score="0" Text="@joojaa Hm. Maybe I´m using &quot;perfect&quot; wrong. What I mean is a &quot;psedo-random pattern which is NOT repeated&quot;. In the post linked by tricho you CAN see some traces of banding if you try really hard because of the dithering pattern being repeated. This becomes even clearer if the gradient is aligned with the display." CreationDate="2016-10-04T17:15:16.083" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5379" PostId="4060" Score="0" Text="Nevermind last comment. Got that mixed up with another image, sorry! My point was a small repeating pattern may look bad." CreationDate="2016-10-04T17:26:50.783" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5380" PostId="4077" Score="1" Text="Thanks, this will help me to work out how and what to implement and will give me some foundation to work from. Also, the last paragraph is a good advice!" CreationDate="2016-10-04T18:31:23.817" UserId="5215" ContentLicense="CC BY-SA 3.0" />
  <row Id="5381" PostId="4071" Score="0" Text="I usually get 30-60 on my macbook pro depending on how large the window is (full screen on a WQHD monitor lowers it a lot). (edit: accidentally hit enter too soon) I was hoping you could go more into some specific techniques to maximize performance. Are my initial ideas for lower quality on track?" CreationDate="2016-10-04T19:41:06.807" UserId="5220" ContentLicense="CC BY-SA 3.0" />
  <row Id="5382" PostId="4078" Score="0" Text="Simplest solution would be to render the polygon and map the screen coordinates of the polygon to your desired texture coordinates in fragment shader" CreationDate="2016-10-05T03:09:20.103" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5383" PostId="4076" Score="0" Text="You means, DX12 only has one pipeline, but it optimized comprise process of PSO with multi-thread and multi-cores. Vulkan has 2 pipelines, reside on different threads, each thread has its own commands managements, but only 2 threads ?" CreationDate="2016-10-05T03:11:17.507" UserId="5222" ContentLicense="CC BY-SA 3.0" />
  <row Id="5384" PostId="4060" Score="0" Text="You mean stochastic?. But if so are elements still asumed to be in a grid." CreationDate="2016-10-05T03:58:23.247" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5385" PostId="4078" Score="0" Text="sure if there was only one texture. How would this work with more than one texture like I specified?" CreationDate="2016-10-05T08:23:08.357" UserId="5093" ContentLicense="CC BY-SA 3.0" />
  <row Id="5386" PostId="4078" Score="0" Text="You just put the textures into a texture array or atlas" CreationDate="2016-10-05T11:14:38.860" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5387" PostId="2059" Score="0" Text="For reference, here is the source for a low discrepancy sample (blue noise) generator https://github.com/bartwronski/BlueNoiseGenerator" CreationDate="2016-10-05T11:15:57.443" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="5388" PostId="4080" Score="1" Text="That is coming from the man himself! I'm not sure if it'll be helpful, but here's a raytraced implementation in shadertoy:&#xA;https://www.shadertoy.com/view/4lV3zV" CreationDate="2016-10-05T14:07:22.863" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5389" PostId="4049" Score="0" Text="How are the components drawn? Are they images or generated by a frag shader? Is the triangle drawn as a single triangle, or as part of the outer circle? I don't think it's possible to make a useful suggestion without knowing how all the components relate in terms of draw calls." CreationDate="2016-10-05T15:26:35.230" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5390" PostId="4085" Score="0" Text="Please see the screenshot. The issue is not regarding the scaling. I already scaled cylinders to match the length between the spheres." CreationDate="2016-10-05T15:48:17.403" UserId="5121" ContentLicense="CC BY-SA 3.0" />
  <row Id="5391" PostId="4049" Score="0" Text="I tried doing an offscreen fbo that leveraged the fact that I'm using additive blending but that cut fps in half.  I am still working on figuring out stencil buffer." CreationDate="2016-10-05T15:59:35.110" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5392" PostId="4049" Score="0" Text="So the outer ring with a triangle is a GLQUAD and the inner circle is another GLQuad. The outer one never gets transformed while the inner one does get rotated." CreationDate="2016-10-05T16:00:37.247" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5396" PostId="2429" Score="0" Text="I'd really like to absorb this answer but I'm getting stuck on the bit about approximating the solid angle as an area of circle of radius $r_\text{light}/r$ using similar triangles. I don't get the denominator, and I can't see how similar triangles come into play. Any help appreciated." CreationDate="2016-10-05T21:22:31.023" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5397" PostId="4079" Score="2" Text="The projection matrix transforms points into **clip space**, which is a 2x2x2 cube centered at the origin, **not** what you're calling &quot;screen space&quot;. Homogeneous coordinates means you have a `w` in addition to your spatial coordinates(x,y and possibly z). It has nothing to do with normalization. You never divide by Z - always W. The point of W is that you can perspective divide and  keep depth (again, in clip space), and nicely lets you add translation into your matrix rather than having a separate step. You're mixing up a lot of different concepts." CreationDate="2016-10-05T22:23:09.067" UserId="5240" ContentLicense="CC BY-SA 3.0" />
  <row Id="5398" PostId="4058" Score="0" Text="What @JulienGuertault said. There's a difference between a  physically correct simulation, and something that's good for gameplay. Also, this isn't really a graphics question. Have you tried gamedev.stackexchange?" CreationDate="2016-10-05T22:38:06.607" UserId="5240" ContentLicense="CC BY-SA 3.0" />
  <row Id="5401" PostId="4087" Score="0" Text="TL;DR - the Z-buffer holds the depth of the closest polygon to the camera. At the end of the frame, all other things being equal, the closest poly is the closest poly, so the depth buffer value will be the same regardless of draw order." CreationDate="2016-10-05T22:51:14.313" UserId="5240" ContentLicense="CC BY-SA 3.0" />
  <row Id="5402" PostId="2429" Score="1" Text="@PeteUK it comes about because you're projecting a light source that's a distance $r$ away onto the unit sphere. (The solid angle is equivalent to area on a unit sphere.) So, distances get divided by $r$. If that doesn't help, I can draw a diagram." CreationDate="2016-10-05T22:57:07.367" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5403" PostId="4062" Score="0" Text="Rasterizers convert vector polys into a bunch of pixels that we can light. When we no longer have pixels, or stop using vector geometry, we will no longer  need rasterizers. No matter  what the rest of your pipeline looks like, at the end of the day (or  frame), you need pixels. The rasterizer just tells us which pixels we are concerned about for a given triangle. All of that is programmable - if you want different output from the rasterizer, send different triangles its way. Or just draw everything to a render texture in a compute shader and blit it to the screen with a view-aligned quad." CreationDate="2016-10-05T22:59:17.863" UserId="5240" ContentLicense="CC BY-SA 3.0" />
  <row Id="5404" PostId="4059" Score="0" Text="I wanted to read your question but that animated gif is too distracting." CreationDate="2016-10-05T23:07:14.267" UserId="5240" ContentLicense="CC BY-SA 3.0" />
  <row Id="5405" PostId="3593" Score="0" Text="I teach an OpenGL class about once per year. We **start** with 4.3 core and shaders. Despite claims the fixed pipeline being better for teaching, my (undergrad) students picked it up pretty quickly. And learned a useful skill." CreationDate="2016-10-05T23:12:46.950" UserId="5240" ContentLicense="CC BY-SA 3.0" />
  <row Id="5406" PostId="4031" Score="0" Text="n = (v1 - v0) x (v2 - v0), where v0, v1, and v2 are the vertices of the (triangle) face in question. The order is importan.  Normalize it if you really need to. (and `x` is cross product)" CreationDate="2016-10-05T23:17:56.073" UserId="5240" ContentLicense="CC BY-SA 3.0" />
  <row Id="5407" PostId="4077" Score="0" Text="@DavidLively I just rephrased that paragraph. &gt; The SIMT and SIMD models have nothing to do with bandwidth / &quot;transfer latency.&quot;. I actually intended to describe how SIMT hides the memory access latency, not the transfer overhead. I think this is a major difference between SIMT and SIMD. I hope it is more clear now. Thank you for pointing out these issues." CreationDate="2016-10-06T00:04:26.267" UserId="120" ContentLicense="CC BY-SA 3.0" />
  <row Id="5408" PostId="4088" Score="0" Text="I wonder if his teacher was referring to non determinism of the floating point operations? Not sure if that actually happens on the GPU but I could see it possibly happening. A pretty obscure thing though, so maybe not likely what was being talked about?" CreationDate="2016-10-06T02:00:27.687" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5409" PostId="4088" Score="0" Text="@AlanWolfe That could happen with a floating-point render target, I suppose. Not sure if it's an issue for integer formats (even with blending)." CreationDate="2016-10-06T02:35:30.260" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5411" PostId="1707" Score="0" Text="The problem with either method (set constant buffer GPU address with SetGraphicsRootConstantBufferView() or using secondary lookups to the descriptors in the root table) is that if there are not enough GPU addresses available to render all objects then the application developer must implement a strategy to re-use constant buffer memory. The application may have to close and submit the current command list (Close() then Reset()) which means you have to duplicate the state (PSO and other state) into a new command list. How should an application use constant buffer memory in a ring buffer strateg" CreationDate="2016-10-06T00:59:46.477" UserDisplayName="user5241" ContentLicense="CC BY-SA 3.0" />
  <row Id="5412" PostId="1707" Score="0" Text="Looks like SE cut off the post in the conversion to comment. It continues with &quot;...y? In the past drivers would notice no-overwrite was set or rename the memory if the previous memory was busy. How do you implement these strategies in DX12?&quot;" CreationDate="2016-10-06T08:08:03.640" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="5414" PostId="4087" Score="0" Text="At the end of the render the frame buffer may contain different values but, provided you haven't played with the Z test mode, the Z buffer should be consistent." CreationDate="2016-10-06T11:27:56.957" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5415" PostId="4077" Score="0" Text="@TheyBusyTypist Much better. Thanks for updating. :)" CreationDate="2016-10-06T14:55:52.160" UserId="5240" ContentLicense="CC BY-SA 3.0" />
  <row Id="5416" PostId="4060" Score="0" Text="@joojaa Sure that´s one way of putting it." CreationDate="2016-10-06T17:24:52.130" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5417" PostId="3959" Score="0" Text="Maybe this falls into one of your categories, but isn't old fashioned render-to-all-faces-of-a-cubemap technically a fully realtime technique?  Also, isn't it possible to augment basic ambient with a environment cubemap for diffuse reflections as well?" CreationDate="2016-10-06T23:34:19.767" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="5418" PostId="4100" Score="0" Text="Exactly I mean this geometric modeling" CreationDate="2016-10-07T14:30:54.867" UserId="5253" ContentLicense="CC BY-SA 3.0" />
  <row Id="5419" PostId="4100" Score="0" Text="Good to hear! BTW I would also look at procedural content generation techniques!" CreationDate="2016-10-07T14:32:40.087" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5420" PostId="4100" Score="0" Text="Unfortunately I am not so familiar with the problems that graphics professionals involve with,I must search more! Thanks so much !" CreationDate="2016-10-07T15:09:38.957" UserId="5253" ContentLicense="CC BY-SA 3.0" />
  <row Id="5421" PostId="4102" Score="0" Text="Thanks for your answer. I'll move on to writing the path tracing bit :) You suggested tracing paths of length X will produce an unbiased image but my book says when looking at techniques to prevent paths growing too long: &quot;A first technique is cutting off the recursive evaluations after a fixed number [...] but important light transport might have been ignored. Thus, the image will be biased&quot;. Are you suggesting to set X fairly high (e.g. 10) to ensure important long paths are considered?" CreationDate="2016-10-08T08:49:35.463" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5422" PostId="4102" Score="1" Text="Yes, you should set X to a fairly large number or cut the length adaptively until the path weight factor drops below a certain threshold" CreationDate="2016-10-08T11:37:33.657" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5423" PostId="4101" Score="0" Text="Here's a nice resource about basic path tracing: http://blog.demofox.org/2016/09/21/path-tracing-getting-started-with-diffuse-and-emissive/" CreationDate="2016-10-09T02:01:01.330" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5424" PostId="4107" Score="0" Text="And what about light.diffuse (diffuse light color?)" CreationDate="2016-10-09T09:56:08.623" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5425" PostId="4107" Score="0" Text="Oh I forgot to add in that, I've updated my answer with it." CreationDate="2016-10-09T10:05:21.773" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5426" PostId="4101" Score="0" Text="@AlanWolfe Thank you. I actually read through that yesterday :) and have found some of your posts on CGSE helpful too. Have downloaded the PTBlogPost1 code and will look through it when I've finished with SmallPt." CreationDate="2016-10-09T12:55:07.580" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5427" PostId="4101" Score="0" Text="Awesome! I'm really glad to hear that (:" CreationDate="2016-10-09T13:50:08.667" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5428" PostId="4109" Score="0" Text="I'm confused by why there are vertical lines at all. Is it deliberate that the texture is aligned with the image plane instead of the triangle? What happens when you rotate the texture (mapping)?" CreationDate="2016-10-10T08:59:29.987" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5429" PostId="4109" Score="0" Text="@DanHulme It's actually aligned with the triangle, you just can't see it in the screenshot. I tried to change the uv mapping but no difference.Weird thing is though, if I rotate the triangle 90° it's perfect. I think this might have something to do with the triangle itself. The problems occur if one side of the triangle is aligned with the x-axis. When aligned with the y-axis it works..." CreationDate="2016-10-10T12:44:07.617" UserId="4571" ContentLicense="CC BY-SA 3.0" />
  <row Id="5431" PostId="1579" Score="0" Text="Here's a good introduction to monte carlo path tracing, which does what you describe - it uses monte carlo integration to try and solve the rendering equation.  It makes for very nice results, but takes a long time to render with the naive implementation.  It's still a very active are of research.&#xA;http://blog.demofox.org/2016/09/21/path-tracing-getting-started-with-diffuse-and-emissive/" CreationDate="2016-10-10T15:53:05.530" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5433" PostId="4110" Score="0" Text="Current answers are avoiding your question why 0.999 is used instead of 1. Besides it making no real visual difference, I suspect that this number is chosen to avoid floating point errors which could result in an energy gain. Could anyone more familiar with IEEE 754 comment on that?" CreationDate="2016-10-11T08:27:53.623" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="5434" PostId="4091" Score="0" Text="Is 1/(1/w) a typo or something I've misunderstood?" CreationDate="2016-10-11T09:03:41.303" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5436" PostId="4091" Score="2" Text="Not a typo. You interpolate 1/w linearly over the screen and calculate 1/(1/w) per pixel. Then multiply linearly interpolated u/w and v/w with the per-pixel 1/(1/w) value. Just a trivial optimization of turning divs to muls to avoid multiple divisions per pixel." CreationDate="2016-10-11T12:41:19.703" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5437" PostId="4117" Score="0" Text="Isn't there still a downside if not all the waves want to take the same path?  Won't it do both paths for the wave? I was also wondering what you meant in the last paragraph about newer cards doing double the logic? Thanks for the answer and info (:" CreationDate="2016-10-11T14:34:50.643" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5438" PostId="4114" Score="0" Text="I'm thinking how the concept of a specular colour makes sense when talking about perfect mirrors? Are you saying it's possible for all incoming light to be reflected at the interface and have it's colour (frequency) changed? Would the angle in incidence = angle of reflection? What physical process would bring about the change of colour? Sorry if I sound like a kid that asks more questions after just been given an answer." CreationDate="2016-10-11T15:14:28.313" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5439" PostId="4114" Score="0" Text="It's a bit more complicated. To try and fit the explanation in this comment: when light hits a surface, some will enter it (refraction), some will not and bounce (reflection). Whether it enters or not can depend on the light wavelength (hence the graphs in @JarkkoL's answer) and that's what gives gold its yellow appearance." CreationDate="2016-10-11T15:27:29.477" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5440" PostId="4114" Score="0" Text="In case that wasn't clear, I was referring to the copper and aluminum triple graphs: they represent the reflectance for red, green and blue wavelengths." CreationDate="2016-10-11T15:34:50.497" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5441" PostId="4107" Score="0" Text="Shouldn't it be `color = diffuse + ambient` ?" CreationDate="2016-10-11T18:34:46.690" UserId="5254" ContentLicense="CC BY-SA 3.0" />
  <row Id="5442" PostId="4114" Score="0" Text="Thanks for your replies. Regarding the light hitting the surface that doesn't bounce and gets refracted: we're not talking about perfect mirror (or ideal specular surface) anymore, right? Is this refraction and re-emission called a specular highlight (i.e. a bright spot)? Back to the contrived ideal specular surface where **all** light gets reflected: is there a possibility for the incident and reflected light to have different colours?" CreationDate="2016-10-11T18:52:47.273" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5443" PostId="4118" Score="0" Text="Yeah, that's kind of what I expected. But it has to work somehow..." CreationDate="2016-10-11T19:12:49.050" UserId="4571" ContentLicense="CC BY-SA 3.0" />
  <row Id="5444" PostId="4118" Score="0" Text="I already have bilinear filtering but I like to have both. This gives it sort of a retro look" CreationDate="2016-10-11T19:13:44.920" UserId="4571" ContentLicense="CC BY-SA 3.0" />
  <row Id="5445" PostId="4109" Score="0" Text="The triangle edge jaggies in the top-right corner (against black background) strike me odd though. Why some pixels appear bigger than others? They should all be the same size." CreationDate="2016-10-11T20:37:13.647" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5446" PostId="4109" Score="0" Text="@JarkkoL that's because I scaled the picture in Photoshop in order to make the issue more visible :)" CreationDate="2016-10-11T20:46:03.460" UserId="4571" ContentLicense="CC BY-SA 3.0" />
  <row Id="5447" PostId="4109" Score="0" Text="I thought you might have (: You should scale it properly to make it less confusing." CreationDate="2016-10-11T20:47:57.477" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5448" PostId="4109" Score="0" Text="Would scaling to a power of 2 multiple of the original size ensure all the scaled up pixels are consistent sizes? (To make the diagram more relevant for people answering.)" CreationDate="2016-10-11T21:20:21.493" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5449" PostId="4119" Score="0" Text="No,no!   A is  the world reference system (the coordinate system, Oxyz) and B is the reference system of the camera (Ouvn).   Maybe I had not specified.   In light of these explanations it is correct what I said in my original post?" CreationDate="2016-10-11T21:45:02.143" UserId="4981" ContentLicense="CC BY-SA 3.0" />
  <row Id="5450" PostId="4119" Score="1" Text="@Umbert why would world be anything other than identity? I mean lack of identity world would mean there is something more fundamental than world in the scene. So the identity basis is present no matter what even if its below your world meaning world is a object space of somekind. There is always a explicit assumption that something identity is at the lowest level of a transformation chain." CreationDate="2016-10-12T03:39:58.227" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5451" PostId="4117" Score="0" Text="Ignore the last paragraph, I'll edit my answer. I thought you were attempting to do something else and I made a bad assumption. If the whole warp doesn't go through the same path, then you're right, then you have to go through the &quot;if&quot; and the &quot;else&quot; block even for newer cards." CreationDate="2016-10-12T04:09:04.017" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5452" PostId="3959" Score="0" Text="@racarate Sorry it took me awhile to respond, but yes, you're right! I think I meant to mention that, but forgot. :) Anyway, I added it. (I did mention using a cubemap for diffuse, up in the first bullet point.)" CreationDate="2016-10-12T04:54:00.447" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5453" PostId="4118" Score="0" Text="@Leon2806 see update re not doing per-pixel increments." CreationDate="2016-10-12T08:14:38.900" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5454" PostId="4121" Score="0" Text="It depends how much control you have over the image. e.g. making all your images 1 pixel large would have a huge benefit, but that probably isn't useful to you." CreationDate="2016-10-12T10:33:13.833" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5455" PostId="4121" Score="0" Text="@DanHulme I'm thinking about quality-loss optimization of existing PNG images. The questions is: how to process such images. First idea - pixelize with unicolor blocks. Second idea - decrease palette to have possibly big unicolor shapes. Or maybe something else. I don't know how PNG compression works and what change will result with best effect." CreationDate="2016-10-12T10:50:36.910" UserId="5285" ContentLicense="CC BY-SA 3.0" />
  <row Id="5456" PostId="4121" Score="0" Text="PNG is not stored in blocks, so no that is useful but not optimal" CreationDate="2016-10-12T10:52:19.673" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5457" PostId="4119" Score="0" Text="@Umbert It's important to say coordinate system relatively to what. &quot;World&quot; is usually considered global &quot;root&quot; frame joojaa explained in his answer. If $A$ is transformation to your &quot;world&quot; then where is it transformation from? Global frame? The answer I gave still holds if both $A$ and $B$ are defined relatively to the same frame, just use different names (world=global frame, object=world)." CreationDate="2016-10-12T12:25:14.963" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5458" PostId="4118" Score="1" Text="I'm not sure if I completely  understand what you mean. Should I multiply the step value by x? because that gives me the exact same result." CreationDate="2016-10-12T12:44:28.677" UserId="4571" ContentLicense="CC BY-SA 3.0" />
  <row Id="5459" PostId="4109" Score="0" Text="What's strange though is that some of the jaggies in the image are 7 pixel wide while some are 3 or 4 pixel wide. I would expect 1 pixel deviation in the image if using non-integer upscaling, but that doesn't explain 7px vs 3/4px. Anyway, you should just upscale the image by some integer multiply (e.g. 800% in Photoshop)." CreationDate="2016-10-12T13:05:21.903" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5460" PostId="4118" Score="0" Text="In real mathematics it does, but not in the world of floating point.  I believe the appropriate quote is _&quot;Floating point numbers are like piles of sand; every time you move them around, you lose a little sand and pick up a little dirt.”_ ( — Brian Kernighan and P. J. Plauger).  Every time you do an operation (e.g an add or a mul) you'll typically lose up to 1/2 a ULP of accuracy.  Replacing a sequence &quot;+C+C+C...+C&quot;  with +N*C will thus improve your sand to dirt ratio :-)" CreationDate="2016-10-12T13:17:32.900" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5461" PostId="4118" Score="0" Text="Thanks for the explanation, now it makes sense to me :)   Unfortunately it does not change anything. But I think another problem regarding accuracy might be that I'm calculating a new step value for each scanline, which is unnecessary because it should be constant across the whole triangle. That's not only more expensive but also gives me slightly different results every time which causes the visual artifacts." CreationDate="2016-10-12T14:19:22.677" UserId="4571" ContentLicense="CC BY-SA 3.0" />
  <row Id="5462" PostId="4124" Score="0" Text="This sounds interesting. Could you expand on the basic idea and explain why it works?" CreationDate="2016-10-12T16:04:54.783" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5463" PostId="4124" Score="0" Text="It's quite clever. I will study it more." CreationDate="2016-10-12T17:03:38.600" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5465" PostId="4107" Score="0" Text="Yeah, you should not be multiply ambient, if it is 0 you will end up with 0 diffuse. It should be diffuse + ambient" CreationDate="2016-10-13T03:36:54.063" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5466" PostId="4126" Score="2" Text="Beautifully explained, I kinda related the fact to the perspective division but wasn't sure, now I am, thanks !" CreationDate="2016-10-13T04:40:09.687" UserId="116" ContentLicense="CC BY-SA 3.0" />
  <row Id="5468" PostId="4122" Score="0" Text="My question is simple. I must aplly the view transfrormation. I supposed that the matrix for pass from Object space to world  has already been applied.Thus I'm in world coordinates (Oxyz).I must pass in camera coordinates (Ouvn).(change of coordinates)Then I overlap the 3 axes (x,y,z) on 3 axes (u,v,n) with a transformation namely Rx*Ry*T. Later for map a point P of the coordinate system of the world (Oxyz) to the Ouvn I do (T^-1)*(Ry^-1)*(Rx^-1)*P . My doubt is: is right the transformation Rx*Ry*T for ovelaps the 2 axes?Or the right transformation is  T*Rx*Ry and then (Ry^-1)*(Rx^-1)*(T^-1)P?" CreationDate="2016-10-13T10:38:06.540" UserId="4981" ContentLicense="CC BY-SA 3.0" />
  <row Id="5470" PostId="4129" Score="1" Text="I think IneQuation means the gaussian filter is a box filter in the sense that it applies to n*n pixels. It's then worth separating it since you go from n*n to 2*n (muliplications per pixel)" CreationDate="2016-10-13T14:20:43.133" UserId="5254" ContentLicense="CC BY-SA 3.0" />
  <row Id="5472" PostId="4129" Score="0" Text="Given the code linked in the answer, this is the first one." CreationDate="2016-10-13T19:15:05.960" UserId="5254" ContentLicense="CC BY-SA 3.0" />
  <row Id="5473" PostId="4047" Score="0" Text="Welcome to ComputerGraphics.SE! It's normally a good idea to make answers on Stack Exchange self contained (e.g. in case links go down and also in general so that people don't have to follow links to be able to tell if an answer is useful to them). You might want to improve your answer by including a short summary of the contents of the paper." CreationDate="2016-10-13T21:22:11.790" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="5474" PostId="4129" Score="1" Text="You are correct, @trichoplax, I meant a square neighbourhood. Fixing. :)" CreationDate="2016-10-14T06:50:51.587" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5475" PostId="4132" Score="1" Text="I suspect that, like myself, many of us don't have Illustrator, so you'll have to be more specific in your description. Are you just clicking to define the end points of the each curve segment and then moving the end point(s) of the tangent associated with an end point?" CreationDate="2016-10-14T08:19:50.713" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5476" PostId="4132" Score="0" Text="It's really hard to explain what I mean but I think if you watch this video from **2:11** to **3:00** you'll understand exactly what I'm trying to say. https://youtu.be/0B_IQK7hMo0?t=2m8s" CreationDate="2016-10-14T09:53:53.187" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5477" PostId="4132" Score="0" Text="How your program interacts with your software depends on your GUI framework. Surely your windowing library has some tools for implementing mouse clicks. That in essesnce is not a graphics problem altough drawing pick buffers might be." CreationDate="2016-10-14T10:16:32.163" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5478" PostId="4132" Score="0" Text="Detecting Mouse Clicks isn't an issue but if you watch this video you'll understand what I'm trying to say  youtu.be/0B_IQK7hMo0?t=2m8s watch from **2:11** to around **3:00**. If you notice the technique in which he draws, when he still holds his mouse down after the click he can tune the curve just by moving the mouse. That is basically what I am trying to implement..." CreationDate="2016-10-14T10:23:40.980" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5479" PostId="4132" Score="0" Text="I might be wrong, but I think you basically just add a control point at each extremity of your &quot;pen tool&quot;" CreationDate="2016-10-14T10:32:23.877" UserId="5254" ContentLicense="CC BY-SA 3.0" />
  <row Id="5480" PostId="4132" Score="0" Text="Could you explain this with some code or pseudo code? I'm struggling to understand." CreationDate="2016-10-14T13:16:54.640" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5481" PostId="4134" Score="0" Text="This answer, this answer is perfection. Thanks." CreationDate="2016-10-14T13:53:41.843" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5482" PostId="4134" Score="0" Text="So to clarify I create the Hermite spline and then the next spline will use the previous splines tangent as the start tangent + it will also use the previous point as the start point." CreationDate="2016-10-14T14:02:15.427" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5483" PostId="4134" Score="0" Text="Yes, exactly. Just the opposite sign (direction) for the start tangent just like in the AI pen tool" CreationDate="2016-10-14T14:10:55.183" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5484" PostId="4132" Score="0" Text="Its just drawing out the tangent of the next spline segment the previous segment just inherits the inverse vector magnitude tangent. A bit like jarkkoL's answer but no need to do hermite. Yes i use illustrator all the time." CreationDate="2016-10-14T14:50:38.847" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5485" PostId="4129" Score="2" Text="I think he's not asking about the blurred home screen icons, but about the colored &quot;clouds&quot; that slowly fade in and out. See the cloudy green/cyan area in the mid-bottom of the screenshot, or watch the linked video." CreationDate="2016-10-14T15:38:24.240" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5486" PostId="4129" Score="0" Text="Huh, okay, I didn't notice it before you pointed it out to me. Anyway, I'd try splatting some random, slowly moving, colourful particles to a small texture, up-sampling (while Gaussian-blurring) to screen size and additive-blending them onto the screen. Should I edit the answer or add a new one?" CreationDate="2016-10-14T16:40:50.653" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5487" PostId="4136" Score="0" Text="Is there a tangible D3D11 book you advice (like the red, blue, orange books for OpenGL)? Or is it just better to stick to Microsoft's webpages?" CreationDate="2016-10-15T11:15:50.843" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="5488" PostId="4136" Score="1" Text="@Matthias I learned from the online API docs and other online resources. Maybe someone else can recommend a good book; I'm sure there are some out there." CreationDate="2016-10-15T14:01:46.413" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5489" PostId="4138" Score="0" Text="This answer corrected my way of thinking of PBR. Your entire answer was very helpful. So to summarize, in the near future graphics programming will shift towards improving the current techniques towards a physically based approach in order to improve visual quality. Also are there any new techniques that are being widely adopted? What I mean is how all games adopted shadow mapping, ambient occlusion, specular &amp; diffuse lighting." CreationDate="2016-10-15T15:37:49.083" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5490" PostId="4138" Score="1" Text="The shift towards PBR is a continuous process that keeps gaining a foothold as more programmers and srtists (don't forget artists!) keeps gaining better understanding of it. And more research is done in the domain and better HW enable more of the real-time rendering algorihms to be implemented in a physically based way" CreationDate="2016-10-16T04:35:25.473" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5491" PostId="4138" Score="1" Text="Not sure how widely adobted these techniques are, but GGX Is quite dominant BRDF at least. I imagine split-sum approximation for environment lighting is pretty popular too. Various area lighting approximations are gaining popularity and use of proper photometric units to define light illuminance. Also techniques for PBR texture acquisition is of interest to many. To name a few" CreationDate="2016-10-16T04:55:02.093" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5495" PostId="4141" Score="0" Text="Thanks for the answer! I guess I'll comeback to Physically Based Area lights in a while since I'm only 15 and all this Math is confusing the hell out of me. Hopefully as time passes there will be more implementation details. and I should be able to understand all of the jargon. I've also read both of those publications you suggested but I've not understood much of it correctly." CreationDate="2016-10-16T16:02:48.033" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5496" PostId="4142" Score="1" Text="A static or semistatic background is easy to create in 3D all you need is some reliable tracker points and you are set to go." CreationDate="2016-10-16T20:01:03.903" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5497" PostId="4145" Score="0" Text="There might also be some ways to optimize the Snell's law calculations, any advice are also welcome on that." CreationDate="2016-10-17T15:50:07.530" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5498" PostId="4141" Score="3" Text="A bit of a tangent but great job on pursuing this at 15 years old.  Reading papers might seem confusing now and have a lot of complex math, but each one you read gets a little bit easier.  Keep at it and you'll be way ahead of the game by the time you hit college and/or the workforce." CreationDate="2016-10-17T17:36:53.067" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5500" PostId="4142" Score="0" Text="...so long as parallax is minimal ;-)" CreationDate="2016-10-17T19:23:21.980" UserId="4494" ContentLicense="CC BY-SA 3.0" />
  <row Id="5501" PostId="4143" Score="0" Text="Interesting... I had no idea that motion controlled cameras were actually a common solution. That seems like such a complicated setup... but I guess that goes to show how easy they make it look, when it's done well ;-)" CreationDate="2016-10-17T19:24:31.967" UserId="4494" ContentLicense="CC BY-SA 3.0" />
  <row Id="5502" PostId="4147" Score="3" Text="Keeping all the data on the GPU and swapping in/out buffers as you're doing should be fine. It's called &quot;ping-ponging&quot; and is a very common technique in graphics. I can't spot the bug in your code, but your compute shader is quite complicated. I'd advise commenting out everything except a simple write to the buffer and see you can make it work that way. Then gradually build it back up again, testing each piece as you go. That should help narrow down the issue." CreationDate="2016-10-18T01:33:25.707" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5503" PostId="4147" Score="0" Text="Thanks for looking at it! I'll give it some more investigation. I figured it was a swapping issue since if I print out how many pixels are 0x1 after each iteration, it goes something like (made up numbers) 1000, 900, 860, 890, 860, 890, 860, 890. However, if I just run one part of the switch case over and over, it will decrease monotonically until it can't anymore. Glad to know I'm not crazy about ping-ponging, though! :)" CreationDate="2016-10-18T02:21:33.393" UserId="5316" ContentLicense="CC BY-SA 3.0" />
  <row Id="5504" PostId="4146" Score="0" Text="What is your use case? Are you working on a game? Photo-realistic rendering for cinema? CAD? That makes a huge difference to whether people will notice or not." CreationDate="2016-10-18T02:29:45.093" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5505" PostId="4142" Score="0" Text="Well no actually for the 3D tracker to work paralax has to be quite high. PS motion controlled camera = Multiaxis robot. Which is something even kids can do these days." CreationDate="2016-10-18T02:44:38.623" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5506" PostId="4146" Score="0" Text="This is for a game but do people anyway notice it in a CAD Visualization or in Cinema Rendering?" CreationDate="2016-10-18T05:33:41.820" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5507" PostId="4147" Score="0" Text="For information: [Cross posted from Stack Overflow](http://stackoverflow.com/questions/40054643/reusing-bindbufferbase-and-opengl-compute-shader)" CreationDate="2016-10-18T11:35:14.093" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5508" PostId="4146" Score="4" Text="Certainly in cinema rendering it would be noticeable, especially if it was mixed with live action footage. For CAD, it would probably be desired for renderings shown to, say, an architectural client, but probably not as necessary when doing the modeling, I would think. Most games I've played have not had the greatest shadows. I usually notice, but don't know how much a typical person would. Anyway, I just wanted to clarify the use case because it can make a difference between whether you need realtime or not and what quality vs. time tradeoffs you might be willing to make." CreationDate="2016-10-18T16:19:55.693" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5510" PostId="4150" Score="1" Text="By talking about traversing edges, it sounds like you don't simply want a bitmap of the silhouette, but some vector description. Is that right? What kind of output are you trying to generate?" CreationDate="2016-10-19T14:22:11.967" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5511" PostId="4150" Score="0" Text="Actually, all kinds of description is OK. For example, a sequence of edges, or start with one point then a path. And my confusion is that is there any method through which I can get the description of silhouette without project the model into a 2-D plane?" CreationDate="2016-10-19T16:37:08.633" UserId="5105" ContentLicense="CC BY-SA 3.0" />
  <row Id="5512" PostId="4150" Score="0" Text="If you just use GL to render it into a framebuffer with a fragment shader that outputs a constant colour, you'll get a bitmap of the silhouette that's correct regardless of concavity. It's so easy that I don't think I've correctly understood what you're trying to do, which is why I'm trying to clarify." CreationDate="2016-10-19T16:48:49.693" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5515" PostId="4150" Score="0" Text="well, I think rendering it into a frambuffer is same as project the model on a 2D plane right? And I want algorithms Or method other than projection  to find silhouette for concave model." CreationDate="2016-10-20T03:22:46.603" UserId="5105" ContentLicense="CC BY-SA 3.0" />
  <row Id="5516" PostId="4151" Score="0" Text="I'm voting to close this question as off-topic because it's about representing floating-point numbers; nothing here is specific to graphics." CreationDate="2016-10-20T11:40:46.117" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5517" PostId="4151" Score="2" Text="@DanHulme There seem to be quite a lot of questions on this SE that are &quot;how does X work&quot; with respect to GPUs and opengl/dx. I happen to be only indirectly using the results for drawing things, but that doesn't affect whether the question is useful to other people. Closing feels like splitting hairs to me, and the line I'm over is not at all clear to new users. You're going to be spending all your mod time closing questions if they all have to be sufficiently abstract computer science tasks and no concrete &quot;make GPU do X&quot; questions." CreationDate="2016-10-20T13:38:50.750" UserId="5328" ContentLicense="CC BY-SA 3.0" />
  <row Id="5518" PostId="4152" Score="0" Text="Use clipping, good idea. Thank you very much." CreationDate="2016-10-20T16:07:56.697" UserId="5105" ContentLicense="CC BY-SA 3.0" />
  <row Id="5519" PostId="4151" Score="1" Text="We're still quite a new site, so we don't yet have a lot of consistency in closing questions. This one in particular feels to me like a better fit for SO than for us, because it's &quot;how do I do this specific thing with this library&quot; - about WebGL as a programming environment rather than as a graphics platform. But I'm only one close-voter, so we'll see how the community thinks as a whole. Either way, thank you for helping us to think about our scope." CreationDate="2016-10-20T17:22:25.103" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5520" PostId="4151" Score="0" Text="On the subject of the question itself, do you get the same problem just from calling the two functions, with no texture? Can you be sure the errors aren't being introduced by some texture filtering?" CreationDate="2016-10-20T17:25:02.990" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5521" PostId="4151" Score="0" Text="@DanHulme Not sure what you mean by &quot;just calling them&quot;. I do have [Javascript equivalents of the two functions](https://github.com/Strilanc/Quirk/blob/master/src/webgl/ShaderCoders.js#L313). The javascript variants work on all the given test values, though that was easier since the intermediate calculations are done with 64 bit precision. I use the JS methods as a comparison tool when debugging the shaders. The shaders do give the right answers on my laptop... just not on tablets and phone." CreationDate="2016-10-20T17:56:50.407" UserId="5328" ContentLicense="CC BY-SA 3.0" />
  <row Id="5522" PostId="4151" Score="4" Text="I personally feel this is pretty on topic.  The heart of real time computer graphics is often &quot;how can i get my data to the GPU in an efficient and accurate way&quot; and getting a float to a shader is right in that vein.  This comes up all the time when doing shadertoy things for example, and likely the best person to answer would be one of those folks who have solved it there, on shadertoy or in demoscene type code, who would be here IMO." CreationDate="2016-10-20T17:58:25.827" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5523" PostId="4153" Score="0" Text="I'm going to go out on a limb and say that it's because the edges are infinitely thin (which follows from the fact that the flat surfaces on each side of the cube are infinitely thin), therefore they have zero surface area, therefore they don't need normals." CreationDate="2016-10-20T20:55:30.627" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5524" PostId="4146" Score="0" Text="_&quot;Is it worth it?&quot;_ entirely depends on *your use case*. You can (and should) look at what is done in products with a similar use case and set of constraints as yours, but in the end only you can make that decision." CreationDate="2016-10-21T03:09:10.190" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5525" PostId="4153" Score="1" Text="Could you detail what makes you think you cannot use lambertian shading on a cube?" CreationDate="2016-10-21T06:04:38.600" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5526" PostId="4153" Score="0" Text="@ Julien Guertault    I mean that for a mathematical cube there aren't normals on the edge points. The formula of Lambertian shading needs a surface with normals on every point. I know that software applications use some tricks, but with this view pratical computer graphics is not an approximation of theoric computer graphics." CreationDate="2016-10-21T11:56:52.870" UserId="1636" ContentLicense="CC BY-SA 3.0" />
  <row Id="5528" PostId="4161" Score="0" Text="How would you blend material parameters like `Metalness`, `Roughness`?" CreationDate="2016-10-22T12:54:27.683" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5529" PostId="4161" Score="0" Text="You could just linearly blend them. Also one option is to use height threshold to just &quot;switch&quot; between material layers, e.g. if you want things like sand between rocks because linear blend would look unnatural" CreationDate="2016-10-22T13:05:31.997" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5533" PostId="4138" Score="0" Text="Are there any good resources other than siggraph to track advances in real time rendering?" CreationDate="2016-10-23T14:04:36.053" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5534" PostId="4158" Score="1" Text="One of the answers in http://computergraphics.stackexchange.com/questions/2161/modern-screen-space-ambient-occlusion-techniques links to a useful paper: http://frederikaalund.com/a-comparative-study-of-screen-space-ambient-occlusion-methods/ It goes through a few different AO techniques in some detail." CreationDate="2016-10-22T16:04:58.150" UserId="554" ContentLicense="CC BY-SA 3.0" />
  <row Id="5535" PostId="4165" Score="0" Text="Why the down vote? :P" CreationDate="2016-10-23T15:22:22.593" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5536" PostId="4165" Score="0" Text="It's not true that spherical harmonics are not good at representing fine details. Like the Fourier transform, they can reconstruct the original signal exactly if you keep all the frequencies. It's just that they make it easy to save space by throwing away high frequencies if you don't need them." CreationDate="2016-10-23T15:32:19.540" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5537" PostId="4165" Score="0" Text="And sorry, I thought after casting the vote it was harsh to downvote for a single dubious claim in an otherwise helpful answer, but my vote was locked in by the time I had second thoughts." CreationDate="2016-10-23T15:33:10.210" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5538" PostId="4165" Score="0" Text="Ah. Yeah it's possible, in the same way that it's possible to represent any data set with a polynomial, but in practice, both are bad choices for needing close fits to many data points.  With a polynomial you need N terms of an N order function to exactly fit N data points for instance, which makes it a worse choice than just an array since it's calculation, not lookup, to get a data point out.  Similarly, in practical terms, spherical harmonics are a bad choice for spherical data with high frequency content that you want to preserve. It's not a good choice in those situations." CreationDate="2016-10-23T15:52:05.053" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5539" PostId="4165" Score="0" Text="That's true if you want to be able to reconstruct individual samples, but not all uses of Fourier require that - similarly for not all uses of SH. If you're going to do a convolution, it's much cheaper to do that in the frequency domain before transforming back to samples. Would you mind me proposing an edit to make this clearer in your answer, after I've finished my own answer?" CreationDate="2016-10-23T15:59:41.780" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5540" PostId="4165" Score="0" Text="Good point about convolution! Sure, go for it." CreationDate="2016-10-23T16:06:30.587" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5541" PostId="4166" Score="0" Text="Great answer this really clarified everything! So basically Light Probes are a easy way to calculate the lighting on moving characters which prevent us from re calculating the GI for the entire scene. Spherical Harmonics on the other hand are used to filter out the high frequencies. (Correct me if I'm wrong, I'm just trying to see if I have the right understanding)." CreationDate="2016-10-23T16:19:44.507" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5543" PostId="4166" Score="0" Text="&quot;if you use the same number of sine waves as there were samples (pixels) in the original image, you can reconstruct the image exactly&quot;, actually not true. For example square wave requires infinite number of frequencies for exact representation" CreationDate="2016-10-23T19:50:30.880" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5545" PostId="4167" Score="1" Text="Could you show what you have tried, and explain what you currently understand?" CreationDate="2016-10-23T22:03:19.600" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5546" PostId="4155" Score="0" Text="Are you able to include your code in the question? I recommend removing most of the code until you stop getting the error message, so you can then provide just enough code to cause the error message. This will help narrow down the source of the problem and is likely to make it easier to answer your question." CreationDate="2016-10-23T22:09:37.427" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5547" PostId="4166" Score="0" Text="@JarkkoL Sure, a true square wave does. But if you've discretized it by sampling, then you only need the same number of frequencies to make the error less than the sampling error. It's a handy outcome of Nyquist's theorem (that the highest frequency present in the sampled signal is half the sample rate)." CreationDate="2016-10-23T23:06:43.267" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5548" PostId="4155" Score="0" Text="The problem is not my Code, it used to work fine with OS X El Capitan. The Error I get has something todo within the underlaying window management of OS X Sierra. I have contacted Apple already and they are working on resolving the issue" CreationDate="2016-10-24T06:58:24.050" UserId="5334" ContentLicense="CC BY-SA 3.0" />
  <row Id="5549" PostId="4168" Score="0" Text="I'm not sure there are that many true 24bit displays. I suspect many will probably pad each pixel out to 4 bytes, thus becoming 3145728, i.e. 3MB." CreationDate="2016-10-24T07:19:39.417" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5550" PostId="4155" Score="0" Text="If there's some aspect of your code that worked with the old version but doesn't work with the new version, then we would need to see your code to narrow down what needs to change to work with the new version. If the problem is a bug in closed source software provided by a company, then that would make this question off topic here as we have no way of fixing the problem. If you think there's a chance it is the first, providing a [minimal working example](http://meta.computergraphics.stackexchange.com/questions/126/should-we-require-minimal-working-examples-mwes) will allow us to investigate." CreationDate="2016-10-24T12:55:40.133" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5552" PostId="4168" Score="1" Text="XGA is a specific standard from a while back, when they didn't have the memory to pad to 32bit. Check it out here:&#xA;https://en.wikipedia.org/wiki/Graphics_display_resolution#XGA_.281024x768.29" CreationDate="2016-10-24T14:04:49.003" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5553" PostId="4166" Score="0" Text="Ah yes, that's true of course. I suppose you need half the frequency but complex (vs real) frequency domain results. Or use DCT &amp; real domain with twice the frequency of DFT." CreationDate="2016-10-24T14:28:11.923" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5555" PostId="4153" Score="0" Text="Ah ok. So, pixels are infinitely small sample points on a grid.  The edge of a cube is infinitely thin - a line segment.  Because of this, we don't often see edges in rendering. When we do, they show up as holes in the cube, z fighting at the edge and similar. In practice though, you aren't ever shading the edge, just the faces." CreationDate="2016-10-25T01:06:25.817" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5559" PostId="4138" Score="1" Text="@ArjanSingh I think one way to find this out, is to look at the references section at the end of any SIGGRAPH article, to see where the references were published.  Those might indicate some other good resources." CreationDate="2016-10-25T14:05:36.290" UserId="5356" ContentLicense="CC BY-SA 3.0" />
  <row Id="5560" PostId="4172" Score="0" Text="Don't know the details but in PenTile subpixels are shared between framebuffer pixels so you can't control them individually. e.g. could be that if you have 3x3 PenTile subpixels, they map to 2x2 framebuffer pixels and the intensity of subpixel is determined by the overlap &amp; framebuffer RGB values. This is purely a guess though" CreationDate="2016-10-25T14:23:50.283" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5563" PostId="4172" Score="0" Text="@JarkkoL That's the point of my question. I know the subpixels are shared, but I need to know what function determines the intensity of each subpixel, so I can predict what results I'll get from certain framebuffer values." CreationDate="2016-10-25T15:44:46.733" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5567" PostId="4172" Score="0" Text="Your question indicated otherwise thus the clarification. If you know the native resolution vs subpixel reso, it might give some idea about the transform" CreationDate="2016-10-25T17:23:26.737" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5568" PostId="4178" Score="2" Text="If the input photo is only looking in one direction then this will be impossible without distortion. You can either use a collection of photos covering all the different directions, or you can have a distorted result, which will also have a discontinuity at the left and right edges." CreationDate="2016-10-25T19:26:04.347" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5569" PostId="4178" Score="0" Text="If you want this picture to be shown constantly then, use the UI image provided by unity." CreationDate="2016-10-25T20:11:02.463" UserId="3437" ContentLicense="CC BY-SA 3.0" />
  <row Id="5570" PostId="4179" Score="0" Text="Including your initial ideas may help people to tailor the explanation and highlight any misconceptions." CreationDate="2016-10-25T22:14:08.147" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5571" PostId="4180" Score="1" Text="You wouldn't just get a weird seam if you tried to use this image as an equirectangular map: it would also get increasingly distorted towards the poles. The stars would get squished together in one axis, so they'd look like lines." CreationDate="2016-10-26T08:07:44.140" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5573" PostId="4181" Score="0" Text="Sorry, maybe I was not clear in the question. Of course I know what you're explaining in your answer. I wanted to know why are we multiplying by $M^{−1}$ in one case and by $M$ in another. In which case do we use $M$ and in which case do we use $M^{−1}$?" CreationDate="2016-10-26T11:13:08.850" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5574" PostId="4181" Score="0" Text="Because $M$ transforms from one space to another (e.g. object$\rightarrow$world space), inverse of the matrix defines the opposite transformation (e.g. world$\rightarrow$object space)" CreationDate="2016-10-26T12:55:32.787" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5575" PostId="4181" Score="0" Text="Ok, so let me try to be more specific: why $M$ (and not $M^{-1}$) transforms from object to world space?" CreationDate="2016-10-26T12:58:07.683" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5576" PostId="4181" Score="0" Text="It's natural to define how objects are oriented in the world rather than how world is oriented in objects. Mathematically nothing prevents you from defining $M$ as world$\rightarrow$object transform and use $M^{-1}$ to define object$\rightarrow$world transform though, it's just unnatural (:" CreationDate="2016-10-26T13:51:01.993" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5577" PostId="4180" Score="0" Text="That's true about the distortion. However, I think there would be a weird seam because the purple band wouldn't meet itself. It would just suddenly stop with a hard edge and restart higher up with another hard edge." CreationDate="2016-10-26T14:32:17.553" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5578" PostId="4180" Score="1" Text="Yes, you'd get a seam as well. But you already mentioned a way to reduce that problem, so I thought I'd better caution the OP that even after fixing the seam they'll still have distortion." CreationDate="2016-10-26T16:26:56.160" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5579" PostId="4181" Score="0" Text="@nbro It might help to think of $M^{-1}$ as undoing whatever $M$ does. If you apply $M$ and then $M^{-1}$ you get back to where you started. So $M$ takes you from object to world, and $M^{-1}$ takes you from world to object. It doesn't matter what you call the matrix. Whatever you did to get from object to world, you have to do the opposite to get back from world to object." CreationDate="2016-10-26T17:16:30.310" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5580" PostId="4183" Score="0" Text="Thanks for the answer!  Interestingly, I have had some limited success at this, thanks to some ideas from someone on twitter.  I'm experimenting more to come up with some final info &amp; a blog post, and will post an answer here with the details.  It's not a general solve, but it is still a bit interesting.  Maybe extensible.  Also had a friend mention he has been able to do this with other limited success by faking modulus mathematically, like using a badlimited saw wave.  I'll share some shadertoy links in the next comment for the curious." CreationDate="2016-10-26T17:30:58.593" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5581" PostId="4183" Score="1" Text="Shadertoy from sebbi showing his idea: https://www.shadertoy.com/view/lly3Rc&#xA;Me exploring and formalizing some things: https://www.shadertoy.com/view/MlK3zt&#xA;Limited success for ray vs infinite layers of concentric circles: https://www.shadertoy.com/view/4tyGDK&#xA;I'm currently working on ray vs infinitely repeating pillars. Looking promising so far.&#xA;In the end these things are interesting but not in general very useful.  Maybe they can be extended. ::shrug::" CreationDate="2016-10-26T17:34:08.840" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5582" PostId="4181" Score="0" Text="@JarkkoL well, its not nesseserily so unnatural if you build out of basic operations. Rotate is just reverse angle, translate is just reverse angle, scale is inverse number, skew is reverse. Forming the matrix from a vector will be weird at first. But the concept would be like moving the world to move object, which is really really odd and unnatural." CreationDate="2016-10-26T17:57:25.637" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5583" PostId="4184" Score="0" Text="This is probably a a dumb question, but do you mean uv, instead of uwv? Are there 3 components to your texture coordinate? And if so, why is it uwv, not uvw?" CreationDate="2016-10-26T18:24:31.963" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5584" PostId="4184" Score="1" Text="@AlanWolfe The titlebar in the screenshot says &quot;Edit UVWs&quot; so the questioner probably means that." CreationDate="2016-10-26T18:45:16.803" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5585" PostId="4183" Score="0" Text="Regarding there needing to be an inverse operation for modulus, there is the form: i = 3N where N is in Z, as a reverse of i%3 = 0.  If we had a solution of that form, we could plug in a value for N, presumably 0?" CreationDate="2016-10-26T19:46:23.730" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5586" PostId="4184" Score="0" Text="@AlanWolfe I have a lack of experience in 3D graphics, so I called it according to 3DSMax modifier name &quot;Unwrap UVW&quot;. Renamed to UVs to avoid misunderstanding." CreationDate="2016-10-26T21:04:43.367" UserId="5363" ContentLicense="CC BY-SA 3.0" />
  <row Id="5587" PostId="4184" Score="0" Text="Instead of using multiple UV's for different texture types (albedo, normal, etc.), you should use single UV and multiple textures for different types." CreationDate="2016-10-26T21:55:38.840" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5588" PostId="4185" Score="0" Text="Can you clarify what you mean by a &quot;spherical Fourier transform&quot;? I googled, but didn't turn up anything that sounds like it would match this question." CreationDate="2016-10-27T04:24:36.350" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5589" PostId="4185" Score="0" Text="I'm thinking like taking a unit sphere, breaking it into the two angles that parameterize it, so you have a 2d function, then using 2d DFT on that." CreationDate="2016-10-27T04:30:17.043" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5590" PostId="4185" Score="0" Text="Related: http://math.stackexchange.com/questions/17479/how-to-perform-a-fourier-transform-in-spherical-coordinates" CreationDate="2016-10-27T05:39:55.723" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5591" PostId="4184" Score="0" Text="@JarkkoL, texture of greater size is needed in case of single UVs. For example, my model should have bump on the top part and image on the rest part - then single UVs would be divided into two parts: for bump and for image. Each texture map will use only a half of avalable space (image with &quot;sun&quot; illustrates a cube UVs with 6 different texture maps for each side)." CreationDate="2016-10-27T07:16:42.600" UserId="5363" ContentLicense="CC BY-SA 3.0" />
  <row Id="5593" PostId="4151" Score="0" Text="@Craig Gidney example 1 is incorrect but the fixed version works well with values between 0 and 1 (i use a similar version to convert a 24 bit depth buffer value into rgb8 texture with no precision loss)." CreationDate="2016-10-27T12:57:41.497" UserId="5364" ContentLicense="CC BY-SA 3.0" />
  <row Id="5594" PostId="4184" Score="0" Text="In general models have all types of textures used for all the parts with matching UV space thus single UV channel is fine. E.g. you would allocate both normal &amp; albedo textures for the top part of your model. Furthermore you should try to have constant pixel density for a good UV map. You are talking about very specific UV space optimization but this isn't the way models are UV mapped in general, IME." CreationDate="2016-10-27T13:37:59.750" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5599" PostId="4151" Score="0" Text="@Raxvan How did you &quot;fix&quot; it? Floats get more precise as you travel to 0, but the linked example-1 method is a 32-bit fixed-point thing. There's nearly a billion representable values in the range from 2^-126 to 2^-32, and they all get totally trashed by that kind of approach." CreationDate="2016-10-27T14:51:57.663" UserId="5328" ContentLicense="CC BY-SA 3.0" />
  <row Id="5602" PostId="4151" Score="0" Text="@Craig Gidney take the example 1 as it is and replace **all** 256.0 values with 255.0. just remember that that method works for values between 0 and 1 actually ever 1 just [0..1)" CreationDate="2016-10-27T15:40:17.413" UserId="5364" ContentLicense="CC BY-SA 3.0" />
  <row Id="5603" PostId="4151" Score="0" Text="@Raxvan Oh. You should replace some of the 256s, but not all of them. And it does round the many values under 2^-32 into 0." CreationDate="2016-10-27T15:42:42.603" UserId="5328" ContentLicense="CC BY-SA 3.0" />
  <row Id="5604" PostId="4186" Score="0" Text="Would there be much point in mapping the sphere differently to compensate for the poles problem? Maybe something taking the vertical angle as a percent and squaring it, to pull the sample points towards the equator non linearly?" CreationDate="2016-10-27T15:49:31.127" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5605" PostId="4186" Score="1" Text="The other reason why SH is handy is it's a rotationally invariant representation. Rotating an SH basis function (by any axis and angle) transforms it to a linear combination of other SH basis functions of the same order. So, you can rotate an SH-represented function by [applying an appropriate matrix to the SH coefficients directly](http://www.filmicworlds.com/2014/07/02/simple-and-fast-spherical-harmonic-rotation/). You don't get this if you do a 2D Fourier transform in lat-long parameter space." CreationDate="2016-10-27T17:49:50.580" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5606" PostId="4192" Score="2" Text="Very good answer, especially for the pointing to the Wikipedia arcticle that I could not reach in google searches." CreationDate="2016-10-27T20:34:58.403" UserId="5367" ContentLicense="CC BY-SA 3.0" />
  <row Id="5608" PostId="4186" Score="2" Text="@NathanReed Yes, I was thinking about that, but neglected to include it in the answer. I've updated it now with some more thoughts about that. Thanks for mentioning it." CreationDate="2016-10-27T22:51:03.530" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5611" PostId="4192" Score="1" Text="I would add that aliasing is a general phenomenon that can be found in contexts other than (screen) &quot;spatial aliasing&quot; or &quot;temporal aliasing&quot;: http://en.wikipedia.org/wiki/Aliasing" CreationDate="2016-10-28T06:27:13.267" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="5612" PostId="4194" Score="0" Text="errrr I assign range from [-2,+2] to [0, 800] and [-1.5, 1.5] to [0, 600] and not the other way around. I thought this was clear from my question." CreationDate="2016-10-28T07:50:09.040" UserId="5371" ContentLicense="CC BY-SA 3.0" />
  <row Id="5613" PostId="4194" Score="1" Text="@quantum231 unless you get a coordinate in the [0, 600][0, 800] space and need to know what it was in the [-1.5, 1.5][-2, 2] space to sample it." CreationDate="2016-10-28T08:28:04.170" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5614" PostId="4194" Score="2" Text="Yeah, but that's not correct. You need to do the inverse to evaluate the Mandelbrot equation for each pixel in your image exactly once. I should have probably emphasized that in my answer." CreationDate="2016-10-28T08:34:57.753" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5616" PostId="4184" Score="0" Text="You may want to split this up into two separate questions because they are different use cases (might help with getting answers). For your first question though, you say you want to tile a texture (presumably to save space). What do you mean by this? Are talking about overlapping the four triangles, or something totally different?" CreationDate="2016-10-29T03:50:42.547" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5617" PostId="4177" Score="0" Text="What are the benefits of using this method over the simpler Image based Lighting?" CreationDate="2016-10-29T11:45:29.620" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5618" PostId="4177" Score="0" Text="You can have better quality low-frequency lighting with similar storage and performance requirements than say using cubemaps." CreationDate="2016-10-29T14:36:46.703" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5619" PostId="4197" Score="0" Text="Does `glGetError()` return any errors during the process?" CreationDate="2016-10-29T15:45:29.803" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5620" PostId="4196" Score="0" Text="Radiosity is a way to compute global illumination under the assumption that all surfaces are perfectly diffuse (Lambertian)." CreationDate="2016-10-29T17:24:47.147" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="5621" PostId="4197" Score="0" Text="No, it returns 0 after each step." CreationDate="2016-10-29T18:27:31.640" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5622" PostId="4198" Score="0" Text="Do most engines like UE4, Unity, Frostbite etc use this as a global illumination technique alongside other methods? I know UE4 has Lightmass and Unity uses Enlighten, are they basically radiosity 'engines'? What other ways can you bake light or calculate indirect lighting in real time?" CreationDate="2016-10-30T05:49:09.223" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5623" PostId="4198" Score="2" Text="All real-time engines combine some kind of indirect illumination approximation (radiosity based or something else) with other techniques for more complete rendering equation. Your question about different techniques to approximate indirect illumination warrants its own question though as it's too broad of a question to answer in comments." CreationDate="2016-10-30T16:38:57.687" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5624" PostId="4201" Score="0" Text="How would that work? gl_FragCoord are the screen UVs. If the mesh occupies part of the screen it's not going to be textured correctly from these." CreationDate="2016-10-31T09:07:49.493" UserId="3331" ContentLicense="CC BY-SA 3.0" />
  <row Id="5625" PostId="4201" Score="0" Text="@aces Yes I agree, I don't really get how this could work." CreationDate="2016-10-31T09:48:04.310" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5626" PostId="4200" Score="0" Text="Could you explain further the end result. Are you after decal projection? A picture of the expected result would help." CreationDate="2016-10-31T10:16:53.860" UserId="3331" ContentLicense="CC BY-SA 3.0" />
  <row Id="5627" PostId="4200" Score="0" Text="This is still for eye rendering :) post updated." CreationDate="2016-10-31T10:47:44.343" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5628" PostId="4200" Score="0" Text="@MaT based on the images i don't think a normal map is enough , you also need a height map. you need to compute the direction vector on the uv space and then offset it based on surface &quot;depth&quot;" CreationDate="2016-10-31T12:35:33.907" UserId="5364" ContentLicense="CC BY-SA 3.0" />
  <row Id="5629" PostId="4200" Score="0" Text="My question seems to be really badly formulated :P&#xA;The Normal screenshot shows the eyes without any rotation therefore the occlusion map is well applied but as soon as you rotate the eye, the occlusion classical behaviour breaks the effect.&#xA;That's why I am trying apply the occlusion map not only in object space to avoid the effect of the eye rotation, but I don't see how." CreationDate="2016-10-31T12:40:35.797" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5632" PostId="4201" Score="0" Text="@Syntac_ The picture makes the intent clearer, but it certainly can be textured correctly with this if you save the rendering of the pre-rotated mesh onto a texture and used screenspace coordinates to look it up. See my edit for a more detailed explanation." CreationDate="2016-11-01T02:58:26.640" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5633" PostId="4076" Score="0" Text="Vulkan is using system-on-a-thread ? Were as DX12 is fully multi-threaded?" CreationDate="2016-11-01T04:03:00.227" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="5634" PostId="4207" Score="0" Text="Thanks for hints. I corrected glTexImage3D function as you pointed but the problem seem to persist. What I noticed is that when I setup texture as RGBA32F, RGBA, GL_FLOAT and then bind it as RGBA32I image and use iimage in shader it works. But if I setup texture as RGBA32I, RGBA_INTEGER, GL_INT is is empty. So it is something with texture setup still. But it *is* correct and glGetError returns 0... I updated my nvidia drivers but this is not driver fault." CreationDate="2016-11-01T11:40:31.747" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5635" PostId="4208" Score="0" Text="I would suggest using uniforms for most of these parameters, such as transformation matrices. You can use uniform buffers to quickly switch uniforms when rendering. Also, the vao just stores the attribute bindings and not the vbo itself." CreationDate="2016-11-01T15:04:41.280" UserId="4768" ContentLicense="CC BY-SA 3.0" />
  <row Id="5637" PostId="4076" Score="0" Text="Am not sure if DX12 is using one pipeline, might be worth looking at the documentation for DX12 for the specifics." CreationDate="2016-11-01T19:48:27.887" UserId="4829" ContentLicense="CC BY-SA 3.0" />
  <row Id="5638" PostId="4076" Score="1" Text="@PaulHK, according to the DX12 article, that would be case. Looks, like VUlkan is using multiple threads as well: https://www.khronos.org/registry/vulkan/specs/1.0/xhtml/vkspec.html#fundamentals-threadingbehavior" CreationDate="2016-11-01T19:52:59.323" UserId="4829" ContentLicense="CC BY-SA 3.0" />
  <row Id="5639" PostId="4212" Score="0" Text="http://stackoverflow.com/questions/1313259/tiling-simplex-noise" CreationDate="2016-10-27T08:26:31.723" UserDisplayName="Christophe Roussy" ContentLicense="CC BY-SA 3.0" />
  <row Id="5640" PostId="4213" Score="0" Text="Thanks for your response! Two questions: 1) The wiki article uses `R' G' B'` in the equation which corespond to &quot;gamma-compressed&quot; values. So wouldn't give me this wrong numbers because I'm using linear RGB? 2) Could you explain why you've chosen this formula from the article? I'm confused why this does apply to my case and not the one with `0.2126 ...`" CreationDate="2016-11-01T20:58:45.803" UserId="5351" ContentLicense="CC BY-SA 3.0" />
  <row Id="5641" PostId="4213" Score="0" Text="@PeteParly It's a matter of choice. I tend to use this formula for calculating the brightness of colors in linear space, because it gives subjectively more pleasing results than the rather aggressive Rec. 709 formula, at least on my monitor. As for gamma-compressed colors, the wiki article mentions the same formula is arbitrarily used for both gamma and linear color spaces, despite it giving different results." CreationDate="2016-11-01T21:41:27.770" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="5642" PostId="4213" Score="0" Text="@PeteParly The best thing you can do is to try out multiple formulas and choose the one that works for you. I'm not aware of any particular standard, besides the Lab color space, which tries to more closely approximate human vision: https://en.wikipedia.org/wiki/Lab_color_space&#xA;&#xA;Using it is a bit more involved, though. To get just the gray color, you'd have to convert your RGB color to Lab, then set the a and b components to 0, effectively removing chrominance, and then convert it back to the RGB space of your choice." CreationDate="2016-11-01T21:44:07.637" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="5643" PostId="4213" Score="0" Text="I think this &quot;The best thing you can do is to try out multiple formulas and choose the one that works for you.&quot; answers a bit of my confusion in my head :) I'll try that and accept your answer" CreationDate="2016-11-01T21:48:25.713" UserId="5351" ContentLicense="CC BY-SA 3.0" />
  <row Id="5645" PostId="4214" Score="0" Text="Are you looking for a hash function that gives an *identical* hash for images similar enough to be different sized versions of the same original, or one that gives more *similar* results for more similar inputs?" CreationDate="2016-11-02T01:46:27.097" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5646" PostId="4212" Score="3" Text="From Wikipedia: &quot;Simplex noise has no noticeable directional artifacts (is visually isotropic), though noise generated for different dimensions are visually distinct (e.g. 2D noise has a different look than slices of 3D noise, and it looks increasingly worse for higher dimensions[citation needed]).&quot;, so sounds like what you got" CreationDate="2016-11-02T01:50:10.323" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5647" PostId="4214" Score="0" Text="I don't think this is answerable without clarifying the purpose of the hash function, so I've put it on hold until it is clarified and then it can be edited and reopened." CreationDate="2016-11-02T01:57:49.660" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5648" PostId="4214" Score="0" Text="Sounds like OP wants a robust way to compare if two images are the same even if they have gone through resize or lossy compression, by generating a hash key from it. Sounds a bit something OpenCV might help with" CreationDate="2016-11-02T02:02:17.523" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5649" PostId="4214" Score="0" Text="Similar question with &quot;Feature Matching&quot; answer: http://stackoverflow.com/questions/11541154/checking-images-for-similarity-with-opencv" CreationDate="2016-11-02T02:17:13.103" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5651" PostId="4198" Score="0" Text="Could you recommend any good tutorials on the topic?" CreationDate="2016-11-02T08:23:22.180" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5653" PostId="4217" Score="0" Text="Thanks! And you are right, the OpenGL-Class has a static member `static GLuint NumberAddedAttributes` which is increased every time an attribute is added. And if each VAO really has its own attribute space, this would mean that i only have to make this member non-static and everything should work. But when i tried this yesterday, i ran into some problems. But now since I read your answer I'll check it again." CreationDate="2016-11-02T09:41:51.020" UserId="5399" ContentLicense="CC BY-SA 3.0" />
  <row Id="5654" PostId="4217" Score="0" Text="I would like to extent the question to SSBO (shader storage buffer object): I also use them and create the shader preamble automatically. I guess in case of SSBO binding points the counter has to be global for all shader?" CreationDate="2016-11-02T10:04:27.940" UserId="5399" ContentLicense="CC BY-SA 3.0" />
  <row Id="5655" PostId="4215" Score="0" Text="Lighting a scene and final rendering the lit triangles are orthogonal." CreationDate="2016-11-02T12:04:34.873" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5656" PostId="4213" Score="0" Text="@PeteParly you can do better if you have a colorimeter, than you can make a icc profile and be more exact for your monitor" CreationDate="2016-11-02T17:02:03.867" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5657" PostId="4214" Score="0" Text="@trichoplax yes I am looking for a hash function that produces an 'identical' digest regardless (or at least to a good degree) of the size, orientation and skewness." CreationDate="2016-11-02T18:27:25.123" UserId="5401" ContentLicense="CC BY-SA 3.0" />
  <row Id="5658" PostId="4214" Score="0" Text="@picolo it sounds like you would prefer a slightly changed image to show as a 100% match (identical hash), but you would also be happy with a high percentage match (similar hash) if there isn't a way to make it 100%. If that's the case, you can edit the question to make it clear what you would find acceptable and it can be reopened." CreationDate="2016-11-02T21:15:34.340" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5659" PostId="4212" Score="4" Text="Also from wikipedia &quot;For higher dimensions, n-spheres around n-simplex corners are not densely enough packed, reducing the support of the function and making it zero in large portions of space.&quot;" CreationDate="2016-11-02T23:05:05.443" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5660" PostId="4212" Score="1" Text="You might want to also insure that all dim are returning full range values..some implementation do not." CreationDate="2016-11-03T08:56:42.733" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="5661" PostId="26" Score="0" Text="The patent is narrowly defined and includes the bit-twiddling permutation method.  SEE claim 1: https://www.google.com/patents/US6867776" CreationDate="2016-11-03T09:07:59.743" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="5662" PostId="4215" Score="1" Text="Like, at a right angle to each other? Dude that is deep." CreationDate="2016-11-03T09:14:44.753" UserId="5403" ContentLicense="CC BY-SA 3.0" />
  <row Id="5663" PostId="4215" Score="0" Text="No I mean that once you have the lighting information you can simply leave out the ceiling geometry in the final render of the lit scene." CreationDate="2016-11-03T09:20:12.143" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5667" PostId="4227" Score="0" Text="Will an implementation clamp the resolved value into the bit depth of the source before writing to the destination? OR, can I assume the precision is &quot;enough&quot; throughout the resolve to not lose any information?" CreationDate="2016-11-03T21:00:38.740" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5668" PostId="4226" Score="0" Text="Current GPUs only support up to 8 samples per pixel, BTW, so your scenario with 32 samples in 4-bit color unfortunately isn't possible using hardware multisampling." CreationDate="2016-11-03T21:45:00.907" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5669" PostId="4226" Score="1" Text="@NathanReed Provide a reference of your claim. Yesterday i queried an Nvidia Quadro 4000 of possible framebuffer configurations and up to 64 samples was provided." CreationDate="2016-11-03T21:46:04.597" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5670" PostId="4226" Score="1" Text="@NathanReed: Not true at all. There are [many implementations that support a sample count greater than 8](http://opengl.gpuinfo.org/gl_stats_caps_single.php?listreportsbycap=GL_MAX_COLOR_TEXTURE_SAMPLES). Indeed, pretty much every GPU that supports GL 4.x supports more than 8 samples. Not because 4.x requires it, but simply because they can. Even many of Intel's GPUs support 16. The only exceptions seem to be AMD GPUs that don't use their GCN core." CreationDate="2016-11-03T22:35:04.340" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="5671" PostId="4222" Score="0" Text="Would you know of any online tutorials that do this?" CreationDate="2016-11-04T05:56:49.953" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5672" PostId="4222" Score="1" Text="@ArjanSingh The UE4 source code has examples of how to do this exact method." CreationDate="2016-11-04T08:31:54.987" UserId="3331" ContentLicense="CC BY-SA 3.0" />
  <row Id="5673" PostId="4222" Score="1" Text="It's a bit more complicated in UE4 because it also generates lens flares, and the shader system requires some engine know-how to understand, but yeah, `AddBloom()` [here](http://tinyurl.com/hswdnqg) is a good starting point. However, for a super-simple technique that involves an optional &quot;lens dirt&quot; effect, you could check out the post-processing library I wrote back at university: [the CPU side](http://tinyurl.com/zoclj9k) (lines 222-241) and the [bright pass](http://tinyurl.com/zjt3lgh) and [compositing](http://tinyurl.com/jdjdwrw) shaders. Apologies for TinyURL, comment was too long." CreationDate="2016-11-04T09:09:22.000" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5674" PostId="4222" Score="0" Text="Also, [this blogpost](https://extremeistan.wordpress.com/2014/09/24/physically-based-camera-rendering/) is a pretty cool resource for camera effects such as this. It links to [Intel's tutorial](https://software.intel.com/en-us/articles/compute-shader-hdr-and-bloom) on the subject, with compute shaders in mind, for instance." CreationDate="2016-11-04T09:20:08.297" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5677" PostId="4220" Score="0" Text="Are you trying to group all photos taken from the same location together, or to identify the location a photo was taken from, or to arrange the photos in a grid according to where they were taken from? Is it just their relative arrangement that is important, or also their absolute location?" CreationDate="2016-11-04T16:21:38.627" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5678" PostId="4220" Score="0" Text="Are these &quot;360 photos&quot; 360 degree panoramic photographs, looking horizontally in all directions? Or are they spherical photographs, looking in all directions, including up and down as well as horizontally?" CreationDate="2016-11-04T16:24:11.263" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5679" PostId="4220" Score="0" Text="I have put the question on hold until it can be clarified. Once it has been edited to make clear what is provided and what is required, it can be reopened if it is on topic." CreationDate="2016-11-04T16:26:37.400" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5680" PostId="4227" Score="0" Text="@Andreas: I would not suggest attempting to use multisampling to make precise calculations of anything. You should use it for its intended purpose: making pictures. If you need guaranteed precision of some calculation, multisampling is the wrong tool to employ; it has too many implementation-dependent variables." CreationDate="2016-11-04T16:57:26.603" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="5682" PostId="4227" Score="0" Text="For the record my (current) mission is trying out order independent translucency and specifically smoke by altering the number of samples covered and their depth. I probably need many samples, at least 8. To make that work I need some guarantees of how OpenGL resolves the multisample surface. And I´d prefer it to be hardware accelerated. I´ll be back with more questions if I screw up." CreationDate="2016-11-05T11:25:48.593" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5684" PostId="4230" Score="4" Text="Some diagrams or screenshots of what you mean might be helpful. When you say the two new points are a threshold apart, does that mean they aren't evenly spaced along the edge? I don't quite get how that helps with temporal stability." CreationDate="2016-11-05T15:29:02.347" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5686" PostId="1518" Score="1" Text="`GL_SHADER_STORAGE_BARRIER_BIT` Wrong barrier. The barrier you provide states how you will *use* the memory. Not how the shader wrote to it. You are doing a pixel transfer, so you need to use `GL_TEXTURE_UPDATE_BARRIER_BIT`" CreationDate="2016-11-05T17:57:19.030" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="5691" PostId="4236" Score="0" Text="Maybe there are another stackexhange places, where this question would be more appropriate to ask?" CreationDate="2016-11-06T08:51:54.730" UserId="5430" ContentLicense="CC BY-SA 3.0" />
  <row Id="5692" PostId="4235" Score="1" Text="So after everything (sky, fog, lighting, shading etc) goes physically based, what comes next? Will most people just try to improve current approximations? Where do you think rendering will head to in the coming years?" CreationDate="2016-11-06T10:08:26.243" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5693" PostId="4236" Score="0" Text="This is on topic here. The only thing I would suggest is explaining what you have already tried, and your current understanding of why this is incorrect. Sometimes describing what is wrong can give a clue about what is causing it. Describing what you have already tried is required on any Stack Exchange site though - it isn't a reason to move the question." CreationDate="2016-11-06T10:39:58.827" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5695" PostId="4236" Score="0" Text="Are you using floating point numbers?" CreationDate="2016-11-07T01:00:29.867" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="5696" PostId="4236" Score="0" Text="@DanielMGessel Yes." CreationDate="2016-11-07T02:04:36.943" UserId="5430" ContentLicense="CC BY-SA 3.0" />
  <row Id="5697" PostId="4226" Score="0" Text="Ahh, I stand corrected. Somehow missed when GPUs crossed the 8-sample threshold. That was the limit for a long time, IIRC." CreationDate="2016-11-07T04:07:02.297" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5698" PostId="4233" Score="0" Text="Yeah, it was a bit hasty answer, added some edits" CreationDate="2016-11-07T04:42:08.213" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5699" PostId="4235" Score="1" Text="For fun of it, I calculated that we would need ~250,000x the GPU performance to reach today's movie quality in real-time (30ms vs 2h per frame). This may sound a lot but if we continue with ~1.5x perf increase per year, we could get there in ~30 years (: It's obviously more complicated than that (mem bandwidth, algorithm improvements, etc.) but gives some idea in what kind of timeframe we might get into full real-time path tracing (:" CreationDate="2016-11-07T04:51:04.797" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5700" PostId="4236" Score="0" Text="If switching from 32 bit floats to 64 bit floats makes the errors rarer, that's an strong indication that you're running into rounding problems - I haven't done much computational geometry in a while, but my recollection was that exact results were often critical." CreationDate="2016-11-07T06:18:25.163" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="5701" PostId="4236" Score="0" Text="Just adding to @DanielMGessel 's comment, many years ago I wrote a general polygon triangulation algorithm (i.e. it handled self-intersections) and until I replaced the float representations with exact rational maths it was a nightmare trying to track down/fix bugs." CreationDate="2016-11-07T10:00:09.693" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5702" PostId="4233" Score="0" Text="You've given so many detailed answers recently that I guessed you had more you could say on this one. +1" CreationDate="2016-11-07T11:16:34.237" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5703" PostId="4240" Score="0" Text="FPGAs have some internal memory, not much. I shall give thought to your description." CreationDate="2016-11-08T08:46:48.323" UserId="5371" ContentLicense="CC BY-SA 3.0" />
  <row Id="5704" PostId="4239" Score="0" Text="Since you are not doing blending of the lines, you should be able to just write to the FB (instead of read-write). That should simplify things quite a bit I believe." CreationDate="2016-11-08T13:42:04.727" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5705" PostId="4239" Score="0" Text="what do you mean by fb?" CreationDate="2016-11-08T21:15:29.760" UserId="5371" ContentLicense="CC BY-SA 3.0" />
  <row Id="5706" PostId="4239" Score="0" Text="framebuffer...." CreationDate="2016-11-08T21:17:17.283" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5709" PostId="4220" Score="0" Text="@trichoplax Yes. All the photos were taken in the same room. I want to arrange them in their absolute location. These are spherical photos." CreationDate="2016-11-09T23:20:41.153" UserId="5412" ContentLicense="CC BY-SA 3.0" />
  <row Id="5710" PostId="4230" Score="0" Text="It sounds a little bit like ROAM, but hard to tell...&#xA;https://www.youtube.com/watch?v=PPjWW8uPp3o" CreationDate="2016-11-09T23:21:40.360" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5711" PostId="4248" Score="2" Text="Can I just suggest you look at &quot;supersampling&quot; https://en.wikipedia.org/wiki/Supersampling and perhaps also  https://en.wikipedia.org/wiki/Distributed_ray_tracing?" CreationDate="2016-11-10T08:52:01.940" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5712" PostId="4248" Score="2" Text="I can also recommend reading this chapter of PBRT http://pbrt.org/chapters/pbrt_chapter7.pdf and reading this paper http://lgdv.cs.fau.de/get/785 (which explains a different technique than the one implemented in pbrt)." CreationDate="2016-11-10T09:05:57.517" UserId="4655" ContentLicense="CC BY-SA 3.0" />
  <row Id="5713" PostId="4248" Score="1" Text="`foreach pixel : p{acc = 0; foreach subsample : s { acc+=sample_scene(s);} store(p, acc);}`" CreationDate="2016-11-10T09:36:25.710" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5714" PostId="4251" Score="0" Text="So basically I render an image to a large size and when saving it to an image, downscale it to a lower size? That seems quite simple :)! Is this the super sampling method?" CreationDate="2016-11-10T14:27:47.367" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5715" PostId="4251" Score="2" Text="@Arjan Singh yes it's https://en.wikipedia.org/wiki/Supersampling , but this is the slowest of them all , raytracing allows you to easily do adaptive supersampling, which can perform a lot better" CreationDate="2016-11-10T14:54:05.530" UserId="5364" ContentLicense="CC BY-SA 3.0" />
  <row Id="5716" PostId="4253" Score="0" Text="Great Answer! What would be the benefits of using this method opposed to the method @Raxvan used? Will I get the same results by rendering to a large size and then downscaling to a smaller size?" CreationDate="2016-11-10T15:28:52.937" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5717" PostId="4253" Score="0" Text="Fundamentally, with ray tracing you just don't need to render a bigger image then scale it down. That means you have a lot more flexibility: you can have a lot of samples, you can vary the number of samples depending on the region, and simply, you don't have to add the rescale step." CreationDate="2016-11-10T15:36:18.933" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="5718" PostId="4253" Score="2" Text="On the topic of jittering, this turns out to be a fairly complex topic. Here is a great paper analyzing the state-of-the-art a few years back http://graphics.pixar.com/library/MultiJitteredSampling/paper.pdf" CreationDate="2016-11-10T15:44:10.223" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="5719" PostId="4220" Score="0" Text="If you're not sure how to edit the wording to make clear what you want, feel free to drop into [chat]." CreationDate="2016-11-10T17:16:36.807" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5720" PostId="4255" Score="0" Text="What do you mean by &quot;downscaling&quot;, in this regard?" CreationDate="2016-11-10T20:12:03.710" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="5721" PostId="4255" Score="1" Text="Maybe I used the wrong term, but I meant the following: my monitor has a native 1920x1080 resolution, the game is offering me (by default) to render 40% more pixels than my monitor can actually display, so in the end, the image will be &quot;downscaled&quot; to the actual size my monitor is actually able to display." CreationDate="2016-11-10T20:15:04.110" UserId="5458" ContentLicense="CC BY-SA 3.0" />
  <row Id="5722" PostId="4241" Score="0" Text="&quot;*since you must also handle synchronization with multiple command buffers*&quot; What synchronization, in particular, are you referring to?" CreationDate="2016-11-10T20:21:47.943" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="5723" PostId="4252" Score="0" Text="I would also mention that LDR is probably more suited to eye-side ray-tracing as you don't need to accumulate a lot of small parts that often." CreationDate="2016-11-11T01:53:34.100" UserId="5462" ContentLicense="CC BY-SA 3.0" />
  <row Id="5724" PostId="4241" Score="0" Text="I edited the question to be more specific. Basically, there are two types of synchronization that I'm concerned about, one with the framebuffers, and one with command buffer submission/frequency." CreationDate="2016-11-11T02:50:51.243" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5725" PostId="4257" Score="0" Text="I really dont think this is what he's talking about. What he is saying is that the default resolution of the game is higher than his monitor's actual resolution." CreationDate="2016-11-11T06:07:54.683" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5726" PostId="4259" Score="1" Text="Physically Based Rendering: From Theory to Implementation has loads of code in it. It explains a lot line by line. You also have the pbrt implementation on Github too. It's a great book." CreationDate="2016-11-11T08:58:18.880" UserId="3331" ContentLicense="CC BY-SA 3.0" />
  <row Id="5728" PostId="4259" Score="0" Text="I've only been able to read sample chapters but based on what you've said I might buy this book." CreationDate="2016-11-11T10:35:52.217" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5729" PostId="4259" Score="0" Text="PBRT is written with a _[literate programming](https://en.wikipedia.org/wiki/Literate_programming)_ approach, which means that the book not only explains the whole theory but also the entire implementation almost line by line.&#xA;&#xA;&#xA;It is an amazing book and well worth a read if you're interested in the topic. Make sure to get the newly released 3rd edition ;)" CreationDate="2016-11-11T11:08:33.657" UserId="1930" ContentLicense="CC BY-SA 3.0" />
  <row Id="5730" PostId="4257" Score="1" Text="Yes, and that is precisely how supersampling is implemented. You render to a higher resolution render target and then downscale to the screen size, and capture sub-pixel detail in the process." CreationDate="2016-11-11T11:42:05.843" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5732" PostId="4257" Score="0" Text="Can you provide evidence that any game actually consciously makes the choice to have a larger than native resolution as a way of doing anti aliasing? The heuristics for auto detection are already complex enough without this added complexity, I'd be really surprised." CreationDate="2016-11-11T13:23:48.050" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5733" PostId="4259" Score="0" Text="@tizian As a beginner who has never written a single ray tracer before should I buy it? I plan to do the scratchapixel tutorials online but once I'm done with that should I buy the book?" CreationDate="2016-11-11T13:30:00.680" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5734" PostId="4260" Score="1" Text="This book is very great. It's written in a very pleasant way and doesn't scare with advanced maths very early on." CreationDate="2016-11-11T13:56:27.457" UserId="13" ContentLicense="CC BY-SA 3.0" />
  <row Id="5735" PostId="4257" Score="1" Text="Here you go, Battlefield 1: https://www.reddit.com/r/battlefield_one/comments/50b4m1/psa_42_resolution_scale_is_your_native_res/&#xA;&#xA;As for the &quot;heuristics&quot;, you'd be suprised. There are practically none. No game I've ever shipped had anything more complex than a simplistic, synthetic benchmark on the first run, and/or custom configurations per device IDs determined via trial-and-error QA testing on a wide range of hardware." CreationDate="2016-11-11T14:56:45.320" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5736" PostId="4259" Score="0" Text="@ArjanSingh PBRT might be better suited if you already have some prior knowledge about ray tracing and want to make the jump to state-of-the-art physically-based rendering.&#xA;&#xA;I heard many great things about Peter Shirley's &quot;minibook&quot; series mentioned in another answer here already; this might be a good place to start instead." CreationDate="2016-11-11T15:10:21.233" UserId="1930" ContentLicense="CC BY-SA 3.0" />
  <row Id="5737" PostId="4257" Score="2" Text="that trial and error is what i was talking about, but i guess this doesn't add complexity in that case since it doesnt really add extra testing.  Interesting stuff.  I was definitely wrong about what I said!" CreationDate="2016-11-11T15:13:23.833" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5740" PostId="4259" Score="1" Text="@ArjanSingh although book recommendation questions are off topic here on main, you could ask about it in [chat]." CreationDate="2016-11-12T09:40:24.407" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5741" PostId="4233" Score="0" Text="&quot;With your current adaptive tessellation strategy you'll end up splitting flat surfaces, which unnecessarily increases your triangle count.&quot; - It´s exactly the opposite. I guess you (and many others) don´t know what a polar coordinate system is and how it warps cartesian 3D-models depending on its projected location. I do agree with you that the threshold can be more elaborate than length on display but deriving it becomes difficult. And besides my target GPU does not have tesselation capability. I´ll update my answer with more info on later occation." CreationDate="2016-11-12T10:01:46.587" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5742" PostId="4230" Score="0" Text="@NathanReed Added further explanations and pictures." CreationDate="2016-11-12T10:55:49.923" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5744" PostId="4230" Score="0" Text="@AlanWolfe I´d say the algorithm is in the same ball park with a few differences. Good find :-)" CreationDate="2016-11-12T11:02:34.960" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5745" PostId="4264" Score="0" Text="First of all, thanks for answering. Actually I tried to interpolate with the background color, but for some reason the results didn't seem to be correct. I will try again and I will make you know." CreationDate="2016-11-12T23:34:28.770" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5746" PostId="4264" Score="0" Text="See my edit in my question. Basically I think interpolation between the background color and green is being done correctly, but maybe that's not what I need to do..." CreationDate="2016-11-12T23:53:30.073" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5747" PostId="4264" Score="0" Text="@nbro Based on your edit, you're not doing interpolation with the background color correctly. You can't use `I = (int)255*I0` and add/subtract `I` from the colors in that way; it doesn't work when the initial color values are anything other than 0 or 255. Sorry if I wasn't clear enough about that. You need to actually implement the interpolation formula, which would involve scaling the colors by `I0` and `1-I0` and then adding them together." CreationDate="2016-11-13T00:08:35.863" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5748" PostId="4264" Score="0" Text="Now I'm doing something like `(1 - I0) * background_color + I0 * green` and it seems to interpolate correctly between green and the background color, but it isn't smooth as it should be (second picture in my question). Also for some reason the line does not do the same movement as before, i.e. it doesn't enter the sphere anymore, but moves always in front of it.. see my edit that follows to see the output." CreationDate="2016-11-13T00:34:12.130" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5749" PostId="4264" Score="0" Text="@nbro Looks to me like you have the interpolation weights backward—try swapping the `I0` and `1 - I0`. About the occlusion: do you still have the z-buffer check? In your original code you had `if ( (zBuffer[x+y*w] &lt; 0) || z &lt; zBuffer[x+y*w] )` which looks like should be responsible for hiding the sections of the line that are behind the sphere." CreationDate="2016-11-13T00:39:05.127" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5750" PostId="4264" Score="0" Text="Actually I also swaped `y` and `nextY` (when setting the pixels) and now the output is more smooth (the problem of **not** entering the sphere still persists)..." CreationDate="2016-11-13T00:39:49.020" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5751" PostId="4264" Score="0" Text="I still have the `z-buffer` check as before... I heard about the inverse of `z` which can be used (somewhere), but I didn't understand why and how.." CreationDate="2016-11-13T00:44:28.040" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5752" PostId="4264" Score="0" Text="For some reason the occlusion problem was solved...thanks a lot for all the help and time!" CreationDate="2016-11-13T01:29:45.770" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5753" PostId="4265" Score="1" Text="Is this rasterization happening on CPU or GPU? Also I'm curious if there's a reason you need it to be faster than polygons as it might help shape an answer. Thanks!" CreationDate="2016-11-13T15:56:04.997" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5754" PostId="4265" Score="0" Text="GPU, OpenCL. Also why shouldn't it be faster than polygons? Why should I be looking for an algorithm, that's slower than polygons? :)" CreationDate="2016-11-13T17:13:38.957" UserId="5476" ContentLicense="CC BY-SA 3.0" />
  <row Id="5755" PostId="4265" Score="2" Text="Faster vs just using polygons hehe.  As in... Do you have an actual usage case where you need an algorithm faster than triangles? If so, what are the details of that usage case. If it's just curiosity though, that's fine too." CreationDate="2016-11-13T17:22:36.353" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5756" PostId="4265" Score="0" Text="Im trying to rewrite my voxel octree raycaster to a work without raycasting..if you know, what i mean. I dont want to use polygons since it seems stupid, rendering 12 individual triangles for the sake of rendering a cube, that by itself is only defined by 4 numbers and has itself some limitation of course, like rotation being impossible to be applied directly to the cube, which on the other hand is something i dont need for my renderer." CreationDate="2016-11-13T17:44:16.733" UserId="5476" ContentLicense="CC BY-SA 3.0" />
  <row Id="5759" PostId="4271" Score="0" Text="That's quite a clarified explanation for that somewhat tricky problem." CreationDate="2016-11-14T17:44:24.810" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5760" PostId="4270" Score="0" Text="I'm looking for the inverse of that. but cant do it that way.. need to do it as an affine transform. I'm setting up the coordinate system apriori with translate() and scale commands applied to the http://docs.oracle.com/javafx/2/api/javafx/scene/canvas/GraphicsContext.html associated with the Canvas object which takes up the whole window and whose coordinate (0,0) is the upper-left hand of the window... currently im using something like this&#xA;&#xA;&gt;     gc.restore();&#xA;&gt;     gc.translate( 0, height / 2 );&#xA;&gt;     gc.scale( width / xrange, height / yrange )&#xA;&gt;     gc.translate( -minx, 0 );" CreationDate="2016-11-15T05:13:56.523" UserId="5478" ContentLicense="CC BY-SA 3.0" />
  <row Id="5762" PostId="4272" Score="0" Text="i may be seriously oversimplifying this but what difference would mapping it as a 2d shape make? with your horizontal cylinder flattened, could the hight (or width with a vertical cylinder?) be exactly the same as the degree? do you need a specific unit of measurement i.e. mm? in which case you need the circumference of the cylinder (or any measure from which the circumference can be calculated) which would then be the hight once flattened. do these rectangles move or grow from the initial position? i am not sure what you are asking for." CreationDate="2016-11-14T23:21:19.023" UserId="5486" ContentLicense="CC BY-SA 3.0" />
  <row Id="5763" PostId="4272" Score="0" Text="@Ryan I need to map the  rectangle on the pipe surface to  2D plane" CreationDate="2016-11-15T11:48:13.640" UserId="5482" ContentLicense="CC BY-SA 3.0" />
  <row Id="5764" PostId="4270" Score="0" Text="Usually you don't want the inverse to map each pixel on the screen exactly once to your function. Here's similar question to yours: http://computergraphics.stackexchange.com/questions/4193/what-is-the-correct-order-of-transformations-scale-rotate-and-translate-and-why/4194#4194&#xA;Would have voted for close, but this is a more generic answer." CreationDate="2016-11-15T13:30:43.350" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5765" PostId="4207" Score="0" Text="It was also part of the problem so I'll accept this answer." CreationDate="2016-11-15T17:25:51.340" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5766" PostId="4270" Score="0" Text="Thank you. I'm actually doing the opposite of that, I'm looping thru each pixel x,y and mapping that to precisely one point in the complex plane to evaluate something like the Mandelbrot set. Also, the mouse click handler returns point clicked in pixels, and I need to map that onto the complex plane as well. I realize each &quot;pixel&quot; is actually a rectangle on the complex plane but the resolution is small enough that just sampling the center of this rectangle is fine for display purposes." CreationDate="2016-11-15T22:04:06.230" UserId="5478" ContentLicense="CC BY-SA 3.0" />
  <row Id="5767" PostId="4277" Score="0" Text="I can't give you a deeper answer unfortunately (someone else surely will!) but the 2 comes from integrating the BRDF over the hemisphere.&#xA;You didn't ask, but you may also be wondering where the $\pi$ went.  It's assumed that your color values already include the adjustment for $\pi$.  You can read more about that here: https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/" CreationDate="2016-11-15T22:38:57.493" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5768" PostId="4277" Score="1" Text="Thanks but why is the BRDF being integrated over the hemisphere at this stage of the algorithm when we're dealing with a single incident direction?" CreationDate="2016-11-15T22:43:12.273" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5769" PostId="4278" Score="0" Text="Ok, thanks for trying to clarify all of these! But I would to know in particular why they are giving us the points in global coordinates and how does it affect my calculations of the pixels that are set." CreationDate="2016-11-16T00:10:51.863" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5770" PostId="4278" Score="1" Text="I've expanded my explanation to show how it relates to your problem. I suspect there are parts you might not be required to do yet, as the problem doesn't specify a view matrix (or camera position and orientation). So it might not be a 1:1 mapping of your problem, but hopefully will get you going in the right direction." CreationDate="2016-11-16T00:56:00.087" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5771" PostId="4265" Score="0" Text="it feels like maybe you could translate the 3d points and project them into 2d (think: x/z, y/z types of transforms), and then you could use some &quot;convex hull&quot; finding algorithm on the points to come up with a 2d poly, instead of starting with triangles.  The fact that it's symmetric (since it's a cube) likely means you can do less work than transforming all of it's 8 points, or even rasterizing the whole thing.  You can likely rasterize half or a quarter, and mirror it or something." CreationDate="2016-11-16T01:07:35.150" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5772" PostId="3978" Score="1" Text="+1 for giving me a simple/understandable introduction to importance sampling." CreationDate="2016-11-16T09:37:45.923" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5773" PostId="1687" Score="0" Text="Requests for software recommendations are off topic here, but you may be interested in [softwarerecs.se]." CreationDate="2016-11-16T14:38:49.280" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5774" PostId="4280" Score="0" Text="Nice one, thanks. Trying to connect this to what my book has for MC estimator for the light going to the viewer: $\left \langle L_r(x\rightarrow\Theta) \right \rangle = \frac{1}{N}\sum_{1}^N\frac{L(x\leftarrow\Psi_i)f_r(x,\Theta\leftrightarrow \Psi_i)\cos(\Psi_i,N_x)}{p(\Psi_i)}$. The division is by the pdf which for non-cosine-weighted hemisphere sampling is $p(\Psi_i) = \frac{1}{2\pi}$. Dividing by a fraction is multiplication by reciprocal, and constant coefficient $2\pi$ gets pulled out from the sum. Does that make sense?" CreationDate="2016-11-16T14:53:17.753" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5775" PostId="4280" Score="0" Text="Although I've now confused myself again because that Wikipedia article says &quot;in the naive case above, there is no particular sampling scheme, so the PDF turns out to be 1&quot;. But isn't the pdf $\frac{1}{2\pi}$?" CreationDate="2016-11-16T15:24:45.793" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="5776" PostId="4282" Score="2" Text="Can you clarify what the problems with floating-point precision are? I don't see anything in this method that would cause precision problems, unless the scale is really extreme. Also, worth noting that this method may fail if the matrix was composed from a sequence of matrices that includes both non-uniform scales and rotations. The $\mathbf{R}$ matrix will turn out not to be a rotation in that case, but will include some shear." CreationDate="2016-11-16T16:11:22.850" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5777" PostId="4280" Score="1" Text="Yes, pdf is $\frac{1}{2\pi}$ for uniform sampling over the hemisphere." CreationDate="2016-11-16T18:05:27.777" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5778" PostId="4282" Score="2" Text="All floating point numbers have intrinsic (bounded) error. Any time you perform operations, and particularly addition or subtraction, you compound the error, increasing the magnitude of the bounds.  Hidden in the decomposition algorithm are many addition operations (both in the matrix multiplication and the scale magnitude calculation) and a square root (in the scale).  Further decomposition will introduce further error." CreationDate="2016-11-16T19:14:17.193" UserId="5498" ContentLicense="CC BY-SA 3.0" />
  <row Id="5779" PostId="4282" Score="1" Text="@Timbo There isn't any full matrix multiplication here though, just multiplying the columns of the matrix by the inverse scales. And a vector magnitude involves adding all positive quantities, so there's no catastrophic cancellation there; it doesn't produce much relative error, AFAICT. Anyway, the author clarified that they're talking about further decomposing the rotation matrix into Euler angles or suchlike, which makes more sense." CreationDate="2016-11-16T20:31:34.843" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5780" PostId="4283" Score="0" Text="Only the yaw is inverted, the pitch works as expected." CreationDate="2016-11-16T20:41:15.820" UserId="5488" ContentLicense="CC BY-SA 3.0" />
  <row Id="5781" PostId="4283" Score="0" Text="Are you saying that the rotation doesn't happen to the counterclockwise direction with increasing angles in your coordinate system?" CreationDate="2016-11-16T20:55:11.100" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5782" PostId="4283" Score="0" Text="It is counterclockwise on the Y axis (the yaw), i.e. increasing the angle (or looking right) causes the fixed object the camera is looking at to go right, but clockwise on the X axis (the pitch), increasing the angle (looking up) causes the object to go down as expected." CreationDate="2016-11-16T21:09:44.730" UserId="5488" ContentLicense="CC BY-SA 3.0" />
  <row Id="5783" PostId="4239" Score="0" Text="It sounds like that you are saying you read a 1 byte value from the frame buffer, and that part is slow / cumbersome?  If so, why are you doing the read from frame buffer, instead of a &quot;blind write&quot;? If that makes sense.  If that is what's going on, a similar thing happens in real GPUs when doing alpha blending.  Alpha blending needs a read of the pixel value already there to mix with the new pixel value, so is slower than a non alpha write, which is just a write, without a read." CreationDate="2016-11-16T22:34:25.883" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5784" PostId="4283" Score="0" Text="What's the coordinate system you are using (x=right, etc)?" CreationDate="2016-11-17T22:18:57.007" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5785" PostId="4283" Score="0" Text="Right handed system with +x is right, +y is up" CreationDate="2016-11-17T22:45:38.080" UserId="5488" ContentLicense="CC BY-SA 3.0" />
  <row Id="5786" PostId="4283" Score="0" Text="Ok, so in that case both rotate to the same direction about their respective rotation axes (x- and y-axis for pitch &amp; yaw respectively) and you get the behaviour you defined, as expected. If YOU expect different rotation direction, then you just have to transpose the matrix like you said, or negate the angle." CreationDate="2016-11-17T23:28:39.883" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5787" PostId="1721" Score="0" Text="@teodron Might you have any insights on mean curvature for border vertices? Can such a thing be defined?" CreationDate="2016-11-18T12:00:45.223" UserId="3294" ContentLicense="CC BY-SA 3.0" />
  <row Id="5788" PostId="4284" Score="0" Text="Not really, actually. I we have already treated global and local coordinates systems, but my question was more related to the fact to why, for example, as you're saying, would I need to convert the coordinates to global coordinates, if they were given as local coordinates. I mean, what's the relevance of the coordinate system in this case/problem given in my question." CreationDate="2016-11-18T12:35:10.697" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5789" PostId="1721" Score="0" Text="@Museful I am a bit worried about the mean curvature not being negative, no matter the surface type. If the Laplacian-like operator is defined on a border vertex, then it's a matter of evaluating the same expression including only the triangles that make up the faces of the surface incident at $v_i$. There are more recent papers on discrete curvatures, however.." CreationDate="2016-11-18T12:50:42.583" UserId="3444" ContentLicense="CC BY-SA 3.0" />
  <row Id="5790" PostId="4284" Score="0" Text="You need to convert from local to global coordinate system to define objects position and orientation in the world. In local coordinate system objects are defined relatively to their pivot point (e.g. object center), so that you can move and rotate the object about that point in the world, by defining the local$\rightarrow$global transformation. The &quot;global coordinates&quot; is there to disambiguate the coordinate system in question and that you don't need to worry about this transformation. What's confusing though is that there's no disambiguation of the camera coordinate system." CreationDate="2016-11-18T14:21:28.443" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5791" PostId="4285" Score="1" Text="A transform like this is not a linear transform!" CreationDate="2016-11-18T15:13:30.600" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5792" PostId="4283" Score="0" Text="Negating the yaw won't make the matrix consistent with the definition of the view matrix (i.e. the inverse of the camera world matrix). It would mean that `Ry Rpᵀ T⁻¹` while it should have been `Ryᵀ Rpᵀ T⁻¹`." CreationDate="2016-11-18T15:52:35.830" UserId="5488" ContentLicense="CC BY-SA 3.0" />
  <row Id="5793" PostId="4283" Score="0" Text="If you negate the yaw angle, you don't need to transpose the yaw matrix. Personally I think it's better to negate the angle." CreationDate="2016-11-18T16:02:54.773" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5795" PostId="4287" Score="0" Text="If I define the transformation as a function of height, how can I then express it as a matrix ? or does this mean that I cannot keep it as the traditional matrix form and there should be input inside my rotation matrix? and if so, can I use such matrix in OpenGL to achieve the same results?&#xA;Thank you" CreationDate="2016-11-18T17:45:23.680" UserId="5495" ContentLicense="CC BY-SA 3.0" />
  <row Id="5796" PostId="4287" Score="2" Text="You can't express it as a constant matrix, because that's able to do only linear transformations (rotate, translate, skew, scale). Instead you would have to create the matrix for every vertex you transform based on the y-coordinate of the vertex." CreationDate="2016-11-18T18:12:43.130" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5799" PostId="4293" Score="1" Text="Can you link to the slides? It may help to answer the question better if we can put it in context." CreationDate="2016-11-19T16:43:50.810" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5800" PostId="4293" Score="0" Text="@NathanReed Probably not... but if something is unclear, I can try to give you more information.." CreationDate="2016-11-19T16:51:46.587" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5801" PostId="4293" Score="1" Text="Well, I'm not sure we can really answer the question as it's not clear from the slide how &quot;screen coordinates&quot; are defined relative to &quot;projected points&quot;. There are various different coordinate systems that might be used; I don't know how the slide author has chosen to set things up." CreationDate="2016-11-19T16:54:05.323" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5802" PostId="4293" Score="0" Text="@NathanReed Indeed the slides are very simplified and in my opinion far from being easily understandable.. I can tell you we're in the context of the _z-buffer_ and _rasterization_.. We have treated global and local coordinate systems.." CreationDate="2016-11-19T16:59:51.577" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="5804" PostId="4291" Score="2" Text="Normals are not transformed with the same transformation matrix as positions. You need to calculate inverse of the transpose of the 3x3 sub-matrix to properly transform normals for transformations with non-uniform scaling and/or skewing." CreationDate="2016-11-19T21:37:13.017" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5805" PostId="4291" Score="0" Text="@JarkkoL yeah that is true, you are right with that. It is best to not use the same matrix, but depending on the implementation, it is done. Most of the times people do not care about the skewing of the normals that much, because they either do not use non-uniform scaling or scaling at all. That part about transforming positions and normals was more about that it could be useful to use one container." CreationDate="2016-11-19T21:54:07.537" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="5806" PostId="4288" Score="2" Text="This doesn't have anything to do with your question but you seem like you probably would be interested to hear about it.  You should check out cosine weighted sampling if you haven't heard of it.  Instead of multiplying by cos theta, you make your rays be distributed based on cos theta. The easiest way to do this is choose random point on a disk (x/z), and then make y such that it's a normalized vector. You end up sampling more meaningful directions and get faster convergence." CreationDate="2016-11-19T22:08:04.640" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5807" PostId="4292" Score="0" Text="This answer would be more complete if you noted that, in actual [homogeneous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates), $(wx, wy, wz, w)$ for any $w \ne 0$ is also a valid representation of the point $(x, y, z)$. When converting from ordinary 3D coordinates to 4D projective coordinates, it's convenient to choose $w = 1$, but allowing other values of $w$ in the inverse conversion lets us represent [perspective transformations](https://en.wikipedia.org/wiki/Perspective_transform) using 4D matrix multiplication, too." CreationDate="2016-11-20T07:24:49.697" UserId="525" ContentLicense="CC BY-SA 3.0" />
  <row Id="5808" PostId="4286" Score="0" Text="@trichoplax yes but my reputation was not high enough to put more than 2 link so I changed it and then forgot to change my sentence sorry =/ I will fix it" CreationDate="2016-11-20T17:50:45.323" UserId="5506" ContentLicense="CC BY-SA 3.0" />
  <row Id="5810" PostId="4289" Score="0" Text="My CPU load reach rarely more than 30% when I'm bellow 20FPS so I am GPU bound. For my glMultiDraw* problem for now I use matrices to move my objects (the UpdateTransformUniform() call) is this possible to link glMultiDrawElements to a buffer of uniform or something like this?  For my buffers, my final buffers are 4 1920x1080 (I'm using deffered rendering) and my bounces are 4 64x64 (will be replaced to 1 64x64 soon). For the algorythm: should I loop over my bounces in every mesh using instances rendering or loop glMultiDraw my mesh for every bounces." CreationDate="2016-11-20T18:36:59.713" UserId="5506" ContentLicense="CC BY-SA 3.0" />
  <row Id="5813" PostId="4289" Score="0" Text="Actually, I think this is what you would want if you're using uniform transformation matrices: glDrawElementsInstancedBaseVertexBaseInstance (OpenGL 4.2). Here's more info about it: http://www.gamedev.net/topic/682969-minimizing-draw-calls-and-passing-transformmaterial-data-to-a-shader-in-opengl-3x/" CreationDate="2016-11-21T05:31:32.107" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5814" PostId="4289" Score="0" Text="I ask a similar question to the once you are asking here: http://computergraphics.stackexchange.com/questions/4241/instanced-stereo-rendering-vs-multiple-command-buffers" CreationDate="2016-11-21T05:41:44.900" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5815" PostId="4289" Score="0" Text="Either way, it's not going to make a huge difference if your performance limited by the GPU. With a 64x64 grid, you are going to be doing a lot of overdrawing, even for a deferred renderer, so you should start optimizing there. But in terms of your followup question, I would go through that slideshow I linked and see if that can help you in your situation (extending it to a grid rather than two screens)." CreationDate="2016-11-21T05:42:22.317" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5816" PostId="4288" Score="0" Text="It's very difficult to understand what you try to say. Can you clarify your question? Are you talking about progressive path tracing and importance sampling?" CreationDate="2016-11-21T14:44:38.000" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5821" PostId="4194" Score="0" Text="Hi, did you derive that equation?" CreationDate="2016-11-22T14:11:39.677" UserId="5371" ContentLicense="CC BY-SA 3.0" />
  <row Id="5823" PostId="4286" Score="0" Text="I (finally) fixed it" CreationDate="2016-11-22T17:47:01.777" UserId="5506" ContentLicense="CC BY-SA 3.0" />
  <row Id="5839" PostId="4295" Score="0" Text="Normals don't work because they're not vectors.  Don't know of a good intro to the concept though." CreationDate="2016-11-24T13:14:12.800" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="5840" PostId="4309" Score="0" Text="Thank you for your answer and your interesting insights. Ropes++ is just way to traverse a kd tree without having to maintain a stack, so I have not paid attention to ray coherency so far." CreationDate="2016-11-24T14:48:18.440" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="5841" PostId="4309" Score="0" Text="Yeah, once you try to increase the ray coherency, it will likely mess up any earlier GPR optimizations you have done. I prefer to defer GPR optimizations to a later stage, though you still need to be aware of it earlier (e.g. to avoid algorithms with deep nested loops that tend to increase the pressure, etc.)" CreationDate="2016-11-24T14:56:36.483" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5842" PostId="4289" Score="0" Text="Okay I'm almost good, last question: it looks like the stereo instanced rendering seems to use 1 render target split in 2 but I have multiple FBO. I cannot find a way to use instances across multiple FBO, is this possible, if yes how ?" CreationDate="2016-11-25T20:09:15.917" UserId="5506" ContentLicense="CC BY-SA 3.0" />
  <row Id="5844" PostId="4315" Score="0" Text="When i copy/paste that into shadertoy I don't get anything like the output you are seeing, and just have a small green diamond.  Did you post the right code?" CreationDate="2016-11-26T22:04:13.613" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5845" PostId="4315" Score="0" Text="Correct the shader is just a triangle. It should be glowing. Those are lines I marked in to express where I am seeing edges in the gradient. They are not apart of the shader code." CreationDate="2016-11-26T22:31:22.193" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5846" PostId="4316" Score="0" Text="Just updated the code so you can see what I see now" CreationDate="2016-11-27T03:04:38.090" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="5847" PostId="4316" Score="0" Text="Ah ok. The negative was a red herring.  Answer updated." CreationDate="2016-11-27T05:26:56.953" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5848" PostId="4320" Score="4" Text="It's an extremely high-resolution normal map, with many texels per screen pixel, and the paper introduces a special technique to calculate highlights from all normals in a screen pixel at once. Can you be more specific about what you understand already and where you're getting lost? It's hard to help otherwise." CreationDate="2016-11-27T16:01:01.193" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5852" PostId="4289" Score="0" Text="Take this with a grain of salt because I haven't implemented it before (and you should probably post a separate question about this specific technique as I'm sure some have and can give better advice). From my understanding, the idea is to scale each instance's NDCs down depending on the side you are rendering and then set the clip planes in a shader. This should be able to be generalized to your approach (instead of two sides, have a grid). You need to pass a region number (side) to each instance." CreationDate="2016-11-27T22:35:53.753" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5853" PostId="4272" Score="1" Text="I think it's not clear *which* surface. Can this be imagined as &quot;projecting&quot; the given rectangles on the (3D!) pipe, then &quot;cutting open&quot; the pipe and &quot;unwrapping&quot; it, to obtain the 2D pipe surface? (And: Why is the width of the rectangle given in degrees?)" CreationDate="2016-11-27T23:41:05.467" UserId="1649" ContentLicense="CC BY-SA 3.0" />
  <row Id="5855" PostId="4321" Score="4" Text="Have you taken a look at [this](https://en.wikipedia.org/wiki/Color_difference)? RGB isn't a great color space for doing comparisons related to human perception." CreationDate="2016-11-28T02:37:02.827" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="5856" PostId="4321" Score="0" Text="Good info thanks! I was looking at cielab but that article says that isn't the best.  I'm doing work with RGB source data unfortunately so have to figure out how to convert from RGB to something better, but the challenge seems to be that RGB is device dependant, while eg cielab are not.  Fortunately, a lesser approximation is good enough for my needs, if device independence isn't really feasible with RGB source data." CreationDate="2016-11-28T02:48:28.220" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5857" PostId="4320" Score="1" Text="That explanation was quite helpful, where I am getting lost is the part about 4D Gaussians &amp; Position Normal Distributions. What are they &amp; what do they do? In the paper they show the benefits of their method as opposed to just a simple normal map." CreationDate="2016-11-28T05:13:50.033" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5858" PostId="4321" Score="0" Text="Check out Bruce Lindbloom's site, especiall the various DeltaE metrics: http://www.brucelindbloom.com/" CreationDate="2016-11-28T12:53:42.593" UserId="385" ContentLicense="CC BY-SA 3.0" />
  <row Id="5859" PostId="4326" Score="3" Text="Welcome to Computer Graphics SE! In general, link-only answers are strongly discouraged on SE, because they might become useless should those links ever go down. Please include a short summary of their content, so that people can still figure out what exactly you're actually suggesting without having to rely on the links." CreationDate="2016-11-28T13:15:20.570" UserId="16" ContentLicense="CC BY-SA 3.0" />
  <row Id="5860" PostId="4282" Score="0" Text="Thanks – great answer. Follow-up: to get the original matrix back, I am assuming we need to follow a certain order of operations, starting from the identity matrix. Would this order be TRS?" CreationDate="2016-11-28T15:45:37.537" UserId="5493" ContentLicense="CC BY-SA 3.0" />
  <row Id="5861" PostId="4272" Score="0" Text="@Marco13 Yeah. rectangles are on a 3 D pipe" CreationDate="2016-11-28T20:46:02.093" UserId="5482" ContentLicense="CC BY-SA 3.0" />
  <row Id="5862" PostId="4272" Score="0" Text="I would guess the width is in degrees just to give the same unit of measurement as height.  I'm betting you calculate it as if it were the height, then use that value for the width." CreationDate="2016-11-28T23:22:52.157" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5865" PostId="4325" Score="0" Text="Once you get into this, you're going to have to come to terms with homogeneous coordinates. [Here](http://www.tomdalling.com/blog/modern-opengl/explaining-homogenous-coordinates-and-projective-geometry/) is a nice introduction to projection." CreationDate="2016-11-29T13:30:15.670" UserId="5574" ContentLicense="CC BY-SA 3.0" />
  <row Id="5867" PostId="4247" Score="1" Text="[OpenEXR](http://openexr.com/index.html) is widely used in industry, with everything you'd expect: HDR, 32-bit float and 'half' float channel support, and support for color management, which is critical when you're using profiles to make sure the colors on your monitor match up with rushes you're viewing on, say, a projector. A lot of what it offers might not be useful to you, but the software is free, and there are plenty of minimal examples." CreationDate="2016-11-29T13:47:18.257" UserId="5574" ContentLicense="CC BY-SA 3.0" />
  <row Id="5871" PostId="1983" Score="0" Text="Q&amp;As like these are always difficult for moderators. Apart from the asking for sources - which is kind of begging the question - this is a gem. I wouldn't mind seeing the 'question' protected', until there's something like a S.O.'s documentation implemented. Maybe that's not a bad place for it - under some 3D tags?" CreationDate="2016-11-29T21:57:53.133" UserId="5574" ContentLicense="CC BY-SA 3.0" />
  <row Id="5874" PostId="1816" Score="0" Text="See also http://stackoverflow.com/questions/34342038/how-to-triangulate-polygons-in-boost/40892650#40892650 for code" CreationDate="2016-11-30T15:54:23.753" UserId="5582" ContentLicense="CC BY-SA 3.0" />
  <row Id="5875" PostId="4335" Score="0" Text="It also seems like maybe the size of the spheres inside the cube correspond to the number of pixels of that color in the image. So it's a 3D histogram, basically. Is the pie wedge maybe the CIE XYZ &quot;shark fin&quot;?" CreationDate="2016-11-30T21:11:18.790" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5876" PostId="4335" Score="0" Text="@user1118321 I can't tell from the image whether the size of the circles inside the cube correspond to sphere size (with some unspecified meaning) or just to proximity to the camera (giving an idea of 3 dimensional position in the cube)." CreationDate="2016-11-30T21:32:36.887" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5877" PostId="4338" Score="0" Text="It's funny, I joined this stack site because of you :) (i read your tags after you answered my UDP question on gamedev). Cheers!" CreationDate="2016-11-30T23:34:36.953" UserId="5586" ContentLicense="CC BY-SA 3.0" />
  <row Id="5878" PostId="4338" Score="0" Text="Small world!  There's a great graphics community on twitter too.  Check out who i'm following (@Atrix256) if you want to get into that.  BTW, now I'm wondering what sort of remote control robot application needs to do OpenCL path tracing.  It sounds like you run an interesting business (:" CreationDate="2016-11-30T23:36:25.700" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5879" PostId="4338" Score="1" Text="That is just the tip of the iceberg... I am writing the path tracer to render images for training a machine learning algo to make a SLAM for robots. Project is not officially launched yet, but site is online to cath some early SEO: http://www.octomy.org/" CreationDate="2016-11-30T23:39:35.780" UserId="5586" ContentLicense="CC BY-SA 3.0" />
  <row Id="5880" PostId="4324" Score="0" Text="The only thing I can think of is to mipmap the data you are ray marching through.  Try it at a lower mip to find a collision, then try a higher mip to find a more detailed answer." CreationDate="2016-12-01T04:49:55.060" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5881" PostId="4339" Score="0" Text="Thanks for the reply joojaa! I understand that relative size of the spacing of knots is irrelevant for the NURBS curve. But I need a normalized knot span for computational purpose in undocumented legacy application. Currently I am facing issues when the knot span is not normalized. Does normalizing the knot span have no effect on the control points of the NURB surface/curve? If so, I can simply normalize the knot span between 0 and 1 using standard normalizing formula without worrying about its effects on control point list. I wish to know if it is safe to do so." CreationDate="2016-12-01T09:18:12.977" UserId="5584" ContentLicense="CC BY-SA 3.0" />
  <row Id="5882" PostId="4339" Score="0" Text="@Vaibhav as long as you scale and offset by a uniform amount yes sure you can normalize thats what example number 2 does." CreationDate="2016-12-01T12:36:15.947" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5883" PostId="4337" Score="1" Text="Vertex attribute interpolation is quite easy, you can simply use each component as the weight for the sum of attributes. I.e. for vertex colours C1, C2 and C3 and barycentric coordinate B, fragment color is F = C1 * B.x + C2 * B.y + C3 * B.z. That is, as long as you either compute the barycentrics in world space, or you perspective-correct them: https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/perspective-correct-interpolation-vertex-attributes" CreationDate="2016-12-01T13:11:46.313" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5885" PostId="4335" Score="2" Text="The white spheres seem to be the quantized colors in the 3D histogram. There are total of 24 of them in the image, i.e. the number of quantized colors." CreationDate="2016-12-01T14:19:26.590" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="5886" PostId="4337" Score="0" Text="Why use a compute shader? You could simply render to a render target as usual, grab the data off the GPU and save it to an image file of your choice. &#xA;&#xA;While possible to do this on the compute shader, you're essentially forcing yourself to perform much of the work that the GPU already knows how to do. Unless you really want to work with compute shaders, I'd advise using the prior technique I mentioned." CreationDate="2016-12-02T02:10:37.197" UserId="5594" ContentLicense="CC BY-SA 3.0" />
  <row Id="5887" PostId="4341" Score="0" Text="Just checking... this is ray tracing right, not path tracing?" CreationDate="2016-12-02T17:54:20.873" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5888" PostId="4341" Score="0" Text="I meant to say Path Tracing, will change the title." CreationDate="2016-12-02T18:17:52.507" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5891" PostId="4342" Score="0" Text="Great Answer this really clarified everything, although I'm going off topic a little would you know of any good resources on how to implement depth of field?" CreationDate="2016-12-03T06:08:12.217" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5892" PostId="4342" Score="1" Text="No sorry, but I am interested too. Ask another question IMO!" CreationDate="2016-12-03T06:09:40.617" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5893" PostId="4344" Score="0" Text="[Related question on depth of field](http://computergraphics.stackexchange.com/questions/66/how-is-depth-of-field-implemented) (not a duplicate as that is focused on approximations rather than path tracing, but gives some insight)." CreationDate="2016-12-03T10:26:58.263" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5894" PostId="4344" Score="1" Text="[Related question on lens model for path tracing](http://computergraphics.stackexchange.com/questions/246/how-to-build-a-decent-lens-camera-objective-model-for-path-tracing). Near to being a duplicate, so I recommend you make the question more specific by explaining what you already understand, so we can see where any confusion is being introduced." CreationDate="2016-12-03T10:36:30.130" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5896" PostId="4345" Score="2" Text="The only thing I know of that sounds remotely related to your question is this paper from SIGGRAPH 2013: https://igl.ethz.ch/projects/make-it-stand/make-it-stand-siggraph-2013-prevost-et-al.pdf" CreationDate="2016-12-03T12:49:24.687" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5898" PostId="4344" Score="0" Text="I understand how to calculate the focal point, focal distance and the secondary ray. But I'm confused on how to put this all together to create the 'blur' that is needed. I haven't found much online so I was hoping that someone here could explain that with some code." CreationDate="2016-12-03T14:28:30.350" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5899" PostId="4346" Score="0" Text="I have heard about a term 'jitter matrix' in a few DOF tutorials. I have no clue what it's for and how it comes to play in Depth of Field. Could you explain what a jitter matrix is and if I need it here?" CreationDate="2016-12-03T15:55:17.323" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5900" PostId="4253" Score="0" Text="The code sample above uses a 4 Sample MSAA, if I wanted to do 8x MSAA what would the matrix look like then? What would I need to change in the jitter matrix shown above?" CreationDate="2016-12-03T15:59:46.453" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="5901" PostId="4345" Score="0" Text="Sorry, I know, could not get the title into one word. However, I believe both are the same problem." CreationDate="2016-12-03T16:13:14.220" UserId="5603" ContentLicense="CC BY-SA 3.0" />
  <row Id="5903" PostId="4346" Score="1" Text="For DOF, you just need to be able to randomly choose points on the disc. If you just choose these uniformly randomly then the method will work. If instead you use a more advanced method of choosing random points then you may be able to get the same quality image with fewer points (speeding up the process of rendering an image). &quot;Jitter matrix&quot; is a way of choosing points with a different random distribution, and you don't need it in order to make depth of field work - it's just a refinement you could add later. It's also applicable to any other situation where you need random points." CreationDate="2016-12-03T16:31:45.933" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5904" PostId="4346" Score="1" Text="I've deliberately kept to just what is essential to make depth of field work, to avoid cluttering the answer with surplus information. You could ask a separate question if you run into any difficulties with different random distributions." CreationDate="2016-12-03T16:32:54.447" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5905" PostId="4346" Score="1" Text="Do you think you could explain the secondary ray?" CreationDate="2016-12-03T16:54:58.993" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5910" PostId="4347" Score="0" Text="I reposted a related question a while back that may also be relevant here: [Are there common materials that aren't represented well by RGB?](http://computergraphics.stackexchange.com/questions/203/are-there-common-materials-that-arent-represented-well-by-rgb)" CreationDate="2016-12-03T23:48:02.100" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5911" PostId="4345" Score="0" Text="I've edited to hopefully make the title and body match. Please revert anything which changes your intention." CreationDate="2016-12-04T00:02:28.307" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5912" PostId="4346" Score="0" Text="@AlanWolfe since the question author mentions already understanding secondary rays, I've tried to just focus on the specific aspect required. Perhaps secondary rays would make a separate question?" CreationDate="2016-12-04T00:11:31.627" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5913" PostId="4346" Score="0" Text="Yeah your question answers the body of the question but not the title. As someone asking the question as listed in the title, your answer doesn't help me. I wonder if we ought to edit the question title or something..." CreationDate="2016-12-04T00:25:54.193" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5915" PostId="4346" Score="1" Text="@AlanWolfe That's a good point. The reason the question was asked is because general descriptions of depth of field are too much information, and just that one narrow topic needed to be addressed alone. I've edited the title to try and better describe the question. The title is always going to be a short approximation, but search engines consider the body too, so it should still be found." CreationDate="2016-12-04T00:57:42.320" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5917" PostId="4349" Score="1" Text="There's bsdf which is the full sphere instead of just the positive hemisphere that you get with brdf.  That makes it include refraction instead of just reflection.  That doesn't handle subsurface scattering though.  There's also &quot;participating media&quot; to think about which is stuff like fog in the air.  Also, you may have different reflection/refraction properties for different wavelengths of light.  How much of this stuff do you care about? (:" CreationDate="2016-12-04T02:47:51.550" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5918" PostId="4347" Score="1" Text="The answers to this are probably useful too.  http://computergraphics.stackexchange.com/q/4321/56.  TL:DR - CIELAB was made to be a measurement of color for human perception.  Unfortunately different displays display things differently so you can't easily convert from RGB to this.  There have also been advancements to CIELAB over the years to make it closer to how color is actually percieved by humans. Not sure if helpful info, but I hope so!" CreationDate="2016-12-04T02:54:34.530" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5919" PostId="4349" Score="0" Text="Im just curious:)" CreationDate="2016-12-04T03:15:46.920" UserId="5586" ContentLicense="CC BY-SA 3.0" />
  <row Id="5921" PostId="4345" Score="0" Text="By using a principal axis rotation, and some shifts, I got what I wanted. Only works because of the special form of a mandibula-scan." CreationDate="2016-12-04T11:08:48.363" UserId="5603" ContentLicense="CC BY-SA 3.0" />
  <row Id="5922" PostId="4349" Score="1" Text="A BSDF (a complete scattering function, including BRDF (reflection part), BTDF (transmission part), and BSSRDF (subsurface part)) as part of the rendering equation, should be &quot;the best&quot; model, just with a couple of caveats. First, there is not one BSDF model but many, each with different tradeoffs. So it really comes down to which BSDF/BRDF you use. Second, they often assume &quot;particle optics&quot; and skip the wave properties of light. That's a drawback that means that you can't model some phenomena. (Polarization, and &quot;CD diffraction&quot;, e.g.)" CreationDate="2016-12-04T21:31:22.893" UserId="4650" ContentLicense="CC BY-SA 3.0" />
  <row Id="5923" PostId="4272" Score="0" Text="You already have a 2D parametrisation, don't you? One of the dimensions is the longitudal axis (in mm?) and the other is the circumferential axis (in degrees). The only problem I see is when you have rectangles wrapping around the 0/360-degree boundary. One workaround for that would be to duplicate each rectangle (or just the ones on the boundary) so that you get one copy on each side of the boundary. Does that help? And if you need to have a distance, and not an angle, for the circular dimension, that is readily available as l = r*a*pi/180, where r is the radius and a is the angle in degrees." CreationDate="2016-12-05T07:08:12.003" UserId="4650" ContentLicense="CC BY-SA 3.0" />
  <row Id="5924" PostId="4008" Score="0" Text="How are the cylinders points defined?" CreationDate="2016-12-05T07:43:39.707" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5925" PostId="4272" Score="0" Text="@Supernormal Please convert the comment as an answer. I could award you the bounty." CreationDate="2016-12-05T08:53:13.470" UserId="5482" ContentLicense="CC BY-SA 3.0" />
  <row Id="5926" PostId="4352" Score="0" Text="Well, I said &quot;possible&quot;, not &quot;imaginary&quot; ;-). But thanks for a good answer!" CreationDate="2016-12-06T01:28:21.200" UserId="5586" ContentLicense="CC BY-SA 3.0" />
  <row Id="5928" PostId="4363" Score="0" Text="If you want to do a rotation, R, around an arbitrary point, X, then the matrix is simply formed by&#xA; Translate(-X)*RotateAroundOrigin(R)*Translate(X)" CreationDate="2016-12-07T12:12:41.387" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5929" PostId="4363" Score="0" Text="@SimonF could you elaborate a little more on your suggestion? Like I mentioned, still in the learning process." CreationDate="2016-12-07T12:14:27.473" UserId="2485" ContentLicense="CC BY-SA 3.0" />
  <row Id="5930" PostId="4363" Score="0" Text="Sorry.... hit return by accident while typing the reply" CreationDate="2016-12-07T12:15:06.797" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5931" PostId="4363" Score="0" Text="@SimonF So this arbitrary point X in my case would be the center of my texture. How could I determine it?" CreationDate="2016-12-07T12:40:13.513" UserId="2485" ContentLicense="CC BY-SA 3.0" />
  <row Id="5932" PostId="4363" Score="0" Text="But if you are placing blades of grass, don't you know where they are being placed?" CreationDate="2016-12-07T16:23:33.583" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="5933" PostId="4363" Score="0" Text="@SimonF I do but is there a way to determine the textures width/height in world-space?" CreationDate="2016-12-07T16:25:38.527" UserId="2485" ContentLicense="CC BY-SA 3.0" />
  <row Id="5935" PostId="4358" Score="0" Text="You're idea of using one dimension at time is a pretty good one! I like it!" CreationDate="2016-12-07T17:08:09.090" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="5936" PostId="4363" Score="0" Text="I'm not sure what this is supposed to do model = glm::translate(model, texturePositions[0]); since texturePositions is filled with 0 vectors. But you are translating before rotating." CreationDate="2016-12-07T19:22:36.620" UserId="5636" ContentLicense="CC BY-SA 3.0" />
  <row Id="5937" PostId="4366" Score="0" Text="You mean the noise effect?" CreationDate="2016-12-07T19:28:47.937" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="5938" PostId="4366" Score="0" Text="@narthex I think so. How do I get rid of it?" CreationDate="2016-12-07T19:44:13.763" UserId="5640" ContentLicense="CC BY-SA 3.0" />
  <row Id="5939" PostId="4352" Score="0" Text="Is there a name for $l(\theta)$? Or do you just say el theta?" CreationDate="2016-12-08T06:50:33.383" UserId="5636" ContentLicense="CC BY-SA 3.0" />
  <row Id="5940" PostId="4363" Score="0" Text="@MatthewWoo as far as I am concerned it is filled with two vec3-objects, which are both set to (0,0,0). Am I worng?" CreationDate="2016-12-08T08:25:23.700" UserId="2485" ContentLicense="CC BY-SA 3.0" />
  <row Id="5941" PostId="4352" Score="1" Text="@MatthewWoo It's a name I came up with on the spot, it's actually the irradiance from DRDF with the extra parameter of wavelength." CreationDate="2016-12-08T09:44:42.080" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5942" PostId="4366" Score="0" Text="anything stopping you from just splitting the object?" CreationDate="2016-12-08T13:08:12.650" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="5943" PostId="4369" Score="3" Text="This is an active area of research so there are a lot of different ways to do it, but as some quick info, from what I've seen, it's commonly both a normal map for small details, as well as vertex transformation for larger details.  Gerstner waves are likely to be interesting to you, and this link probably as well:&#xA;http://http.developer.nvidia.com/GPUGems/gpugems_ch01.html" CreationDate="2016-12-08T18:46:56.440" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5944" PostId="4370" Score="0" Text="Could you mention any other requirements? Do you need the fastest / most memory efficient approach, or the simplest one to implement to get on with prototyping?" CreationDate="2016-12-08T20:02:59.513" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5945" PostId="4370" Score="2" Text="Maybe this question on stackoverflow helps: [link](http://stackoverflow.com/questions/838761/robust-algorithm-for-surface-reconstruction-from-3d-point-cloud)" CreationDate="2016-12-08T22:52:19.647" UserId="5632" ContentLicense="CC BY-SA 3.0" />
  <row Id="5946" PostId="1518" Score="1" Text="Are you sure? According to [the docs](http://docs.gl/gl4/glMemoryBarrier), `GL_TEXTURE_UPDATE_BARRIER_BIT` is used when synchronizing calls to `glTexImage`, and has nothing to do with the memory used in storage buffers. I think you meant `GL_PIXEL_BUFFER_BARRIER_BIT`?" CreationDate="2016-12-09T04:00:06.630" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="5947" PostId="4370" Score="1" Text="Have you tried using [MeshLab](http://meshlab.sourceforge.net/)? It's one of its use cases by design." CreationDate="2016-12-09T09:23:47.947" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="5950" PostId="4370" Score="1" Text="Performance doesn't matter. I think understanding the concepts and accuracy is more important. Sorry if the question is too broad, but I thought that any experiences from experts, how to dive in and which concept is worth it, would be helpful for me and potential future visitors @trichoplax" CreationDate="2016-12-09T12:12:47.310" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="5959" PostId="4375" Score="1" Text="The problem is actually different, it's not that I'm missing an intersection it's that the shading normals are creating rays that are valid for the shading normal, but not for the surface geometry normal." CreationDate="2016-12-10T18:07:07.110" UserId="5653" ContentLicense="CC BY-SA 3.0" />
  <row Id="5960" PostId="4375" Score="0" Text="You mean that some of the rays created are not on the hemisphere of the geometry normal? Like if you would create a dot product with the ray direction and the geometry normal, it ends up being negative? As far as I know this is just ignored. Don't know what kind of artifact you actually get when you have that. Could you maybe show some pictures of the artifacts?" CreationDate="2016-12-10T19:04:28.773" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="5963" PostId="4374" Score="0" Text="How often and how much are they in the opposite direction? It could be a math precision error if they are only occasionally inside, by not very much. If they are often the wrong way, it could be that your tangent and bitangent vectors or math is wrong.  If it's almost always wrong, it could be due to mixing left and right handedness maybe." CreationDate="2016-12-12T04:26:19.290" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5964" PostId="4155" Score="0" Text="I have the same issue with a PDF view when using full screen. To what I found else on the net this looks like a Sierra bug :-(" CreationDate="2016-12-12T13:03:18.890" UserId="5660" ContentLicense="CC BY-SA 3.0" />
  <row Id="5966" PostId="4377" Score="0" Text="Are you able to share your actual code (as opposed to pseudo-code)? That way we can test it rather than just reason about it. Incidentally, that's a lot of semicolons for pseudocode - it almost looks as if it would compile..." CreationDate="2016-12-12T23:11:11.320" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5967" PostId="4374" Score="0" Text="Are you able to share your code?" CreationDate="2016-12-12T23:13:09.203" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5968" PostId="4377" Score="1" Text="Thanks for your answer @trichoplax. Concerning the screenshot it might be awkward as the whole thing is wrong because as you move the cylinder, you can notice that the penumbra doesn't behaves well.&#xA;I would be glad to share the code because I am losing all my hair trying to make true PCSS... I just need to clean the code a bit :)" CreationDate="2016-12-13T00:20:37.173" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5969" PostId="4377" Score="1" Text="Will post some source soon. Hope somebody can help about that." CreationDate="2016-12-13T00:22:03.370" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5970" PostId="4378" Score="2" Text="At first glance, the edge artifacts are somewhat reminiscent of alpha-test edges on a low-res texture. This is the outline rendered from the SDF, correct? Can you show the SDF itself? If that looks right, then I'd check if the texture is set up correctly (interpolation mode etc) and look for math bugs in the final rendering shader." CreationDate="2016-12-13T01:44:54.327" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5972" PostId="4377" Score="0" Text="I meant &quot;the right seems to be correct and the left not&quot;. So, no error in the description." CreationDate="2016-12-13T09:19:24.510" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5973" PostId="4378" Score="0" Text="Well, the generated sdf looks like this (in text form): http://pastebin.com/5GY71Y15" CreationDate="2016-12-13T14:49:37.900" UserId="5661" ContentLicense="CC BY-SA 3.0" />
  <row Id="5974" PostId="4378" Score="0" Text="And the interpolated image for the sdf looks like this: http://imgur.com/G7pAlLT" CreationDate="2016-12-13T14:59:31.720" UserId="5661" ContentLicense="CC BY-SA 3.0" />
  <row Id="5975" PostId="4381" Score="1" Text="Possibly helpful link if you aren't dead set on b-spline.  http://blog.demofox.org/2015/08/09/cubic-hermite-rectangles/" CreationDate="2016-12-13T16:25:09.393" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5976" PostId="4381" Score="0" Text="@AlanWolfe Thanks for the link, that looks interesting but I am looking for a smoothing effect that is gradual, i.e., Runge's phenomenon is not acceptable" CreationDate="2016-12-13T16:35:42.360" UserId="5633" ContentLicense="CC BY-SA 3.0" />
  <row Id="5977" PostId="4380" Score="0" Text="Thank you! So these C constants are just being replaced by those integer values that to me looked like memory adresses which is wrong, but what are those integers actually representing?" CreationDate="2016-12-13T18:06:19.237" UserId="5664" ContentLicense="CC BY-SA 3.0" />
  <row Id="5978" PostId="4380" Score="0" Text="@mbl They don't represent anything, they're just arbitrarily assigned values, defined in the OpenGL spec so that applications and GL drivers can communicate with each other." CreationDate="2016-12-13T18:14:11.060" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5979" PostId="4380" Score="2" Text="@Dan, btw, what do you mean about &quot;single-element structs&quot;? Nowadays I would expect to use enums in C/C++ for this." CreationDate="2016-12-13T18:15:30.123" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="5980" PostId="4380" Score="2" Text="@NathanReed Enums are fine in C++ but in C they're implicitly convertible from integers and other enums, so they only provide the names for constants, not the type-safety. Nowadays I guess you can rely on compilers to warn you at appropriate times, even in C." CreationDate="2016-12-13T18:47:59.790" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="5981" PostId="4385" Score="1" Text="Hi Jay.  The stack exchange sites are question and answer sites.  While it can happen as a side effect, they are not meant for making connections or for having open ended conversations.  I would recomend asking a specific question about something you are having trouble understanding, and then if you get an answer, use the site's chat feature to connect with the individual.  Voting to close this question as is, since it's off topic, but please feel free to modify it into a specific question about the technique and it will be re-opened!" CreationDate="2016-12-13T22:51:10.100" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="5982" PostId="4377" Score="0" Text="I've posted a new answer to avoid making a huuuuge question." CreationDate="2016-12-14T14:07:39.877" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5984" PostId="4377" Score="1" Text="Alright, I'll move it to the question." CreationDate="2016-12-14T14:40:38.620" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5985" PostId="4377" Score="1" Text="Incidentally, it's very well written. I was ready to upvote the answer if it had a solution at the end." CreationDate="2016-12-14T14:41:49.160" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="5986" PostId="4377" Score="1" Text="@trichoplax Thanks I think that it's better to give as much informations as possible especially with this kind of topics. It also might help others.&#xA;If you know somebody that might be able to help don't hesitate ;)" CreationDate="2016-12-14T14:45:13.880" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5987" PostId="4385" Score="1" Text="Thank you I shall revise as follows. What is the reason of SHRINKAGE after Laplacian Smoothing on noisy surface meshes. But this requires someone who is expert on the techniques, right?" CreationDate="2016-12-14T18:28:08.710" UserId="5670" ContentLicense="CC BY-SA 3.0" />
  <row Id="5990" PostId="3699" Score="0" Text="I'm looking for a way to get the DISTANCE MIDPOINT for my quad and cubic bezier curves. I was using t value 0.5 and as you said I just realized it's problematic. What I'm trying to do now with the bezier calculation is moving balls at even speed. But using t value makes it so hard.. I think I need to find proper t values depending on the arc length. Any resource or a small hint for me, please?" CreationDate="2016-12-14T20:59:39.803" UserId="5674" ContentLicense="CC BY-SA 3.0" />
  <row Id="5991" PostId="3699" Score="2" Text="Unfortunately no sinple general closed form solution exists for beziers but a good resource that covers numerically doing this and more can be found [Here](https://pomax.github.io/bezierinfo/) tough i read today at work a even better resource should link to it but i havent got the link on my phone. But perhaps this is enough @Jenix" CreationDate="2016-12-14T21:06:55.993" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="5993" PostId="4380" Score="1" Text="@DanHulme: C is not a type-safe language, so that's to be expected." CreationDate="2016-12-14T21:16:31.887" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="5995" PostId="4377" Score="0" Text="Alright I think I got something by using the distance between the receiver and the blocker. Now I need to apply it correctly." CreationDate="2016-12-15T19:28:18.150" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="5996" PostId="4381" Score="1" Text="@AlanWolfe I was still stuck and I checked out the article again and it actually gave me awesome results! Thanks so much for the tip. Post an answer if you want and I will accept it" CreationDate="2016-12-15T20:17:04.853" UserId="5633" ContentLicense="CC BY-SA 3.0" />
  <row Id="5997" PostId="4381" Score="0" Text="I'm glad it helped, but I've never used the tesselator before, so am not able to make a decent answer.  If you write one up, i'd upvote it (;" CreationDate="2016-12-15T21:42:11.090" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6000" PostId="4392" Score="0" Text="Thanks, I'll try that.  Yes, I'm kinda aware that such condition exists when using this operations. But I thought that it is just `or` (addition) operation so these operation will combine result in the same value no matter the order. just like 2+3+4 == 4+3+2." CreationDate="2016-12-16T10:45:56.117" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6002" PostId="4392" Score="0" Text="In settings bits as occupied positions sense." CreationDate="2016-12-16T13:47:12.690" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6005" PostId="4395" Score="0" Text="I've have seen this other formulation for the Cook-Torrance BRDF, where the equation is multiplied by $\frac{1}{4}$ instead of $\frac{1}{\pi}$. However, in the end, the effect of this modification is very small because we would be substituting 2, present in the final equation, by 1.57 ($= \frac{\pi}{2}$). I have made a test here (just in case...), and indeed the problem persisted." CreationDate="2016-12-16T16:40:44.180" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="6006" PostId="4392" Score="5" Text="@narthex Yeah, it's a commutative operation, but that's not the issue—the problem is, the reads and writes from different threads can be interleaved. For example: thread 1 reads the initial value into a local variable, thread 2 reads the same initial value into its local variable, thread 1 writes a modified value, then thread 2 writes its own modified value that doesn't include the bits that thread 1 had set. So thread 1's contribution ends up getting lost." CreationDate="2016-12-16T18:13:22.347" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6007" PostId="4395" Score="0" Text="@Capagot A factor of $1/\pi$ is sometimes incorporated into light source intensities (by convention) and left out of BRDFs; [see also this question](http://computergraphics.stackexchange.com/questions/3946/correct-specular-term-of-the-cook-torrance-torrance-sparrow-model). But that's more common in real-time rendering than in path tracing. Also you say your Lambertian tests match Mitsuba perfectly, so it seems less likely that this is the issue...still it might be worth looking into." CreationDate="2016-12-16T18:26:21.593" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6008" PostId="4395" Score="0" Text="@Capagot I think you're missing a $\frac{1}{\pi}$ in your distribution function $D$. The paper I linked to includes that factor in the Beckmann distribution, which you use, so having $\frac{1}{4}$ in $f_r$ and $\frac{1}{\pi}$ in $D$ should do the trick." CreationDate="2016-12-16T21:21:21.497" UserId="5632" ContentLicense="CC BY-SA 3.0" />
  <row Id="6011" PostId="4395" Score="0" Text="@NathanReed I've read the article about embedding $\pi$ into the color. However, for the reason that you've mentioned, I was convinced that that was not the problem." CreationDate="2016-12-16T22:00:28.830" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="6012" PostId="4395" Score="0" Text="@wolle Exactly! Actually, I had already taken a quick look at the paper that you've mentioned, but I didn't notice that! I've just altered my implementation to account for the $\frac{1}{\pi}$ in $D$ and $\frac{1}{4}$ in $fr$, and everything now works like a charm! I will include an update to the question with the answer! Thank you!" CreationDate="2016-12-16T22:00:33.740" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="6013" PostId="4395" Score="0" Text="Since comments are not guaranteed to stay around forever, it's worth updating the answer to show both the places that needed to be changed." CreationDate="2016-12-16T22:13:29.653" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6015" PostId="4399" Score="0" Text="Thanks very much for the answers! To get the results I'm looking for, it might be appropriate to switch to shadow volumes. The codeflow link provided under &quot;interpolation&quot; is very useful - thanks!" CreationDate="2016-12-17T13:43:44.380" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="6026" PostId="4400" Score="0" Text="Thanks Nathan ! Is this just a new call or does it also include some performance boost. I mean what does this call imply compared to the D3D9 version ?" CreationDate="2016-12-19T07:13:29.160" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="6034" PostId="4400" Score="1" Text="@MaT It implies you're using D3D10+. ;) It's literally the same operation in hardware; they just redesigned the API and called it a different name." CreationDate="2016-12-19T14:51:52.943" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6056" PostId="4401" Score="0" Text="Part of [this comment](http://computergraphics.stackexchange.com/questions/4393/handling-projective-aliasing-in-shadow-mapping#comment6015_4399) is thanking you for your answer, so I just thought I'd let you know since you won't get a notification from there." CreationDate="2016-12-20T20:51:56.627" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6057" PostId="4401" Score="0" Text="Yes, I saw it but thanks." CreationDate="2016-12-20T21:19:53.050" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6060" PostId="4333" Score="0" Text="Most Color Quantization algorithms do clustering in the RGB cube (splitting the space of colors in big significant chunks). Try to search for NeuQuant, Xiaolin Wu quantization algorithms for more details." CreationDate="2016-12-21T16:02:43.457" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="6065" PostId="4410" Score="0" Text="Thanks for the answer. So this can only be fixed in the shader? Or is there something I am doing wrong with my actual light pov camera? because basically I am taking my normal cameras position and moving the light pov camera to that position every frame instead of just having a static light pov camera for the entire scene high up somewhere." CreationDate="2016-12-22T11:47:40.470" UserId="5709" ContentLicense="CC BY-SA 3.0" />
  <row Id="6066" PostId="4409" Score="1" Text="Do you have `GL_CULL_FACE` disabled when rendering to shadow map?" CreationDate="2016-12-22T15:19:37.290" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6067" PostId="4410" Score="0" Text="Shouldn't the light POV camera be in the same position as the light?" CreationDate="2016-12-22T16:00:40.433" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6068" PostId="4409" Score="0" Text="I tried with it both on and off, it only changes the shadow slightly under the models feet." CreationDate="2016-12-22T19:34:24.083" UserId="5709" ContentLicense="CC BY-SA 3.0" />
  <row Id="6070" PostId="4410" Score="3" Text="@Kachinsky what user1118321 means, I think, is that you're applying the shadow to a surface which is not lit (the light comes from behind). It's what it looks like to me anyway. You must think of the shadow map as something which alters the intensity of a specific light, not something you can just slap onto the object at the end of your shader. If there's no light, there can be no shadow." CreationDate="2016-12-23T15:41:40.463" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6071" PostId="163" Score="1" Text="I would recommend this article from Scratchapixel for an in-depth explanation: https://www.scratchapixel.com/lessons/advanced-rendering/ray-tracing-distance-fields" CreationDate="2016-12-24T12:54:46.943" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6072" PostId="4413" Score="1" Text="First step in any GL debugging problem: have you tried setting up [KHR_debug](http://renderingpipeline.com/2013/09/opengl-debugging-with-khr_debug/), and/or sprinkling your code with [`glGetError`](http://docs.gl/gl4/glGetError) calls, to see if the driver reports any issues?" CreationDate="2016-12-25T18:58:35.793" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6073" PostId="4413" Score="0" Text="Thanks for commenting. Yes i am using debug output extension and it didnt report any problem. I have also checked each iteration of vertex shader with RenderDoc. Everything is updating according to their vertex divisor settings besides texture coordinates. This is weird." CreationDate="2016-12-25T19:02:17.280" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6074" PostId="4413" Score="1" Text="Hmm, what _are_ you getting for the texture coordinates in RenderDoc? When you say they're not updating correctly, what are they doing instead?" CreationDate="2016-12-25T19:14:58.093" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6075" PostId="4413" Score="0" Text="I mean its showing texture coordinates of the first instance for all instances. I checked all the data before sending it to the gpu memory.  I will put the whole code and check it one more time later." CreationDate="2016-12-25T19:24:37.480" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6078" PostId="4415" Score="1" Text="Worth noting also that he could get the behavior he wants using a texture buffer or SSBO, and manually indexing it with `gl_InstanceID * vertex_count + gl_VertexID` in the shader." CreationDate="2016-12-25T23:40:06.227" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6079" PostId="4415" Score="0" Text="@NathanReed: Heh heh heh, no you can't. Instance arrays and `gl_InstanceID` have different interactions with [base instance rendering](https://www.khronos.org/opengl/wiki/Vertex_Rendering#Instancing). And by &quot;different interactions&quot;, I mean `gl_InstanceID` *completely ignores* the base instance. Now, if this were *Vulkan*, `gl_InstanceIndex` could do that. But if you want to do this in GL, you [have to use an extension](https://www.opengl.org/registry/specs/ARB/shader_draw_parameters.txt)... for some reason." CreationDate="2016-12-26T01:29:12.170" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6080" PostId="4417" Score="1" Text="You might want to say what platform you're on, as that will likely change the answer." CreationDate="2016-12-26T06:14:47.743" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6082" PostId="4417" Score="1" Text="There's a list on the OpenGL page... https://www.khronos.org/opengl/wiki/Debugging_Tools" CreationDate="2016-12-26T08:04:31.393" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="6083" PostId="4417" Score="0" Text="Not sure about the quality of the various tools though, generally it's not up to the standard of the DirectX tools that ship with Visual Studio. I tried gDebugger on a project last year but it didn't work with compute shaders at all..." CreationDate="2016-12-26T08:06:14.427" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="6084" PostId="4417" Score="0" Text="That link helped. Thanks. Did you try CodeXL?" CreationDate="2016-12-26T08:10:16.383" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6086" PostId="4417" Score="0" Text="Questions about debugging GLSL are on topic. Questions like [this](http://computergraphics.stackexchange.com/questions/96/how-can-i-debug-glsl-shaders) and [this](http://computergraphics.stackexchange.com/questions/23/how-can-i-debug-what-is-being-rendered-to-a-frame-buffer-object-in-opengl) are fine. However, requests for off site resources (such as a debugging tool) are off topic here. You can ask for recommendations over on Software Recommendations but please read their [on topic](http://softwarerecs.stackexchange.com/help/on-topic) page first." CreationDate="2016-12-26T11:36:28.700" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6087" PostId="4418" Score="3" Text="Fascinating question, though I don't know that we have the expertise on this site to answer it. :) I'm not aware of any displays that work on principles other than additively combining a set of primaries, which as you mentioned only gives a convex polygonal subset of the CIE space. The liquid crystal tunable filter sounds like amazing tech, but I'd guess that it's not in consumer displays just due to some set of engineering tradeoffs vs standard LCDs (like cost, brightness, field of view, switching speed, and suchlike)." CreationDate="2016-12-27T00:35:56.803" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6088" PostId="4415" Score="2" Text="OK, but &quot;no you can't&quot; is a bit too pessimistic—that's only a problem if you're using nonzero base instances to begin with; and if you are, you could also store the base instance in a uniform and account for it in the shader. :)" CreationDate="2016-12-27T00:40:04.973" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6092" PostId="4418" Score="2" Text="One issue I see with this, what content would you play through such a display if it existed? All currently existing digital media is encoded as RGB (or YCrCb, which gets converted to the same thing). So to take advantage of this new colour gamut, graphics programmers would have to completely change the way they write colour code, and CCD manufacturers would also have to create new sensors that capture the full range of colours." CreationDate="2016-12-27T09:54:27.003" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="6093" PostId="4420" Score="1" Text="Thank you for your answer, but don't LCD color filters filter light out too? In fact, the larger the color range a display has, the more narrow the transmittance spectrum of the filter will be, meaning that there will be large losses even for white. With tunable color filters, though, there would be large losses only for saturated colors (which matches what objects do in reality; saturated objects absorb more of the illuminant), but pure white would be created by letting all wavelengths pass through (a &lt; 385 nm, b, c &gt; 745 nm), meaning less losses than LCDs." CreationDate="2016-12-27T13:30:59.843" UserId="5724" ContentLicense="CC BY-SA 3.0" />
  <row Id="6094" PostId="4420" Score="0" Text="Also, if we pick a backlight whose spectrum is the same as the LEDs that light up the room, we could produce all the colors that can possibly be found in objects of that room. I do agree that minaturization would be difficult (although it would probably be easier than other recent display technologies such as OLEDs since Lyot filters use liquid crystals to be tuned)." CreationDate="2016-12-27T13:41:43.123" UserId="5724" ContentLicense="CC BY-SA 3.0" />
  <row Id="6095" PostId="4420" Score="0" Text="@d9584 yes but the LCD has the luxury of having only 3 bands to contend to so it can survive with a light that has less band than full spectrum and it can pass things in wider band." CreationDate="2016-12-27T14:04:51.913" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6096" PostId="4418" Score="1" Text="@russ There's not much content for HDR displays yet either, and they are slowly being adapted. I think that if such a display produced images that look significantly better than what regular RGB displays can do, it would have a chance to eventually see widespread use. That's another big if, though." CreationDate="2016-12-27T14:34:29.260" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="6097" PostId="4213" Score="0" Text="This is copy pasted in terms of weights and incorrect. What is missed in those three weight values is the fact that for any given colour space the weights are different. The weights are determined by the given primary lamp's luminance position, or Y. The values given 0.299, 0.587, and 0.114 are archaic and incorrect. For any given set of encoded values representing sRGB / 709 lights, the correct values are 0.2126, 0.7152, and 0.0722. Do *not* use the weights posted in the answer. It is *not* subjective." CreationDate="2016-12-27T16:20:18.453" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="6098" PostId="4418" Score="0" Text="@Quinchilion yes but most devices are capable of capturing higher dynamic range but not color outside the tristimulus triangle. Besides HDR is just upwards in the chart so and can easily be reranged accetably. Sure no problem dont display color you dont measure." CreationDate="2016-12-27T16:28:35.083" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6101" PostId="4423" Score="0" Text="...but the system I'm describing is not a tri-light system. Instead of using a normal color filter, it uses a filter that can be adjusted using liquid crystals to allow only a certain wavelength range to pass." CreationDate="2016-12-27T19:00:54.370" UserId="5724" ContentLicense="CC BY-SA 3.0" />
  <row Id="6102" PostId="4423" Score="0" Text="That's sort of what we already have. The point is, given that the CIE model is based on a standard observer, it is impossible with any given tristimulus system and the visible light range." CreationDate="2016-12-27T19:07:38.450" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="6103" PostId="4423" Score="4" Text="No, it isn't. Currently the liquid crystals just change the opacity of  the color filters without changing their actual colors. What I'm talking about is using three filters that change the transmittance range of the color filter (like in this image: https://www.rp-photonics.com/img/edge_filter.png). This would mean that the three points in the chromacity diagram are not fixed, and can be moved aroud to produce any color." CreationDate="2016-12-27T19:15:33.477" UserId="5724" ContentLicense="CC BY-SA 3.0" />
  <row Id="6107" PostId="4423" Score="0" Text="@d9584 I can't speak to the hypothetical idea of a wavelength based system, and the outline I tried to answer for was why no full spectral locus display currently exists as per your first question. I also suspect that any such system would need an absolutely absurd emission both in intensity and uniform spectral output to be feasible. Given the constraints on the latter, it would need a light-year jump in technology to be even remotely feasible." CreationDate="2016-12-27T22:52:05.343" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="6113" PostId="4407" Score="0" Text="Isn't rendering at a resolution higher than the display's output resolution, then having the resultant bitmap downsampled to the target resolution by the GPU, a very common technique for increasing overall graphics quality in games, specifically for things like anti-aliasing? I imagine it's not so odd to have a game offer significantly higher render resolutions than the display is capable of natively displaying." CreationDate="2016-12-28T14:36:54.387" UserDisplayName="user5291" ContentLicense="CC BY-SA 3.0" />
  <row Id="6114" PostId="4060" Score="0" Text="I would say there's no general rule to speak of. There are so many different color quantization algorithms developed over the years, and they all seem to have various advantages or disadvantages specifically for smooth gradient dithering, but I'm probably wrong! This is a topic that I'm very interested in. Here's a great article that compares many dithering algorithms and explains dithering in general: http://www.tannerhelland.com/4660/dithering-eleven-algorithms-source-code/" CreationDate="2016-12-28T14:45:38.603" UserDisplayName="user5291" ContentLicense="CC BY-SA 3.0" />
  <row Id="6119" PostId="4432" Score="4" Text="Wow. They probably hire toughest minds in the world to make up most distinct names for same things." CreationDate="2016-12-28T21:10:42.407" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6120" PostId="4432" Score="0" Text="That chart is great, thanks for having taken the time to write it down!" CreationDate="2016-12-28T21:45:46.330" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="6121" PostId="4432" Score="3" Text="&quot;*texture array - layered image*&quot; OpenGL also calls them Array Textures; the term &quot;layered image&quot; is used primarily around their attachments in FBOs. Also, you should probably mention OpenGL sampler objects and their D3D equivalent, sampler state." CreationDate="2016-12-28T23:21:42.820" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6122" PostId="4432" Score="0" Text="@NicolBolas Thanks. I've edited and hopefully clarified that." CreationDate="2016-12-29T04:14:50.003" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6123" PostId="4434" Score="0" Text="Nice, this is great work!" CreationDate="2016-12-29T11:06:17.883" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="6124" PostId="4422" Score="0" Text="Comments are not for extended discussion; this conversation has been [moved to chat](http://chat.stackexchange.com/rooms/50883/discussion-on-question-by-wil-directx-openglvulkan-concepts-mapping-chart)." CreationDate="2016-12-29T11:34:17.840" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6127" PostId="4435" Score="0" Text="This is more than thread safety. A mutex would make any global RNG thread safe but still slow and unpredictable with multiple threads. Per-pixel seed is indeed the way to go for reproducibility, although per-thread state is enough to fix the performance problem." CreationDate="2016-12-29T15:33:50.220" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6128" PostId="4435" Score="0" Text="With thread-safe I don't mean use of mutex etc. but just making of an RNG class that has no static members. The clib version is bad as it has a global state shared across threads. I'll fix the answer to be more clear" CreationDate="2016-12-29T15:51:48.517" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6129" PostId="4434" Score="0" Text="I think it would be fair to say that `VkDescriptorPool` and `ID3D12DescriptorHeap` are similar in function (in that they are how you allocate descriptors), but quite different in form, due to the differences in the overall way descriptors are handled between the APIs. Also, I imagine that the D3D12 equivalent to `VkBufferView` is typed buffers, just as for D3D11." CreationDate="2016-12-29T16:24:42.290" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6130" PostId="4434" Score="0" Text="You're right on the descriptor heap. Updated. Regarding the buffer views, both API have a concept of views. I couldn't tell if DX12 has broken away from the DX1 convention of typed buffers or not since I don't have much experience with DX11." CreationDate="2016-12-29T16:37:50.477" UserId="5745" ContentLicense="CC BY-SA 3.0" />
  <row Id="6131" PostId="4435" Score="0" Text="PBRT already [has that](https://github.com/mmp/pbrt-v2/blob/master/src/core/rng.h) so the problem is likely how it is used (shared ?). I unfortunately don't know the PBRT code well enough to know what's going on." CreationDate="2016-12-29T17:04:04.390" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6133" PostId="4434" Score="0" Text="Could you say that D3D12 render targets are the equivalent of VkFramebuffer?" CreationDate="2016-12-30T06:03:19.123" UserId="125" ContentLicense="CC BY-SA 3.0" />
  <row Id="6134" PostId="4434" Score="0" Text="@JorgeRodriguez - I updated response to address your question." CreationDate="2016-12-30T08:48:56.470" UserId="5745" ContentLicense="CC BY-SA 3.0" />
  <row Id="6135" PostId="4432" Score="0" Text="Nice chart, but usually I find that `Multiple Render Targets` and `Occlusion Query` are also called the same in OpenGL, in general." CreationDate="2016-12-30T09:32:58.847" UserId="5002" ContentLicense="CC BY-SA 3.0" />
  <row Id="6136" PostId="4437" Score="0" Text="I can't tell from the animation what the specific causes and consequences of the gimbal lock are. Could you explain more about what happens and whether it is reversible?" CreationDate="2016-12-30T12:11:07.313" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6137" PostId="4437" Score="0" Text="It halped! Thank you for taking some time out for this." CreationDate="2016-12-30T13:06:37.233" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6138" PostId="4437" Score="2" Text="Oh sorry, rotation in this case is around axis (shown as circles) and by rotating on one axis, you rotate the other axis. The consequence is that with normal rotation sphere (the sphere in the animation) in animation, you don't see these axis being rotated, so if you get a gimbal lock, one axis is the same as another one, but with a normal rotation sphere it still tries to rotate as if that axis wasn't the same as another one, this means that it uses weird values to get to that rotation. I thought that the full post made that clear and that Ankit had some knowledge about it, @trichoplax" CreationDate="2016-12-30T14:58:35.853" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="6139" PostId="4437" Score="0" Text="For me, the extra explanation in your comment makes a big difference, so it might be worth editing it into the answer for future readers (comments aren't guaranteed to stay around)." CreationDate="2016-12-30T15:16:59.590" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6140" PostId="4437" Score="0" Text="Gimbal lock has nothing to do with matrices." CreationDate="2016-12-30T19:32:16.730" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6141" PostId="4432" Score="2" Text="@CpCd0y Yeah, they're colloquially called that, but my intent here was to say what those things are called / how they're represented in API-ese." CreationDate="2016-12-30T22:02:14.560" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6142" PostId="4437" Score="0" Text="The way I see it, is that knowing a little bit about how the rotation matrix is made from Euler angles can be useful to understanding Gimbal locking @joojaa  But you do have a point." CreationDate="2016-12-30T22:18:29.197" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="6143" PostId="4437" Score="1" Text="@bram0101 sure. Or you can actually not think so abstractly and think of it as actual physical 3 axis gimbal" CreationDate="2016-12-30T22:27:50.870" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6144" PostId="4432" Score="1" Text="@NathanReed: MRT is not used in the OpenGL specification, but &quot;occlusion query&quot; very much is. The term &quot;samples passed&quot; is merely one kind of occlusion query; there are also &quot;any samples passed&quot; and &quot;conservative any samples passed&quot;." CreationDate="2016-12-31T03:26:10.050" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6145" PostId="4435" Score="0" Text="A thread-safe RNG is not enough because that it only solves the race condition. But it serializes the execution and cache invalidation will still be present." CreationDate="2016-12-31T04:22:16.790" UserId="2448" ContentLicense="CC BY-SA 3.0" />
  <row Id="6146" PostId="4435" Score="0" Text="I'm not suggesting thread-safe implentation of a RNG class but implemeting RNG class that you use thread-safely, by not sharing the state across threads." CreationDate="2016-12-31T04:48:29.783" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6147" PostId="4437" Score="0" Text="&quot; The xyz Euler angles can be converted to matrices so that it can be used in the rotation. That is where something called rotation order comes in &quot; cleared my confusion. Thanks." CreationDate="2016-12-31T05:42:47.707" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6148" PostId="4439" Score="0" Text="Have you seen that drawing works if you take out the degenerate triangles? If not, then what makes you think that degenerate triangles have *anything* to do with your problem? &quot;*The geometry itself is correctly formed because it works on Nvidia cards.*&quot; No, that's not a reasonable conclusion to come to from that evidence. &quot;Works on NVIDIA&quot; only means &quot;works on NVIDIA&quot;. There's plenty of stuff NVIDIA's GL implementations do that are outside the specification." CreationDate="2016-12-31T17:41:26.053" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6149" PostId="4439" Score="1" Text="&quot;*On ATI it displays the gray scissor clear box but that's it.*&quot; What &quot;gray scissor box&quot; are you referring to?" CreationDate="2016-12-31T17:42:33.573" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6150" PostId="4439" Score="0" Text="The only way to resolve problems like this is to boil them down to the simplest elements: draw one triangle. Get that working. Then keep adding elements until you find the one that causes things to disappear. That's your problem." CreationDate="2016-12-31T17:44:28.373" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6151" PostId="4439" Score="0" Text="I do not suspect that the degenerates have anything to do with the issue. I don't think I said that I did. I was simply describing the format of the geometry that is being sent that is not displaying correctly.By saying that the geometry itself is correctly formed what I meant is that the indices create triangle strips with the correct vertex indices specified in the correct order which produce the correct number of triangles with the correct alternating winding order. I was simply referring to the actual vertex data in `heightMapVertexData`. It is what i expect it to be." CreationDate="2016-12-31T17:52:30.003" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6152" PostId="4439" Score="0" Text="&quot;*I do not suspect that the degenerates have anything to do with the issue. I don't think I said that I did.*&quot; But that is what you ask in the title of the question: &quot;Are ATI cards lacking some feature that would allow them to draw triangle strips connected by degenerates with a single draw call using a vao?&quot; So either that's what you're asking about or the title is misleading." CreationDate="2016-12-31T17:55:49.543" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6153" PostId="4439" Score="0" Text="I am very familiar with debugging things from the ground up by adding one element at a time. That's how I got the system working on my Nvidia cards in the first place. That and reading the OpenGL spec.&#xA;&#xA;Unfortunately I have no access to ATI hardware where I am right now so that's not an option for debugging this issue for another few months when I can get back home where I have ATI hardware.&#xA;&#xA;I was hoping there was some simple well known reason ATI cards don't work with what I am doing that I just missed. That way I could get more eyes on my application sooner." CreationDate="2016-12-31T17:58:22.927" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6154" PostId="4439" Score="0" Text="I have changed the title to be less specific about the geometry being drawn." CreationDate="2016-12-31T18:01:26.533" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6155" PostId="4439" Score="0" Text="The gray scissor box I'm referring to is the one drawn by:" CreationDate="2016-12-31T18:03:17.857" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6156" PostId="4439" Score="0" Text="glEnable(GL_SCISSOR_TEST)&#xA;glEnable(GL_MULTISAMPLE)&#xA;glClearColor(background.r, background.g, background.b, background.a)&#xA;glScissor(xPosition, flippedY, width, height)&#xA;glClear(GL_COLOR_BUFFER_BIT or GL_DEPTH_BUFFER_BIT)" CreationDate="2016-12-31T18:03:40.903" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6158" PostId="4439" Score="0" Text="Ok. I have added a better description of the gray scissor box to the question as you recommended." CreationDate="2016-12-31T21:37:34.640" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6159" PostId="4439" Score="0" Text="Could AMD be optimizing out vertex attributes, throwing things off?" CreationDate="2016-12-31T22:41:01.130" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="6160" PostId="4439" Score="0" Text="Yes the ATIs (and Nvidias) optimize out attributes if they aren't used in the body of the shader. `if (positionAttribute.location &gt;= 0) {` guards against enabling specific attributes if they are optimized out by the shader compiler. This had no impact on the output on ATI cards but it did fix an 0x501 error from the Nvidia cards when trying to enable an inactive attribute. The Nvidias still rendered fine regardless of the `0x501 GL_INVALID_VALUE` error however." CreationDate="2016-12-31T22:48:39.403" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6161" PostId="4439" Score="0" Text="In the case of the shaders in this question both vertex attributes shown here `positionAttribute` and `uvAttribute` are non-negative. The code is shared in other locations and the guard against the negative attributes is for those other cases. The 0x501 I mentioned was from those other cases as well." CreationDate="2016-12-31T22:54:37.843" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6162" PostId="4439" Score="0" Text="AMd cards are capable of drawing using this technique; there is either a driver bug or some in spec variability. Uninitialized variables in shaders were a common case for a while (NV defaulted vectors to 0001 and AMD left garbage in them) and we already discussed compiler variability in optimizing out/placing attributes. If I think of anything else..." CreationDate="2017-01-01T01:00:30.800" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="6163" PostId="4439" Score="0" Text="In glClear, shouldn't you be using bit wise or (|) not logical &quot;or&quot; (which is like ||, right)? Doubt it's anything..." CreationDate="2017-01-01T01:43:19.143" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="6164" PostId="4439" Score="0" Text="It's Kotlin. I didn't think the language was too important in the post in the is case since the GL calls are all the same. I'll update the question to mention that. `or` in this way is bitwise in kotlin. `||` is logical or in Kotlin." CreationDate="2017-01-01T12:35:51.620" UserId="5763" ContentLicense="CC BY-SA 3.0" />
  <row Id="6165" PostId="4442" Score="0" Text="You mean accurate atmospheric scattering? Why not render vertices which form skydome which is centered at camera position and color it in fragment directly?" CreationDate="2017-01-01T14:32:07.170" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6166" PostId="4444" Score="0" Text="Great Answer but wouldn't having a hemisphere around the entire scene centered at the camera that has a sky texture (created with a sky rendering model) on the inside do the trick fine? I mean I will of course represent the sun as a disk light and apply some sort of physically based color transform on the disk light (will have to check a siggraph paper I read for the actual name). What are the advantages of using the method I described over the method you said?" CreationDate="2017-01-01T18:24:19.897" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6167" PostId="4444" Score="1" Text="Using the method I described, the scene will appear illuminated by the whole environment. Think of the indirect illumination and bounces between the scene geometry too. What you are describing is a model with a texture, and no emission. If you only set emission on the disk and illuminate the scene just with that, it is the same as having an area light on that solid angle of the disk, you are not emitting the color of the whole sky onto the objects and making that radiance bounce between them as indirect illumination." CreationDate="2017-01-01T18:35:22.263" UserId="5519" ContentLicense="CC BY-SA 3.0" />
  <row Id="6168" PostId="4444" Score="1" Text="Of course, using the sun light source separated from the sphere would work if you wanted to simulate the whole atmosphere physically, with scattering. But it would be way more involved than my answer and I doubt that is what you want. Generally, we pick an image of the sky and emit that image onto the whole scene as light (the process I described)." CreationDate="2017-01-01T18:42:18.053" UserId="5519" ContentLicense="CC BY-SA 3.0" />
  <row Id="6171" PostId="4450" Score="0" Text="Yeah CPU cores. I just edited my question. Thank you" CreationDate="2017-01-02T18:28:13.587" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6172" PostId="4454" Score="1" Text="Could you let us know what you mean by &quot;doesn't get correctly loaded&quot;? What specifically goes wrong that indicates this?" CreationDate="2017-01-02T23:30:19.500" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6173" PostId="4454" Score="0" Text="Hi @trichoplax, the problem is that the array doesn't contain any data. I'm sure of this because the data I'm trying to load is used to calculate the colours of objects using tristimulus values, but the object is always black. In fact, if I tested now that if I try to pass an array of 4 values and the block in the shader contains for example a single variable of type vec4, the code above works. Thanks." CreationDate="2017-01-02T23:33:45.547" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="6174" PostId="4456" Score="0" Text="Thank you @Nicol_Bolas. Now it's clear (and I will use a standard uniform to pass my data to the shader). I will delete my other post on stack overflow, but can you remove the down vote, please? Thank you again." CreationDate="2017-01-02T23:58:42.610" UserId="2237" ContentLicense="CC BY-SA 3.0" />
  <row Id="6175" PostId="4456" Score="0" Text="@FabrizioDuroni a number of people have viewed this post. We don't know who the downvoter was." CreationDate="2017-01-03T00:11:04.987" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6176" PostId="3713" Score="0" Text="Convert radians to revolutions by multiplying by 1/(2pi); have your texture represent one complete revolution of sin/cos/whatever and lookup with GL_REPEAT. Please post what kind of speedups you get - different HW has very different performance characteristics - AMD has generally supported single cycle sin/cos instructions through HW lookup tables and texture lookup tables were, at least at one time, a slowdown. Keeping the table small so it lives in cache as GroverManheim suggested, is important." CreationDate="2017-01-03T01:04:27.483" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="6177" PostId="4455" Score="4" Text="Many raytracers implement analytical shapes. In fact we started 3d thatway and progressed to dicing of analytical shapes and only then to using polygons." CreationDate="2017-01-03T04:57:01.723" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6178" PostId="4455" Score="1" Text="You may be interested to read about the [ellipsoid rendering of the Ecstatica series](http://gdalgorithms-list.narkive.com/qbLh34tI/ellipsoid-engine-used-in-game-ecstatica-how-its-done) back in the 1990s.&#xA;&#xA;![Ecstatica screenshot](http://vignette4.wikia.nocookie.net/ecstatica/images/f/f7/Ecs_firstencounter.jpg/revision/latest?cb=20120813160048)" CreationDate="2017-01-03T09:28:43.610" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="6179" PostId="4459" Score="1" Text="Have you got any additional information about the overlay? Perhaps it's not plain alpha blending, but another operator. You could try some of the [Photoshop layer operator equations](http://photoblogstop.com/photoshop/photoshop-blend-modes-explained) instead." CreationDate="2017-01-03T11:26:27.667" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="6180" PostId="3713" Score="0" Text="When you say &quot;really old iDevice&quot; how old do you mean? For example, [page 8 of the series 6](http://cdn.imgtec.com/sdk-documentation/PowerVR+Series6+Compiler.Instruction+Set+Reference.pdf) gives the &quot;fred&quot; instruction which is meant for trig range reduction (admittedly for Radians), which can then be followed by fsinc. It's been a while since I worked on these (i.e. developing the HW algorithms) but can't imagine there's a much faster way of calculating sine/cosine." CreationDate="2017-01-03T11:28:56.160" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6181" PostId="4460" Score="1" Text="Are you talking about on a PC (where the GPU has its own memory) or a mobile platform (where the GPU accesses main memory directly)?" CreationDate="2017-01-03T11:34:07.330" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6182" PostId="4462" Score="0" Text="convex object of are there concave parts?" CreationDate="2017-01-03T14:16:27.747" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6183" PostId="4462" Score="0" Text="@ratchetfreak I uploaded the pic of the objects. :) Just curious, what is your consideration by asking that question?" CreationDate="2017-01-03T14:31:54.747" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="6184" PostId="4462" Score="1" Text="seeing if the GJK algorithm would work. It requires convex objects." CreationDate="2017-01-03T14:37:42.190" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6185" PostId="4465" Score="0" Text="Yeah I expected such answer. Wait for GPU - I mean that draw call as any other function returns after its finishes work, in this case let's say - ordered work. Yeah there is some confusion here." CreationDate="2017-01-03T15:13:51.317" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6186" PostId="4465" Score="1" Text="@narthex: &quot;*returns after its finishes work*&quot; That all depends on how you define &quot;its work&quot;. The &quot;work&quot; of a draw call is to drop a token into the GPU's command stream. And sometimes not even that much. In all likelihood, by the time that function returns, [nothing has even *started* to render, let alone finished](https://www.khronos.org/opengl/wiki/Synchronization)." CreationDate="2017-01-03T15:23:51.260" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6187" PostId="4465" Score="0" Text="_OpenGL Rendering Commands are assumed to be asynchronous._ I feel like noob now." CreationDate="2017-01-03T15:42:09.103" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6188" PostId="4455" Score="1" Text="For modeling more general curved surfaces 3d splines are also commonly used, such as Bézier patches and NURBS surfaces. Most graphics applications just approximate these curves using triangles, but there are methods for displaying them that do not require such approximations." CreationDate="2017-01-03T22:06:59.027" UserId="4768" ContentLicense="CC BY-SA 3.0" />
  <row Id="6189" PostId="3713" Score="0" Text="If sin instruction and texture lookup are expensive, you might also try approximating sin with a polynomial" CreationDate="2017-01-03T22:22:35.723" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6190" PostId="4466" Score="0" Text="Thanks.. Will take a look and confirm whether it satisfy my needs or not.. :)" CreationDate="2017-01-04T01:09:50.980" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="6191" PostId="4450" Score="0" Text="Could you be a little more specific about what you have tried or found so far?" CreationDate="2017-01-04T09:31:20.760" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6192" PostId="4469" Score="2" Text="The fuzzy look could come from the denoiser that they use. Renderman, the renderer that Pixar and probably Disney uses, has gotten a denoiser a while back. It has been used in Finding Dory, and could be used in Moana if they used Renderman to render it." CreationDate="2017-01-04T10:38:31.070" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="6193" PostId="4469" Score="0" Text="My favorite source that describes what Dan says below is found [here](https://renderman.pixar.com/view/basic-computer-graphics-2) incidenttaly its a pixar publication (document is a bit old but is still valid for the theory). @bram0101 why wouldn't Disney use their own renderer... Disney owns Pixar remember." CreationDate="2017-01-04T10:57:04.207" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6194" PostId="4469" Score="1" Text="Does anyone know what data compression scheme/format the cinematic releases use? Could it potentially be an artefact of skimping on the bit rate?" CreationDate="2017-01-04T11:21:04.717" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6195" PostId="4469" Score="0" Text="Disney has also developed their own renderer, https://www.disneyanimation.com/technology/innovations/hyperion  @joojaa" CreationDate="2017-01-04T14:06:03.223" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="6196" PostId="4470" Score="1" Text="Doesn't this belong on Physics.SE?" CreationDate="2017-01-04T18:31:26.297" UserId="5792" ContentLicense="CC BY-SA 3.0" />
  <row Id="6197" PostId="4472" Score="3" Text="Strictly speaking, light does travel through metals but gets attenuated very rapidly, so that it doesn't penetrate more than a few microns below the surface. (Very thin layers of metal _are_ partially transparent—the gold film on spacesuit helmets, for instance.) That's what the imaginary component of the IOR measures: the rate of attenuation. And Fresnel's law applies just as much to metals as to anything else, as seen in the other answers." CreationDate="2017-01-04T22:03:19.810" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6198" PostId="4462" Score="2" Text="I think a typical approach would be to use a BSP tree to split the object into concave parts, then use BVH sweep and prune (broad-phase) + GJK (narrow-phase) to get exact collision." CreationDate="2017-01-04T22:23:09.090" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6199" PostId="4471" Score="0" Text="Also, they don't send primary rays out in a regular grid pattern (which is effectively what you get by increasing resolution)." CreationDate="2017-01-05T00:47:34.720" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6200" PostId="4471" Score="0" Text="I wonder if there are adaptive AA techniques which decide the successive sample positions based on previous samples rendered for the pixel" CreationDate="2017-01-05T01:25:33.237" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6201" PostId="4462" Score="0" Text="@NathanReed But the broad-phase you mention will require both objects to be collided. Am I right? I prefer to define a collision, if distance between two objects are &lt;= certain value." CreationDate="2017-01-05T01:46:48.700" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="6202" PostId="4462" Score="2" Text="@Bla... You can inflate all the bounding boxes (or bounding spheres or whatever) by the tolerance, or build the tolerance into the broad-phase intersection tests." CreationDate="2017-01-05T01:53:55.270" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6203" PostId="4477" Score="1" Text="I have seen a few people apply machine learning to BRDF's.  Here's one such thing: http://www.iaeng.org/IJCS/issues_v42/issue_1/IJCS_42_1_04.pdf" CreationDate="2017-01-05T05:05:25.083" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6204" PostId="4471" Score="3" Text="@JarkkoL Yes, that's the usual way of doing it. The cost of the adaptive sampler is much less than the cost of additional primary rays, and it lets you get better results with fewer rays." CreationDate="2017-01-05T10:20:38.837" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6205" PostId="4471" Score="2" Text="@NicolBolas That's a good point, so I've added it (and JarkkoL's clarification) to the answer." CreationDate="2017-01-05T10:26:20.250" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6206" PostId="4470" Score="0" Text="Although many computer graphics questions involve physics, this is clearly a question looking for answers from computer graphics experts, and would not be a good fit on physics.SE." CreationDate="2017-01-05T10:40:23.393" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6207" PostId="4473" Score="0" Text="Yep, thank you for adding these questions; some of them I already had them on my list. I am a bit puzzled how to use or create a proper DB for this as it seems to require a lot of accurate resources. I was hoping for an alternate/possible solution.." CreationDate="2017-01-05T11:44:40.893" UserId="5780" ContentLicense="CC BY-SA 3.0" />
  <row Id="6208" PostId="4473" Score="0" Text="@4673_j This is why even Adobe does not implement this function. Even though their users would benefit (and they would get the DB in no time by asking users). Basically its not entirely feasible at current date." CreationDate="2017-01-05T11:47:17.193" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6209" PostId="4473" Score="0" Text="Hm.. I might have a partial solution in my case, as there might be ~10 different monitors/screens. I'm searching now how to extract that information from the system. Thanks for your effort. I think this kind of &quot;feature/info&quot; has to be provided by the OS, not just the resolution; as the OS has direct access to all physical components." CreationDate="2017-01-05T11:55:15.507" UserId="5780" ContentLicense="CC BY-SA 3.0" />
  <row Id="6210" PostId="4473" Score="0" Text="@4673_j Its good that this info did not exist back in 1995 or oherwise the web would still be designed like a print publication. Its a good thing that the os does not try to react to my monitors DPI or it would defeat many normal operations." CreationDate="2017-01-05T11:58:13.053" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6211" PostId="4474" Score="0" Text="This is exactly the kind of detail I was hoping for when posting the question; thanks a lot! I've tried running the numbers again with the complex values, and I get results a lot more close to expected.&#xA;So what you're describing about electrostatic equilibrium is basically $\nabla B=0$?" CreationDate="2017-01-06T00:23:18.867" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6212" PostId="4482" Score="1" Text="&quot;*You might want to convert your code to use that if possible*&quot; His code *is using that*. `glCreateTextures` is specific to the ARB_DSA function. He's using an odd mixture of the two, likely because he couldn't find a `glTextureImage2D` in the ARB version (since the ARB wants to discourage non-immutable textures)." CreationDate="2017-01-06T01:11:18.870" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6213" PostId="4482" Score="1" Text="@NicolBolas Aha, thanks, I hadn't caught that. I've updated the answer again." CreationDate="2017-01-06T03:12:37.617" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6214" PostId="4483" Score="2" Text="Do you have a reference picture of the image you expect to get? It could be easier to first replicate an exiting image and compare your result." CreationDate="2017-01-06T05:47:28.517" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6215" PostId="4483" Score="1" Text="@JulienGuertault thank you so much for the suggestion! That was brilliant and I wish I had thought of it earlier. That would've helped with a lot of hair pulling. See my edited post. T" CreationDate="2017-01-06T06:32:27.000" UserId="5764" ContentLicense="CC BY-SA 3.0" />
  <row Id="6217" PostId="4484" Score="0" Text="Is your CPU code calling into GL on multiple threads? My first guess would be that there's nothing wrong with the shader and it's a cache invalidation issue caused by some non-thread-safe use of your intermediate textures." CreationDate="2017-01-06T14:54:14.840" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6218" PostId="4484" Score="0" Text="Thanks, and no, the CPU code is executed without multithreading. By the way: Now I tried the following: &#xA;&#xA;vec2 basePos	= vec2(mod(pos.x,256),mod(pos.y,192));&#xA;&#xA;where one instance is 256x192 and i still get this fault. It seems that there is a limitation on the number of reads you can perform on a texel?" CreationDate="2017-01-06T15:27:01.333" UserId="5399" ContentLicense="CC BY-SA 3.0" />
  <row Id="6219" PostId="4484" Score="0" Text="Additionally i found out, that the position of the black boxes varies randomly, when using the mod(...) version" CreationDate="2017-01-06T15:34:40.530" UserId="5399" ContentLicense="CC BY-SA 3.0" />
  <row Id="6221" PostId="4483" Score="1" Text="I've edited out the &quot;one more thing&quot; section now that you've posted it as a separate question. Notice that the separate question now appears in the &quot;Linked&quot; section on the right hand side of this page, because that question contains a link to this question." CreationDate="2017-01-07T12:20:26.467" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6222" PostId="4411" Score="0" Text="Thank you joojaa - you seem to know what you are talking about. If I may ask, have you coded Laplacian Smoothing in C++? How can I reach out to you for technical help?" CreationDate="2017-01-07T18:02:13.237" UserId="5670" ContentLicense="CC BY-SA 3.0" />
  <row Id="6223" PostId="4483" Score="0" Text="In case it's helpful, here's a good short read about path tracing basics with some simple c++ source code accompany it: http://blog.demofox.org/2016/09/21/path-tracing-getting-started-with-diffuse-and-emissive/" CreationDate="2017-01-07T18:30:39.693" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6224" PostId="4491" Score="3" Text="Convert the TRS form back to a matrix and check if it's &quot;close enough&quot; to the original matrix?" CreationDate="2017-01-07T22:07:50.357" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6225" PostId="4492" Score="0" Text="This sounds incorrect. Roughness doesn't cause diffuse, it only affects it due to geometric occlusion. A fine polished marble slate still has the same a strong diffuse, just with slightly different repartition of brightness depending on the view angle." CreationDate="2017-01-08T03:18:08.770" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6226" PostId="4490" Score="2" Text="That's an interesting question, but a diagram would help to visualize situation." CreationDate="2017-01-08T11:05:50.293" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6227" PostId="4494" Score="1" Text="Thank you so, so much! Now everything makes a lot more sense to me. That paper is a wonderful resource too as my next step will be implementing glass after I have mirror done." CreationDate="2017-01-08T21:23:39.773" UserId="5764" ContentLicense="CC BY-SA 3.0" />
  <row Id="6229" PostId="4492" Score="0" Text="Roughness might not be the right word for it, but the principle is there." CreationDate="2017-01-09T04:00:33.890" UserId="5816" ContentLicense="CC BY-SA 3.0" />
  <row Id="6230" PostId="4496" Score="0" Text="when you use interger type with shift, same as float-type" CreationDate="2017-01-09T10:12:46.850" UserId="5823" ContentLicense="CC BY-SA 3.0" />
  <row Id="6232" PostId="4434" Score="2" Text="Inspired by this topic, I wrote single header implementations of both Vulkan and DX12 renderers:&#xA;&#xA;renderers:https://github.com/chaoticbob/tinyrenderers" CreationDate="2017-01-09T11:23:24.617" UserId="5745" ContentLicense="CC BY-SA 3.0" />
  <row Id="6233" PostId="4484" Score="1" Text="In your short version, the shaders seem fine. Garbage in a depth or stencil buffer can easily do this kind of thing (make sure depth testing is disabled). On the GL bug side, have you tried forcing a cpu renderer or a different vendor's GPU?" CreationDate="2017-01-09T15:56:37.817" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="6234" PostId="4502" Score="0" Text="Thank you very much! You saved me some headache's. You are right. I completely overseen the vkUpdateDescriptorSets method. And in my setup, the pipelines won't change at all. So I can build them all upfront." CreationDate="2017-01-09T21:06:17.350" UserId="5825" ContentLicense="CC BY-SA 3.0" />
  <row Id="6235" PostId="4503" Score="0" Text="Does it work better if the plane is facing the camera? I don't think you're giving it the correct Z. I don't know that particular unproject function but I suspect it should be Z from your depth buffer which is certainly not constant with the plane orientation shown." CreationDate="2017-01-10T01:45:11.073" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6236" PostId="4503" Score="0" Text="I get a similar result with the plane facing the camera.. I'm not sure how to get that z value from the depth buffer. But a value different from 1 just seem to make it worse." CreationDate="2017-01-10T05:52:19.800" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6237" PostId="4503" Score="0" Text="You are not passing the view matrix. In the StackOverflow link you posted, it states that you should be passing the ModelView matrix on the 'model' parameter. That makes sense, since the function cannot guess which view matrix you are using and it needs to know that to reverse all the transformations." CreationDate="2017-01-10T10:40:57.370" UserId="5519" ContentLicense="CC BY-SA 3.0" />
  <row Id="6238" PostId="4503" Score="0" Text="Ok, yes good point. I think I assumed since the function takes in the window dimensions it had the parameters needed for the &quot;view&quot;. But this makes more sence, I didn't understand why the model matrix was a parameter. Will try this later, thanks." CreationDate="2017-01-10T10:44:55.610" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6239" PostId="4484" Score="0" Text="Thanks @Daniel, I disabled depth testing and nothing changed. And currently I don't have the possibility to use another GPU or do CPU rendering. I found a workaround for my problem. But now I run into another problem with two uniform variables: For the first 5 instances they are read correctly and the 6 is faulty. In the end I guess, that I made a fault in setting up the OpenGL stuff in my C++ code." CreationDate="2017-01-10T10:48:45.847" UserId="5399" ContentLicense="CC BY-SA 3.0" />
  <row Id="6240" PostId="4507" Score="0" Text="The diagram suggests you are looking for the shortest distance between vertices of one polygon and another, which will not always be the same as the shortest distance between the polygons. For example, all of the connections in the diagram are longer than if you made them perpendicular to the edge to the right, rather than meeting a vertex on the right. Is your question specifically looking for vertex to vertex connections?" CreationDate="2017-01-10T11:52:10.643" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6242" PostId="4510" Score="1" Text="What is it that you don't understand or want to do? This is hard to read because it's javascript without any sane abstractions... but at its core, it's only a cross product to compute the normal and addition to average the normals on shared vertices." CreationDate="2017-01-10T19:13:09.497" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6243" PostId="4510" Score="1" Text="Seconded. I don't see a question so have no idea what it is you are asking about." CreationDate="2017-01-10T19:32:03.820" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6244" PostId="4510" Score="1" Text="Never mind the first line and that is Javascript.  I know what the code does. I know how to calculate normal in theory.  But in practice, in the code, I do not understand the index management mechanism. I miss the sense of what is done." CreationDate="2017-01-10T19:33:58.750" UserId="4981" ContentLicense="CC BY-SA 3.0" />
  <row Id="6245" PostId="4503" Score="0" Text="I tried with the view matrix. See edits." CreationDate="2017-01-10T19:48:13.203" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6246" PostId="4507" Score="0" Text="Yes you are right, I'm searching for vertex-vertex connections." CreationDate="2017-01-10T22:04:48.187" UserId="5828" ContentLicense="CC BY-SA 3.0" />
  <row Id="6247" PostId="4511" Score="2" Text="Using a paint program that has a &quot;nearest neighbor&quot; resize mode would give you a quick approximation, though it may not select the exact same rows/columns as your code. I think it would be pretty easy to prototype in Python using [pillow](https://python-pillow.org/) for image manipulation." CreationDate="2017-01-10T22:36:30.300" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6248" PostId="4510" Score="2" Text="Ok, I can't write an answer anymore but as best I can tell, `vs` is a vertex array and `ns` a corresponding normal array. Both are raw float arrays so they are indexed with `i*3+0` (`i*3+x` in the code) for x component of vector number i, `i*3+1` for y, `i*3+2` for z. `ind` is an indirection array which contains vertex (and thus normal too) indices. It defines the triangle strips (the topology) within the arbitrary set of vertices. `v1` and `v2` are two edges of the triangle." CreationDate="2017-01-11T00:07:38.680" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6249" PostId="4511" Score="0" Text="Don't ntsc and pal have overscan areas - could you prepare your video with the decimated/filtered images in the overscan area? It would take 1 scan line of PAL to hold your entire image." CreationDate="2017-01-11T01:04:58.240" UserId="2500" ContentLicense="CC BY-SA 3.0" />
  <row Id="6250" PostId="4510" Score="2" Text="I have tried to reword the question to make it clearer. Feel free to edit or revert changes if I changed the intended meaning." CreationDate="2017-01-11T06:51:39.927" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6251" PostId="4511" Score="1" Text="*&quot;I cannot find any examples of how such downscaling will behave.&quot;*   Did you ever play old 3D games like Doom and see the hideous aliasing artefacts on walls/objects in the distance? That's what it will be like." CreationDate="2017-01-11T09:40:47.443" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6252" PostId="4503" Score="0" Text="Does the plane really have a model matrix equal to identity (i.e. no scales, etc)? Also, glm::unproject returns a vector in object coordinates (model space). How are you using that returned vector when determining where it is in the world?" CreationDate="2017-01-11T12:23:32.477" UserId="5519" ContentLicense="CC BY-SA 3.0" />
  <row Id="6254" PostId="4503" Score="0" Text="The plane is just to visualize the xy world plane (made up of lines).&#xA;I take the x and y value from the returned vector and create a translation matrix with 0.0 as z translation. No scale and rotation.&#xA;That gives me the model matrix which is sent to the shader with ViewM and ProjM." CreationDate="2017-01-11T12:27:00.147" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6255" PostId="4513" Score="0" Text="Are you specifically looking to perform the rendering 4 times, or just to render once and display the result in 4 different places?" CreationDate="2017-01-11T12:50:07.020" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6256" PostId="4513" Score="0" Text="Perform the rendering 4 times. What I wrote is just a simplified example, for example I want the main camera to render a car at the full window, and another camera render the car from the side in a small square in one of the corners." CreationDate="2017-01-11T12:59:31.270" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="6257" PostId="4513" Score="0" Text="If I understood this correctly so you need to split your window to for example 4 parts and in each part render other parts of the scene like [that](http://archive.gamedev.net/archive/reference/programming/features/3dmfc/viewerearly.jpg)?" CreationDate="2017-01-11T14:15:03.163" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6258" PostId="4513" Score="0" Text="Yes, but not strictly 4 parts. I want it to be flexible.&#xA;Flexible size and position of viewport rectangle." CreationDate="2017-01-11T14:38:04.867" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="6259" PostId="4514" Score="0" Text="Thank you, but a little confused.&#xA;&quot;check with your projection matrix what values comes out at the near-plane&quot; I'm no quite sure how I would do that...&#xA;And in your first equation, isn't the first and second component of v the x and y in &quot;clip space&quot;?" CreationDate="2017-01-11T15:00:42.003" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6260" PostId="4514" Score="0" Text="If you transform a point $[0, 0, np]$ from camera space to NDC, what's the value you get for z-coordinate. And to your second question, no, they are in NDC. You have to multiply by w (i.e. near-plane distance in your case) to get them into clip-space" CreationDate="2017-01-11T15:14:31.830" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6261" PostId="4516" Score="0" Text="I made an edit. Yes voxelization can be also done as precomputation. But it seems to have nice performance also in real time as I implemented it for occlusion purporses in RSM based GI. I know that precomputation is crucial for performance but it is just a idea of thesis  - study of &quot;real timeness&quot;, I mean calculated each frame. A screen space GI sounds like oxymoron." CreationDate="2017-01-11T15:21:54.343" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6262" PostId="4516" Score="0" Text="Scene complexity - actually just one model probably sponza. Same complexity to get comparable results." CreationDate="2017-01-11T15:26:34.547" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6263" PostId="4516" Score="0" Text="Performance depends on the scene complexity you need to voxelize per frame, if you can call it real time. You could also call path tracing real time technique given simple enough scene and/or beefy enough hardware, and poor enough quality (:" CreationDate="2017-01-11T15:26:42.080" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6264" PostId="4515" Score="0" Text="So with glViewPort() I can define where I want to draw next, and with glBlendFunc I can define how the GPU should blend the overlapping areas (framebuffers) with each other. Am I correct?" CreationDate="2017-01-11T15:30:29.633" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="6265" PostId="4515" Score="1" Text="Yes, you are free to parametrize that. As parameters of `viewport` are x, y of left bottom corner of chunk  and width, height of chunk. With blending you can experiment." CreationDate="2017-01-11T15:36:45.650" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6266" PostId="4515" Score="1" Text="why blending? how is it related to the question ?" CreationDate="2017-01-11T15:38:59.450" UserId="5364" ContentLicense="CC BY-SA 3.0" />
  <row Id="6268" PostId="4515" Score="0" Text="@Tudvari you can do it in more modern way" CreationDate="2017-01-11T17:27:15.027" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6270" PostId="4515" Score="0" Text="and what is that way?" CreationDate="2017-01-11T17:42:00.107" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="6271" PostId="4515" Score="0" Text="@Tudvari I edited my answer." CreationDate="2017-01-11T17:45:26.173" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6273" PostId="4508" Score="0" Text="Searching for the nearest point is an easy task. There is another problem. What if the nearest point cause an intersection with another polygon. Or what if the connection intersects the first polygon itself. Please look at the second image." CreationDate="2017-01-11T20:11:17.843" UserId="5828" ContentLicense="CC BY-SA 3.0" />
  <row Id="6274" PostId="4519" Score="0" Text="This is interesting because TAA between frames is cheap but hard to implement.  I used temporal blending but on voxel grid to get smooth light movement which is quite easy to implement in this case." CreationDate="2017-01-11T21:17:15.123" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6275" PostId="4518" Score="4" Text="Hello! Can you explain (or better yet, show!) how it isn't working, and how you expect it to work?" CreationDate="2017-01-11T21:26:26.927" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6276" PostId="4519" Score="0" Text="I know the usage of TAA on shadows in screen space or volumetric fog rendering but never thought it can be applied to GI in that way." CreationDate="2017-01-11T21:29:44.110" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6278" PostId="4515" Score="3" Text="You don't need blending or multiple framebuffers. Rendering will not write to any pixels outside the current glViewport rectangle, so you can just set up and draw each viewport in turn. BTW you can also use the [scissor test](https://www.khronos.org/opengl/wiki/Scissor_Test) to restrict clears to a certain rectangle, in case you want overlapping viewports." CreationDate="2017-01-12T01:02:54.347" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6279" PostId="4519" Score="0" Text="Remember me if you publish a famous paper (;" CreationDate="2017-01-12T03:16:44.017" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6282" PostId="4515" Score="2" Text="Blending has nothing to do with this and makes the answer more confusing. The setting up of the viewports is all that is needed" CreationDate="2017-01-12T15:34:46.420" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="6283" PostId="4515" Score="0" Text="@NathanReed No problem feel free to edit  or add info about scissor test" CreationDate="2017-01-12T15:48:10.990" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6287" PostId="4411" Score="0" Text="Is there a way to contact members privately? I am trying to get some help from &quot;joojaa&quot;." CreationDate="2017-01-12T18:13:07.170" UserId="5670" ContentLicense="CC BY-SA 3.0" />
  <row Id="6288" PostId="4527" Score="0" Text="&quot;immediately get the up, right(*), forward vectors&quot;: could you please specify which rows or columns represent the X, Y or Z axis, and why?" CreationDate="2017-01-12T18:38:44.687" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6289" PostId="4411" Score="0" Text="@Jay i have once done a Laplacian smoothing  algorithm but in 2D. But i dont know if im very good at helping you with your code." CreationDate="2017-01-12T19:11:45.493" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6290" PostId="4527" Score="0" Text="let me know if that helps nbro!" CreationDate="2017-01-12T19:31:53.480" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6291" PostId="4527" Score="0" Text="Yes, that was helpful, thanks! I've another doubt. You say &quot;you can look at it and immediately get the up, right(*), forward vectors&quot;, you're saying of the local coordinate system? According to the answer provided by @Oliver, this matrix defines a new coordinate system with respect to another, but this is often implicit. This kind of makes sense to me if I imagine at the end that this matrix represents a transformation. But what I'm confused about now is if the columns (or rows) of the transformation matrix actually define the basis of the new space... I should go back to review this." CreationDate="2017-01-12T19:47:49.870" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6292" PostId="4527" Score="0" Text="Yes. In my example, you can use the columns to get the basis vectors of the coordinate system it describes.  The columns are in the coordinate system of whatever the parent coordinate system is.  If there is no parent coordinate system, they are in global space.  Else, they are in the coordinate system of the parent. HTH but i understand that sometimes it takes time and multiple attempts to absorb new concepts :P" CreationDate="2017-01-12T19:54:42.953" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6293" PostId="4527" Score="0" Text="When you say &quot;The columns are in the coordinate system of whatever the parent coordinate system is&quot; you mean &quot;the columns are in the vector space of whatever...&quot;?" CreationDate="2017-01-12T19:57:11.667" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6295" PostId="4527" Score="0" Text="You are over thinking it. Look at the identity matrix. The first column says that the x axis of the transform is 1,0,0.  If you had some rotation / scaling matrix, it may give you a different answer like 0,2,0, but that is still the x axis of the transform that the matrix represents.  Matrices can be parented off of each other, but that is probably too confusing to visualize as you are just trying to learn things.  Just imagine that if there was a matrix hierarchy, that all the matrices are multiplied together into one matrix M then you can ignore hierarchies. So, col 1 = x axis. Easy stuff (:" CreationDate="2017-01-12T21:18:29.960" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6296" PostId="4527" Score="0" Text="I'm not really overthinking it, in my opinion. I'm just trying to put together things from linear algebra and the terminology used in computer graphics. In computer graphics, people talk about spaces and coordinate systems almost interchangeably, but that's clearly not correct from a linear algebra's point of view. The &quot;parenting&quot; thing is another thing I've never heard in linear algebra and I guess it just another vague term." CreationDate="2017-01-12T21:29:13.383" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6297" PostId="4528" Score="0" Text="Just thinking out loud, it may be possible to use some equations for [spherical lenses](http://www.physicsinsights.org/simple_optics_spherical_lenses-1.html) and treat it like calculating defocus blur. The caustic brightness would go like $1/r^2$ of the circle of confusion, or something like that." CreationDate="2017-01-12T22:04:24.320" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6298" PostId="4527" Score="0" Text="Fair enough! If you want more rigorous details, I'll leave that to someone else. I'm not real clear on vector spaces, but am clear on matrices as coordinate systems. I'm more a game programmer than a mathematician. True, you can be both, but in my case, I'm stronger in one vs the other hehe." CreationDate="2017-01-12T22:39:17.257" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6299" PostId="4528" Score="0" Text="That sounds really promising.  I like how that seems like it could be exact for spheres, whereas the answer about using surface normal and refractive index would be decent approximations for general shapes." CreationDate="2017-01-12T22:40:35.067" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6300" PostId="4493" Score="0" Text="Thank you. This is the kind of thing I was looking for. There is an edge case I have found which is not detected - a negative scale in the original transform, but from what I've read I'm not sure its possible to detect this." CreationDate="2017-01-13T00:17:14.220" UserId="4509" ContentLicense="CC BY-SA 3.0" />
  <row Id="6301" PostId="4493" Score="1" Text="@sebf You can check the sign of the determinant of the 3×3 submatrix. That will detect if there's a handedness flip, which is caused by a negative scale along either 1 or 3 axes. However, if there was a negative scale along 2 axes originally, that's equivalent to a 180° rotation, so it's not possible in that case to tell the difference just from the final transform matrix." CreationDate="2017-01-13T00:35:30.210" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6303" PostId="4527" Score="0" Text="@nbro that is a good separate question. But basically it goes like this linear algebra deals with one transform from one space to another. If you have 3 spaces (for convenience because tranforms are easy, other reasoning not so easy), A, B and C. Then you would need to know which transform goes from which space to what space. Now if you know transform A-B and A-C  you also know all other combinations. So we store those transforms for later use. This is herarchially as if B and C belomg under A and indeed if you had a space W and a transform W-A then changed W-A then B and C would move with A." CreationDate="2017-01-13T06:08:37.293" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6304" PostId="4523" Score="0" Text="It does not matter if its row oriented column oriented or arbitrary otder or inverse its still the same case. This is not because of math but what we are trying to acieve. Math does not deal with the why as much as you like to believe, math deals with properties of abstract things, the interpretation has to come from oitside of math. But its better you smartly ask the space question you originally tried ton" CreationDate="2017-01-13T07:33:01.333" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6305" PostId="4527" Score="0" Text="@joojaa I didn't under this last part &quot;then changed W-A then B and C would move with A. &quot;" CreationDate="2017-01-13T11:40:50.577" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6306" PostId="4527" Score="0" Text="@nbro they are defined in relation to A therefore if that relation does not change they will transform as if a fixed part of A" CreationDate="2017-01-13T11:49:46.717" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6307" PostId="4527" Score="0" Text="@joojaa I'm going to ask a separate question." CreationDate="2017-01-13T12:39:50.313" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6308" PostId="4532" Score="0" Text="&quot;*What is it meant by matrix hierarchy?*&quot; It is a [hierarchy](http://www.dictionary.com/browse/hierarchy) of matrices." CreationDate="2017-01-13T14:01:35.140" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6309" PostId="4532" Score="1" Text="You should read [this scene graph page](https://en.wikipedia.org/wiki/Scene_graph). The scene graph is where the parent and hierarchy concepts come from." CreationDate="2017-01-13T14:07:50.670" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6310" PostId="4532" Score="0" Text="@NicolBolas I know what's a hierarchy, and I know that it starts to make sense to talk about matrices hierarchies, if there are parent-child relationships between matrices." CreationDate="2017-01-13T14:09:11.563" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6311" PostId="4531" Score="0" Text="&quot;*This index can be used in the Vertex Shader to set the Viewport to which the scene is rendered.*&quot; No, it cannot. Well, not without the ARB_shader_viewport_layer_array extension, or the AMD equivalents. None of those are standard in GL 4.5. You are perhaps thinking of Geometry Shaders." CreationDate="2017-01-13T14:38:50.593" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6312" PostId="4532" Score="1" Text="BTW, if you dont get a good answer here nbro, mathematics stack exchange may be able to help more!  http://math.stackexchange.com/" CreationDate="2017-01-13T16:59:38.267" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6313" PostId="4533" Score="1" Text="I now established that I will use stm32f767 which – not looked myself yet – has DSP commands. I might do some computations after all, clock is 216 MHz and DMA is said to be flexible. I will read ADV7280 output clocked at 54 MHz, though (interlace -&gt; progressive enabled)." CreationDate="2017-01-13T17:45:04.403" UserId="5832" ContentLicense="CC BY-SA 3.0" />
  <row Id="6314" PostId="4535" Score="2" Text="It took me a second to understand what you meant by a rotation gizmo. I added an image of one in case it's helpful (i think you can't add images until you have more reputation sadly!).  If you have a more appropriate image in mind, feel free to let me know and i can change the image." CreationDate="2017-01-13T18:54:45.193" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6315" PostId="4535" Score="0" Text="Thanks for adding the image Alan. You are right, I think it will make things clearer." CreationDate="2017-01-13T18:57:27.513" UserId="5841" ContentLicense="CC BY-SA 3.0" />
  <row Id="6316" PostId="4532" Score="0" Text="@AlanWolfe Yes, thanks for trying to help!" CreationDate="2017-01-13T19:30:28.490" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6317" PostId="4540" Score="0" Text="UV-mapping the surface?" CreationDate="2017-01-14T02:57:15.713" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6318" PostId="4540" Score="0" Text="Hrm yeah I guess that's true if you have   a unique range of u and v per face!" CreationDate="2017-01-14T03:13:50.313" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6319" PostId="4536" Score="0" Text="When you say simulation, do you mean simulating the physics of things like hair or rendering things more accurately?" CreationDate="2017-01-14T05:41:19.520" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6320" PostId="4536" Score="0" Text="I mean things like simulating the motion of hair and water and things yeah, not the light physics." CreationDate="2017-01-14T05:43:28.783" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6321" PostId="4541" Score="0" Text="Anything you deem necessary" CreationDate="2017-01-14T10:59:07.507" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6322" PostId="4540" Score="0" Text="2 time 2 angle pairs? See the third rotation about the own axis is not needed to represent a arrow in any direction." CreationDate="2017-01-14T11:00:48.843" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6323" PostId="296" Score="2" Text="&gt; &quot;If you are quite good at writing programs in e.g. Java, you won't have that much trouble learning C++&quot; — this is not true. *Everybody* will have colossal amounts of trouble learning *and using* C++. Comparing it to Java or any other safe language is completely inappropriate. For other languages you listed (Objective-C, Swift, or some other object-oriented language) that statement is mostly true, though." CreationDate="2017-01-14T13:19:32.250" UserId="5848" ContentLicense="CC BY-SA 3.0" />
  <row Id="6324" PostId="31" Score="0" Text="keep in mind that `mozjpeg` compressor has a special trick to produce less distortion on these kinds of images. (and it's more efficient in general than usual jpeg writers too.) did you try it?" CreationDate="2017-01-14T13:43:18.547" UserId="5848" ContentLicense="CC BY-SA 3.0" />
  <row Id="6325" PostId="4542" Score="0" Text="So, essentially, the screen is the real $2D$ screen (or image on the screen) whereas $X$ would be half of the opening width of the virtual &quot;panel&quot; or &quot;board&quot; created by the opening angle of the camera in the $3D$ world (coordinate system)? So, clearly, this virtual &quot;panel&quot; or &quot;board&quot; maybe not cover the whole screen, right?" CreationDate="2017-01-14T13:48:21.087" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6326" PostId="4540" Score="0" Text="That's what I was thinking too with the spherical coordinates. You don't need roll! But getting the angles takes some trig.  However, this lets you do a ray intersection against a generic object as a table lookup so maybe the trig ops aren't a bad trade off.  Nathan Reed's idea doesn't have that though which is interesting." CreationDate="2017-01-14T14:30:11.073" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6327" PostId="4542" Score="0" Text="@nbro With the maths here, the &quot;board&quot; has to cover the whole screen, but in general, yes, they don't have to be the same. It's more usual to call this the *image plane*: the plane onto which the 3D scene is projected to create the image." CreationDate="2017-01-14T15:27:38.857" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6328" PostId="4542" Score="0" Text="The image plane can be thought as a plane in the $3D$ world, right?" CreationDate="2017-01-14T15:29:14.983" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6329" PostId="4542" Score="0" Text="@nbro Yes, that's right." CreationDate="2017-01-14T15:31:23.497" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6330" PostId="4535" Score="0" Text="Fwiw I think you are right. Using color based selection isn't going to work for this." CreationDate="2017-01-14T15:34:05.063" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6331" PostId="4542" Score="0" Text="Sorry for asking too many question, but I would like to make this thing clear, and books or articles around do not make it clear, IMHO. How would the math work if for example the _image plane_ was smaller than the screen? Another question. Is the _image plane_ in the &quot;world space&quot; or in the &quot;camera space&quot; (if this question even makes sense)? I mean, the _image plane_ coordinates are usually represented using a triple of coordinates, so I suppose they are represented in the world coordinate system..." CreationDate="2017-01-14T15:34:14.480" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6332" PostId="4542" Score="0" Text="I've a slide where we want to find the $3D$ coordinates corresponding to the $2D$ coordinates (or two indices) of a pixel... so it seems to me that the _image plane_ can be thought really as the $3D$ counter-part of the $2D$ screen..." CreationDate="2017-01-14T15:36:47.693" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6333" PostId="4542" Score="0" Text="This Wiki article https://en.wikipedia.org/wiki/Image_plane seems to have helped a little bit. Apparently the _image plane_ is also called the _screen space_, so I guess that its coordinates are defined with respect to the local coordinate system of the screen space (and not with respect to the world coordinate system). I would still appreciate if you could answer my questions." CreationDate="2017-01-14T15:46:21.087" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6334" PostId="4544" Score="1" Text="From a [dictionary](http://www.dictionary.com/browse/monitor): monitor - _the screen component of a computer, especially a free-standing screen_. So, in English these words have the same meaning? I don't know. I my native language monitor is generally a piece of hardware and screen is a flat part of the monitor onto which viewport picture is projected." CreationDate="2017-01-14T17:40:54.250" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6335" PostId="4545" Score="0" Text="So, do you think that if I talk about screen (referring to the real screen of a device) and screen space (or image plane, which is the infinite abstract geometric plane that identifies) in the same sentence, there could be ambiguities. I'm basically writing something explaining that the screen space is somehow mapped to the screen, but after reading your answer, it seems a little bit ambiguous." CreationDate="2017-01-14T18:00:53.663" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6336" PostId="4545" Score="0" Text="By the way, just to be even sure. The screen space is a $3D$ object with respect to the camera, right? I know in computer graphics people talk about the camera space (which I know it's a synonym for eye or view space), but now it's not clear the difference between camera space and screen space. I thought that the screen space was, as you said, an abstract plane on the $3D$ world, but with respect to the camera..." CreationDate="2017-01-14T18:04:27.887" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6337" PostId="4536" Score="1" Text="... although light physics is sometimes a part of the more advanced simulation. For instance, in the movie interstellar, they rendered the black hole based on what we know of black hole physics and actually ended up publishing a research paper about it!" CreationDate="2017-01-14T18:22:43.947" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6338" PostId="4546" Score="0" Text="R11F_G11F_B10F is an RGB format." CreationDate="2017-01-15T01:44:27.053" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6339" PostId="4546" Score="0" Text="Keep in mind that the docs don't list all possible formats. There are often vendor-specific formats, and formats that came after the docs were written." CreationDate="2017-01-15T03:04:50.213" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6340" PostId="4545" Score="0" Text="&quot;However, a built-in display such as the one in a laptop, tablet, or phone is never called a monitor.&quot; That is not true. The display on a laptop, tablet or phone is sometimes referred to as a monitor in casual parlance in US English, at least." CreationDate="2017-01-15T03:06:47.597" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6341" PostId="4511" Score="0" Text="Note that PAL and NTSC are a 4:3 aspect ratio (despite NTSC seeming like 3:2  - it has non-square pixels at 720x480). 32x16 is 2:1, so you will either be stretching the video, or pillar boxing it. In my opinion, pillar box is far better, but given the abstractness of your description, maybe you'd prefer to stretch it?" CreationDate="2017-01-15T03:34:38.620" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6343" PostId="4552" Score="0" Text="Offcourse it is possible to unmult this  apha if one so wishes. Im a bit unsure about whether this answers the question though, incredibly vague as it is." CreationDate="2017-01-15T07:56:33.693" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6344" PostId="4545" Score="0" Text="@nbro screen space and camera space are usually different things." CreationDate="2017-01-15T08:03:24.053" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6345" PostId="4552" Score="0" Text="@joojaa It isn't quite as straightforward as you have to be certain to preserve unpremult values at RGB 0.0. The only time this is required is on colour operations, otherwise associated alpha is always the correct format." CreationDate="2017-01-15T08:11:15.523" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="6346" PostId="4553" Score="0" Text="I did check out the presentation, it said 'omission of effects results in images a little bit too dark' I'm a little curious as to what that really means." CreationDate="2017-01-15T09:17:54.053" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6347" PostId="4552" Score="0" Text="Sure, but alpha in images is a problematic concept in many situations anyway." CreationDate="2017-01-15T09:47:06.293" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6348" PostId="4546" Score="0" Text="@NicolBolas Yeah because it has 32 bits overall, now I see it." CreationDate="2017-01-15T10:19:44.533" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6349" PostId="4553" Score="0" Text="It means several things: One is that path tracing cannot sample all possible light/scene interactions (see section 8.3 of Eric Veach's thesis). The other is that for production purposes, scenes are often rendered without caustic paths and a limited number of bounces (omitting those paths can significantly reduce noise). Terminating those paths removes light from the final result, making it darker than it would be if all light transport paths were calculated." CreationDate="2017-01-15T11:29:40.427" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="6350" PostId="4545" Score="0" Text="@joojaa But objects in both spaces are represented with $3$ coordinates, even though in the case of screen space usually $z$ is fixed, e.g., to $1$ (or $-1$), am I right?" CreationDate="2017-01-15T11:46:37.853" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6352" PostId="4557" Score="3" Text="Discouraging people from experimenting in graphics and putting it on par with shipping hand crafted crypto algorithms is ridiculous." CreationDate="2017-01-15T19:34:55.247" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6353" PostId="4557" Score="0" Text="@AlanWolfe Given that there are quite a few extremely brilliant PhD types out there that have already spent countless hours and effort solving the problems in the original question, I find your context of ridiculous ridiculous. Not to discourage one from filling their boots and attempting to reinvent the wheel." CreationDate="2017-01-15T20:51:54.340" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="6355" PostId="4557" Score="0" Text="You should hear the simple hacks recommended by active graphics researchers. Such as &quot;dot product RGB, it really does work amazingly well&quot; from Peter Shirley." CreationDate="2017-01-15T20:56:23.097" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6356" PostId="4557" Score="0" Text="@AlanWolfe I am sure there are some. Sadly the use of IPT and JCh for example, are almost trivial and yield remarkably good results. I edited the post and comment as I realized it read as pure snark. It wasn't intended as such. Dot product is also colour space dependent sadly, and it too fails." CreationDate="2017-01-15T20:58:05.607" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="6357" PostId="4557" Score="0" Text="Apologies if my response was also snark. Thanks for the info Troy!" CreationDate="2017-01-15T21:02:01.033" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6358" PostId="4557" Score="0" Text="@AlanWolfe I would be interested in the context of what you are trying to solve. There is a tremendous amount of top shelf research going on at the moment, specifically relating to game programming and motion picture work. I might be able to point to a more specific bit of code." CreationDate="2017-01-15T21:03:17.003" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="6359" PostId="4557" Score="2" Text="I've moved on but there's application of wave function collapse for procedural image and content creation. It works in part by exact matching pixels so works best with pixel art. I was looking at seeing it be able to do softer matching for use with more realistic images, or for less strict procedural content rules. Check out this link for the basic thing: https://github.com/mxgmn/WaveFunctionCollapse" CreationDate="2017-01-15T23:04:42.363" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6360" PostId="4557" Score="0" Text="@AlanWolfe Wow. I have already seen your work. It is pretty incredible. Would be remarkable to see you extend it." CreationDate="2017-01-15T23:25:11.960" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="6361" PostId="4557" Score="2" Text="It's not my work but I was trying to extend it. I totally agree, it's cool stuff!  Off topic but here's my unrelated work hehe. http://blog.demofox.org/2016/02/22/gpu-texture-sampler-bezier-curve-evaluation/" CreationDate="2017-01-15T23:26:58.830" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6362" PostId="4531" Score="0" Text="@Nicol, Thanks for this hint! I forgot to mention, that you have to include the extension. I will edit my answer." CreationDate="2017-01-16T07:26:44.073" UserId="5399" ContentLicense="CC BY-SA 3.0" />
  <row Id="6363" PostId="4556" Score="0" Text="I'm voting to close this question as off-topic because it doesn't seem to be about computer graphics, but rather video/image/signal processing." CreationDate="2017-01-16T14:04:26.587" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6364" PostId="4558" Score="0" Text="&quot;*Swapping buffers doesn't really copy anything these days.*&quot; I wouldn't be too sure. Lots of implementations swap via copy, especially if your application is windowed." CreationDate="2017-01-16T16:13:36.873" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6366" PostId="4558" Score="0" Text="@NicolBolas Well OK, if you're going into a compositor, there will probably be a copy *somewhere* along the line. I was more thinking of the case where the output is going into the display controller directly. My point is that OP was thinking that each `eglSwapBuffers` or equivalent is pausing while it copies a bunch of data, but the problem is something completely different." CreationDate="2017-01-16T16:43:50.140" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6367" PostId="4556" Score="2" Text="I think it's an interesting question. Essentially temporal aliasing problem which is too often ignored by CG people. There are a huge number of algorithms out there which look cool on still frames but break down when you start animating input parameters." CreationDate="2017-01-16T19:20:57.480" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6368" PostId="4556" Score="1" Text="I meant quantization, not aliasing, but I still think it's worth discussing why this fails to be smooth and what can be done about it besides the obvious &quot;get a toolkit with the proper floating point zoom support&quot;." CreationDate="2017-01-16T19:30:47.860" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6369" PostId="4558" Score="0" Text="You only explained what I already knew.&#xA;My question is, Why does my screen slow when I do not swap the buffers?" CreationDate="2017-01-16T19:48:21.557" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="6370" PostId="4558" Score="0" Text="Lol I tried my code again and now it doesn't slow down the screen. Anyway, thanks for all." CreationDate="2017-01-16T20:16:31.603" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="6371" PostId="4563" Score="2" Text="I have not more than 15 score points so I cannot vote up your awesome answer. Thanks!!!" CreationDate="2017-01-17T00:26:54.780" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="6372" PostId="4563" Score="1" Text="No worries. Glad I could help." CreationDate="2017-01-17T00:27:52.163" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6373" PostId="4564" Score="1" Text="The MIP map level is the (base 2) logarithm of the scaling of the texture and so the top level map will also get all the &quot;negative&quot; values, i.e., when magnification of the texture is occurring, so it's not that &quot;unfair&quot; :-)&#xA;&#xA;Further, the aim of MIP mapping is primarily to eliminate aliasing though at the risk of over filtering. Choosing ceil thus aims for &quot;no aliasing&quot; but, having said that, you should be able to set a LOD bias to shift this to suit your taste." CreationDate="2017-01-17T12:00:42.780" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6374" PostId="4321" Score="0" Text="There is a whole Python package for color science that includes some transformations: http://colour-science.org/ ." CreationDate="2017-01-17T14:53:41.950" UserId="5870" ContentLicense="CC BY-SA 3.0" />
  <row Id="6375" PostId="4556" Score="0" Text="We have a [tag for image-processing](http://computergraphics.stackexchange.com/questions/tagged/image-processing), and there is support for the place of image processing in our scope on Meta [here](http://meta.computergraphics.stackexchange.com/questions/38/is-this-site-only-about-3d-topics) and [here](http://meta.computergraphics.stackexchange.com/questions/9/how-do-we-draw-the-line-between-questions-that-are-appropriate-for-computer-grap)." CreationDate="2017-01-17T15:06:35.160" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6376" PostId="4565" Score="0" Text="What have you tried so far? If you describe what you already know how to do we'll have more chance of giving a relevant answer. For example, are you more stuck on how to solve this mathematically, or how to express it in JavaScript?" CreationDate="2017-01-17T16:07:17.027" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6377" PostId="4565" Score="1" Text="Oh, thanks. I'm a competent JavaScript programmer. It's the math. I'll put what I've tried in the question so it's easier to read." CreationDate="2017-01-17T16:11:06.143" UserId="5871" ContentLicense="CC BY-SA 3.0" />
  <row Id="6378" PostId="4565" Score="2" Text="What you're after is [vector projection](https://en.wikipedia.org/wiki/Vector_projection). Do you understand basic vector math? If not, I'm sure someone can break it down to the the basic operations." CreationDate="2017-01-17T16:23:03.913" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6380" PostId="4569" Score="1" Text="Just checking: do you have updated graphics drivers installed? And you're not using remote desktop or anything? Do other graphics applications work (e.g. 3D games) on the machine?" CreationDate="2017-01-17T21:47:08.967" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6381" PostId="4565" Score="0" Text="I don't understand vector math, but it looks like the code I'm using actually works. I think the problem I'm having is elsewhere." CreationDate="2017-01-17T22:03:15.633" UserId="5871" ContentLicense="CC BY-SA 3.0" />
  <row Id="6382" PostId="4569" Score="0" Text="I very recently updated the NVIDIA drivers (version 376.62). I'm not using a remote desktop, and it's a work machine so I don't know if other programs work. The program I'm trying to run does display properly at first, but then crashes." CreationDate="2017-01-17T22:27:38.867" UserId="5874" ContentLicense="CC BY-SA 3.0" />
  <row Id="6383" PostId="4565" Score="0" Text="Your $y_p$ and $x_p$ look reversed from what I would expect.  Is that intentional? If not, that might be the source of whatever problems you are having." CreationDate="2017-01-17T23:51:20.963" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6384" PostId="4565" Score="0" Text="Math.atan2 takes (y,x) as arguments: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/atan2/. Is that what you mean?" CreationDate="2017-01-18T01:37:35.317" UserId="5871" ContentLicense="CC BY-SA 3.0" />
  <row Id="6385" PostId="4295" Score="0" Text="@MBReynolds In a mathematical sense, normals are as _vectors_ as points or directions. The problem here is that the transformations that we apply to the points of a surface to transform them do not apply to the normals." CreationDate="2017-01-18T13:05:23.133" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6386" PostId="4295" Score="2" Text="surface normals are bivectors, not vectors.  We can find a normal by the cross product of two vectors, the result is a bivector.  SEE Per Vogensen's: https://gist.github.com/pervognsen/c6b1d19754c2e8a38b10886b63d7bf2d" CreationDate="2017-01-18T13:55:49.310" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="6392" PostId="4559" Score="0" Text="THis answer really needs a picture" CreationDate="2017-01-19T07:49:33.577" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6393" PostId="4573" Score="3" Text="Yes you should use the BRDF when you intersect with something inside the  glass that isn't the glass. Are you going to use the same light direction or are you going to refract that too ? (as objects inside the glass are light by a refracted lightsource" CreationDate="2017-01-19T08:00:28.267" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6395" PostId="4573" Score="0" Text="I can undelete it, i skim read your question and didn't realise you already understood how to cast rays through refractive materials, so my answer is mostly redundant." CreationDate="2017-01-19T08:04:17.300" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6397" PostId="4572" Score="1" Text="Cook-torrance is popular these days, it takes micro geometry into account so attenuates differently to lambert." CreationDate="2017-01-19T08:48:24.647" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6401" PostId="4566" Score="0" Text="a quaternion is built as `quat(cos(angle/2), axis*sin(angle/2))` and you know that the dot product us equal to cos(angle) times the product of the lengths and the cross product is the axis times sin(angle) times the product of the lengths. Normalize it and you can easily get the quaternion of the same rotation but with double the angle. To get the proper quaternion you can do `slerp(unit, quat, 0.5)` which after inlining ends up as `normalize(unit+quat);`" CreationDate="2017-01-19T10:18:51.607" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6405" PostId="4572" Score="1" Text="Disney's BRDF is pretty much the top of the line as far as diffuse BRDFs go, yes." CreationDate="2017-01-19T18:30:25.120" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="6406" PostId="4508" Score="0" Text="Either I must hear overlooked the second image or you added after I typed my response. Either way, in 3D this is typically solved with Ray tracing and an appropriate spatial subdivision, and I would expect the same to work in 2D." CreationDate="2017-01-11T20:45:23.587" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="6407" PostId="4576" Score="0" Text="To make this into an answer, could you explain the algorithm?" CreationDate="2017-01-19T19:00:20.697" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6408" PostId="4181" Score="0" Text="I would have another question or doubt. You talk about a matrix $M$ that transforms an object to world space. This matrix encodes the operations I've described in my question, but in the same question I'm not talking about transforming object to world spaces or world to object spaces: I'm simply talking about transformations (encoded as matrix multiplications) applied to an object. What's confusing to me is that you're talking about these transformations from spaces to spaces, whereas in my question I'm simply applying transformations to an object..." CreationDate="2017-01-19T19:25:43.210" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6409" PostId="4182" Score="0" Text="Could you please point me to the right sources to read more about this parent-child relationships? I would like to understand this from the mathematical view point, if it makes sense, which I think it doesn't, because you talk about spaces in computer graphics in a different way than people in mathematics talk for example about vector spaces, that's I find this notation and terminology in computer graphics very ambiguous." CreationDate="2017-01-19T19:32:33.107" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6410" PostId="4182" Score="0" Text="@nbro like i said it has nothing to do with mathematics. It has to do with practical use of modeling. Its only purpose is to make all vector spaces reachable by the computert so that you the user dont have to book keep all transforms separately. Why you would want the spaces in the first place is just a question of practicality. Its a tool to make the modeling easy." CreationDate="2017-01-19T19:38:05.387" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6411" PostId="4182" Score="0" Text="But then how can I imagine a space in computer graphics? Can I imagine it as being a coordinate system defined with respect to &quot;something&quot;?" CreationDate="2017-01-19T19:40:57.540" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6413" PostId="4182" Score="0" Text="@nbro its a relationship to the the previous entry in your model tree (not a binary tree just a tree like a filesystem or a xml file), where the root is the world that contains everything." CreationDate="2017-01-19T19:43:30.130" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6414" PostId="4182" Score="0" Text="By relationship I assume you're talking about transformations (matrix multiplications in practice), since at the very end this is exactly what happens. Are you saying that a space would be a transformation with respect to the previous &quot;what&quot;? Sorry for being so pedantic, but I just would like to make this thing clear, and after having read a lot of resources, no one has been able to really make this thing clear, but maybe I just didn't read the appropriate ones." CreationDate="2017-01-19T19:47:22.153" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6415" PostId="4182" Score="0" Text="@nbro previous transformation in the chain of events, previous coo9rdinate system. You can not tell form mathematics what this thing is because its your job to describe this thing, mathematics does not know." CreationDate="2017-01-19T19:57:00.160" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6416" PostId="4182" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/52088/discussion-between-joojaa-and-nbro)." CreationDate="2017-01-19T19:58:11.470" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6419" PostId="4578" Score="0" Text="It might help to describe the reason that you need this. There are [many different centres of a triangle in 2D](https://en.wikipedia.org/wiki/Triangle_center) so I imagine there may be many approaches for a sphere surface triangle. If we know what your underlying purpose is we will have a better idea of whether &quot;average&quot; is the best approach." CreationDate="2017-01-20T01:29:34.900" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6423" PostId="4579" Score="2" Text="@trichoplax sorry about that I edited this myself a bit to fix the mistakes/typos." CreationDate="2017-01-20T08:28:10.123" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6424" PostId="4532" Score="0" Text="Please check https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/" CreationDate="2017-01-20T09:30:26.850" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6425" PostId="4523" Score="0" Text="Please check https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry" CreationDate="2017-01-20T09:30:55.617" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6427" PostId="4579" Score="0" Text="Great Answer! The Diagram and the additional explanation was great, I think since you're an author of the scratchapixel tutorials you should add this so people with the same question aren't in doubt. Also, will you guys do a photon mapping lesson? That would be really awesome!" CreationDate="2017-01-20T10:32:19.057" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6428" PostId="4523" Score="0" Text="@user18490 Yes, in this case, this question arose after reading a lesson from scratchpixel. In my opinion it's not understandable enough the explanation there." CreationDate="2017-01-20T10:32:33.570" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6429" PostId="4532" Score="0" Text="@user18490 I've read that lesson about geometry. There's nothing which talks about the concept I'm describing in this question, from what I remember." CreationDate="2017-01-20T10:38:04.227" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="6430" PostId="4466" Score="1" Text="What almost satisfy my need is this paper: Accurate and Fast Proximity Queries Between Polyhedra Using Convex Surface Decomposition.." CreationDate="2017-01-20T11:01:01.880" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="6431" PostId="4579" Score="1" Text="@Arjan. I didn't write this lesson but I will pass on your comment. Also yes we have a plan to write a lesson on Photon Mapping but only in Volume 3, we need to finish Volume 2 this year... If you wish to support this initiative please promote it around you))" CreationDate="2017-01-20T11:16:05.167" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6432" PostId="4532" Score="0" Text="@nbro that's actually true, though it's pretty clear that if you ask the question you don't understand what matrices are? so maybe you should start from there? But you are right about this website, maybe you should contact them and ask them to add this to the lesson?" CreationDate="2017-01-20T11:32:19.337" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6434" PostId="4290" Score="1" Text="In complement to all the other answers and because other people already answered this question in length elsewhere you can check:&#xA;&#xA;https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/transforming-points-and-vectors" CreationDate="2017-01-20T11:49:26.210" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6435" PostId="4579" Score="0" Text="I'd like to contribute to this project in a more meaningful way, are there any ways to contact your team? I messaged the scratchapixel facebook page but I didn't get a reply." CreationDate="2017-01-20T12:02:57.173" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6436" PostId="4583" Score="0" Text="You probably also need to know about matrix inverses ;)" CreationDate="2017-01-20T12:08:07.667" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6437" PostId="4532" Score="1" Text="@user18490 no nbro understands what matrices are he just didnt undersatand why you would have hierarchies. He is looking at it form very mathematical perspective. There is no such concept in nonapplied mathematics. Having chains of transforms makes only sense for modeling reasons." CreationDate="2017-01-20T12:09:59.790" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6439" PostId="4584" Score="0" Text="Are you looking for pairs of (point, face) that are separated by less than x, or a list of all points and faces that are less than x from some point/face on the other patch?" CreationDate="2017-01-20T14:12:59.767" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6440" PostId="4584" Score="0" Text="Should be the later." CreationDate="2017-01-20T14:17:09.473" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="6441" PostId="4579" Score="0" Text="Really? I will let them know. I think we have a contact email at the bottom of the website's home page)" CreationDate="2017-01-20T15:48:21.557" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6442" PostId="4586" Score="0" Text="Thank you, I do understand the difference between the two and I understand how they both work, I was just curious as to whether such a thing was exclusive" CreationDate="2017-01-20T16:00:09.673" UserId="5887" ContentLicense="CC BY-SA 3.0" />
  <row Id="6443" PostId="4586" Score="0" Text="So it's not))) now I hope you know." CreationDate="2017-01-20T16:00:35.163" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6444" PostId="4585" Score="1" Text="I think that user18490 answered the question quite well, but just in case you would like to see some actual examples, here are two tutorials that do reflection without ray casting or ray tracing. They used OpenGL for it.   https://www.youtube.com/watch?v=GADTasvDOX4     and    https://www.youtube.com/watch?v=xutvBtrG23A     basically one renders the reflection out through a camera that is positioned mirror like and the other uses cube maps for reflections. Cube maps can be rendered out for every frame to be able to get reflections of the actual dynamic scene. Hope that helped a bit!" CreationDate="2017-01-20T16:07:00.340" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="6445" PostId="4587" Score="0" Text="This is only one technique though among others. But the question is more fondamental. The fact is you can also use rasterisation to &quot;solve&quot; for visibility between a shaded point and the rest of the scene for reflection &quot;rays&quot; if you really want to. This is highly ineffective but possible. So as the OP said in his comment &quot;they are not exclusive&quot;." CreationDate="2017-01-20T18:02:48.117" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6446" PostId="4589" Score="0" Text="But you miss the fondamental point to some extent. Cube maps/planar reflections are not accurate nor are most of the &quot;tricks&quot; that are used with rasterisation to mimic reflections/refractions. The point is **you can** compute true reflections/refractions using rasterisation, not fake ones. You can use a technique similar to radiosity cube maps for instance, and I believe that's what the OP needs to know." CreationDate="2017-01-20T21:48:12.960" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6447" PostId="4589" Score="2" Text="@user18490 You seem to be taking a very narrow interpretation of the question, focusing purely on whether perfect reflections are theoretically possible, regardless of practicality. I'm coming at it from a different angle: techniques useful in production real-time graphics." CreationDate="2017-01-20T23:22:04.947" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6448" PostId="4591" Score="0" Text="Thank you for your reply Nicol. I`ve fixed the argument mistake and found out the root cause. I have specified 'transformation' as my uniform value and was trying to use 'transform'.... It works llike a charm now." CreationDate="2017-01-21T00:39:31.387" UserId="5890" ContentLicense="CC BY-SA 3.0" />
  <row Id="6449" PostId="4586" Score="0" Text="If they are useful and relevant to the question, feel free to include specific links. It's only [link only answers](http://meta.stackexchange.com/a/8259/258647) that are problematic." CreationDate="2017-01-21T11:15:37.290" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6450" PostId="4588" Score="0" Text="Can this be extended to give a list of all faces that are closer than x to the other mesh, or will it only work to give a single pair of faces?" CreationDate="2017-01-21T11:20:42.250" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6451" PostId="4586" Score="0" Text="I think these 2 links to scratchapixel tutorials will be good on learning about the differences about Rasterization and Ray Tracing. https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-overview" CreationDate="2017-01-21T13:22:33.673" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6453" PostId="4593" Score="1" Text="`glGetIntegerv(GLFW_CONTEXT_VERSION_MAJOR` I'm pretty sure that doesn't work. OpenGL doesn't know what to do with `GLFW_CONTEXT_VERSION_MAJOR`." CreationDate="2017-01-21T18:43:27.343" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6455" PostId="4589" Score="0" Text="]-) yes because i think this is what the OP wants to know. But I think your answer is complementary so it's good. The question is whether you can computer &quot;accurate&quot; reflection/refractions with a given algorithm. I would say that if it's not possible with a given visibility algorithm, unless you fake it (aka reflection look like reflections but are not correct geometrically, physically) then I would say this algorithm can not handle things such as reflection/refractions, which is the OP's questions." CreationDate="2017-01-22T09:32:51.783" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6457" PostId="4597" Score="0" Text="There are plenty of articles out there on the web on initialising of lines for example http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter22.html. This was a very popular topic in the 80's so you might find good articles in books like Graphics Gem. It's more a matter of Google, searching and reading yourself. But that's a VERY well document topic." CreationDate="2017-01-22T09:36:54.640" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6458" PostId="4600" Score="0" Text="Could you link the other several books?" CreationDate="2017-01-22T10:22:11.803" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6459" PostId="4597" Score="1" Text="@user18490 its a very well documented topic, only many of those documents contain systematic flaws and assuptions. So it has a lot of documentation but lot of it is simply misleading." CreationDate="2017-01-22T13:27:15.337" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6460" PostId="4596" Score="0" Text="[Discussion on meta about cross site duplicates and whether they are a problem](http://meta.computergraphics.stackexchange.com/questions/250/how-do-we-deal-with-duplicates-of-so-questions)" CreationDate="2017-01-22T14:26:03.410" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6461" PostId="4599" Score="0" Text="Although recommendations are off topic on the main site (as they are on nearly every Stack Exchange site), you are welcome to discuss recommendations in [chat]." CreationDate="2017-01-22T14:36:26.430" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6462" PostId="4596" Score="1" Text="If you have additional questions, please post them as new questions rather than editing them into an existing one. This way the answers to separate questions can be compared independently. You can still include links between the questions if they are related." CreationDate="2017-01-22T17:39:32.763" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6463" PostId="4597" Score="0" Text="@jooja, I respect your opinion but if there is a topic on which there is a lot of  information it is on the topic of line anti-aliasing. It is one of the first topics that was ever researched in CG and you will find algorithms for doing this specially that were developed in the 70s/80s. I pointed the OP to Graphics Gems books in which he/she will find information. On such basic questions you have to encourage OPs to search on Google before asking such questions on here (and show they researched the topic before coming here). Maybe you can also write an answer)" CreationDate="2017-01-22T19:04:59.580" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6464" PostId="4597" Score="2" Text="@user18490 point is that lot treatises of computer graphics neglect to define scope and limits of their assumptions. So for example many coverage based calculations forget to mention 1. That you can not assume box filter being the best possible filter. 2. Algorithm is only right is you draw one line/curve but they do not stack up correctly (leading to the conflation issue many vector engines exhibit, even tough we have known of the cause since 1980's), 3. Your output  frame buffer is not linear. etc etc." CreationDate="2017-01-22T19:21:40.667" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6466" PostId="25" Score="0" Text="I still use gDEbugger from time to time for older GL versions. CodeXL is the more modern version of it and can be found here: https://github.com/GPUOpen-Tools/CodeXL&#xA;Note that there's a comprehensive list of graphics debugging tools on the apitrace page here: https://apitrace.github.io/#links" CreationDate="2017-01-23T13:52:24.060" UserId="5908" ContentLicense="CC BY-SA 3.0" />
  <row Id="6467" PostId="4601" Score="0" Text="This solution is kinda obvious, and far from the question asked. Goal of the question is to find a solution that operates to the *uncombined* tranform components, before creating any model matrix." CreationDate="2017-01-23T14:56:55.413" UserId="5855" ContentLicense="CC BY-SA 3.0" />
  <row Id="6468" PostId="4601" Score="0" Text="But you can apply this solution to uncombined transform components.&#xA;You can of course simplify it, e.g. change sign in position instead of multiply it by matrix but idea is the same.&#xA;Applying it to model matrix will apply it also to mesh vertices and you don't need to change models." CreationDate="2017-01-23T15:03:59.267" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="6469" PostId="4601" Score="1" Text="Of course if you apply this matrix to scale you don't need to worry about meshes but scale like `(1,1,-1)` isn't intuitive." CreationDate="2017-01-23T16:26:18.133" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="6470" PostId="4569" Score="0" Text="Can anyone help me out with this?" CreationDate="2017-01-23T17:55:42.033" UserId="5874" ContentLicense="CC BY-SA 3.0" />
  <row Id="6471" PostId="4603" Score="0" Text="`If you are truly normalizing the data, the only cause of overflow I can think of is due to rounding issues which can definitely happen.` Yeah, but  If I put value 400 to this formula (which can happen when I add a high number to the image) I get a normalized value of 555 which is not in the expected range :D There's something wrong with these extremes O.o" CreationDate="2017-01-23T18:52:40.677" UserId="5910" ContentLicense="CC BY-SA 3.0" />
  <row Id="6473" PostId="4601" Score="0" Text="Modifying mesh vertices for some simple transformation doesn't seem a good idea, seems more like it will complicate things to a new level." CreationDate="2017-01-23T18:55:58.677" UserId="5855" ContentLicense="CC BY-SA 3.0" />
  <row Id="6474" PostId="4603" Score="0" Text="No problem.  Let's say that after adding 400 to each channel of each pixel, the minimum value seen in any channel is 400 and the maximum is 555.  You would then run through every channel value of every pixel and apply the normalization formula.&#xA;&#xA;Output = (Input - 400) / (555-400)&#xA;&#xA;If you plug an input value of 400 into that equation you get an output of 0.  If you plug an input value of 555 into that equation you get an output value of 1.  For values between 400 and 555 you'll get values between 0 and 1.&#xA;&#xA;This result should then be multiplied by 255 to make it [0,255]." CreationDate="2017-01-23T18:56:10.043" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6475" PostId="4603" Score="0" Text="Now you've said something complately opposite to your answer, cause you've said that I should use the old image's extremes and now, that new ones :D But according to your explanation - it works, I just tested it (swapped so the app takes extremes from the new image). Strange results, cause the image looks a little bit brighter, even if I added 1000 to it. Will test some and give some time for the question cause I really need to be sure that this is the correct way. There's no talking with my stubborn teacher, he can't even explain what he meant." CreationDate="2017-01-23T19:04:51.223" UserId="5910" ContentLicense="CC BY-SA 3.0" />
  <row Id="6476" PostId="4603" Score="1" Text="I think the confusion is coming from the addition and the normalization being seen as a single operation.  it's really two operations.&#xA;1) Add 400 to each channel in a pixel.  You will need the pixel values to be in some storage type which can handle values outside of the 0-255 range. Floats could be a good quick and easy choice. Doubles if you need precision. Fixed point if you need speed.&#xA;2) Now do the normalization step. Find min and max values, normalize all values (all channels of all pixels) muliply by 255.&#xA;3) A final third pass does a clamp and conversion from float back to uint8." CreationDate="2017-01-23T19:13:52.507" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6477" PostId="4603" Score="0" Text="That's what I'm doing right now. I read the first image, store it in the array A, now, I add a value 100 to each pixel, now I copy the array to array B and save array A to see results (before normalization). I find extremes on array B, normalize array B, clamp it and save it as the &quot;after&quot; image. Here's a link to all three images: https://drive.google.com/open?id=0B9DujHxzmzAySl9SU2VKNjhsNDA&#xA;&#xA;I have them in PCX format cause I chose it as a working for lessons (it's  the least complicated one :D). You'll need a photoshop or pcx viewer to view it, but you probably know that ;D" CreationDate="2017-01-23T19:29:12.553" UserId="5910" ContentLicense="CC BY-SA 3.0" />
  <row Id="6478" PostId="4601" Score="0" Text="When I think about it now, maybe I was wrong. With translation and rotation like this meshes should look alright and no changing in culling should be needed." CreationDate="2017-01-23T21:23:09.497" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="6482" PostId="4578" Score="0" Text="Could you detail a little more what the context is (why you are trying to do this), what your constraints are (for example, do you know what the sphere center and radius are, or do you only have the triangle?) and what formula you are using (random floating points values are a lot harder to read than equations)?" CreationDate="2017-01-25T03:42:34.323" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6483" PostId="4606" Score="1" Text="dupe of one of the following: http://computergraphics.stackexchange.com/q/2508/137 http://computergraphics.stackexchange.com/q/259/137 http://computergraphics.stackexchange.com/q/280/137" CreationDate="2017-01-25T09:13:16.710" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6487" PostId="4603" Score="0" Text="@Spectre It might be smarter to just display both images and screenshot the comparison. You can host a single jpg easily. But looking at your images I think your example is correct. Note that normalizing after addition undoes the addition because you subtract `fmin`." CreationDate="2017-01-26T03:05:41.927" UserId="3204" ContentLicense="CC BY-SA 3.0" />
  <row Id="6488" PostId="4610" Score="1" Text="The effect is typically called &quot;bloom&quot;. Here's [a previous question on bloom implementation](http://computergraphics.stackexchange.com/questions/4221/bloom-in-directx), and [another related question](http://computergraphics.stackexchange.com/questions/3706/tweaking-a-glow-shader-to-make-it-look-better)." CreationDate="2017-01-27T02:26:02.647" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6489" PostId="4610" Score="1" Text="Heh, I just noticed that second one was one you posted. :) Anyway, the key to getting large bloom radius is to do repeated steps of downsampling and blurring—kind of like making a mip chain—then summing together all the blurred textures in the final pass." CreationDate="2017-01-27T02:32:40.660" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6490" PostId="4610" Score="0" Text="@NathanReed Yah! Still working on it. Using that technique got me something quite good looking but it can't run real-time. I have however added a button that freezes the game and then exports it with all the pretty post processing using that technique." CreationDate="2017-01-27T05:18:36.323" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="6491" PostId="4617" Score="5" Text="_&quot;Because traditional dithering algorithms involve giving information from each pixel to its neighbours, they don't parallelise well&quot;_  That's error diffusion dithering but there is also ordered dither which can be done independently on each pixel" CreationDate="2017-01-27T17:29:40.420" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6492" PostId="4617" Score="0" Text="oh yeah, I did think of ordered dither for the old use case of cheap alpha blending, but I don't think it's used much nowadays (for that purpose) so I decided not to include it." CreationDate="2017-01-27T17:31:46.207" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6493" PostId="4617" Score="0" Text="It's in some hardware eg mapping from 24bit to 16bit frame buffers." CreationDate="2017-01-27T17:36:21.817" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6494" PostId="4612" Score="1" Text="Is there a fast way to downscale a texture? I generally create an offscreen FBO of the smaller size and then have the most generic full-screen texture renderer render onto the smaller FBO. After that I have the original texture and a FBO with the smaller texture. Is that the proper way to do it?" CreationDate="2017-01-28T04:24:46.707" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="6495" PostId="4612" Score="0" Text="@J.Doe: That's how I do it too." CreationDate="2017-01-28T06:53:05.113" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6496" PostId="4616" Score="3" Text="Why do you think it does not store correctly? Does your `GL_DEPTH_COMPONENT` is also 32 bit float type?" CreationDate="2017-01-28T10:28:36.200" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6497" PostId="4617" Score="0" Text="@Dan Good answer) but jittering is not specific to ray-tracing. It's about randomising in a way the position of samples within the pixel space and can be used with rasterisation as well." CreationDate="2017-01-28T12:28:29.450" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6498" PostId="4615" Score="2" Text="So you are into ray tracing and you are worried about creating an OpenGL window? :)" CreationDate="2017-01-28T12:52:30.877" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6499" PostId="4618" Score="0" Text="Thanks! I think I will do this with DirectX. So first I need to load the image as a texture on a quad from a specified file path (where the image is being rendered to) and then as the image is being rendered it will show up on the screen. Will I manually need to refresh the image as it is being rendered to see the progress (unload, delete, reload &amp; repeat within DirectX) or is this unnecessary? I did this in C# with a picturebox but I got an error since I cannot read and write the file simultaneously or something like that, will the same happen here?" CreationDate="2017-01-28T13:00:08.580" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6500" PostId="4618" Score="0" Text="I'm on Windows by the way." CreationDate="2017-01-28T13:00:30.210" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6501" PostId="4612" Score="1" Text="If memory usage is not an issue, you can use summed area tables to get arbitrary sized box filtering in just two lookups. I don't have the links right here, but if I'm not mistaken, people have extended that to approximate Gaussian blur with a small series of SAT lookups." CreationDate="2017-01-28T13:00:43.373" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="6502" PostId="4618" Score="0" Text="@ArjanSingh if I understand correctly, this answer will give you somewhere to send your partial image as you create it, rather than displaying the saved file. Treat them as two separate things. Display the image pixel by pixel as you create it in memory, and save it to file at the end when it is complete." CreationDate="2017-01-28T14:00:19.057" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6503" PostId="4615" Score="0" Text="If you really don't want to work with anything except .ppm files, you could render to an array of initially zeroed pixel values in memory, and save a series of .ppm files at regular intervals (probably more like one image per row rather than one image per pixel, otherwise you'll have millions of image files in the folder). You'll then be one step closer to the approach described in the answer." CreationDate="2017-01-28T14:06:58.837" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6504" PostId="4615" Score="0" Text="@Arjan &amp; Trichoplax. Yes that's actually a good idea. I was thinking trying that one day but never got the time which is to write the file to disk and let some JavaScript if your browser checking every say 2s the file on disk, reload it display it in your browser)). It would work not sure about the efficiency probably poor but with canvas now it's easy (don't even need WebGL)." CreationDate="2017-01-28T14:30:44.460" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6505" PostId="4618" Score="0" Text="@trichoplax How can I copy the data from the image buffer to a DirectX texture? Loading, and displaying the texture isn't hard but how do I create the texture with the pixel data? Should I make this a separate question?" CreationDate="2017-01-28T14:42:34.333" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6506" PostId="4618" Score="1" Text="@ArjanSingh does sound like a separate question." CreationDate="2017-01-28T14:46:30.603" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6507" PostId="4620" Score="0" Text="If I wrote my ray tracer as a .DLL and used P/Invoke in C# to use it, could I then use the image buffer to send the image buffer to the picturebox? This is quite similar to what you said which got me thinking if I could do this with C#." CreationDate="2017-01-28T14:50:53.220" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6508" PostId="4620" Score="0" Text="Yeah probably. I'm not into c# but Visual Studio with c# libs is pretty robust IDE too." CreationDate="2017-01-28T14:52:44.720" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6509" PostId="4618" Score="0" Text="@ArjanSingh Though I have to say Arjan that they are plenty of tutorials out there on how to do that ... &quot;How can I copy the data from the image buffer to a DirectX texture? ...&quot;.  I think it is important you also look for such tutorials before asking your questions here and only ask if you really can't find... but that's a very common topic." CreationDate="2017-01-28T15:00:06.260" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6510" PostId="4620" Score="0" Text="@narthex: it's a solution but saying that dealing with Qt is easy is not so true if you are a beginner in programming. Yes QtCreator simplified things a lot but you already introduced to the OP a lot of complex concepts (like signal/slots), etc which will take time to digest. The OP is interested in learning graphic, and so learning a graphics API is more a priority than learning Qt;-))) He can do this later ... + plus keep in mind that Qt UI are drawn in OpenGL and DX)))" CreationDate="2017-01-28T15:02:33.843" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6511" PostId="4620" Score="0" Text="After looking online, I'm gonna write my ray tracer as a .DLL then use the data from the image buffer use `PictureBox.SetPixel(x, y, color)` I wanted to give my Ray Tracer a UI so writing the ray tracer in .DLL was inevitable. The OpenGL/DirectX method seems quite interesting and if the SetPixel method is too slow I'll just go with that." CreationDate="2017-01-28T15:11:18.157" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6512" PostId="4620" Score="0" Text="@user18490 OP stated that he is looking for simple solution to view his image in UI application and this is in my opinion the simplest and involves the least lines of code instead of tedious graphical API. Also  making UI application as OP posted in image not only in Qt invevitably involves learning  such concepts as GUI thread and some kind of signals or events." CreationDate="2017-01-28T15:21:53.410" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6514" PostId="4621" Score="1" Text="Needs more clarification. Are `a` and `b` intended to be points defining the line? How are the barycentric coordinates defined relative to those points? Is `point` assumed to be _on_ the line, or might it be somewhere off the line? What is the intended role of `epsilon`?" CreationDate="2017-01-29T00:04:15.003" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6515" PostId="4619" Score="2" Text="Note that the method by Jakob et al. relies on rendering of tabulated BSDF data in some specialised Fourier basis representation. For details, also refer to the corresponding [technical report](http://rgl.s3.eu-central-1.amazonaws.com/media/papers/Jakob2014Comprehensive_1.pdf).&#xA;An open-source implementation is also available in the newest, 3rd edition of [PBRT](https://github.com/mmp/pbrt-v3/). The BSDF files can be generated with [layerlab](https://github.com/wjakob/layerlab/) in Python." CreationDate="2017-01-29T00:07:57.050" UserId="1930" ContentLicense="CC BY-SA 3.0" />
  <row Id="6516" PostId="4621" Score="0" Text="yes, a and b are intended to be points defining the line. yes, How are the barycentric coordinates defined relative to those points? point can be on or somewhere off the line. epsilon is the error range" CreationDate="2017-01-29T00:25:58.703" UserId="5943" ContentLicense="CC BY-SA 3.0" />
  <row Id="6518" PostId="4617" Score="0" Text="@user18490 I know it's not specific to ray-tracing, but that was the most obvious example. I'd welcome an edit adding an unrelated example to bulk up that section." CreationDate="2017-01-29T09:01:41.177" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6519" PostId="4623" Score="0" Text="Step 1: Stop confusing the act of creating a buffer with the act of using that buffer with a VAO." CreationDate="2017-01-29T14:58:39.057" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6520" PostId="4625" Score="0" Text="Thank you. I made it so that the &quot;shared VBOs&quot; are only calling glBufferData once, then they are rebounded and set to a vertex attribute for each VAO. So I don't need to call glBufferData for 10 time with the same data:)" CreationDate="2017-01-29T16:06:53.177" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6521" PostId="4621" Score="0" Text="Would you expect the barycentric coordinates of the point to be the same if the point is projected to the line?" CreationDate="2017-01-29T17:08:25.487" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6522" PostId="4621" Score="0" Text="yes, barycentric coordinates of the point should be the same if the point is projected to the line" CreationDate="2017-01-29T20:12:19.987" UserId="5943" ContentLicense="CC BY-SA 3.0" />
  <row Id="6523" PostId="4627" Score="0" Text="Very similar question: [Mirror Reflections: Ray Tracing or Rasterisation?](http://computergraphics.stackexchange.com/q/4585/48)" CreationDate="2017-01-29T20:12:52.383" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6524" PostId="4621" Score="1" Text="Baryenctric coordinates are used for shapes likes triangles or more complex shapes. Not sure how you can define them with respect to a line? Doesn't make a lot of sense to me? It would be good if you could edit your question and explain in more details what you are looking for (what's the problem at hand)" CreationDate="2017-01-29T21:26:36.653" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6525" PostId="4626" Score="0" Text="Maybe best asking your question on Game Development Stack Exchange" CreationDate="2017-01-29T21:28:13.413" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6526" PostId="4628" Score="0" Text="I just want to make an addition to this. This article explains ray marching really well, but it uses a function to generate a height map when you give it the x and y value. You can change it so that instead of that function, it reads it from the depth buffer. http://www.iquilezles.org/www/articles/terrainmarching/terrainmarching.htm" CreationDate="2017-01-29T21:28:52.213" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="6527" PostId="4626" Score="0" Text="Good idea. http://gamedev.stackexchange.com/questions/136525/multiple-buffers-and-calling-glbuffersubdata/136526#136526" CreationDate="2017-01-30T06:54:52.577" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6528" PostId="4617" Score="0" Text="Thanks for the awesome answer @DanHulme ! Any reference to nice dithering and jittering ?" CreationDate="2017-01-30T07:35:53.400" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="6529" PostId="4617" Score="0" Text="@user18490 What would be a good example of jittering in a rasterized renderer ?" CreationDate="2017-01-30T07:38:04.350" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="6530" PostId="4617" Score="1" Text="@MatT: any renderer would implement some sort of pixel sampling as a the most common approach to oversampling (to fight the main issue that you get with point sampling which is aliasing). The idea is to create several samples in your pixel and check whether the triangles these samples are contained within the rendered triangles. Then you accumulate their results using some sort of filtering. REYES (old), OpenGL, they all offer that feature. Check seminal paper: http://graphics.pixar.com/library/StochasticSampling/paper.pdf" CreationDate="2017-01-30T10:26:23.327" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6531" PostId="4620" Score="0" Text="@Narthex: very true and I don't try to argue with you or pick up a fight)). There are strong divergence of opinion on the topic. I worked for 20 years with many engineers on developing complex software and I just point out that from my experience it is better to take the time to learn yourself the concepts your describe than just using a very complex API such as Qt that hides a lot of these things for you and yet that you have to use. I strongly disagree with the idea that these concepts are hard to learn. This is my opinion I have proven it many times in my work, but won't go on a crusade." CreationDate="2017-01-30T10:30:36.010" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6532" PostId="4628" Score="0" Text="i will keep that in mind. Thank you for taking out some time for answering." CreationDate="2017-01-30T12:56:28.400" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6538" PostId="4630" Score="0" Text="What exactly are you looking for here? This looks like Dot-graph territory, not API territory. Obviously, since Dot is just a text file, you can write it with any programming language. Are you looking for a program to create such things or a programmatic way to generate such an image?" CreationDate="2017-01-30T17:33:26.417" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6539" PostId="4621" Score="0" Text="Barycentric coordinates apply to all SIMPLICES (plural of simplex).  A line is a 1 dimensional simplex." CreationDate="2017-01-30T19:40:55.330" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6541" PostId="4617" Score="1" Text="@MatT: Wiki in fact has a good article -&gt; https://en.wikipedia.org/wiki/Supersampling" CreationDate="2017-01-30T21:51:13.847" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6543" PostId="4635" Score="0" Text="Your example is not visible. Do you work in 2D or 3D?" CreationDate="2017-01-30T21:53:01.200" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6545" PostId="4636" Score="0" Text="Thank you for this detailed answer. Good input for me to start planing my setup." CreationDate="2017-01-30T22:09:10.037" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6546" PostId="4635" Score="0" Text="Strange, I can see it. 2D. Here is another link to the example: http://bit.ly/2kIc4Wg" CreationDate="2017-01-30T22:30:51.333" UserId="5961" ContentLicense="CC BY-SA 3.0" />
  <row Id="6547" PostId="4630" Score="0" Text="I'm voting to close this question as off-topic because it is not about computer graphics. I'm am not sure what would be a good place on SE to ask it though. Also, as Nicol points out, it need to be better formulated." CreationDate="2017-01-31T03:01:23.627" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6549" PostId="4628" Score="0" Text="No problem. If the answer did satisfy you, don't forget to accept the answer. Otherwise, we can try to discuss further the question ;)" CreationDate="2017-01-31T07:06:31.467" UserId="5002" ContentLicense="CC BY-SA 3.0" />
  <row Id="6550" PostId="4638" Score="0" Text="Could you please give me a snippet code example that shows how to use one VAO for multiple VBOs? All examples I find on the internet call to glVertexAttribPointer before drawing a buffer, but if you have to call that method everytime you want to draw a buffer, then VAOs doesn't make sense." CreationDate="2017-01-31T07:36:35.220" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="6551" PostId="4640" Score="0" Text="As you imply, a plane can be represented by its normal, N, and a scalar, d. A point X is on the plane if  N.X+d = 0.  &#xA;As for inside and outside typically you define a rule, e.g., that the normal points &quot;out&quot; from the plane which just means that if N.X+d &gt; 0, then X is outside the plane, while N.X+d&lt;0, implies it's inside." CreationDate="2017-01-31T09:40:37.783" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6554" PostId="4640" Score="0" Text="Clearly my brain is not in gear this morning. Will delete." CreationDate="2017-01-31T10:24:39.147" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6555" PostId="4642" Score="0" Text="If you represent your plane by the Normal and a scalar you can avoid the unnecessary expensive of the vector subtract." CreationDate="2017-01-31T11:10:05.117" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6556" PostId="4642" Score="0" Text="@SimonF. Sure ... feel free to edit my answer and add that as a quicker solution... I just made an answer)." CreationDate="2017-01-31T11:29:02.997" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6557" PostId="4642" Score="0" Text="Given the number of typos I'm making today ( &quot;expensive of the&quot;, good grief!), I don't think I dare.  :-)" CreationDate="2017-01-31T13:52:44.580" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6558" PostId="4638" Score="0" Text="I added some code per OP's request. Hope it's ok. It will need to be approve by Nicol to show up" CreationDate="2017-01-31T16:46:02.490" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6561" PostId="4642" Score="1" Text="@SimonF - I'll do it for you..." CreationDate="2017-01-31T19:59:13.457" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6562" PostId="4631" Score="0" Text="yEd is also nice. altough in this case creating a dot file is the way to go." CreationDate="2017-01-31T20:59:38.480" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6564" PostId="4640" Score="0" Text="@Mojomo. Please accept the answer if it is satisfactory otherwise people will not make the effort to keep answering questions;-)))" CreationDate="2017-02-01T07:14:48.757" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6565" PostId="4638" Score="1" Text="@4dr14n31t0r Th3 G4m3r Note that in the code below you will need to make the call all these functions each time in the rendering loop if you render more than 1 object. Also please accept an answer if acceptable." CreationDate="2017-02-01T07:24:08.307" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6566" PostId="4611" Score="1" Text="Can you not 'monte-carlo' the material layers? E.g. Weight each layer according to their reflectivity and pick one at random based on that.  Deeper layers will need some attenuation based on the sum of absorption of all layers above them." CreationDate="2017-02-01T08:59:27.680" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6574" PostId="4642" Score="0" Text="@user18490 yes, that answers my question. Thank you" CreationDate="2017-02-01T15:58:49.493" UserId="5943" ContentLicense="CC BY-SA 3.0" />
  <row Id="6575" PostId="4635" Score="0" Text="You can also raytrace lines. Cast a ray perpendicular to line and test the lines in the scene. Like ray-tracing but in 2D. That would work and would give you accurate results." CreationDate="2017-02-01T20:03:15.827" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6576" PostId="4648" Score="0" Text="What platform? What libraries are you familiar with? Do you have any requirements or limitations in doing this? Your question as posed seems a bit too broad. Can you narrow it down a bit?" CreationDate="2017-02-02T06:20:00.547" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6578" PostId="4648" Score="1" Text="You could try SDL as that supports software rendering / direct FB access and is in some ways simpler to use. Otherwise you could stick with openGL and use texImage2D to upload your software framebuffer to the GPU." CreationDate="2017-02-02T09:38:42.257" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6579" PostId="4648" Score="0" Text="Thanks for mentioning about the SDL. And yes texImage2D can be used to put image in gpu but then what to do? Is there any function in OpenGL so that i can copy the image data directly to default framebuffer?" CreationDate="2017-02-02T09:47:06.780" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6580" PostId="4642" Score="1" Text="One thing we should point out that is, if using floating point, it is unlikely, *in general*, that you will ever get  _(vDotPlaneNormal == 0.0)_.  You may need to add a &quot;within epsilon&quot; test" CreationDate="2017-02-02T10:04:08.660" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6582" PostId="4648" Score="1" Text="On win32 you can get away with GDI, flushing memory to screen is relatively fast. Look for CreateDIBSection" CreationDate="2017-02-02T11:20:17.607" UserId="5364" ContentLicense="CC BY-SA 3.0" />
  <row Id="6583" PostId="4649" Score="0" Text="guis in games usually boil down to textured squares." CreationDate="2017-02-02T14:08:38.250" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6584" PostId="4649" Score="0" Text="But how about programs?" CreationDate="2017-02-02T14:23:33.627" UserId="5985" ContentLicense="CC BY-SA 3.0" />
  <row Id="6585" PostId="307" Score="0" Text="On your UBO link: &quot;Lastly, they can be used to share information between different programs. So modifying a single buffer can effectively allow uniforms in multiple programs to be updated.&quot;" CreationDate="2017-02-02T18:07:02.573" UserId="5254" ContentLicense="CC BY-SA 3.0" />
  <row Id="6587" PostId="4635" Score="0" Text="In your picture, Segment 2 does face a polygon of a different colour; Polygon D, but you say it should return false for Segment 2.  This implies you need to see if the path is occluded by polygons of the same colour, right?" CreationDate="2017-02-02T23:27:43.193" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6588" PostId="4654" Score="0" Text="&quot;*If I on the other hand choose OpenCL, will it be possible for other people to use my implementation without installing OpenCL on their computer?*&quot; That's just as true of OpenGL. The only difference being that GL typically comes with graphics drivers, while CL typically requires an explicit download." CreationDate="2017-02-03T01:54:14.327" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6589" PostId="4648" Score="0" Text="To supplement my answer: You will need to draw a full screen quad with texture mapping enabled, you can follow a simple texture mapping tutorial for this, it's fairly straight forward." CreationDate="2017-02-03T04:17:56.890" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6590" PostId="4635" Score="0" Text="Yes. That's another piece of it. The segment will return false if the _closest_ edge which the edge is facing is of the same type." CreationDate="2017-02-03T06:22:33.780" UserId="5961" ContentLicense="CC BY-SA 3.0" />
  <row Id="6591" PostId="4656" Score="0" Text="what if the vertex used both matrices (blended) to calculate the normal" CreationDate="2017-02-03T08:35:46.477" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6592" PostId="4657" Score="0" Text="This is not much different to the one above, is it? :-) The point is that you need to output really small alphas to keep as much of Bg as possible and rely on large scales of output Colours to compensate for the suppression that those alphas will cause to the accumulated colour. Making sure that the denominator is not zero as well." CreationDate="2017-02-03T09:02:00.963" UserId="270" ContentLicense="CC BY-SA 3.0" />
  <row Id="6594" PostId="4654" Score="0" Text="Why was my question downvoted?" CreationDate="2017-02-03T18:27:44.983" UserId="5989" ContentLicense="CC BY-SA 3.0" />
  <row Id="6595" PostId="4654" Score="0" Text="@NicolBolas That may be true. So what frameworks do media players and other video software usually use to accelerate video rendering?" CreationDate="2017-02-03T18:34:35.123" UserId="5989" ContentLicense="CC BY-SA 3.0" />
  <row Id="6596" PostId="4657" Score="0" Text="There is a difference. Note that the alpha is multiplied by the color in the numerator and that fixes the problem" CreationDate="2017-02-03T20:03:52.660" UserId="5881" ContentLicense="CC BY-SA 3.0" />
  <row Id="6597" PostId="4661" Score="0" Text="Thanks for taking out some time for answering the question!" CreationDate="2017-02-04T17:54:24.943" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="6598" PostId="4662" Score="0" Text="The full project is at https://github.com/kourbou/pythree-gtk. The lecture I used was [here](https://www.youtube.com/watch?v=mpTl003EXCY)." CreationDate="2017-02-04T20:46:23.770" UserId="5998" ContentLicense="CC BY-SA 3.0" />
  <row Id="6599" PostId="4661" Score="0" Text="@Ankit np, the rules are that you accept this as an answer if you do;)" CreationDate="2017-02-05T10:29:51.077" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6600" PostId="4659" Score="0" Text="I am not too sure to understand your question. SH coefficients assuming you rendered them in a pre-pass already encode visibility information (assuming you are talking about the SH coefficients computed at the vertex of your objects, not the SH coefficients of your env map for example). So are you asking how to convolve the SH coeffs from the env map with the SH coeffs from the vertices?" CreationDate="2017-02-05T10:33:48.910" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6601" PostId="4659" Score="0" Text="@user18490 Yeah convolve two sets but nevermind, why would I need that actually,  reconstruction and multiplication will work." CreationDate="2017-02-05T10:49:26.050" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6603" PostId="4662" Score="0" Text="Please refer to https://www.scratchapixel.com/lessons/3d-basic-rendering/get-started and https://www.scratchapixel.com/lessons/3d-basic-rendering/perspective-and-orthographic-projection-matrix. There is ample information on the perspective projection process out there (and helas debugging people's program is not what a forum is about). Eventually that's teacher's jobs)))" CreationDate="2017-02-05T11:01:28.473" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6604" PostId="4670" Score="0" Text="That's one of the problems you have. If you try to debug code without knowing what the library you use do, then you will not be able to fix much. Use libraries you either understand or write your own code for this." CreationDate="2017-02-05T12:39:50.210" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6605" PostId="4670" Score="0" Text="@user18490 I wrote my own code... I just had to translate my code from Java into Python here and I don't use `numpy` specifically." CreationDate="2017-02-05T12:41:09.183" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6606" PostId="4670" Score="0" Text="Please check the reference I provided you. Do you understand the difference between column and row-major order matrices? Also how can you tell the matrix you give above is column-major?" CreationDate="2017-02-05T12:44:44.427" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6607" PostId="4669" Score="0" Text="I'm not familiar with Maya, so I wouldn't know what it looks like there. If it's relevant to you, I wrote the code to load Wavefront models exported from Blender; they load/render fine, so there's no issue there. I did mention that I have a working look-at matrix applied to the camera (which needs 3 parameters, not 2: `eyePosition`, `targetPosition`, and `upDirection` vectors). The transformation is actually contained in a `SceneNode` that the `Camera`, or `Dog`, is attached to, hence the question for a way to make it work regardless of which one needs to 'look' at some specified direction." CreationDate="2017-02-05T12:50:09.070" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6608" PostId="4670" Score="0" Text="@user18490 I'm going over the reference (it's 5am here, though), which is trying to explain how a view-matrix works, but that should be kept in the *other* question... don't mix stuff up cross-questions. Also, I understand the difference between row and column-major matrices... not sure what you're trying to get at?" CreationDate="2017-02-05T12:52:29.397" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6609" PostId="4669" Score="0" Text="I already replied to your question. Take the dog eye as the `eyePosition` of your `lookout`  method. If your camera is looking the other way around then apply a negative scale along the camera z-axis. There is no reason one or the other solution should not work. Again if you understand what the lookAt method works, you wouldn't ask the question;-) So do you understand what it does?" CreationDate="2017-02-05T12:55:15.577" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6610" PostId="4670" Score="0" Text="I don't know what you mean by mixing stuff. You have code that doesn't work. You say this matrix you provide (in form of code) is column-major. I tell you 'no' it's not. The reference I pointed you to, explains how a perspective matrix is actually built, which is what you are trying to do. The fact that it's 5am your place is not relevant to the question: tare the time to digest the theory (before posting anything else). Then it will help you fix your code." CreationDate="2017-02-05T12:59:18.693" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6611" PostId="4669" Score="0" Text="I *am* using the dog's  eye as `eyePosition` when trying to turn it towards the target position. I'm also using camera position as `eyePosition` when creating a separate look-at for the camera. However, the logic that allows the camera to look in the correct direction (i.e. face the target) leaves the dog facing in the opposite direction; the dog is not the camera position. The camera is looking at the dog which is trying to look at something else. If I scale the matrix, then the camera looks in the opposite direction I intend it to." CreationDate="2017-02-05T13:03:55.553" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6612" PostId="4670" Score="0" Text="@user18490 You're coming across as if you thought I had posted this question. The code I based this answer on actually does work." CreationDate="2017-02-05T13:46:32.437" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6614" PostId="4670" Score="0" Text="sorry I got you and the OP mixed up indeed. Though I see that now you have corrected the matrix to be column-major ordered;-)))" CreationDate="2017-02-05T14:00:28.367" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6615" PostId="4670" Score="0" Text="@user18490 Yes, it had been a translation typo. See why 6am *is* relevant? ;)" CreationDate="2017-02-05T14:01:58.877" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6616" PostId="4670" Score="0" Text="All understood." CreationDate="2017-02-05T14:02:31.187" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6617" PostId="4670" Score="0" Text="Alright thank you @ray ! It works perfectly. I have no idea why the lecture had column-major matrices. I ended up using your matrix." CreationDate="2017-02-05T16:05:30.137" UserId="5998" ContentLicense="CC BY-SA 3.0" />
  <row Id="6618" PostId="4671" Score="0" Text="Yes, I'm sorry. I'm not actually in a Computer Graphics class, just trying to teach myself. May I ask why this matrix (from the reference) uses -f/(f-n) and -f*n/(f-n)? Do you end up with the same results with the matrix @ray gave?" CreationDate="2017-02-05T16:08:07.080" UserId="5998" ContentLicense="CC BY-SA 3.0" />
  <row Id="6619" PostId="4665" Score="1" Text="Thank you! That makes sense. Intuitively I knew it was required but I didn't recognize that it was an optimization." CreationDate="2017-02-05T16:38:18.003" UserId="6001" ContentLicense="CC BY-SA 3.0" />
  <row Id="6620" PostId="4671" Score="0" Text="@Kourbou: doesn't the matrix look the same to you? I you learn computer graphics it is important you learn how to read equations (as well as code)" CreationDate="2017-02-05T17:25:03.033" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6621" PostId="4665" Score="5" Text="Just to make sure this is explicit.. not only is cosine weighted hemisphere an optimisation because it takes fewer instructions, it's also an optimisation because it converges more quickly. It takes fewer samples to get a better result. This is a form of importance sampling." CreationDate="2017-02-05T21:06:42.837" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6622" PostId="4665" Score="0" Text="Exactly - that was my desire as I'm trying to reduce the number of samples for indirect calculations in my lightmapper." CreationDate="2017-02-05T22:30:06.860" UserId="6001" ContentLicense="CC BY-SA 3.0" />
  <row Id="6627" PostId="4655" Score="0" Text="Thanks. So do you mean that scatter-gather algorithms work better in OpenCL than in OpenGL?" CreationDate="2017-02-06T07:42:54.883" UserId="5989" ContentLicense="CC BY-SA 3.0" />
  <row Id="6629" PostId="4655" Score="0" Text="Which part of the algorithm don't you think would be well suited for an OpenGL shader? The algorithm uses interpolation by combining the pixels in a small neighborhood linearly with coefficients that it gets from a lookup table. The table key is obtained by looking at the strength, angle and spread respectively of the gradients in a local neighborhood and combining the quantizations of those values. It also calculates the local binary patterns feature." CreationDate="2017-02-06T07:58:19.903" UserId="5989" ContentLicense="CC BY-SA 3.0" />
  <row Id="6630" PostId="4670" Score="0" Text="@kourbou In mathematical texts I've come across, the &quot;standard&quot; convention has always been to use column-major orderings. What's really important (I think) is to be aware of which convention the systems you're using have adopted, be aware of the differences in expected results, and to be consistent (i.e. don't mix up row/column-major orderings)." CreationDate="2017-02-06T11:02:24.810" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6631" PostId="4669" Score="0" Text="I've made several updates to the question, which I think now does a better job at explaining the issue, removing ambiguity by showing the code I've been using, and showing the reference on which it was based, etc. I did go over your reference, but it explains the same concepts. I did spot what looks like a typo: it says &quot;(mind the direction of this vector: it is `To − From` not `From − To`)&quot;, but then you go on to write the line of code  `Vec3f forward = Normalize(from - to);`, which contradicts what you were trying to clarify. Let me know if you still think my question can be improved." CreationDate="2017-02-06T12:07:12.190" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6632" PostId="4669" Score="0" Text="I replied in the edit." CreationDate="2017-02-06T12:24:22.183" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6633" PostId="4669" Score="0" Text="Thanks for replying quickly. I'm wondering if my updated post clarified anything? Your edit seems to only be about what I initially thought could be was a potential typo. My look-at matrix is meant to be used with OpenGL. Does my look-at matrix look incorrect to you? From what I understand in what you insisted so much that I read (which is not the first article/reference I had consulted), it should work just fine regardless of whether the `eyePosition` is that of the `Dog` or the `Camera`, which is what I always understood to be the case, until I ran into the issue I'm trying to solve." CreationDate="2017-02-06T12:32:28.203" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6634" PostId="4669" Score="0" Text="Just saw your other edit; the book reference is explicitly states that it's using column-major vectors and matrices, not row-major." CreationDate="2017-02-06T12:32:52.820" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6635" PostId="4669" Score="0" Text="@Ray. Then if they use column-major ordering the way the matrix is written down is just plain wrong. This is not a good book. Can you point the reference please?" CreationDate="2017-02-06T12:34:01.213" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6636" PostId="4669" Score="0" Text="Well, I would've expected the `side` vector to be the left-most column only, not the top row (i.e. the transpose of that is what I would've expected), but I don't see the reference stating that it was transposing the pictured matrix. It could've been a typo or something in its errata." CreationDate="2017-02-06T12:37:13.863" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6637" PostId="4669" Score="0" Text="@Ray: why are the vector for the camera coordinate system in row form and the translation part in column form? If it's column-major matrix vectors should be written as **columns**. I gave you source code. That's how it's done in OpenGL. Many things can go wrong in your code, so follow this example and you will get it right ... code uses row-major matrices. The matrix from your ref is ALL wrong. It writes the vectors of the coordinate system in row form and the translation in column form. It's a mix of both!!! WRONG WRONG you won't go anywhere with that. You need to understand how matrices work" CreationDate="2017-02-06T12:42:01.123" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6638" PostId="4669" Score="0" Text="I had been sent a draft copy of the reference, so any &quot;title&quot; I provide would be meaningless, and would probably be a tangential discussion." CreationDate="2017-02-06T12:42:23.850" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6639" PostId="4669" Score="0" Text="Just saw your last edit. Will go over it." CreationDate="2017-02-06T12:44:25.860" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6640" PostId="4669" Score="0" Text="I understand how matrices work; please don't confuse a problem specific to the look-at matrix with not knowing anything about matrices. It comes across as condescending, even if that's not your intention :/" CreationDate="2017-02-06T12:46:14.383" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6641" PostId="4669" Score="0" Text="@Ray: you mind this useful as well: http://stackoverflow.com/questions/4124041/is-opengl-coordinate-system-left-handed-or-right-handed. I am just trying to help you. You assume you know but you don't if you can't see that the matrix you publish on this forum is not a row-major or column-major order I am sorry but I believe yes you should first really understand this and has someone who has 20 years of CGI programming behind if you don't want to learn then that's up to you, but I am just trying to mentor you here... and if I come as condescending maybe being humble is good too;-)" CreationDate="2017-02-06T13:50:45.467" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6642" PostId="4668" Score="0" Text="oIts the exactly same computation." CreationDate="2017-02-06T14:10:03.280" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6643" PostId="4669" Score="0" Text="The reason the rows and colums are weirdly flipped is that they have done a 90 degree turn/flip in the matrix. While it works, it is a bit magick and should be avoided IMHO. Which is also why op is unable to do what he wants." CreationDate="2017-02-06T14:20:57.527" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6644" PostId="4655" Score="0" Text="The part you described sounds like it involves sampling in an area around the pixel currently being operated on to calculate a value to then use in a lookup table. So you your &quot;gathering&quot; by sampling in an area around the current pixel. Then your doing a dependent texture lookup. Both of those can be expensive depending on the hardware you're running on. If the area sampled in both the input texture and the lookup table is small enough, it might not be a bottleneck, but I suspect that's where it would happen. It seems OpenCL might be a better fit for that reason (to me)." CreationDate="2017-02-06T17:18:56.600" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6647" PostId="4675" Score="0" Text="you probably can't unless you flatten all the data in a single batch and draw that at once which would work if they all have the same shader." CreationDate="2017-02-06T22:24:09.813" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6649" PostId="4676" Score="1" Text="Is VSync enabled? It could be the CPU is waiting for the next available frame. If the GPU takes say 20ms to complete its tasks, the CPU will wait for the next frame which would occur at ~33ms (16.6x2ms), meaning the GPU will idle for ~13ms." CreationDate="2017-02-07T03:06:21.333" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6650" PostId="4673" Score="0" Text="For sake of OpenGL 2.X, I'll probably stick with CPU skinning. Good to hear I'm not misunderstanding things." CreationDate="2017-02-07T03:24:59.937" UserId="5991" ContentLicense="CC BY-SA 3.0" />
  <row Id="6651" PostId="4668" Score="0" Text="@joojaa &quot;olts&quot;? Not sure I understood your comment." CreationDate="2017-02-07T03:58:05.867" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6652" PostId="4668" Score="0" Text="Typo, it should read 'its'. The source you use uses a transpose trick just clear the matrix. Usually you shouldnt do this many steps beween calls as it confuses other programmers . Anyway the code is the same minus the extra matrix manipulation. I wont be writing code because your code does not tell me what all your conventions are." CreationDate="2017-02-07T04:49:40.500" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6654" PostId="4668" Score="0" Text="@joojaa When you say &quot;minus the extra matrix manipulation&quot;, do you mean that the matrix from the reference above should simply be transposed so that it ends up in a column-major order or were you referring to something else? My intention is to use column-major matrices, but the one above was arranged in row-major based on how it was shown in the reference (which could've been a consistency issue fixed later in the ref) I've been going over my stuff and re-checking my understanding and other things, which is why it's taking a bit longer than usual for me to reply to comments. Thanks." CreationDate="2017-02-07T11:16:54.617" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6655" PostId="4669" Score="0" Text="@user18490 I understand and appreciate that you're trying to help, and I've been re-checking my stuff under the working hypothesis that I really understand absolutely nothing (even other refs), trying to discover which part(s) I misunderstood. I never intended to come across as arrogant, if that's your impression (really, I never would've posted a public question if that were the case), but it's a bit over the top to suggest that I know absolutely nothing about it, which is also too generic/vague to help me figure out which particular detail(s) I may have misunderstood. Hope that makes sense." CreationDate="2017-02-07T11:26:46.943" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6656" PostId="4668" Score="0" Text="@rayits From the look of it its  inverted after initial placement. See the ivnerse of a orthogonal matrix (a pure rotation) is its transpose. And ofcourse the inverse of a move is just negative move. Thats why it looks like its wrog order. Its right, just that many operations are stacked in one line of code." CreationDate="2017-02-07T13:09:50.567" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6659" PostId="4669" Score="0" Text="@user18490 Well, I've been going over my other references (including OpenGL Super Bible 6 on matrices, model-view transform, and look-at matrix topics), but I'm still unable to figure out what part I've misunderstood to such a spectacular degree. (The code in edit-3 did not work for me.) I had noticed SB6 referring to the 'look-at' matrix as the 'view matrix' (p.77, 79) and thought that both were the same thing until I got burned (look-at and view matrices get built differently), so now I think of them as different matrices. I mention this in case it's comes across as related to my issue." CreationDate="2017-02-08T09:23:27.527" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6660" PostId="4669" Score="0" Text="@user18490 BTW, I'm also familiar with [this other article](http://www.3dgep.com/understanding-the-view-matrix/#The_View_Matrix) on the topic, which I had read a while back, but I'm re-reading yet again for good measure." CreationDate="2017-02-08T09:24:47.733" UserId="6003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6661" PostId="4682" Score="0" Text="which ones are missing? It looks correct at first glance." CreationDate="2017-02-08T09:34:48.043" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6663" PostId="4682" Score="0" Text="[I put a hand-drawn circle into each pentagon (blue from first iteration and black for second iteration](https://i.stack.imgur.com/96y1X.png) I counted 25 black circles and 5 blue ones." CreationDate="2017-02-08T13:20:14.277" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6664" PostId="4684" Score="1" Text="carefull vulkan and opengl expect different Z bound in clipspace. GLM should have a #define for that somewhere." CreationDate="2017-02-08T13:43:35.713" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6665" PostId="4682" Score="0" Text="The ones in the middle of each large side. The tips from these pentagons would stick out of the big pentagon. If I add one more recurrence one sees the &quot;cracks&quot; opening up." CreationDate="2017-02-09T00:24:01.297" UserId="6023" ContentLicense="CC BY-SA 3.0" />
  <row Id="6666" PostId="4683" Score="0" Text="I know it's not possible to tile the plane only with pentagons. I'm trying to use pentagons and rhombuses (for the gaps), which is possible in another configuration, but not in the above one. If you use the code I provided with n_iter=3, the figure generated will have big cracks of white space in it. I'm trying to avoid that." CreationDate="2017-02-09T00:44:03.217" UserId="6023" ContentLicense="CC BY-SA 3.0" />
  <row Id="6667" PostId="4683" Score="0" Text="One strategy for growing the pentagon &quot;lattice&quot; above, would be to add another pentagon to each side of the small pentagons whenever possible, without overlapping one another. It would be like a crystal growth. One gets a quasicrystal like in this link: http://www.pnas.org/content/93/25/14271/F4.large.jpg, in this case one needs 3 basic shapes. I think though the recursive method won't work." CreationDate="2017-02-09T00:56:56.663" UserId="6023" ContentLicense="CC BY-SA 3.0" />
  <row Id="6668" PostId="4683" Score="0" Text="Ah, sorry, I misunderstood what you were trying to do. I'll see if I can come up with something that addresses your actual question (no promises!)." CreationDate="2017-02-09T05:13:34.417" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6669" PostId="4689" Score="0" Text="which matrix lib?" CreationDate="2017-02-09T11:03:04.430" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6670" PostId="4689" Score="0" Text="@ratchetfreak vecmath" CreationDate="2017-02-09T11:05:49.667" UserId="6029" ContentLicense="CC BY-SA 3.0" />
  <row Id="6671" PostId="4688" Score="0" Text="I needed something similar but used a biased approach in the end: Fibonacci spiral sampling (Quasi-Monte Carlo)." CreationDate="2017-02-09T11:17:16.930" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="6672" PostId="4685" Score="0" Text="Let me get this straight. So the 3 arguments to lookAt are eye, center, up.  Eye is the position of the camera.  I'm kind of confused on what center does.  Up is the where the top of the camera is pointing?" CreationDate="2017-02-09T13:19:08.063" UserId="6025" ContentLicense="CC BY-SA 3.0" />
  <row Id="6673" PostId="4691" Score="1" Text="And where is the stratification? This is just uniform cosine weighted sampling?" CreationDate="2017-02-09T13:29:54.207" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="6674" PostId="4685" Score="0" Text="`center` is the point the eye is looking at, i.e. the point that will be in the middle of the screen. `up` is the direction (not a point) the top of the camera is pointing; i.e. if you draw a line from `center` in the direction `up`, this line will be in the 12 o'clock position on the screen." CreationDate="2017-02-09T13:49:41.377" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6675" PostId="4692" Score="0" Text="I just looked up homography on wikipedia and it looks to me, for 2D, it's performing a mapping much like perspective texturing which means (using homogeneous coordinates) you'll need a 3x3 matrix where the bottom row isn't just 0,0,1.  You can't form that with just rotations and translations as their matrices will always have 0,0,1 in the bottom row and, hence, so will their products." CreationDate="2017-02-09T13:49:51.433" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6676" PostId="4691" Score="1" Text="The projection preserves stratification. I will update the answer with that precision." CreationDate="2017-02-09T13:53:37.973" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6677" PostId="4691" Score="0" Text="How does one control the number of samples per stratum? Or do we need to?" CreationDate="2017-02-09T14:49:17.190" UserId="6001" ContentLicense="CC BY-SA 3.0" />
  <row Id="6678" PostId="4691" Score="1" Text="@Steven I think you'll usually want one sample per stratum (eg. one sample per grid cell in 2D) to make best use of stratified sampling. Unless you have some reason to use a fixed stratification with a variable number of samples." CreationDate="2017-02-09T17:55:37.713" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6679" PostId="4693" Score="0" Text="Although it's the case for the obvious mapping here, not all mappings preserve stratification. The [Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) is a common one which does not." CreationDate="2017-02-09T17:58:12.093" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6680" PostId="4693" Score="1" Text="@Olivier Box–Muller maps e.g. disjoint boxes in the 2D input space to disjoint annular sectors of the 2D output space, so it preserves stratification in that sense at least. Do you mean that it doesn't if you consider it as a random 1D mapping?" CreationDate="2017-02-09T18:12:17.870" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6682" PostId="4693" Score="0" Text="I ended up successfully using this technique - I simply used my cosineWeightedSample(u,v) function and stratified my u,v inputs. Thanks" CreationDate="2017-02-09T22:55:53.107" UserId="6001" ContentLicense="CC BY-SA 3.0" />
  <row Id="6683" PostId="4693" Score="0" Text="@NathanReed yes, I believe you can put it that way. You can't generate a properly stratified 1D normal distribution with it. I suppose you could consider the 2D output stratified, if in a weird way." CreationDate="2017-02-10T00:06:17.437" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6684" PostId="4698" Score="1" Text="If you've found the solution, please go ahead and answer your question. :)" CreationDate="2017-02-10T10:32:10.340" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="6685" PostId="4689" Score="0" Text="Have you printed out the original matrix to ensure its constructed OK? Can you step through code to check the scalar multiplication operator is being called and verify that a temporary matrix/vector isn't being constructed? Can you post a link to this vecmath library?" CreationDate="2017-02-11T00:19:18.350" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6687" PostId="4698" Score="0" Text="You have ample resources on the Web to learn about basic geometry and basic principle of 3D rendering such as https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/math-operations-on-points-and-vectors and https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading for an introduction to cross product and shading. It will be **very** hard for to try to get something as advanced as bump mapping working if you don't even understand what a cross product is. Learn the basic first, then progress step by step. People are not here to teach." CreationDate="2017-02-11T15:27:54.290" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6688" PostId="4704" Score="1" Text="if you want to model the sun as an area light then you can also make it a directional light but instead of a single angle make it centered around a cone" CreationDate="2017-02-11T16:00:31.973" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6689" PostId="4698" Score="6" Text="@user18490 although we expect people to attempt to solve things themselves first, this question shows effort and describes the calculations and learning done so far, and I don't see a problem with it. I like your links to relevant resources but I strongly disagree with &quot;People are not here to teach&quot;. There are lots of people here who put huge amounts of their time in to writing very helpful explanations in their many answers." CreationDate="2017-02-12T01:06:30.010" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6690" PostId="4698" Score="0" Text="@trichoplax. I understand your point but try in a constructive way to prove my point. Site like stack overflow are essentially there for people to ask questions and people to answer these questions. While you can always argue that any question is a valid question, answering things such as &quot;how do we render an image of a 3D model&quot; or in that case, &quot;how I do I do bump mapping on a sphere while I don't even know what a cross product is&quot; are just for people willing to help on these forum, questions that are unarguably too large and can't be answered unless you write a book. That's teaching." CreationDate="2017-02-12T08:24:19.817" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6691" PostId="4710" Score="0" Text="Dude i've read all of those. And maybe i forgot to mention but i am working with floats. What do you expect when i say everything is in range [0,1] surely it can't be 0 and 1. &#xA;&#xA;Again i think you failed to understand my questions 1) and 3). I've read books and i know Ks and Kd are blah blah blah coefficients. But what exactly are those when it comes to putting numbers? Are those the diffuse and specular color or something else? Let me update my question with books references." CreationDate="2017-02-12T09:05:45.763" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="6692" PostId="4710" Score="0" Text="@wandering-warrior: your commend is border line polite. I am not a dude to start with. Please stay within the limits of correctness. Second I spent a share amount of time answer your question so I would appreciate some appreciation of that. If you feel I don't answer your problem, maybe you start questioning whether your questions are properly formulated." CreationDate="2017-02-12T09:10:37.233" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6693" PostId="4710" Score="0" Text="Well sorry i can't tell your gender sitting in front of my LED. My native language isn't english so didn't know people get pissed off on the net if called a 'dude'. Yes i feel you don't answer the question because you didn't read it carefully. I clearly wrote &quot;If Ks and Kd are the surface color or the reflectance ratio and if they are the ratio what their value should be?&quot; You failed to answer what their value should be?" CreationDate="2017-02-12T09:18:03.270" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="6694" PostId="4710" Score="2" Text="Sorry, cant help it but I'm *considering* downvoating based on op's attitude and not on the questions merit. Dude, loose the attitude and have a little courtesy. We're all here to help.@user18490 made a more than decent effort to answer your question." CreationDate="2017-02-12T10:19:32.433" UserId="3128" ContentLicense="CC BY-SA 3.0" />
  <row Id="6696" PostId="4687" Score="0" Text="Thank you for the guidance. FBM seems like a suitable noise function to recreate the footage." CreationDate="2017-02-12T11:55:14.707" UserId="6028" ContentLicense="CC BY-SA 3.0" />
  <row Id="6697" PostId="4696" Score="0" Text="Thank you for the guidance. The codebase you linked and the sample you provided helped me understand and implement my own function." CreationDate="2017-02-12T11:57:50.093" UserId="6028" ContentLicense="CC BY-SA 3.0" />
  <row Id="6698" PostId="4710" Score="0" Text="@wandering-warrior: I answered your question very precisely. But you need to get outside of the context of the code that you are using. I said that Kd and Ks are coefficients (they are not colors) and that should take values between 0 and 1 and that their sum should be ideally equal to 1. I also mentioned that you need to multiply these coefficients by the functions diffuse() and specular() which return the diffuse and specular responses of the surface. These functions are defined by shading model you use. If you are not familiar with these concepts you need to study them first." CreationDate="2017-02-12T12:08:02.057" UserId="1608" ContentLicense="CC BY-SA 3.0" />
  <row Id="6699" PostId="4710" Score="0" Text="*ahem* Ok you answered half.  &quot;For example if i have a color rgb(66, 170, 244) which is somewhat light blue. What should the Ks and Kd be for the above color?&quot; If these aren't the color where can i get their values? Any reference?" CreationDate="2017-02-12T12:47:49.320" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="6700" PostId="4698" Score="0" Text="@user18490 I agree we do not want questions that are too broad. It seems we disagree on whether this particular question is too broad (I see the body of the question as more specific than the broad title). However, this is a community decision, and you are part of the community, so I encourage you to suggest closing the question, and then the community will vote on whether to close. [More information on flagging to close questions here.](http://meta.stackoverflow.com/questions/334115/without-close-flag-privileges-how-do-i-bring-a-low-quality-question-to-the-comm/334118)" CreationDate="2017-02-12T13:39:22.857" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6702" PostId="4709" Score="1" Text="CGPP is dated regarding PBR and statements like Kd+Ks&lt;=1 isn't correct, assuming they stand for diffuse &amp; specular albedo. It's rather that integral of BRDF over hemisphere &lt;= 1 for energy conservation" CreationDate="2017-02-12T14:18:49.130" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6703" PostId="4710" Score="1" Text="@wandering-warrior please be respectful to the people who are giving their time to help you. Whether you intended so or not, the repeated use of the phrase &quot;you failed&quot; comes across as aggressive and ungrateful. As for the confusion over the term &quot;dude&quot;, it's worth noting that it is a [**slang term with a long history**](https://en.wikipedia.org/wiki/Dude) and a wide range of meanings, from complimentary to insulting. It's safest to avoid slang terms here as the intended meaning will not always match the understood meaning." CreationDate="2017-02-12T14:48:42.383" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6705" PostId="4709" Score="0" Text="@JarkkoL - does that mean when i normalize blinn phong, there is no need for Kd+Ks &lt;= 1?" CreationDate="2017-02-12T15:18:05.287" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="6706" PostId="4710" Score="0" Text="@Erik Thanks for speaking out about this. Hopefully I've made it clear that disrespectful behaviour is not acceptable here. Note that you can flag any question, answer or comment for moderator attention, and we will appreciate the chance to step in and calm things down." CreationDate="2017-02-12T15:51:04.800" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6707" PostId="4704" Score="0" Text="Thanks for the answer. What I basically try to calculate is the Sun direct irradiance at few surface points(in a room floor). Knowing the clock time and site latitude, longitude I can get sun angular position. Is it then valid to use this single Sun direction for all the sensors(assuming I go with your first answer, Sun as directional source)?" CreationDate="2017-02-12T20:01:24.350" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6708" PostId="4704" Score="1" Text="Ratchet, did I understand it correctly that you mean instead of sending a shadow ray to a fixed direction, sampling the direction from a solid angle around that fixed direction?" CreationDate="2017-02-12T20:08:37.037" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6709" PostId="4713" Score="0" Text="I was suspicious of precision problems, only I was looking at compression until I got overwhelmed. Thank you so much, this helped me fix it." CreationDate="2017-02-12T20:46:53.437" UserId="6049" ContentLicense="CC BY-SA 3.0" />
  <row Id="6710" PostId="4698" Score="2" Text="I was attempting to derive from first principles how to bump map a ray-traced sphere, and had what I thought was a reasonable question (exactly how do you know which order u x v or v x u will get you a normal pointing away from the sphere center.) I don't consider my question a question in basic geometry..." CreationDate="2017-02-13T00:40:04.110" UserId="6033" ContentLicense="CC BY-SA 3.0" />
  <row Id="6711" PostId="4698" Score="0" Text="Also, I wasn't able to find an answer to my question in any of the 3D graphics texts I had or on the Web. If you've got such a link, please post it." CreationDate="2017-02-13T00:47:44.100" UserId="6033" ContentLicense="CC BY-SA 3.0" />
  <row Id="6712" PostId="4712" Score="1" Text="I actually posted a relevant answer on a more general SO question. http://stackoverflow.com/questions/12524623/what-are-the-practical-differences-when-working-with-colors-in-a-linear-vs-a-no/12894053#12894053" CreationDate="2017-02-13T09:53:15.420" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6715" PostId="4703" Score="0" Text="Are you looking to preserve the *ratio* between angles, to keep things in proportion, or to preserve the exact angle values? In general the angles cannot be preserved as angles around a point do not necessarily add up to 360 degrees in a non-planar surface." CreationDate="2017-02-13T16:42:38.533" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6716" PostId="4703" Score="0" Text="The ratio. I will edit the post. Thank you for pointing this out." CreationDate="2017-02-13T17:13:07.877" UserId="6044" ContentLicense="CC BY-SA 3.0" />
  <row Id="6717" PostId="4703" Score="0" Text="@trichoplax Angles around a point do add up to 360° as long as the surface is smooth (a small patch around the point looks like a plane). You might be thinking of the angles inside a polygon? For instance, the interior angles of a triangle don't add up to 180° on a curved surface in general." CreationDate="2017-02-13T22:20:27.567" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6720" PostId="4703" Score="0" Text="Please see Nathan's comment which explains my misunderstanding (thanks Nathan - that is indeed what I was confusing it with)" CreationDate="2017-02-14T14:45:05.090" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6721" PostId="2004" Score="0" Text="Have you found any demos with Maximum mipmaps? Thanks." CreationDate="2017-02-14T15:31:06.090" UserId="5756" ContentLicense="CC BY-SA 3.0" />
  <row Id="6722" PostId="4703" Score="0" Text="@NathanReed I'm doubting myself now, but I think my error was in terminology (talking about surface instead of mesh), and that the intended point stands. Although the smooth surface that is being approximated by a triangle mesh is locally planar, the mesh itself, as a polyhedron, does not have angles around each vertex that sum to 360°. So an angle preserving projection can be made from a surface to a plane, but the angles are dependent on which surface was chosen to fit to the vertices of the mesh prior to projection (unless the original surface is known). Is this relevant here?" CreationDate="2017-02-14T16:09:47.237" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6723" PostId="4718" Score="1" Text="`max_vertices = 93) out; //MAX 128` What does that mean? You specified a max of 93, so why does your comment say otherwise?" CreationDate="2017-02-14T16:44:53.840" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6724" PostId="4718" Score="3" Text="Also, it's not clear what this subdivision form is trying to do. It looks vaguely like you're linearly interpolating between the triangle positions (in clip space, no less). I'm not sure what that's supposed to accomplish or what the &quot;correct&quot; normals for that would even be." CreationDate="2017-02-14T17:02:22.967" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6727" PostId="4723" Score="1" Text="Not sure I'm understanding your question. Are you asking how do you sample from the mixture of $n$ lights? If you want to sample two or more lights, you would just repeat the procedure to sample a single light (with fresh random numbers), right? Or is there something else you're asking?" CreationDate="2017-02-14T23:18:15.520" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6728" PostId="4721" Score="0" Text="Thanks for the suggestions. I've read Valve's paper on Radiosity and Normal Mapping. They describe projecting the results of the radiosity onto the three basis vectors. They don't distinguish between direct and indirect lighting. I don't think you want to consider normal mapping in the baker - my understandings is that the point of the three basis is so that the normals can access the full hemisphere of radiosity at runtime when performing bump mapping." CreationDate="2017-02-15T04:54:26.423" UserId="6001" ContentLicense="CC BY-SA 3.0" />
  <row Id="6729" PostId="4716" Score="0" Text="Thanks. I'm struggling to follow the derivation somewhat. If I'm not wrong, every entry of the $H_1$ matrix should be multiplied by a factor of $\frac{1}{8}$." CreationDate="2017-02-15T05:08:55.283" UserId="5905" ContentLicense="CC BY-SA 3.0" />
  <row Id="6732" PostId="4703" Score="0" Text="For meshes angles do not add up to 360°." CreationDate="2017-02-15T14:12:25.827" UserId="6044" ContentLicense="CC BY-SA 3.0" />
  <row Id="6734" PostId="4721" Score="0" Text="Of course. The whole point of Radiosity is that it is indirect, so Normal mapping isn't really part of the total energy distribution solution. Of course, you could subdivide the surface even further to the level of individual bumps (in the normal map), but at that point you're at a subpixel level and that brings whole lot of other issues. Then again, if you had a CUDA Radiosity shader, it should still be doable." CreationDate="2017-02-15T16:05:52.540" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="6735" PostId="4721" Score="0" Text="Wait - what? Light maps are almost always lower resolution than the diffuse and bp maps. That's why i am trying to generate directional lightmaps to encode the static lighting so i can &quot;light&quot; the normal maps at runtime." CreationDate="2017-02-15T16:22:35.593" UserId="6001" ContentLicense="CC BY-SA 3.0" />
  <row Id="6736" PostId="4721" Score="0" Text="Yeah, well that's the decision that only you can make, as only you know how high/low frequency the diffuse maps are and what is the actual visual style you are aiming for. But with Radiosity, you really don't want to use low-resolution lightmaps. I mean, the shadow boundary that Radiosity produces is one of the most beautiful things in computer graphics. You don't want to loose that to save some memory, now do you ? Do you happen to have any screenshots you are willing to share so we know what exactly are we talking about here ?" CreationDate="2017-02-15T16:26:52.767" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="6738" PostId="4727" Score="0" Text="Have you checked the MaxSDK ? I imagine here may be a good start &gt; http://docs.autodesk.com/3DSMAX/16/ENU/3ds-Max-SDK-Programmer-Guide/index.html?url=files/GUID-40ED5D02-BBCF-4EE3-9EE7-E59425B49CBB.htm,topicNumber=d30e2631" CreationDate="2017-02-16T08:05:48.203" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6739" PostId="4727" Score="3" Text="Alternatively you could use the open source AssImp library to import scenes and render them inside your own program, rather than getting your program to interoperate with Max, which I guess would involve more effort." CreationDate="2017-02-16T08:07:52.733" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6740" PostId="4723" Score="0" Text="Thanks Nathan, you are correct you as to do the procedure again for multiple lights. But I wonder if the above mixture density allows one to pick two or more samples from the distribution." CreationDate="2017-02-16T12:08:29.090" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6741" PostId="4729" Score="0" Text="Thanks, this kind of method for fractionally distributing brightness among pixels is what I was looking for. Has this technique a name? I  thought to add brightness to pixels based on the distance between their center and the point coordinates (up to a certain threshold) but this is simpler." CreationDate="2017-02-16T14:18:26.937" UserId="6072" ContentLicense="CC BY-SA 3.0" />
  <row Id="6742" PostId="4726" Score="1" Text="I'm not sure I've understood what's going wrong, but couldn't you just repeatedly render to a texture and then just draw a single textured quad when your texture is how you like it?" CreationDate="2017-02-16T16:55:58.680" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6743" PostId="4726" Score="2" Text="Have you remembered to glClear() between each iteration of the loop (or ensure that you're drawing over every pixel in your &quot;function to adjust&quot;)?" CreationDate="2017-02-16T16:56:59.633" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6744" PostId="4313" Score="0" Text="Thank you, you explained it really well and I was able to visually understand what you just wrote. Also, sorry for replying so late. -Sami" CreationDate="2017-02-16T17:08:08.920" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6747" PostId="4727" Score="1" Text="What would you like to compare your scenes with? With the output of the internal 3DS Max renderer? I was told by a Corona developer that the MaxSDK was painful to work with and also not well documented (at least in previous versions), so be prepared for that ;-)" CreationDate="2017-02-16T19:15:53.963" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="6748" PostId="4726" Score="1" Text="I have tried doing glCear(GL_BACK_LEFT) between each iteration but it is not clearing, I'm wondering if I'm missing a line of code before the glClear, or if I'm calling it right. I think the problem is that the back buffer isn't clearing" CreationDate="2017-02-16T21:09:07.387" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="6750" PostId="4731" Score="0" Text="It doesn't show a different thumbnail for me when downloaded." CreationDate="2017-02-17T05:48:42.393" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6751" PostId="4731" Score="1" Text="Probably this feature got lost when imgur reapplied compression. Some formats embed a separate thumbnail, if they do then nothing stops the thumbnail from being a different picture." CreationDate="2017-02-17T06:03:53.650" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6752" PostId="4731" Score="0" Text="also possibly its same as http://thume.ca/projects/2012/11/14/magic-png-files/" CreationDate="2017-02-17T06:10:56.710" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6753" PostId="4730" Score="0" Text="What do you think would be faster and easier to implement? Using the MaxSDK or using Asslmp to import the scenes into my ray tracer? If I use the Asslmp method will it just import geometry will I have to define materials &amp; textures myself?" CreationDate="2017-02-17T07:38:37.913" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6754" PostId="4730" Score="2" Text="AssImp imports materials and uvs. IIRC AssImp uses a key/value system for materials, so it can handle every material parameter type you can imagine. For me personally, I would prefer to import via AssImp, I hear MaxSDK can be frustrating." CreationDate="2017-02-17T09:59:17.703" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6755" PostId="4730" Score="2" Text="Yeah, I've worked with the Max SDK before. It's badly designed and implemented, full of pitfalls and undocumented &quot;features&quot;, and a lot of the sample code doesn't work. I'd recommend avoiding it if you can. That said, I don't know how good AssImp is, and I don't know if it will evaluate materials for you or if you'll have to reimplement all the materials yourself - and of course I wouldn't expect it to work with other Max shader or light plugins." CreationDate="2017-02-17T11:05:58.453" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6756" PostId="4726" Score="2" Text="There's a clear mask that affects which buffers are cleared. But I'm just guessing at things here, since you haven't posted a minimal, verifiable example." CreationDate="2017-02-17T16:47:30.573" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6758" PostId="4733" Score="0" Text="Actually I read that it is possible with PNG files, but the thing which amused me and brought me here is that, how this trick is applied to JPEG format." CreationDate="2017-02-18T14:43:02.830" UserId="6074" ContentLicense="CC BY-SA 3.0" />
  <row Id="6759" PostId="4737" Score="2" Text="Have you made any headway on this by yourself, or any ideas?" CreationDate="2017-02-19T18:32:26.620" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6760" PostId="4737" Score="3" Text="Its pretty common to feel this way in higher education, i know i did. In reality its a issue with yourself. Sure the educators aren't stellar but that is simply because your coming nearer the boundary of know how and your ability to work in several different levels of abstractions and several levels of uncertainty comes to play. (something that is very poison to certain mind frames. You'll get over this in about 5 years). The question itself is quite good. If you ever do any robotics, 3D graphics or engineering you'll be glad. It measures wether you undertood the 3 first thing you were taught." CreationDate="2017-02-19T18:56:18.617" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6761" PostId="4736" Score="0" Text="Well, kind of, if your target is to do the half toned image. But the target is usually to do the original image and your now 2 steps away form achieving that. Its a bit like swapping a fillet stake dinners ingredients with that of a sandwich and saying its easier to do. Sure." CreationDate="2017-02-19T19:05:20.150" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6762" PostId="4723" Score="1" Text="Are you worried about anything in particular with just repeating the process multiple times? Were you concerned about sometimes choosing the same light more than once?" CreationDate="2017-02-19T20:54:59.087" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6763" PostId="4676" Score="0" Text="Hey PaulHK sorry my stack overflow account here is brand new, and something messed up with my registration so I cannot comment on my own post. Your idea that it is v-sync is an interesting one. *[converted from an answer - rest of content too long for a comment moved to the question - trichoplax]*" CreationDate="2017-02-07T05:13:19.720" UserId="6016" ContentLicense="CC BY-SA 3.0" />
  <row Id="6764" PostId="4723" Score="0" Text="@trichoplax. No. I just want to understand mathematically how one choose multiple samples from a merge density." CreationDate="2017-02-20T12:53:42.807" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6765" PostId="4736" Score="0" Text="Vector formats also support colour gradients etc, for which palette reduction would be highly detrimental.There have been papers that describe ways of making use of this." CreationDate="2017-02-20T15:12:33.060" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6766" PostId="4716" Score="0" Text="Thanks for the link, but in general [link-only answers are discouraged on StackExchange](http://meta.computergraphics.stackexchange.com/questions/98/how-should-we-deal-with-link-only-answers-here). I'm not suggesting you paste the entire derivation in, but perhaps you could summarize the key points (e.g. assumptions that lead to the specific coefficients) in your answer?" CreationDate="2017-02-20T16:00:30.567" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6768" PostId="4733" Score="1" Text="@AdeshTamrakar Transparency is supported in [JPEG 2000 format](https://en.wikipedia.org/wiki/JPEG_2000)." CreationDate="2017-02-20T16:32:31.600" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6770" PostId="4741" Score="3" Text="Is the region always convex, or might it be concave?" CreationDate="2017-02-21T00:49:07.873" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6771" PostId="4741" Score="1" Text="It could be either - it's guaranteed to be enclosed, though." CreationDate="2017-02-21T00:50:45.137" UserId="6100" ContentLicense="CC BY-SA 3.0" />
  <row Id="6772" PostId="4740" Score="1" Text="For generating SDFs, you can brute force it doing something like: For every empty pixel, find the nearest 'solid' pixel by searching the entire bitmap (smallest distance wins). Not a cheap method but plenty of room of optimisation later. For downscaling you should be taking averages of distance over the area you are sampling from." CreationDate="2017-02-21T04:13:14.657" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6773" PostId="4740" Score="0" Text="Will using ctx.drawImage would work for the downscaling?" CreationDate="2017-02-21T04:55:55.473" UserId="6099" ContentLicense="CC BY-SA 3.0" />
  <row Id="6775" PostId="4740" Score="0" Text="Also, how do I normalize the distance representation into apha values" CreationDate="2017-02-21T07:10:51.820" UserId="6099" ContentLicense="CC BY-SA 3.0" />
  <row Id="6776" PostId="4740" Score="0" Text="If you are encoding as 8bit alpha you would normalise with something like    Alpha[n] = distField[n] * 255 / MaxDist;    were MaxDist is the largest distance in your field, this requires you to pass MaxDist as a uniform to your shader so it can unpack your alpha buffer properly. You could maybe get away with not not needing MaxDist if you have fixed texture dimensions. Ideally you should be using a floating point format though." CreationDate="2017-02-21T07:16:01.577" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6777" PostId="4742" Score="0" Text="One method to make this work faster is to perform a prepass on your array of vertices, duplicating or merging very close vertices and removing vertices that are in a straight line between two neighbouring vertices. They won't contribute to the resulting mesh in a meaningful way so they don't need to be considered. The amount of error you want to consider valid is up to you. This is a good approach when dealing with user input which is inherintly 'dirty'" CreationDate="2017-02-21T16:41:57.947" UserId="6001" ContentLicense="CC BY-SA 3.0" />
  <row Id="6778" PostId="4744" Score="0" Text="Better question would be, should you have scene graph in rendering engine at all ;)" CreationDate="2017-02-22T04:04:18.427" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6779" PostId="4753" Score="0" Text="It should be $L = \frac{d\phi}{d\omega dAcos(\theta)}$" CreationDate="2017-02-22T13:54:47.150" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6780" PostId="4753" Score="0" Text="Yeah, sorry, fixed that now." CreationDate="2017-02-22T14:14:19.107" UserId="1831" ContentLicense="CC BY-SA 3.0" />
  <row Id="6781" PostId="4747" Score="0" Text="Interesting, thanks for that. I'm using rust and GLium, which I don't think has bindings to this but I could look into contributing a patch." CreationDate="2017-02-22T17:38:33.153" UserId="6100" ContentLicense="CC BY-SA 3.0" />
  <row Id="6782" PostId="4743" Score="1" Text="This is a really neat technique thanks for that. I think I managed to find an off-the-shelf tesselator I can use, but I might have to try this out just because it's a nifty idea." CreationDate="2017-02-22T17:40:27.177" UserId="6100" ContentLicense="CC BY-SA 3.0" />
  <row Id="6783" PostId="4756" Score="0" Text="I know calculating irradiance, for example, cancels out the term, but this doesn't answer my question regarding radiance itself.  More precisely, how can it approach infinity when theta is approaching 90." CreationDate="2017-02-22T17:53:08.380" UserId="1831" ContentLicense="CC BY-SA 3.0" />
  <row Id="6784" PostId="4740" Score="0" Text="Hi Paul, thank you for your answer. Can you elaborate on the downscaling?" CreationDate="2017-02-22T18:03:20.280" UserId="6099" ContentLicense="CC BY-SA 3.0" />
  <row Id="6785" PostId="4756" Score="1" Text="Because if the flux stays constant, you need infinite radiance that there's any flux through a surface that's perpendicular to the light source" CreationDate="2017-02-22T18:05:59.653" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6788" PostId="4753" Score="1" Text="I've posted an answer to this, but have been thinking about it and think I can see what you might be getting at. If there was some differential flux (even a teeny weeny bit) at a grazing angle and $\theta$ was **very close** to $90°$, then the radiance would be very large because of the very small denominator. Is this what you're getting at? If so, I'm not sure what the explanation would be and wonder &quot;is there any surfaces that would  have a significant $d\phi$ at a grazing angles?&quot;" CreationDate="2017-02-22T20:11:00.083" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6791" PostId="4756" Score="0" Text="It makes sense that the light &quot;has&quot; to be more intense in this context, indeed." CreationDate="2017-02-22T21:25:36.877" UserId="1831" ContentLicense="CC BY-SA 3.0" />
  <row Id="6794" PostId="4760" Score="0" Text="billboards, flat squares rotated to always face the camera" CreationDate="2017-02-23T08:13:40.710" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6795" PostId="4761" Score="1" Text="The divide by 2 is because you're taking the neighbouring vertices height, which happen to be 2 units apart (see the diagram in your second link which lables in/out and left/right neighbour vertices). It's to normalise height (Y) with X/Z." CreationDate="2017-02-23T09:11:48.560" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6797" PostId="4744" Score="0" Text="@JarkkoL why wouldn't you? how could you otherwise impose relative movements in the scripting of a game engine for instance?" CreationDate="2017-02-23T13:53:58.917" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="6798" PostId="4352" Score="0" Text="In fact you should consider the complete EM spectrum. Furthermore, the question does not specify whether a human eye must perceive the resulting images." CreationDate="2017-02-23T13:57:14.153" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="6799" PostId="4744" Score="0" Text="@Matthias just explicitly update transformation of such objects. relative movement is extremely rare in practice and should be dealt as an exception, not as part of common engine data structure." CreationDate="2017-02-23T14:03:19.187" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6801" PostId="4767" Score="0" Text="Where are you getting this infinity from? Is this a formula you're using, or something you've read, or something about a particular shader?" CreationDate="2017-02-24T12:58:23.320" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6802" PostId="4767" Score="0" Text="@DanHulme Question updated." CreationDate="2017-02-24T13:39:03.750" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="6803" PostId="4767" Score="0" Text="What's your maths background? Are you familiar with the Dirac delta distribution?" CreationDate="2017-02-24T13:40:04.643" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6804" PostId="4767" Score="0" Text="@DanHulme Haven't heard of that... Learning CG always makes me worry about my math." CreationDate="2017-02-24T13:44:02.973" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="6806" PostId="4769" Score="0" Text="Thanks for your time and patience! I've understand what you mean. However, I doubt that the total area under the BRDF curve(you mean the surface in the hemisphere coordinate system?) is the albedo. In my another question [Energy conservation of BRDF](http://computergraphics.stackexchange.com/questions/4768/energy-conservation-of-brdf), the last equation $\forall\Phi:\int_{\Omega_x}f_r(x,\Phi\to\Theta)cos(N_x,\Theta)d\omega_\Theta\le1$ is the necessary and sufficient condition for energy conservation.(To be continued)" CreationDate="2017-02-24T16:37:34.243" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="6807" PostId="4769" Score="1" Text="So I think albedo maybe equal $\int_{\Omega_x}f_r(x,\Phi\to\Theta)cos(N_x,\Theta)d\omega_\Theta$ (I'm not sure since I haven't found any relative document for the time). An additional factor $cos(N_x,\Theta)$ is in the formula. From this formula, if we consider the situation that it equals 1 and the BRDF is constant, we can get the BRDF equals $\frac{1}{\pi}$ but not $\frac{1}{2\pi}$ since $\int_{\Omega_x}cos(N_x,\Theta)d\omega_\Theta=\pi$." CreationDate="2017-02-24T16:37:41.743" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="6808" PostId="4394" Score="0" Text="Not sure if you're still visiting this site, but I've got a question about your Fresnel equation and have posted it [here](http://computergraphics.stackexchange.com/questions/4771/fresnel-and-specular-colour)" CreationDate="2017-02-24T21:27:40.853" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6809" PostId="4768" Score="0" Text="I'm working through the same book and coincidentally went through that chapter (again) in the last few days. Are you aware of the [UC Davis lectures on YouTube](https://www.youtube.com/playlist?list=PLslgisHe5tBPckSYyKoU3jEA4bqiFmNBJ) that use this book? Unfortunately the lecturer doesn't address your questions specifically in the [BRDF lecture](https://www.youtube.com/watch?v=ytRrjf9OPHg). I was formulating an answer for you but I'm not **fully** understanding it either, so I'll not reply as I don't want to misinform. I will try to write my thoughts about it when I have time though..." CreationDate="2017-02-24T23:12:27.960" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6811" PostId="4772" Score="0" Text="Thanks. So to come up with the specular colour for gold, I'd need to lookup the wavelengths for red, green, and blue components. With those wavelengths I lookup the corresponding refractive indices for gold. Then plug them in the formula for $R_0$ and those 3 values are the components of the specular colour?" CreationDate="2017-02-25T00:56:53.017" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6812" PostId="4772" Score="2" Text="@PeteUK Yep, as an easy way, you can use representative wavelengths for R, G, B like that. Probably a more accurate way is to integrate $R_0(\lambda)$, multiplied by one of the [CIE color matching functions](https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_RGB_color_space), over wavelength." CreationDate="2017-02-25T00:59:43.433" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6819" PostId="4779" Score="0" Text="Thanks! So your saying I need to create a Win32 Console Application -&gt; Import all Files and I should be good to go?" CreationDate="2017-02-26T12:12:36.847" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="6820" PostId="4779" Score="0" Text="Try this first: https://msdn.microsoft.com/en-us/library/jj620919.aspx Then drag and drop your files and your main function instead. (And remember what I said about Precompiled Headers)" CreationDate="2017-02-26T12:43:27.577" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="6821" PostId="4777" Score="0" Text="Do you mean subsurface scattering?" CreationDate="2017-02-26T13:07:04.467" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="6823" PostId="4737" Score="0" Text="Hello PeteUK. Yes, I have edited the question and added my progress so far." CreationDate="2017-02-26T22:37:10.360" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6824" PostId="4772" Score="1" Text="@PeteUK In complement to the answer posted by -Nathan, I would cite this article (https://seblagarde.wordpress.com/2011/08/17/feeding-a-physical-based-lighting-mode), by Sebastién Lagarde, where he briefly discusses the convertion between IOR and RGB values to be used in the Fresnel equation (Schlick's approximation). The ZIP file containing the source code of the converter is disguised as a PDF and the link can be found in the section &quot;Specular color&quot;." CreationDate="2017-02-27T04:32:50.693" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="6825" PostId="4772" Score="0" Text="Taking a closer look at the conversion program (IOR-&gt;RGB) I mentioned above, I see that it actually converts IOR values into sRGB (gamma compressed) values. Wouldn't it be the case to convert just to RGB, since the final rendered (path traced) image will be gamma compressed?" CreationDate="2017-02-27T04:46:58.820" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="6826" PostId="4772" Score="0" Text="@ChristianPagot Perhaps it's for storing in a sRGB specular color texture. The GPU would then decode to linear RGB when it's sampled." CreationDate="2017-02-27T04:53:13.213" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6828" PostId="4781" Score="1" Text="As I understand it, this is the math used for the &quot;french curve form&quot; https://en.wikipedia.org/wiki/Euler_spiral&#xA;I guess you would start by understanding that, an apply curve-fitting to that type of functions.&#xA;How easy it is to learn curve-fitting depends on your math background. But the literature should be easy to find." CreationDate="2017-02-28T07:25:35.960" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6834" PostId="4764" Score="0" Text="To be honest, I didn't read the code. Can you first clarify your question?" CreationDate="2017-02-28T11:49:07.603" UserId="6125" ContentLicense="CC BY-SA 3.0" />
  <row Id="6835" PostId="4781" Score="0" Text="@remi000 So the method you are recommending would be similar to regression/statistical curve fitting (maybe with a more appropriate distance/error function than those typically used in statistics), and simply using Euler spiral/French curve functions as the function being fit?" CreationDate="2017-02-28T16:44:58.193" UserId="6136" ContentLicense="CC BY-SA 3.0" />
  <row Id="6836" PostId="4764" Score="0" Text="Sorry, things got muddled while writing. How do I find the position of all joints for an inverse kinematic chain that is longer than 2 segments?" CreationDate="2017-02-28T18:14:40.730" UserId="2076" ContentLicense="CC BY-SA 3.0" />
  <row Id="6837" PostId="4764" Score="0" Text="I think you can perhaps find explicit formulas here http://courses.csail.mit.edu/6.141/spring2011/pub/lectures/Lec14-Manipulation-II.pdf starting on slide 32. Tell me if it's not what you're looking for." CreationDate="2017-02-28T19:41:17.920" UserId="6125" ContentLicense="CC BY-SA 3.0" />
  <row Id="6838" PostId="4786" Score="0" Text="what I have a problem with is the &quot;stride&quot; value in glVertexAttribPointer. Why would you not tightly pack an array?" CreationDate="2017-02-28T19:58:18.517" UserId="5476" ContentLicense="CC BY-SA 3.0" />
  <row Id="6839" PostId="4746" Score="0" Text="Thanks! For now, I have the scene graph on the main thread, but it would be nice to have it in a web worker then. I would also like to do things like `node.rotation.x = 30` from the main thread, and have that update the scene graph node in the worker (effectively the main thread API is just an interface that updates the actual node in the worker). Does that seem like a good approach? I'm going to try it here: https://github.com/trusktr/infamous" CreationDate="2017-02-28T20:37:15.440" UserId="4991" ContentLicense="CC BY-SA 3.0" />
  <row Id="6840" PostId="4786" Score="1" Text="when interleaving data by using a array of structs then the stride is the sizeof the struct." CreationDate="2017-02-28T20:46:17.737" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6841" PostId="4764" Score="0" Text="It's close, but not exactly it. These formulas illustrate a typical 3 joint IK solve (e.g. arm, leg) with a single bend. I'm looking for the formula that factors in additional joints and bends in the IK solve (e.g. 4 joints with 2 bends)." CreationDate="2017-03-01T02:18:30.270" UserId="2076" ContentLicense="CC BY-SA 3.0" />
  <row Id="6842" PostId="4781" Score="0" Text="Yes something like that. I remember there is some very general method for curve fitting. And you can use this method on any kind of curve after defining the curve-parameters of which can vary (http://web.iitd.ac.in/~pmvs/courses/mel705/curvefitting.pdf)&#xA;I've never used one of these French or Hip curves, but I guess you need to move it around to draw the exact curve you want, so this would be multiple segments like you said.. This might make it more complex.&#xA;Anyway, this sounds like an interesting problem!" CreationDate="2017-03-01T07:33:29.097" UserId="3434" ContentLicense="CC BY-SA 3.0" />
  <row Id="6844" PostId="4787" Score="0" Text="Do you mean underconstrained?" CreationDate="2017-03-01T12:25:08.923" UserId="6125" ContentLicense="CC BY-SA 3.0" />
  <row Id="6845" PostId="4787" Score="0" Text="@StinkySkunk oh yes" CreationDate="2017-03-01T12:46:01.710" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6846" PostId="4788" Score="0" Text="at some point you have no choice but to call glDraw* multiple times with different uniforms." CreationDate="2017-03-01T15:07:14.863" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6847" PostId="4788" Score="0" Text="I don't think that you can treat these 3 objects as one because they belong to different `vaos`. You can't bind more than one `vao` at the time." CreationDate="2017-03-01T16:29:05.997" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="6848" PostId="4787" Score="0" Text="To be honest, I'm not sure which option I'm searching for. What I do know is that I'm after a planar solution that moves like this: &#xA;&#xA;https://youtu.be/3sqd_CZNq7s?t=6s&#xA;&#xA;It's short, but I'm looking at the part where the color chain is red-orange-yellow-blue." CreationDate="2017-03-01T18:12:40.767" UserId="2076" ContentLicense="CC BY-SA 3.0" />
  <row Id="6849" PostId="4787" Score="0" Text="That does not seem very useful @GregGunn It does not even seem to solve any ik. Atleast we do not know if it is because we cant see the target.  It looks more like a system that moves the links based on proxility which is more like hbrid fk/ik. the only reason we even suspect its ik is because the title says so... Titles lie." CreationDate="2017-03-01T19:00:52.033" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6850" PostId="4787" Score="0" Text="@joojaa If that reference is my goal—regardless of it's IK or not—how would you suggest I build something that moves like that?" CreationDate="2017-03-01T19:30:16.597" UserId="2076" ContentLicense="CC BY-SA 3.0" />
  <row Id="6851" PostId="4737" Score="0" Text="I'm doubting that the third column should be all zeros. It's late here and will try to look at it more tomorrow, but think that column should be the basis vector for your $N$." CreationDate="2017-03-02T00:05:51.620" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6852" PostId="4737" Score="0" Text="I'm also doubting that $1$ in row 2, column 4 of your rotation matrix. That's the translation part of the matrix." CreationDate="2017-03-02T00:09:34.523" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6853" PostId="4789" Score="0" Text="You also need to invert the matrix in the end" CreationDate="2017-03-02T02:59:47.487" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6854" PostId="4789" Score="0" Text="@JarkkoL, I don't know why it needs to invert, can you explain?" CreationDate="2017-03-02T04:06:18.087" UserId="6140" ContentLicense="CC BY-SA 3.0" />
  <row Id="6855" PostId="4789" Score="0" Text="OP is asking for view (world-&gt;camera) matrix, while you are building object-&gt;world matrix" CreationDate="2017-03-02T04:13:51.003" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="6856" PostId="4781" Score="1" Text="Just so you know, the Bezier curve was created so that designers didn't have to deal with French curves anymore since they are imprecise and the resulting curves are hard to communicate between people. Also, if you are looking to fit data points with a polynomial, you should check out least squares fitting, which is an O(1) operation - no looping or gradient descent type stuff required. One other thing, bezier curves can be approximated with line art, like with strings. Check out figure 2 here: https://plus.maths.org/content/bridges-string-art-and-bezier-curves" CreationDate="2017-03-02T06:06:51.307" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="6857" PostId="4789" Score="0" Text="@JarkkoL Thank you very much, I misunderstood the transformation. already change the answer." CreationDate="2017-03-02T06:08:12.630" UserId="6140" ContentLicense="CC BY-SA 3.0" />
  <row Id="6858" PostId="4787" Score="0" Text="@GregGunn There is a inifinite mumber of solutions that does something. _You_ need to decide what is useful, i cant help you if there is no aim other than a abstract &quot;something&quot;. You need to have a clear vision of what problem your ik is trying to solve. Onnce you can describe what the vide does with its 4th link and why thats useful then i can help. The trick to underconstrained problems is that you need to do the reasoning because the computer or anybody else who does not k.ow the purpose and specification can not do it for you." CreationDate="2017-03-02T06:43:18.940" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6859" PostId="4789" Score="0" Text="This is great. I will check with my lecturer and once he approves, I'll accept the answer." CreationDate="2017-03-02T10:33:50.193" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6860" PostId="4797" Score="0" Text="You are talking about GL_LINEAR_MIPMAP_LINEAR filtering method, but if you use GL_NEAREST_MIPMAP_LINEAR instead then you first do a nearest interpolation of the higher-res and lower-res textures, and then mix the resulting colors giving more preference to the nearest mipmap and less preference to the furthest. Am I correct?" CreationDate="2017-03-03T00:30:08.370" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="6861" PostId="4797" Score="1" Text="Actually, for nearest, you would just pick either the high res or low res texture (which ever is nearest), and then within that texture do a normal 2D nearest-neighbor look-up." CreationDate="2017-03-03T00:51:02.213" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6862" PostId="4789" Score="0" Text="@S.A Thank you, your question is very interesting." CreationDate="2017-03-03T09:34:23.553" UserId="6140" ContentLicense="CC BY-SA 3.0" />
  <row Id="6863" PostId="4798" Score="1" Text="Without looking at your code I'd guess the error is perspective correction, similar to [this question](http://computergraphics.stackexchange.com/questions/4079/perspective-correct-texture-mapping?rq=1)" CreationDate="2017-03-03T10:22:11.410" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6864" PostId="4793" Score="0" Text="Unrelated, but your `imageArray2` loop would be faster as `for (int i = 0; i &lt; 100 * 100 * 3; i += 3) { imageArray2[i] = 0; imageArray2[i + 1] = 0; imageArray[i + 2] = 255; }` because it avoids a division inside the loop. I think it's clearer this way too." CreationDate="2017-03-03T10:35:24.940" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6865" PostId="4793" Score="2" Text="Obviously mipmaps are turned off in the program as-is: the thing you're debugging is why you get a black screen when you uncomment the line that turns them on. First rule of GL debugging is to use [`glGetError`](https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/glGetError.xhtml) everywhere and see if any of the gl* function calls are failing." CreationDate="2017-03-03T10:37:35.947" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6866" PostId="4798" Score="0" Text="you are interpolating in screenspace, you should be interpolating in clipspace." CreationDate="2017-03-03T11:42:49.877" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6867" PostId="4796" Score="1" Text="I think you can do this vith a L-system. Like the penrose tiling exaple on [this page](http://www.kevs3d.co.uk/dev/lsystems/), see examples on the right. (a L-system is pretty easy to program, if you dont know how its one of those cassic CS things). An early alpha version of a L-System generator for illustrator can be found [here](https://bitbucket.org/joojaa/jooillustratorscripts/src/99aaf05584e32a9c694bbdb0857169201ff9af26/jooLSystem.jsx?at=Lsys&amp;fileviewer=file-view-default)" CreationDate="2017-03-03T12:10:00.967" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="6868" PostId="4795" Score="0" Text="Thank you, this is very helpful and gives me a lot of jumping-off points. I considered splines, but they aren't typically used in this drafting context AFAIK, so I don't think they would be ideal for at least some of the applications." CreationDate="2017-03-03T16:13:43.990" UserId="6136" ContentLicense="CC BY-SA 3.0" />
  <row Id="6869" PostId="4800" Score="1" Text="Fantastic reference. Always fascinating to read original papers which introduced new technologies that we all but take for granted these days." CreationDate="2017-03-03T22:09:16.563" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="6870" PostId="2286" Score="1" Text="The [Disney BRDF paper](https://disney-animation.s3.amazonaws.com/library/s2012_pbs_disney_brdf_notes_v2.pdf) section 5.3 refers to the equation in your answer and then goes on to specify a different model. I don't claim to understand any of it as I've just started my glossy BRDF implementation!" CreationDate="2017-03-03T23:02:17.397" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="6871" PostId="4793" Score="0" Text="@Dan Hulme How can I turn on mipmaps? The tutorial I'm following doesn't make mention of how to turn on/off mipmaps." CreationDate="2017-03-03T23:36:23.977" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="6872" PostId="4793" Score="1" Text="Setting the `GL_TEXTURE_MIN_FILTER` to `GL_NEAREST` means the mipmaps won't be used, which is why you only see level 0 when you do this." CreationDate="2017-03-04T09:47:34.247" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6873" PostId="4803" Score="0" Text="If it were me, I'd avoid OpenGL and use Metal, DirectX 12, or Vulkan, as OpenGL is now a legacy library that probably won't be updated in the future." CreationDate="2017-03-04T20:58:08.330" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6874" PostId="4803" Score="0" Text="@user1118321 I plan to move to Vulkan after the project, OpenGL is just more covered and accessible right now. Also, my professor doesn't have experience in Vulkan sadly." CreationDate="2017-03-05T01:35:50.443" UserId="6167" ContentLicense="CC BY-SA 3.0" />
  <row Id="6875" PostId="4806" Score="0" Text="Thank you! This helps a lot to get to a good starting point. I really like the idea of Fast Ray-Liquid intersection testing, but might be out of scope. I'm going to look into it." CreationDate="2017-03-05T03:16:59.317" UserId="6167" ContentLicense="CC BY-SA 3.0" />
  <row Id="6876" PostId="4657" Score="7" Text="In order to help people assess whether to use this approach, could you add an explanation of why this solves the problem with the approach in the question?" CreationDate="2017-03-05T13:38:49.680" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6877" PostId="4812" Score="0" Text="But the scale factor is 3 dimensional and the tex coordinate is 2 dimensional." CreationDate="2017-03-06T07:55:50.450" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="6878" PostId="4812" Score="0" Text="For example if the scaling factor is 1, 10, 1, then the texture coordinates of a face will be scaled based on which face we render currently. Like if we render the top quad it will be scaled x = 1 and z = 1. If we look at a side quad it will scaled by x = 1 and y = 10 or z = 1 and y = 10. How can I check which face are working on?&#xA;Or do I overcomplicate it?" CreationDate="2017-03-06T08:03:08.383" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="6881" PostId="4801" Score="0" Text="I deleted my answer as I think that a blending solution won't fix this problem.  I think you probably need to utilize the stencil buffer somehow to avoid writing the darkness to the same area twice." CreationDate="2017-03-06T15:26:17.937" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="6882" PostId="4801" Score="0" Text="Yes, I have thought the same. I will pursue that idea for the moment. Thanks for trying." CreationDate="2017-03-06T15:28:36.117" UserId="6164" ContentLicense="CC BY-SA 3.0" />
  <row Id="6883" PostId="4812" Score="0" Text="Without knowing more about your architecture and seeing more of your code, I don't have any way to answer that question. I would assume that you know which faces you're drawing when. You do need to know which face you're drawing and what the scale factors are in order to do this correctly." CreationDate="2017-03-06T17:17:20.910" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6884" PostId="4814" Score="0" Text="Very nice @StinkyShunk ! I came up to this already too but this explains it well tp.  I've planned to answer the question myself but now I think It'll accept your answer and post mine according 3D as additional information too." CreationDate="2017-03-06T18:33:40.403" UserId="6165" ContentLicense="CC BY-SA 3.0" />
  <row Id="6886" PostId="4814" Score="0" Text="After all, thank you very much @StinkySkunk. I really hope this helps other people trying to understand it out there too." CreationDate="2017-03-06T19:14:43.197" UserId="6165" ContentLicense="CC BY-SA 3.0" />
  <row Id="6887" PostId="4814" Score="0" Text=":) I am happy I could confirm your thinking. Please do post your more thorough answer as well for the benefit of others!" CreationDate="2017-03-07T00:07:21.773" UserId="6125" ContentLicense="CC BY-SA 3.0" />
  <row Id="6888" PostId="4796" Score="0" Text="Based on the answer by @Mikhail V and by looking at the fractal structure given in my code with number of iterations up to 5, I wonder if one could make a &quot;fractal complement&quot; (or dual) to fill in the gaps/cracks, since they are also self-similar. Then one could patch both fractals together in the end." CreationDate="2017-03-07T01:46:23.227" UserId="6023" ContentLicense="CC BY-SA 3.0" />
  <row Id="6889" PostId="4813" Score="0" Text="Usually the printer will tell you the minimum DPI required. I believe that most printers use 300 DPI for printing photos. They may let you get away with less (hence the bad and average categories), but their hardware prints at 300 dpi so your image should have enough pixels for that." CreationDate="2017-03-07T05:39:43.930" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6891" PostId="4818" Score="0" Text="Do I really need 3 correspondences, or is 2 enough?    &#xA;&#xA;Additionally, how can I get the angle of rotation from this?" CreationDate="2017-03-07T11:30:49.107" UserId="6175" ContentLicense="CC BY-SA 3.0" />
  <row Id="6892" PostId="4818" Score="1" Text="2 is enough but the third lets you verify that it's indeed a rotation. As for angle you can project the point and it's rotated point onto the axis get the direction to the points and dot product." CreationDate="2017-03-07T11:35:55.860" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6893" PostId="4818" Score="0" Text="I guess two can have degenerate cases if they're coplanar with the axis of rotation, so three can help, but three can also be bad if they're still coplanar with the axis.  Like for a triangle on a door that rotates open, all the &quot;midpoint-planes&quot; are the same." CreationDate="2017-03-07T11:41:46.897" UserId="6175" ContentLicense="CC BY-SA 3.0" />
  <row Id="6894" PostId="4818" Score="1" Text="@ginsunuva there is a solution for that, see edit. The only remaining issue is a degenerate triangle which is parallel with the rotation axis but that has multiple solutions." CreationDate="2017-03-07T11:43:54.037" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6895" PostId="4818" Score="0" Text="Great, also how do I actually do the project/find angle thing? How can I get two points to be in this new 2d coordinate system defined by the orthogonal plane to the rot-axis, so I can do a dot product?" CreationDate="2017-03-07T12:42:01.103" UserId="6175" ContentLicense="CC BY-SA 3.0" />
  <row Id="6896" PostId="3675" Score="0" Text="Great answer! I completely agree with shadows and ambient occlusion being the main factors contributing to the &quot;3D-ness&quot; of the other apps. Fog is less important for small, enclosed spaces but is required for performance in large, open spaces and is rather trivial to implement." CreationDate="2017-03-07T13:51:14.027" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="6898" PostId="4812" Score="0" Text="@user1118321 Could you expand on the part about passing a scale matrix into the fragment shader? How do you use that to adjust U,V coords for non-uniform scaling? Is it possible?" CreationDate="2017-03-07T13:59:25.173" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="6900" PostId="4820" Score="2" Text="Welcome to the CGSE! To get relevant help, you will have to add more context to your question. What tool or language are you using? Is &quot;Processing&quot; some setting of that tool?" CreationDate="2017-03-07T15:56:34.323" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="6901" PostId="4820" Score="1" Text="Thanks for your welcome. Processing is a tool for developing basic computer graphics programs. I edited my post." CreationDate="2017-03-07T16:00:56.363" UserId="6183" ContentLicense="CC BY-SA 3.0" />
  <row Id="6902" PostId="4812" Score="0" Text="OK, I've added an example above." CreationDate="2017-03-07T17:20:05.297" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6903" PostId="4816" Score="0" Text="One more question - how did you count aspect ratio of these images?" CreationDate="2017-03-07T22:27:35.540" UserId="6179" ContentLicense="CC BY-SA 3.0" />
  <row Id="6904" PostId="4801" Score="2" Text="You could try using Max blending : http://stackoverflow.com/questions/2143690/is-it-possible-to-achieve-maxas-ad-opengl-blending" CreationDate="2017-03-08T06:41:27.103" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6905" PostId="4812" Score="0" Text="I couldn't figure out how could I know which face I am drawing. The drawing part of my architecture is simple: Load the vertices, normals, indices, uv-s from an .obj file, then just push it to the shader. The vertex shader will multiple it with MVP, then the fragment shader samples from teh texture based on the UV coord. Somehow I have to get that which face looks towards which axis before the MVP multiple. (because for example a &quot;rotate around Y axis 90 degree&quot; would make this very hard, if not impossible)&#xA;Maybe I could something with the indices?" CreationDate="2017-03-08T09:38:08.610" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="6906" PostId="4812" Score="0" Text="Can you modify the .obj file? If so, you can group sets of polygons in meaningful ways using [the `g` command](https://en.wikipedia.org/wiki/Wavefront_.obj_file#Referencing_materials). This will allow you to make 2 triangles into a single cube face, for example. Then, when reading it in, you can know which face is which." CreationDate="2017-03-08T16:52:12.200" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6907" PostId="4813" Score="0" Text="Keep in mind that those suggestions are mostly useless. You won't see anywhere near 300 DPI from common commercial printers on typical photo paper. It's closer to 100-150 DPI in my experience. And you won't be able to see even that much on larger prints from a reasonable viewing distance so you can get away with less." CreationDate="2017-03-08T20:53:03.950" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="6908" PostId="4827" Score="1" Text="There looks to be a number of rather odd things in your answer, but _&quot;Jpg uses fast fourier transform&quot;_ is certainly incorrect.   JPEG uses Discrete Cosine Transforms (DCTs). There are several reasons why this is a better choice than FFT but that's a little difficult to explain with limited time and the restrictions of S.O. comments :-)." CreationDate="2017-03-09T09:57:40.637" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6909" PostId="4827" Score="1" Text="@SimonF Thanks for your comment, I believe DCT is a special form of FFT since DCT only uses cosine waves but FFT uses both sine and cosine waves, you may set the sine expansion =0  and solve the equation using the constraint to have a DCT out of A FFT, but YES YOU ARE RIGHT the exact name of algorithm is DCT for jpeg lossy data. Thanks again" CreationDate="2017-03-09T12:59:10.150" UserId="537" ContentLicense="CC BY-SA 3.0" />
  <row Id="6911" PostId="4830" Score="1" Text="Maybe because the implementation modifies in place running LtR? That would mean the pixel above got changed and one got inserted to the left but not to the right." CreationDate="2017-03-09T13:23:47.710" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="6917" PostId="4830" Score="0" Text="Ok, but modifying in place would mean that, looking at the 2nd picture, the downmost circled blank pixel turns into a red one. Then, the green seed right next to it should take that information and turn into a red one too, but it remains green..." CreationDate="2017-03-09T15:06:11.080" UserId="5406" ContentLicense="CC BY-SA 3.0" />
  <row Id="6918" PostId="4832" Score="1" Text="Please don't ask the same question on multiple sites. It means that any answers get spread across different places, making that information harder to find in future." CreationDate="2017-03-09T17:02:26.930" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6919" PostId="4828" Score="1" Text="There's not a lot of information to go on here. What OS are you on? What are you trying to take a screenshot of? What is it going to be output on? What makes you think that it's even possible to take a screenshot at a higher resolution than the screen? Also, you mention rendering in your last sentence. What are you rendering? And how are you rendering it?" CreationDate="2017-03-10T03:45:10.670" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6920" PostId="4828" Score="0" Text="I guess you could divide your view fustrum into a grid and render it to several separate render targets. Your post processing shaders need to be able to deal with sub sections (say a full screen vignette would have UV coordinate inputs, you would be passing in sub-rectangles of UVs to render sections of it), you should be able to restitch the output textures together without artefacts." CreationDate="2017-03-10T08:34:43.047" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6921" PostId="4828" Score="0" Text="@user1118321 I edit post with more details.&#xA;@PaulHK In your solution screenspace effects like `SSR` will be broken due to lack information." CreationDate="2017-03-10T11:49:16.753" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="6922" PostId="4830" Score="0" Text="[Somewhat related](http://computergraphics.stackexchange.com/questions/2102/is-jump-flood-algorithm-separable) but definitely not a duplicate (disclaimer: I wrote an answer to it)" CreationDate="2017-03-10T21:00:23.380" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6924" PostId="4828" Score="0" Text="What information do your post processing effects require? Do they just need the immediately neighbouring pixels or more distant data?" CreationDate="2017-03-10T21:13:36.843" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6925" PostId="4835" Score="0" Text="Ahh, that's it! So when a pixel finds a colored neighbor it computes the distance between that pixel and the initial seed (whose position it knows from the neighbor). This wasn't clear to me that there is a difference between a colored pixel and a seed. Thank you, you helped me a lot with that!&#xA;&#xA;But one question remains: what about the diagonal pixels in the 4x4 example? They have the same distance to both initial seeds..." CreationDate="2017-03-10T21:23:25.777" UserId="5406" ContentLicense="CC BY-SA 3.0" />
  <row Id="6926" PostId="4835" Score="4" Text="Glad it makes sense now! The diagonals are ambiguous in the definition of the voronoi diagram: the paper mentions (on the first page) that for equidistant seeds, they choose the seeds arbitrarily. Maybe it will look nicer if you consistently choose one color over the other, I have no idea!" CreationDate="2017-03-10T21:36:46.803" UserId="6125" ContentLicense="CC BY-SA 3.0" />
  <row Id="6930" PostId="4837" Score="1" Text="Could you post some code and a screen shot of what it looks like, and perhaps an example of what you want it to look like?" CreationDate="2017-03-11T06:48:55.653" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6931" PostId="4836" Score="1" Text="Do you already have code to do this? If so, have you profiled it to see what its performance is like and where any potential slow-downs might be? As it's asked right now, there are a lot of possible answers, and it's a quite broad question." CreationDate="2017-03-11T06:51:44.690" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6932" PostId="4836" Score="0" Text="No because the results to the experiment will greatly affect how I design my engine. I am completely redesigning my graphics engine and I am trying to have it with my new method in mind." CreationDate="2017-03-11T07:01:14.080" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="6933" PostId="4837" Score="0" Text="@user1118321 Good idea. Added!" CreationDate="2017-03-11T07:01:51.457" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="6934" PostId="4828" Score="0" Text="@trichoplax Post processes need information from whole screen (some needs few previous frames but in this case it can be ignored)" CreationDate="2017-03-11T10:30:08.897" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="6935" PostId="3784" Score="0" Text="I like your explanation very much. I get the arguments that there must be the inverse solid angle factor in BRDF, but what about the cosine factor? If we could drop the cosine term from BRDF, then we could drop if from the integral in the rendering equation, couldn't we? The only reason I can see is in the correct/current formulation the denominator can be seen as irradiance..." CreationDate="2017-03-11T11:50:05.027" UserId="5249" ContentLicense="CC BY-SA 3.0" />
  <row Id="6936" PostId="4811" Score="0" Text="So for designing a PDF for picking a light, is there a specific method that's proven to be effective? Or does everyone have their own combination of techniques such as the output power, orientation, etc as you mentioned? Also, will it make biased if I do that? Or does dividing by the PDF keep it unbiased?" CreationDate="2017-03-11T16:17:44.600" UserId="5764" ContentLicense="CC BY-SA 3.0" />
  <row Id="6937" PostId="4816" Score="0" Text="For real, @Nicol Bolas, how did you determine that 1000x1333 image has aspect ratio of 3:4? Even this site https://www.ninjaunits.com/calculators/aspect-ratio/ does not determine this." CreationDate="2017-03-11T17:59:30.180" UserId="6179" ContentLicense="CC BY-SA 3.0" />
  <row Id="6938" PostId="4816" Score="1" Text="@funguy: The aspect ratio of an image is the ratio of the image's width to its height. You don't need a website to divide 1000 by 1333; you only need a calculator. Note that this ratio is just the ratio of the image itself; some image formats have metadata that specify what aspect ratio the image should be *stretched* to in order to correctly reproduce what it stores. But absent such metadata, the assumption is that the ratio of the pixels is how the image should be displayed." CreationDate="2017-03-11T18:04:59.343" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6940" PostId="4816" Score="0" Text="Yes, @Nicol Bolas, but this is what I want to understand, how to find out aspect ratio with a calculator. If you divide 1000/1333 you will get something like 0.75018... but still, how to add up this final result - 3x4?" CreationDate="2017-03-11T19:26:40.317" UserId="6179" ContentLicense="CC BY-SA 3.0" />
  <row Id="6941" PostId="4811" Score="0" Text="Unfortunately, I don't know any production-proven approaches in detail and even PBRT proposes only very simple methods. I extended the answer with what I know. And, just to be clear, for picking a light source you need to use probability, not PDF (probability density function)." CreationDate="2017-03-11T21:52:16.563" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="6942" PostId="4836" Score="1" Text="Are you only drawing one quad (with the texture tiled)? I think a diagram or what you want to achieve as the end result would help." CreationDate="2017-03-12T04:13:53.533" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="6943" PostId="4840" Score="0" Text="So cos(theta) would equal to 0.8. Thanks!" CreationDate="2017-03-12T09:28:04.413" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6945" PostId="4838" Score="1" Text="Just to be clear, you want the integer ratio and not the actual value? For example, the aspect ratio is just `width` / `height`, so for 1000x1200, it's 1000/1200 = 0.8333... But if you want to print it out as a ratio with a format like 16:9, then it's slightly more complicated (though not much). But it's not clear from your question which one you're asking about." CreationDate="2017-03-12T19:03:31.093" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6946" PostId="4838" Score="0" Text="Yes, it is 1000/1200 = 0.8333, but how you actually get 5:6 by knowing just  this - 1000/1200 = 0.8333 ?" CreationDate="2017-03-12T21:06:23.100" UserId="6179" ContentLicense="CC BY-SA 3.0" />
  <row Id="6948" PostId="4845" Score="0" Text="Can you give an example with numbers? Would appreciate." CreationDate="2017-03-12T23:12:01.090" UserId="6179" ContentLicense="CC BY-SA 3.0" />
  <row Id="6949" PostId="4837" Score="1" Text="Could you expand the explanation of what you intend? Do you want the objects to be opaque in all cases *except* when they are behind a trail? Does this apply only to the trails of other objects, or also to an object's own trails?" CreationDate="2017-03-13T01:00:19.190" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6951" PostId="4844" Score="0" Text="I wonder what the $b$ without $2$ is then. I've seen it used somewhere, but cannot find it now." CreationDate="2017-03-13T16:56:19.970" UserId="6029" ContentLicense="CC BY-SA 3.0" />
  <row Id="6952" PostId="4557" Score="0" Text="@AlanWolfe The research you linked is incredible! Thanks for posting." CreationDate="2017-03-13T21:33:16.357" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="6954" PostId="4852" Score="0" Text="So the divide by w brings the 4d coordinates to 3d and then the 3d points are orthographically projected to 2d? So a perspective projection really also encompasses a final orthographic projection?" CreationDate="2017-03-14T10:08:22.523" UserId="6230" ContentLicense="CC BY-SA 3.0" />
  <row Id="6955" PostId="4845" Score="0" Text="What about other value 1000x1333 it does not add up. It seems like a few people are saying that aspect ratio of this is 3:4, but if I try to count it is always 1000x1333. How they get 3:4 for this?" CreationDate="2017-03-14T11:31:56.570" UserId="6179" ContentLicense="CC BY-SA 3.0" />
  <row Id="6956" PostId="4842" Score="0" Text="Do you have to use vertex normals as opposed to face normals? This makes the problem considerably more difficult. That said, the center of the triangle is always going to be the average of the values at the 3 vertices of the triangle, regardless of the size or direction of the triangle. If you know how to calculate the values at each vertex, you can simply interpolate t=0.5 between all three values." CreationDate="2017-03-14T13:52:29.050" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="6957" PostId="4842" Score="0" Text="@Dan No, it's flat shading, so it should be by face normal." CreationDate="2017-03-14T15:25:54.490" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="6958" PostId="4845" Score="0" Text="That's a case of integer rounding. Technically, 3:4 would be a ratio of 0.75, whereas 1000:1333 is 0.750187546886722... So it's very close to 3:4, but not exact. So another way you could approach this is to have a table of common aspect ratios (like 1:1, 2:1, 3:4, 4:5, 16:9, etc.) and then find the one that's closest. Or you could round the result of the width / height to some decimal amount and then compare." CreationDate="2017-03-14T16:37:34.137" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="6959" PostId="4840" Score="1" Text="For flat shading you'd want to use the triangle's true, geometric normal vector, right? Not the average of the vertex normals or something." CreationDate="2017-03-14T16:58:44.670" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6960" PostId="4855" Score="0" Text="Thanks for your answer. My aim is to calculate daylighting from the visible sky, seen through the glazing portal,on few measurement points. The transmitted ray will hit a different patch of sky with a different distribution than the initial shadow ray. And as you said this will give wrong results." CreationDate="2017-03-14T17:13:53.010" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6961" PostId="4849" Score="0" Text="Thanks for the answer but I have to admit that I don't understand everything. Could you put some concrete example or code so that I can't see more clearly please ?" CreationDate="2017-03-14T17:30:32.940" UserId="5770" ContentLicense="CC BY-SA 3.0" />
  <row Id="6962" PostId="4850" Score="0" Text="What do you mean by &quot;the same side&quot;? I know the general idea of &quot;how to do it&quot; but I don't know how it happens in practice. Povray gives such an accurate and pretty result, I don't understand how it can possibly do it." CreationDate="2017-03-14T17:36:20.940" UserId="5770" ContentLicense="CC BY-SA 3.0" />
  <row Id="6963" PostId="4840" Score="0" Text="Nathan, so cos(theta) is equal to 0.8 still, right?" CreationDate="2017-03-14T17:40:56.410" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6964" PostId="4842" Score="0" Text="@aces So you're agreeing with me? Lol. Your comment contradicts itself.The OP specified 3 normals, one for each vertex, in his question." CreationDate="2017-03-14T17:44:33.870" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="6965" PostId="4855" Score="0" Text="I am not sure any light tracing methods would help either as sampling the infinite area light source(sky) for an indoor scene is not efficient, and considering the window portal as the light source with the background sky distribution wouldn't help either as you sample the window area with a sample direction towards the room but still needs to trace one transmitted ray back to the sky to account for the glazing. Is this correct?" CreationDate="2017-03-14T18:01:18.603" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6967" PostId="4840" Score="0" Text="@NathanReed We don't have the winding direction, so we could potentially compare the average with both flat geometric normal vectors and pick one. What if the triangle has some sort of weighted normal distribution and the vectors are given to compensate for this? This might be too general of a case I'm thinking of though..." CreationDate="2017-03-14T19:28:58.263" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="6968" PostId="4842" Score="0" Text="@Dan Sorry, I interpreted your comment incorrectly." CreationDate="2017-03-14T19:31:07.473" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="6969" PostId="4840" Score="0" Text="I suspect you're overthinking it, since this sounds like a homework problem. :) But it's true the winding direction needs to be considered, or we need to check light vs normal and do double-sided lighting." CreationDate="2017-03-14T19:32:16.863" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="6970" PostId="4840" Score="0" Text="I do like your suggestion though. :) I'll incorporate it into the answer." CreationDate="2017-03-14T19:33:06.447" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="6971" PostId="4840" Score="0" Text="Actually, I decided to compare it against the average normal vector. The reasoning behind this is that the supplied vertex normals can help us determine the winding order if the triangle is single-sided." CreationDate="2017-03-14T20:31:46.313" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="6972" PostId="4849" Score="0" Text="Ok. I haven't got time today, but will try to add some more info soon(ish)." CreationDate="2017-03-15T12:10:07.650" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6973" PostId="4842" Score="0" Text="@aces No big deal. Hopefully your answer helps the OP!" CreationDate="2017-03-15T16:16:15.100" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="6975" PostId="4863" Score="0" Text="But it's required only if one maps a texture to the object?" CreationDate="2017-03-16T06:40:49.887" UserId="6029" ContentLicense="CC BY-SA 3.0" />
  <row Id="6976" PostId="4842" Score="0" Text="My lecturer told me this was wrong. The question asks to use flat shading, which is a different equation." CreationDate="2017-03-16T09:33:21.493" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6977" PostId="4856" Score="0" Text="My lecturer told me this method is not answering the question, because the question asks for flat shading." CreationDate="2017-03-16T09:33:51.503" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6978" PostId="4866" Score="0" Text="To my understanding, this is how to calculate the RGB intensity values using flat shading. I would really appreciate seeing what others have to say about this." CreationDate="2017-03-16T09:45:52.650" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6980" PostId="4849" Score="0" Text="What you do suggest is that I keep using my method of constructing triangles but that I should use a spatial subdivision algorithm to &quot;fasten&quot; the calculation?" CreationDate="2017-03-16T17:00:50.857" UserId="5770" ContentLicense="CC BY-SA 3.0" />
  <row Id="6982" PostId="4869" Score="0" Text="The (surface normal x Light intensity) / diffuse part, shouldn't that be N.L * lightColour * diffuseColour ?" CreationDate="2017-03-17T07:56:16.103" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6983" PostId="4869" Score="0" Text="I didn't add a divide anywhere in the equation. It is lightIntensity*(diffusive reflection coefficient * (N.L))" CreationDate="2017-03-17T08:33:59.497" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6984" PostId="4869" Score="0" Text="@PaulHK Is it possible you could give a reference to where you found the equation for flat shading?" CreationDate="2017-03-17T08:56:44.863" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="6985" PostId="4869" Score="0" Text="OpenGL's fixed function pipeline, I found a decent reference here &gt; http://www.cs.cmu.edu/afs/cs/academic/class/15462-s09/www/lec/02/lec02b.pdf" CreationDate="2017-03-17T09:33:54.467" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6986" PostId="4869" Score="0" Text="This equation doesn't consider flat or smooth shading, that is down to you to use either a fixed normal per triangle or interpolated vertex normals." CreationDate="2017-03-17T09:34:51.223" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="6987" PostId="4849" Score="0" Text="It seems to me you have two basic options though, in some senses, they can both converge to an 'equivalent' result: (a) Interval arithmetic, to weed out regions followed by Newton-Rhapson to solve for the ray intersection or (b) dice (i.e. tessellate)  your object into &quot;sufficiently small&quot; triangles that you don't see discontinuity artefacts (as in your first image). In a sense these are &quot;similar&quot; since in (a) each Newton-R iteration approximates the surface locally as a plane (like a small triangle). I suspect b will be much easier to implement but tricky to estimate ideal subdivision level." CreationDate="2017-03-17T10:19:44.837" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="6989" PostId="4873" Score="0" Text="Thanks for your answer. You mentioned that brdf samples can be shared between direct and indirect. Could you explain this a bit more please." CreationDate="2017-03-18T10:00:31.967" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6990" PostId="4873" Score="0" Text="You're welcome, @ali. I extended the answer. If not clear enough, please, let me know." CreationDate="2017-03-18T14:44:41.553" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="6991" PostId="4868" Score="0" Text="Are you trying to implement a general-use robust path tracer or is it intended to be used in a special (and fairly difficult) scenarios like the one you showed in your picture?" CreationDate="2017-03-18T14:48:00.813" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="6992" PostId="4873" Score="0" Text="Well, a bit struggling to get your idea, but I guess that's what I used to do in this situation: on doing the direct lighting, as the solid angle for visible part of the sky and the external obstruction are same, I could use one pdf to compute the contribution of the both: either from the sky(if no hit) or from the external obstruction. The problem arises on computing the radiance of the external hit as it needs recursive calls(a separate complete path) to get the correct estimate. This works though makes it quite slow and a complex code; it is also advised not to do indirect in direct part." CreationDate="2017-03-18T16:17:19.310" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6993" PostId="4868" Score="0" Text="No no aim for implementing a general-use path tracer. Yet this is sort of a typical scenario in doing daylight calculation and I try to improve the efficiency of path tracer here. I came across an exercise in pbrt book which suggests to flag, and then sampling certain objects which are important source of indirect, along with brdf sampling(I have updated the question with this exercise, please take a look); and thought this could be a solution to my case." CreationDate="2017-03-18T16:29:40.347" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="6994" PostId="4874" Score="2" Text="You seem to keep using the words without any idea of what they mean. While &quot;building&quot; and &quot;compiling&quot; are often used synonymously, I have never seen &quot;rendering&quot; being used as a synonym for them. It's overall unclear what you're really asking about. Perhaps you should get some more programming experience that could better inform you as to what all of these terms mean." CreationDate="2017-03-18T19:03:15.077" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="6995" PostId="4868" Score="0" Text="I've edited for appearance only - I didn't want to change the equations without checking with you first. I'm guessing that in both the description and the pseudocode it should say $W2 = p_2 / (p_2 + p_3)$ rather than $W2 = p_2 / p_2 + p_3$ (since that would evaluate the division before the addition)." CreationDate="2017-03-19T00:44:43.883" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6996" PostId="4848" Score="0" Text="For the specific problem outlined in red in the first image, it looks like you might need to check which side of the surface you are approaching before deciding which way the normal should point, otherwise some regions will be shaded incorrectly. This shows up in particular for a Möbius strip, since there must be a point at which the normals switch direction due to the twist. There must be some adjacent triangles that have almost opposite normal directions (like the light and dark adjacent triangles in the image)." CreationDate="2017-03-19T01:14:16.007" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="6997" PostId="4848" Score="0" Text="Yes, I thought that as well but I am setting a `isInside` flag in the triangle intersection method. If it is true I then negate the normal vector. The flag is set if `det &lt; 0` using an intersection method I don't recall the name for now" CreationDate="2017-03-19T02:33:45.410" UserId="5770" ContentLicense="CC BY-SA 3.0" />
  <row Id="6998" PostId="4848" Score="0" Text="I am using the [Möller–Trumbore intersection algorithm](https://www.wikiwand.com/en/M%C3%B6ller%E2%80%93Trumbore_intersection_algorithm)" CreationDate="2017-03-19T03:04:25.550" UserId="5770" ContentLicense="CC BY-SA 3.0" />
  <row Id="6999" PostId="4868" Score="0" Text="Many thanks @trichoplax. It is correct the way you have written it. My apology as I am new to this group and couldn't find correct fonts/style for maths." CreationDate="2017-03-19T06:45:59.497" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7000" PostId="4868" Score="0" Text="There is a guide to using MathJax for mathematical formatting [over on Worldbuilding Stack Exchange](https://worldbuilding.meta.stackexchange.com/questions/607/how-do-i-add-mathematical-notation-using-latex-mathjax), but there is no obligation to use MathJax - I just edited it in to make it more readable." CreationDate="2017-03-19T13:46:48.897" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7001" PostId="4867" Score="1" Text="It would be great to have a summary of what made the difference and how it helped." CreationDate="2017-03-19T15:13:27.723" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7003" PostId="4863" Score="1" Text="@mavavilj yes texture coordinates are not relevant if you are not using textures." CreationDate="2017-03-19T15:20:16.160" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7005" PostId="4857" Score="1" Text="Are you asking which properties should be treated differently at different distances?" CreationDate="2017-03-19T16:05:11.967" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7006" PostId="4857" Score="0" Text="Exactly, that's what I meant :)" CreationDate="2017-03-19T16:09:44.123" UserId="4646" ContentLicense="CC BY-SA 3.0" />
  <row Id="7007" PostId="4857" Score="1" Text="It might help to edit to explain this - it took me a while to guess what you meant." CreationDate="2017-03-19T16:12:25.123" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7008" PostId="4876" Score="1" Text="You are awesome! you always answer all my questions! Thanks a lot! PS: You have to make an opengl tutorial." CreationDate="2017-03-19T20:42:19.047" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="7009" PostId="4771" Score="1" Text="Inspired by your question, I've posted a [related one](http://computergraphics.stackexchange.com/q/4877/5681) regarding the use of more accurate Fresnel approximations in a RGB-based path tracer.  I am just commenting it here in the case you are interested." CreationDate="2017-03-19T21:06:30.083" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="7010" PostId="4771" Score="0" Text="Brilliant. I have tried to implement gold appearance but don't think it looks right and have been reading more about Fresnel, extinction coefficients, etc. I'm not on this full time so it's going slow. Thanks." CreationDate="2017-03-19T22:32:53.300" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="7011" PostId="4868" Score="0" Text="ali, just to be sure, do I get it right that P2 (the black part of the image) is the geometry which blocks light from the white background visible on your image? And, what do you mean &quot;P2 is same PDF but to account for external surfaces contributions&quot;? I have problems understanding this sentence. Is it the same as P1, or is it somehow changed P1?" CreationDate="2017-03-19T23:22:52.830" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="7013" PostId="4866" Score="0" Text="There are multiple problems with the normal. The first is that it needs to be normalized. The second issue is that it points in the opposite direction as the vertex normals. Maybe that's somehow correct, but since you weren't given a winding order, I would pick the direction that's similar to the direction of the vertex normals:&#xA;http://computergraphics.stackexchange.com/questions/4839/how-do-you-calculate-costheta-for-diffuse-in-flat-shading" CreationDate="2017-03-20T01:50:33.427" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="7014" PostId="4842" Score="0" Text="It isn't a &quot;different equation.&quot; The only difference is how you treat the normals (and possibly the interpolation of vertex information such as color, but that's not necessary in this question since the vertices don't have individual colors)." CreationDate="2017-03-20T01:53:16.443" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="7015" PostId="4856" Score="0" Text="This does answer the question. The only difference between flat shading and smooth shading in this question is the normal you use in the lighting calculation. That's why I edited your earlier question." CreationDate="2017-03-20T01:59:01.477" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="7016" PostId="4856" Score="0" Text="Also, I treat the diffuse reflectance coefficients as the diffuse color (maybe there's some miscommunication here?) because it doesn't make sense for them to be three-component vector of arbitrary values. It's used in energy conservation. The same thing is applicable to the diffuse light attenuation you put in you question; it should only be a single value (although the calculation is the same for this question)." CreationDate="2017-03-20T02:03:36.663" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="7017" PostId="4880" Score="0" Text="They look fine to me, but be aware of row/column order of your C++ side matrices. Although as you say, your symptoms are only slightly wrong so it's probably some other issue." CreationDate="2017-03-20T10:08:24.443" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7018" PostId="4868" Score="0" Text="First thanks for your attention :). The black part of the image, marked as P2, is external buildings that blocks light from the sky for indoor surfaces. Yes P2 is same pdf as P1; P2 is 1/(window area) to sample points on the window to estimate the contribution of external buildings. P1 is same pdf, 1/(window area), again to sample points on window opening; but this time they are shadow rays to get an estimate from the visible sky(direct light);" CreationDate="2017-03-20T11:13:10.697" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7019" PostId="4868" Score="0" Text="Its essentially one estimator in both cases with same pdf, one to get the direct light, the other the indirect from the external buildings. The only different is the Visibility test in the two estimators. In P1 any shadow rays that hits the external objects are dismissed whereas in the case of P2 any ray that returns no intersect is ignored(as the domain is on external buildings). I hope this explains better." CreationDate="2017-03-20T11:13:13.713" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7020" PostId="4880" Score="0" Text="I cant get it to handle the lightingdirection correctly" CreationDate="2017-03-20T11:14:44.540" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7021" PostId="4795" Score="2" Text="@WillGoldie POmax has added a section on his fabulous bezier curve page that actually adresses how to simplify beziers as arcs which might be of use for you: https://pomax.github.io/bezierinfo/#arcapproximation" CreationDate="2017-03-20T11:44:32.707" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7022" PostId="4857" Score="0" Text="Thanks, you are totally right! I tried to rephrase the question, please do tell if it's still hard to understand." CreationDate="2017-03-20T12:00:58.493" UserId="4646" ContentLicense="CC BY-SA 3.0" />
  <row Id="7023" PostId="4881" Score="0" Text="I will keep that in mind but I was talking about generalised methods" CreationDate="2017-03-20T12:36:39.603" UserId="5506" ContentLicense="CC BY-SA 3.0" />
  <row Id="7024" PostId="4880" Score="0" Text="You could fix your normals to point towards the camera (e.g. 0,0,-1) then try a forward light dir of 0,0,1 without using the normal matrix to at least eliminate that as a problem." CreationDate="2017-03-20T13:05:11.300" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7025" PostId="4880" Score="0" Text="To add : Doing the above is to produce predictable results without using a normal matrix, you can then re-introduce the normal matrix to this scheme as a method for testing the normal matrix in your shader" CreationDate="2017-03-20T13:36:19.780" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7026" PostId="4881" Score="0" Text="@newin When you say generalised, do you mean non-flat surfaces, or multiple flat surfaces? Do you require distortion of the reflections? Multiple reflections of mirrors in other mirrors? It would help to edit the question to specify what you do need and what you don't need, then answers can take that into account and focus on just what is necessary." CreationDate="2017-03-20T20:49:48.910" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7028" PostId="4881" Score="1" Text="@trichoplax I edited my question with much more precisions I hope that's enough." CreationDate="2017-03-20T22:13:49.327" UserId="5506" ContentLicense="CC BY-SA 3.0" />
  <row Id="7030" PostId="4885" Score="0" Text="Yes, you are right. When I multiply position with model only, and not view, I get the right angle. Thank you very much. (I think I understand too)" CreationDate="2017-03-21T08:35:29.057" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7031" PostId="4880" Score="0" Text="I was sending the wrong matrix in" CreationDate="2017-03-21T08:46:31.727" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7032" PostId="4880" Score="1" Text="@Charlie If you've now worked out what is wrong, could you please write it up as an answer? It will be helpful to any future visitors who have a similar problem." CreationDate="2017-03-21T11:19:14.023" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7033" PostId="4880" Score="0" Text="@DanHulme I modified the values in the original, but ill post a clarification too" CreationDate="2017-03-21T12:08:46.737" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7034" PostId="4880" Score="1" Text="Don't edit the original to include the answer. It makes it harder for other visitors to see what the original problem was. They made it easy to answer your own question to avoid any confusion over what is the question and what is the answer." CreationDate="2017-03-21T12:43:16.230" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7035" PostId="4883" Score="1" Text="With you can also insert debug messages into the opengl stream with push/pop debug group to get some context. The opengl objects created can also be named with glObjectLabel." CreationDate="2017-03-21T13:07:06.040" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7038" PostId="4890" Score="3" Text="The simplest option is to get higher resolution of your shadow map in areas you care about." CreationDate="2017-03-22T09:04:51.003" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7039" PostId="4890" Score="3" Text="Percentage Closer Filtering" CreationDate="2017-03-22T12:58:04.123" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="7042" PostId="4895" Score="0" Text="maybe it's the scale, when the height you are timing is 1000 m then falling is going to seem slow." CreationDate="2017-03-23T14:17:37.010" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7043" PostId="4895" Score="0" Text="I didn't think about that.  I have an 800x800 window and I have the viewport mapped at 1 meter per pixel.  I just googled and it takes around 19 seconds to fall from 800 meters." CreationDate="2017-03-23T14:30:18.810" UserId="6025" ContentLicense="CC BY-SA 3.0" />
  <row Id="7044" PostId="4895" Score="1" Text="You should print out all the values to debug this code. The first bug is that `last` is only initialized once: it's the time of the first frame, not the time of the previous frame." CreationDate="2017-03-23T17:01:39.963" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7045" PostId="4895" Score="1" Text="actually, it's initialized every frame.  I did debug." CreationDate="2017-03-23T18:36:25.493" UserId="6025" ContentLicense="CC BY-SA 3.0" />
  <row Id="7046" PostId="4902" Score="2" Text="You will see circles, due to diffraction" CreationDate="2017-03-24T01:46:08.027" UserId="6290" ContentLicense="CC BY-SA 3.0" />
  <row Id="7047" PostId="4890" Score="0" Text="PCF isn't really that good. In fact, PCF is the naive brute force solution for shadow map filtering. It definitely does not deserve to be called &quot;the go to&quot; solution. Even the simplest exponential shadow maps can be linearly filtered without destroying performance." CreationDate="2017-03-24T01:47:10.407" UserId="6285" ContentLicense="CC BY-SA 3.0" />
  <row Id="7048" PostId="4890" Score="0" Text="A realistic solution for a game engine is something more like using cascade shadow map partitioning in conjunction with a linearly filterable depth representation that you can pre-filter to the desired degree of smoothness." CreationDate="2017-03-24T01:51:54.443" UserId="6285" ContentLicense="CC BY-SA 3.0" />
  <row Id="7051" PostId="4905" Score="0" Text="Yeah, I've read that article. But does this phenomenon occur if we consider idealized film? Where there is no shadowing by the pixel sensor edges and no attenuation due to nonzero color filers thickness. I'm asking about phenomenon caused just be geometry." CreationDate="2017-03-24T20:08:14.883" UserId="5249" ContentLicense="CC BY-SA 3.0" />
  <row Id="7052" PostId="4899" Score="1" Text="The problem is that there are hundreds of OpenGL calls in my codebase. Is there a list of OpenGL calls which have as side effect the allocation/deallocation of memory?" CreationDate="2017-03-24T20:18:33.017" UserId="5225" ContentLicense="CC BY-SA 3.0" />
  <row Id="7053" PostId="4899" Score="1" Text="Regarding my prior comment, I'm assuming `glBindBuffer` is the key function call to look out for?" CreationDate="2017-03-24T20:23:58.273" UserId="5225" ContentLicense="CC BY-SA 3.0" />
  <row Id="7054" PostId="4902" Score="0" Text="What if we assume infinitesimal pinhole and classic optics (no diffraction)." CreationDate="2017-03-24T20:41:42.987" UserId="5249" ContentLicense="CC BY-SA 3.0" />
  <row Id="7055" PostId="4906" Score="1" Text="Could you add some clarification? Does &quot;looks like it's spinning on its y axis&quot; mean that it spins only during the process of rotating to face the camera, and is stationary thereafter? If the objects already end up facing the camera, could you explain how they currently move and what is unsatisfactory about it?" CreationDate="2017-03-25T13:00:24.043" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7056" PostId="4906" Score="1" Text="It would also help to see the code if you are able to post it." CreationDate="2017-03-25T13:08:23.527" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7057" PostId="4899" Score="1" Text="It depends on which flavor of OpenGL your program uses, I would suggest to monitor `glGenBuffers` / `glDeleteBuffers`, (see the OpenGL wiki's [Buffer Object page](https://www.khronos.org/opengl/wiki/Buffer_Object) ). Also `glCreateProgram` /  `glDeleteProgram`, `glGenSamplers` / `glDeleteSamplers`, etc" CreationDate="2017-03-25T13:26:07.680" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="7058" PostId="4899" Score="0" Text="In my code, there are 20 calls to `glGenBuffers`, and only one call to `glDeleteBuffers`. Does that automatically identify a problem, in that these calls should be 1 for 1 with each other?" CreationDate="2017-03-25T13:28:34.017" UserId="5225" ContentLicense="CC BY-SA 3.0" />
  <row Id="7059" PostId="4899" Score="0" Text="No, it is not necessary a problem, you have to check the handles of the objects generated versus deleted, and see if they match." CreationDate="2017-03-25T13:32:52.043" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="7060" PostId="4899" Score="1" Text="A good-practice for multi-platform graphics application (rendering engines, etc) is to wrap the graphics API specific functions under a generic layer (for example, your program has a structure/class `MyAppBuffer` from which are derived `MyAppOpenGLBuffer`, `MyAppDirectXBuffer`, etc. It has the benefit of having all the graphics API functions in a single place, making it easier to monitor. You could try to refactor your code in a similar way." CreationDate="2017-03-25T13:36:35.343" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="7062" PostId="4907" Score="1" Text="Could you elaborate on the problems? Also just to be sure, this doesn't have anything directly to do with OpenGL right?" CreationDate="2017-03-25T16:13:41.017" UserId="4776" ContentLicense="CC BY-SA 3.0" />
  <row Id="7063" PostId="4907" Score="0" Text="I am facing problem in implementing object picking using obb algorithm.I am not able to understand what does t1 and t2 means and how the algorithm is working. And for opengl i am using glut library if you explain the algorithm that would be enough." CreationDate="2017-03-25T16:32:12.700" UserId="6320" ContentLicense="CC BY-SA 3.0" />
  <row Id="7064" PostId="4815" Score="1" Text="The densest known packing of the regular pentagon is [slightly different](http://blogs.ams.org/visualinsight/2014/12/01/packing-regular-pentagons/)." CreationDate="2017-03-25T22:55:03.243" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7065" PostId="4908" Score="0" Text="thanks for this i just have couple of questions....first what are the equations of this plane and for 3d i should have a pair of z planes right?" CreationDate="2017-03-26T13:48:13.903" UserId="6320" ContentLicense="CC BY-SA 3.0" />
  <row Id="7073" PostId="4905" Score="1" Text="Yes, that's the &quot;natural vignetting&quot; on the wiki page" CreationDate="2017-03-27T03:26:20.610" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7074" PostId="4907" Score="0" Text="First of go with GLFW library rather than going with freeglut And check out this tutorial... http://antongerdelan.net/opengl/raycasting.html" CreationDate="2017-03-27T05:17:16.937" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="7075" PostId="4914" Score="5" Text="That is an anisotropic specular reflection (dominant in 1 direction), an effect usually from brushed metals." CreationDate="2017-03-27T05:37:09.193" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7076" PostId="4914" Score="2" Text="check here &gt; http://sungsoo.github.io/papers/brdf-mdf.pdf" CreationDate="2017-03-27T06:34:22.613" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7078" PostId="4912" Score="1" Text="Could you add a link to the paper? That way it will be easier for someone to help you." CreationDate="2017-03-27T08:54:25.253" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7079" PostId="4907" Score="1" Text="@dkoder Can you edit the question to summarize what you understand already and where specifically you're having trouble? Currently your question is too unclear to really answer. :(" CreationDate="2017-03-27T17:02:34.117" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7080" PostId="4908" Score="0" Text="Yes for 3D you'd just have to add z as well, the concept stays the same. I can write you a short snippet of the algorithm. Do you have a preferred language? C, Python or?" CreationDate="2017-03-28T00:33:58.307" UserId="4776" ContentLicense="CC BY-SA 3.0" />
  <row Id="7081" PostId="4916" Score="0" Text="First of all, have you tries other computers and verified that it isn't your GPU that's the problem? Because I'm facing a similar problem, because my GPU is starting to get a few years behind it. Second of all. Any additional context would greatly help. Did you write the application? Reaching any limits? What does the shader look like? Checked for OpenGL errors?" CreationDate="2017-03-28T00:36:55.363" UserId="4776" ContentLicense="CC BY-SA 3.0" />
  <row Id="7082" PostId="4916" Score="0" Text="I'd look at your post processing first, the black artefact at the start looks like it is being blurred each frame by something. Maybe you are rendering something into the wrong buffer somehow ?" CreationDate="2017-03-28T02:36:40.477" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7083" PostId="4916" Score="0" Text="FWIW, I have seen issues like this when NaNs enter the pipeline, either by being passed from the CPU or being calculated on the GPU. I can't say whether that's your issue or not, but it did end up looking similar to this." CreationDate="2017-03-28T04:36:25.420" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7084" PostId="4920" Score="0" Text="Please dont ask for oppinions. They dont give out clear answers. Yes, it works." CreationDate="2017-03-28T05:05:54.023" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7085" PostId="4919" Score="4" Text="seems to me that this kind of review is a full half of a PhD thesis" CreationDate="2017-03-28T05:08:32.367" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7086" PostId="4918" Score="1" Text="Why scan it? I mean most likely there is a model of the bottle somewhere (ask manufacturer) or then modeling it from scratch which usually is not a big deal. Also scanning is more work than taking a hundred pictures." CreationDate="2017-03-28T05:13:20.123" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7087" PostId="4916" Score="0" Text="@Vallentin I have lots of shaders and framebuffers... that's why i didn't provide any code and in this code there are not many hints of anything.. . There was this issue when my renderer was forward, then I rewrited it to deferred, added some color attachments ping pong (to apply post effects, fog, light) and it worsen the condidtion. There are some gl errors (1280) when adding color attachments to framebuffers which I don't understand because they work at the end. Yeah I didn't check it on other computer. My GPU is quite new (gtx970)" CreationDate="2017-03-28T08:28:39.633" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7088" PostId="4916" Score="0" Text="@PaulHK I'll check postprocess and btw white flashes in first gif are from eye adaptation effect. There is pass to calculate average luminance (downsample to 1x1) and these error values seem to confuse this." CreationDate="2017-03-28T08:38:33.037" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7089" PostId="4916" Score="0" Text="This is just a renderer done in free time to test some advanced graphic techniques, nothing pro." CreationDate="2017-03-28T08:45:18.113" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7090" PostId="4916" Score="0" Text="@PaulHK Yes increasing blur sigma is also increasing the size of squares, so artifacts get blurred." CreationDate="2017-03-28T08:49:14.683" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7091" PostId="4918" Score="1" Text="I don't think a rendering approach will work well for you in this case, as a lot of the bottles on your belt will be crushed, which is quite hard to simulate." CreationDate="2017-03-28T08:57:17.907" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7092" PostId="4915" Score="0" Text="In deferred way you first loop through your models and render albedo, normals and positions to framebuffer color attachments and then apply all your lights in single draw call irrelevantly to number of models." CreationDate="2017-03-28T11:03:20.850" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7093" PostId="4789" Score="0" Text="Hi, I just made a new question: http://computergraphics.stackexchange.com/questions/4917/final-transformation-matrix-to-transform-world-coordinate-into-vrc The problem I am having with your final transformation matrix is that it is not following the steps my lecture slides are telling me. My lecture slides tell me to multiply the rotation matrix with the translation matrix to find the final transformation matrix." CreationDate="2017-03-28T11:18:59.830" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="7094" PostId="4918" Score="0" Text="@DanHulme I dont know we did it for testing a reverse vending machine for testing and it seemed to work quite well. Altough it is true that real thing has different levels of data. But it was quite much easier to have digital reolicates of 2000 bottles than actually go and buy them. as quite many of the models were available from the vendor with some emailing and asking nicely." CreationDate="2017-03-28T14:17:17.487" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7095" PostId="4918" Score="0" Text="@joojaa Yeah, in general it can be a really effective technique, but if it's for post-consumption plastic bottles you can't expect them to be the same shape as new bottles" CreationDate="2017-03-28T14:24:42.787" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7096" PostId="4918" Score="0" Text="@DanHulme Depends on where you are, in Finland where nearly all bottles and cans are recycled they will in fact all be 90% intact by the time they return as manhandling them means you will not get the deposit back. Glass bottles will be reused as bottles." CreationDate="2017-03-28T14:27:44.393" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7098" PostId="4920" Score="0" Text="@joojaa Some answers can be based on widely-held community opinions called &quot;conventions&quot;, which I may not know of, which is why I'm asking. After thinking about it, it doesn't seem like I want *every* shader instance to run a tween that I may only want for a handful of vertices." CreationDate="2017-03-28T17:55:35.973" UserId="4991" ContentLicense="CC BY-SA 3.0" />
  <row Id="7099" PostId="4920" Score="0" Text="Im fine with the other parts except is it a good idea. Since good is not well defined. This is not a convention thing as its merely a optimization problem, neither is clearly better in a context free situation. But we can not optimize in all possible contexts." CreationDate="2017-03-28T18:33:00.740" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7100" PostId="4922" Score="0" Text="Could you explain what you do know about Gouraud shading so we can see what level of explanation you require?" CreationDate="2017-03-28T20:50:01.207" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7101" PostId="4922" Score="0" Text="Aaand yet another homework..." CreationDate="2017-03-28T21:21:01.400" UserId="2479" ContentLicense="CC BY-SA 3.0" />
  <row Id="7102" PostId="4916" Score="0" Text="Do you have a method to visualise all your buffers individually ?" CreationDate="2017-03-29T07:18:17.637" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7105" PostId="4877" Score="0" Text="I'm looking forward to going through this when time permits. Your results look good compared to reference. I'd be interested to see the images from your renderer with Schlick's. In my renderer, I'm trying to use image based lighting. My gold (with Schlick's) doesn't look great, but I'm not sure if that's because of the BRDF, or the lighting. Think I might move to area lights (is that what you use?), and work in a more controlled environment until I've got things right." CreationDate="2017-03-29T08:40:42.400" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="7106" PostId="4922" Score="0" Text="@trichoplax I edited the question. I have a basic understanding of Gouraud shading. I think the intensity values change as you go through the polygon. I don't really know how to answer the question or where to start though as the symbols used in videos and research I have done do not match the information given in the question." CreationDate="2017-03-29T09:03:32.827" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="7107" PostId="4926" Score="0" Text="Great, so we both agree we have to multiply the rotation and translation matrices. I'm going to have to check with my lecturer though which order to do it. He also multiplies the last column of the final transformation matrix (M) by -1 (except for the 1 in M44). Thanks anyways." CreationDate="2017-03-29T10:09:35.977" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="7108" PostId="4926" Score="0" Text="Can you share the result after checking with your lecturer. Thanks." CreationDate="2017-03-29T10:12:55.893" UserId="6140" ContentLicense="CC BY-SA 3.0" />
  <row Id="7109" PostId="4922" Score="1" Text="As an aside, you also need to say whether the colour interpolation is linear in screen space (as would have been the case when the technique was first used) or linear in world (i.e. perspective correct in screen space) as would be the case with modern rendering hardware. (Not entirely sure, but Dreamcast might have been one of the first to do the latter)" CreationDate="2017-03-29T11:05:59.930" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7110" PostId="4926" Score="0" Text="Yeah, once I get it sure." CreationDate="2017-03-29T12:03:53.283" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="7111" PostId="4922" Score="0" Text="@Simon F not really sure. This is a past exam question. I literally copied and pasted all the information from that question. This was all the information given." CreationDate="2017-03-29T12:05:38.497" UserId="5513" ContentLicense="CC BY-SA 3.0" />
  <row Id="7112" PostId="4922" Score="0" Text="In that case, assuming the light intensity is &quot;1.0&quot;, then given constant, diffuse coefficients and a constant direction *to* the light, normalise your light direction, take the dot product with each of your (unit) normals, clamp each to be positive if necessary, average those scalar vals (because you're at the centre of the triangle) add on the ambient, and you can then multiply this by the diffuse surface colour." CreationDate="2017-03-29T13:46:57.390" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7116" PostId="4928" Score="0" Text="Yes, I have added a new computeshader.hlsl" CreationDate="2017-03-29T17:07:26.910" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7117" PostId="4885" Score="0" Text="It is worth mentioning that placing lightposition into viewspace was an even better solution ;)" CreationDate="2017-03-29T20:33:44.697" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7122" PostId="4884" Score="0" Text="If you have found a solution, please post it as an answer rather than editing it into the question. This allows voting on answers. [Self answering is actively encouraged.](http://computergraphics.stackexchange.com/help/self-answer)" CreationDate="2017-03-30T07:01:12.357" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7124" PostId="4908" Score="0" Text="Sorry for the late reply...python code would be nice" CreationDate="2017-03-30T16:21:13.487" UserId="6320" ContentLicense="CC BY-SA 3.0" />
  <row Id="7128" PostId="4937" Score="0" Text="There are several blogs talking about derivatives and it would be cool to have some feedback from people who implemented them in their production engine and why they choose that method." CreationDate="2017-03-31T08:23:22.367" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="7129" PostId="4937" Score="2" Text="One of the big factors of why things don't immediately get adopted is inertia and that the existing solution being good enough." CreationDate="2017-03-31T08:56:47.627" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7133" PostId="4918" Score="0" Text="@joojaa Asking the manufacturer is actually the best idea. You're probably right, i was under the impression that i would need at least a couple of thousand images of each bottle. Can you tell me more about the work you did with the reverse vending machine? How did you guys go about generating the data?" CreationDate="2017-03-31T14:28:52.960" UserId="6340" ContentLicense="CC BY-SA 3.0" />
  <row Id="7135" PostId="4941" Score="1" Text="Have you tried https://groups.google.com/forum/#!forum/skia-discuss? You're unlikely to get help on CGSE as the question isn't about directly computer graphics, but about using a specific API and file I/O. See http://computergraphics.stackexchange.com/help/on-topic" CreationDate="2017-04-01T08:17:51.377" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="7139" PostId="4877" Score="0" Text="@PeteUK Yes, I am using only one area light source (a sphere actually). I did not include rendered images with Schlick's so as not to clutter the question. However, I can send you equivalent images rendered with Schlick's Fresnel formulation along with the precise description of the scene so that you can reproduce on your renderer and compare. My email is available on my Github page (see my profile)." CreationDate="2017-04-01T14:42:49.873" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="7140" PostId="4877" Score="0" Text="Very kind, email on the way..." CreationDate="2017-04-01T16:24:44.620" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="7141" PostId="4942" Score="0" Text="How are you calculating the contrast ratio after you perform the edge detection? What results are you getting? Also did you leave out half a paragraph right below the images? There's a sentence fragment at the end." CreationDate="2017-04-02T02:20:31.480" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7142" PostId="4942" Score="0" Text="@user1118321 I moved it to the bulleted list at the end, nice catch." CreationDate="2017-04-02T03:00:12.897" UserId="6373" ContentLicense="CC BY-SA 3.0" />
  <row Id="7143" PostId="4949" Score="1" Text="Thank you for telling me what the name of the operation is; I found a plugin for paint.net called &quot;Modify Channels&quot;, and it serves my purpose perfectly ^^ (; As for the flipping thing, I suppose that would have worked for the indexed image; however, it would not work for anything like swapping just two channels on a sprite sheet, for one example. I probably could've used a more imaginative image than that indexed one for the purpose of this question, but being honest I was frustrated + thus in a rush when I initially posted this question. Though that 'rush' ended up taking quite a while &gt;.&gt; xD" CreationDate="2017-04-02T05:41:07.743" UserId="6376" ContentLicense="CC BY-SA 3.0" />
  <row Id="7144" PostId="4942" Score="1" Text="How does your luminosity conversion work? It seems the contrast labelling does not match the actual luminosity output you have made (the red F00 is much darker than the grey 777, even though the label indicates that the F00 should have a higher contrast ratio).." CreationDate="2017-04-03T10:43:54.860" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7145" PostId="4942" Score="0" Text="@PaulHK I bet you're onto it. It's the browser's default `luminanceToAlpha`, which is probably fouling up somewhere along the line." CreationDate="2017-04-03T12:37:47.213" UserId="6373" ContentLicense="CC BY-SA 3.0" />
  <row Id="7146" PostId="4950" Score="0" Text="Depending on the field of work, a graphics programmer may have zero knowledge of GPU architecture, or may have very detailed knowledge, or anywhere in between. The term &quot;computer graphics programmer&quot; is too broad to be able to give a meaningful answer to this question. You may be able to guide your research by finding examples of the type of thing you want to be able to work on, which will help narrow down which knowledge areas are required/beneficial." CreationDate="2017-04-03T12:49:11.373" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7147" PostId="4950" Score="0" Text="@trichoplax just updated the description. Hopefully it's a little less broad." CreationDate="2017-04-03T14:25:01.213" UserId="6384" ContentLicense="CC BY-SA 3.0" />
  <row Id="7148" PostId="4950" Score="0" Text="Maybe that people run systems that are diverse and a few years old. Relying on the newest prototypes might cause more harm than good, as they have yet to become standard" CreationDate="2017-04-03T15:33:24.143" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7149" PostId="4942" Score="1" Text="Indeed, it's typical for luminance to give green the most weight and blue the least. For example, [BT.709 luma](https://en.wikipedia.org/wiki/Rec._709#Luma_coefficients)." CreationDate="2017-04-03T15:38:00.563" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7151" PostId="4954" Score="0" Text="Why not density level set on the voxel structure?" CreationDate="2017-04-04T14:30:25.980" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7152" PostId="4950" Score="1" Text="You might enjoy learning about PowerVR-style tile-based renderers.  The usual books (H&amp;P, etc) don't cover that.  As far as writing OpenGL applications, there is often a matching between vendor-specific extensions and vendor-specific hardware.  Take a look at mobile GPU vendor extensions, programmable blending is the first that comes to mind." CreationDate="2017-04-04T17:47:09.437" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="7153" PostId="1755" Score="0" Text="Plenty of L-System examples (including trees other plants) are available in [VLab](http://algorithmicbotany.org/virtual_laboratory/) (for mac) and [L-Studio](http://algorithmicbotany.org/virtual_laboratory/) (for PC). You can reproduce the figure by downloading their software and executing its L grammar (which is partially available in the book)." CreationDate="2017-02-03T07:36:32.633" UserId="5881" ContentLicense="CC BY-SA 3.0" />
  <row Id="7154" PostId="4952" Score="0" Text="I've guessed that the capitalised &quot;PI&quot; was referring to the constant $\pi$ as I couldn't think of an acronym that would fit. If this is not your intention please edit to correct." CreationDate="2017-04-04T21:00:56.893" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7155" PostId="4932" Score="0" Text="I suppose I'd like to animate any numbers of particles to randomly chosen end points. It could be 100000 particles (where a particle could possibly have multiple vertices). It's a personal project for fun, so no time limit. I don't need the data on the CPU side, but I just need to know when the tween is complete on the CPU side." CreationDate="2017-04-04T23:14:38.323" UserId="4991" ContentLicense="CC BY-SA 3.0" />
  <row Id="7156" PostId="4920" Score="0" Text="@joojaa Suppose we're animating 100000 vertices to randomly-chosen end points." CreationDate="2017-04-04T23:15:37.567" UserId="4991" ContentLicense="CC BY-SA 3.0" />
  <row Id="7157" PostId="4958" Score="3" Text="The effect will also be more noticeable at the terminator as it would add some variation to an otherwise perfectly smooth curve." CreationDate="2017-04-05T05:38:49.563" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7158" PostId="4922" Score="0" Text="I would have thought you would take the average of the 3 vertex normals to get the triangle centre normal. Then normalise your light direction (1,2,2). So (in psuedo/glsl) saturate(dot( (Na+Nb+Nc)/3, normalize(L) ) ) * diffuseColor;  ?" CreationDate="2017-04-05T08:27:22.913" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7159" PostId="3800" Score="0" Text="wasn't the original `El` name derived from &quot;emissive light&quot; ? which was not a convolved term, but an additive term in front of the integral. Irradiance should be named `Ir` no ?" CreationDate="2017-04-05T09:14:00.997" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="7160" PostId="3800" Score="0" Text="@v.oddou E is the usual variable for irradiance that I've seen used in texts and papers. Emissive light is usually denoted $L_e$ (L is radiance)." CreationDate="2017-04-05T13:37:02.617" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7161" PostId="4961" Score="0" Text="hey that's very interesting. that's probably way easier since our engine is more based on d3d ways. I'll look into that tomorrow" CreationDate="2017-04-05T13:58:50.483" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="7162" PostId="4957" Score="0" Text="Why the vertical and not the horizontal should be used? The 50px and the 30px is horizontal measurement in the frame..&#xA;And the sizes are what I got by the algorithm, maybe the it not 100% accure" CreationDate="2017-04-05T14:02:35.427" UserId="3305" ContentLicense="CC BY-SA 3.0" />
  <row Id="7163" PostId="4958" Score="2" Text="Even if it is a shadowside slope of a mountain gets less light rhan one without. Even self shadowing by some method might be noticeable." CreationDate="2017-04-05T14:18:44.130" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7164" PostId="4953" Score="0" Text="Hey, I mentioned it in the EDIT that always returning 90 degrees was the mistake I did. But even after that, I got the results as shown in the image. Most of the cylinders went to their correct position but some have deviated a little for some reason. Could be an implementation error again. Thanks for your answer :)" CreationDate="2017-04-06T08:39:04.137" UserId="5121" ContentLicense="CC BY-SA 3.0" />
  <row Id="7165" PostId="4965" Score="1" Text="Do you have any recommendation for a library that is simple (can be implemented using only c++ code, no additional software and/or designers), fast, c++ oriented (opposed to written for c, but usable) and lightweight (without non gui modules) ?" CreationDate="2017-04-06T10:40:08.937" UserId="5215" ContentLicense="CC BY-SA 3.0" />
  <row Id="7166" PostId="4965" Score="1" Text="Myself I got used to working with QtDesigner IDE + Qt libs, where I design gui using built in designer and subclass widget to get gl context and then use glew to get gl extensions. It is only c++. There are more compact c++ libs for only gui making, like wxwidgets, maybe you should check that out, I don't have much experience with them though." CreationDate="2017-04-06T13:20:22.720" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7170" PostId="4967" Score="0" Text="Some of your pros seem to be about not using precomputed tangent space (i.e. deriving the tangent space from UV derivatives per-pixel), which AFAIK is a separate design choice, independent from the choice of derivative maps vs normal maps." CreationDate="2017-04-06T21:32:47.497" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7171" PostId="4970" Score="2" Text="Are you familiar with differentials in calculus, like $dy/dx$ and such? Are you familiar with the concept of solid angle?" CreationDate="2017-04-06T23:33:58.497" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7172" PostId="4970" Score="0" Text="@NathanReed yes I am familiar with dy/dx but I am not familiar with concept of solid angle." CreationDate="2017-04-07T00:31:14.203" UserId="6282" ContentLicense="CC BY-SA 3.0" />
  <row Id="7173" PostId="4970" Score="0" Text="@NathanReed ok soI did more research and I think I have better understanding of solid angle. Solid angle ranges from 0~4 pi. But what does differential of solid angle of the normal vector at &quot;i&quot; mean?" CreationDate="2017-04-07T05:58:46.217" UserId="6282" ContentLicense="CC BY-SA 3.0" />
  <row Id="7174" PostId="4970" Score="0" Text="It looks like an integral that's been quantified." CreationDate="2017-04-07T08:40:24.780" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7175" PostId="4968" Score="0" Text="For some reason glClientWaitSync always fails for me, if timeout is non-0. Even if it's 1, then there's console warning:&#xA;WebGL: INVALID_OPERATION: clientWaitSync: timeout &gt; MAX_CLIENT_WAIT_TIMEOUT_WEBGL" CreationDate="2017-04-07T11:59:24.640" UserId="6404" ContentLicense="CC BY-SA 3.0" />
  <row Id="7176" PostId="4967" Score="0" Text="Thanks for the comment @NathanReed Are you talking about the fact of calculating tangents and binormals ? Could you tell me more about precomputed tangent space ?" CreationDate="2017-04-07T15:12:59.450" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="7177" PostId="4971" Score="1" Text="Also, metalness is often a binary value, and assuming that you will not use roughness gradients (so as to see the impact of reduced precision), you could store metalness and roughness in a single 8 bit channel (1 bit for metalness and 7 for roughness)." CreationDate="2017-04-07T15:33:34.287" UserId="270" ContentLicense="CC BY-SA 3.0" />
  <row Id="7178" PostId="4972" Score="1" Text="Could you be a little more specific as to what you don't really understand? Is it how the waves are generated and the parameters? I'm sure many people here could help you." CreationDate="2017-04-07T19:12:53.157" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7179" PostId="4968" Score="0" Text="Did you verify that you're using WebGL 2.0?" CreationDate="2017-04-07T21:54:48.370" UserId="4776" ContentLicense="CC BY-SA 3.0" />
  <row Id="7180" PostId="4968" Score="0" Text="Of course, context is WebGL2RenderingContext, and I use other WebGL2.0-only features." CreationDate="2017-04-08T00:10:38.807" UserId="6404" ContentLicense="CC BY-SA 3.0" />
  <row Id="7181" PostId="4968" Score="0" Text="Check whether `sync` is `0` or `null`. Also [check whether your browser supports it](https://developer.mozilla.org/en-US/docs/Web/API/WebGL2RenderingContext/fenceSync#Browser_compatibility)." CreationDate="2017-04-08T00:13:30.870" UserId="4776" ContentLicense="CC BY-SA 3.0" />
  <row Id="7182" PostId="4972" Score="0" Text="@ArjanSingh Yes! I updated the question." CreationDate="2017-04-08T02:49:27.947" UserId="6407" ContentLicense="CC BY-SA 3.0" />
  <row Id="7183" PostId="4808" Score="3" Text="Specifically what kind of constraints do you want to include?" CreationDate="2017-04-08T05:29:49.060" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7184" PostId="4968" Score="0" Text="`sync` works fine, I can get it's status with `getSyncParameter`. It is only the `clientWaitSync` that has issues. Chrome 57 in macOS 10.12.4 on MacBook mid-2012." CreationDate="2017-04-08T10:25:07.883" UserId="6404" ContentLicense="CC BY-SA 3.0" />
  <row Id="7185" PostId="4968" Score="0" Text="Try and check `gl.MAX_CLIENT_WAIT_TIMEOUT_WEBGL` maybe `gl.clientWaitSync()` just isn't allowed in Chrome on MacOS." CreationDate="2017-04-08T16:46:35.120" UserId="4776" ContentLicense="CC BY-SA 3.0" />
  <row Id="7187" PostId="4978" Score="0" Text="Since the objective of the chapter is to implement Lambert could you suggest an alternative implementation? Where does the grey color from the sphere come from, is it set anywhere?" CreationDate="2017-04-10T10:54:42.507" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7189" PostId="1793" Score="1" Text="this is a bit late, but [here](https://i.stack.imgur.com/nKqSd.jpg) you can see a STATIC mesh with 24.000.000 Vertices in Blender.&#xA;And i can rotate it SMOOTH with 40 FPS. I think it is just amazing what modern graphic cards can do." CreationDate="2017-04-09T15:42:59.287" UserId="6420" ContentLicense="CC BY-SA 3.0" />
  <row Id="7191" PostId="4983" Score="0" Text="What is the error you get? Do you have the debug layer turned on?" CreationDate="2017-04-10T22:00:24.873" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7192" PostId="4983" Score="0" Text="Yes, I have debug layer enabled.&#xA;when this is called :&#xA;&#xA;ThrowIfFailed(D3DX12SerializeVersionedRootSignature(&amp;computeRootSignatureDesc_madhu, featureData_madhu.HighestVersion, &amp;signature1, &amp;error1));&#xA;&#xA;an exception occures, stepping through the code I see that the handle returned into the signature is invalid" CreationDate="2017-04-10T22:38:55.377" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7193" PostId="4983" Score="0" Text="Ok. So is there any output from the debug layer about the error? If you're using Visual Studio, it would show up in the Output pane. Alternatively, is there any error message generated in the `error1` blob?" CreationDate="2017-04-10T22:44:28.947" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7194" PostId="4983" Score="0" Text="Not sure how to check the error from &quot;error1&quot; blob.&#xA;But the return value is E_INVALIDARG- one or more arguments are invalid." CreationDate="2017-04-10T22:50:55.130" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7195" PostId="4983" Score="0" Text="You can print the error blob to the output pane like this: `if (error1) { OutputDebugStringA((char *)error1-&gt;GetBufferPointer());	}`" CreationDate="2017-04-10T23:04:43.280" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7196" PostId="4983" Score="0" Text="Shader register range of type UAV (root parameter [1], visibility ALL, descriptor table slot [0]) overlaps with another shader register range (root parameter[0], visibility ALL, descriptor table slot [0]).&#xA;&#xA;&#xA;&#xA;This is the error that is printed. &#xA;Is there something that I could do to manage this?" CreationDate="2017-04-10T23:18:34.163" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7197" PostId="4985" Score="0" Text="That makes sense, though I doubt I'm running out of VRAM honestly because last time I checked these textures are only ~40MB each or so and a 960M has 4GB so there should plenty. Also yeah that whole prioritize uninitialized and first in/first out system of managing the tiles should always make sure it updates the oldest freed tile. I'll try that call though to see if it works." CreationDate="2017-04-11T00:34:59.973" UserId="4837" ContentLicense="CC BY-SA 3.0" />
  <row Id="7198" PostId="4985" Score="0" Text="Well I implemented the `glInvalidateTexSubImage` thing when the tiles are freed (assuming I did it right, it calls it on each mipmap level for the layer in question) and it doesn't seem to change anything. I'm going to keep it because it probably will be helping the GPU in some way but I guess sending something to NVIDIA might be the best option now (I'll likely try to make a minimal example of it in a small program later when I have time). For now I'm also going to try initializing the entire texture upfront to see if that helps." CreationDate="2017-04-11T17:58:25.537" UserId="4837" ContentLicense="CC BY-SA 3.0" />
  <row Id="7200" PostId="4989" Score="0" Text="Probably you have problems in D3dcompilefromfile. I would suggest you to print the return of GetAssetFullPath and verify if It has the expected file location." CreationDate="2017-04-11T21:32:50.467" UserId="303" ContentLicense="CC BY-SA 3.0" />
  <row Id="7201" PostId="4989" Score="0" Text="More info:&#xA;when i printed the debug error it says the following:&#xA;D3D12GetDebugInterface: This method requires the D3D12 SDK Layers for Windows 10, but they are not present on the system.&#xA;&#xA;would this mean something?" CreationDate="2017-04-11T21:42:27.780" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7202" PostId="4989" Score="0" Text="You dont have directx sdk properly installed or dont have It linked correctly on the binaries of your program. You should check the linker setup in your compilation routine." CreationDate="2017-04-11T22:13:52.373" UserId="303" ContentLicense="CC BY-SA 3.0" />
  <row Id="7203" PostId="4989" Score="0" Text="But then the NbodyGravity sample runs perfectly well. If the SDk was not properly configured, how is that possible?" CreationDate="2017-04-11T22:23:05.020" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7204" PostId="4989" Score="0" Text="If the sample is running, then you should check if your program is linked with the same libraries as the sample." CreationDate="2017-04-11T22:26:05.793" UserId="303" ContentLicense="CC BY-SA 3.0" />
  <row Id="7205" PostId="4989" Score="0" Text="Yes, all i am doing is adding code to the same sample. i am not deleting or changing any path as such" CreationDate="2017-04-11T22:56:03.413" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7206" PostId="4991" Score="0" Text="If the second error message is accurate, you'd need to download the mentioned SDK layers and see if there is still a problem. If the second error message is not accurate then we'll need to know that before investigating." CreationDate="2017-04-12T07:05:10.973" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7207" PostId="4989" Score="2" Text="Next time you want to add more information to your question, you should use the &quot;edit&quot; link instead of posting a whole new question." CreationDate="2017-04-12T08:55:37.637" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7208" PostId="4982" Score="0" Text="Could it be some kind of false sharing: the memory you're overwriting with the texture upload shares cache lines with parts of the atlas that the GPU is reading at the same time? What size &amp; shape is the region of the atlas you're overwriting?" CreationDate="2017-04-12T09:04:46.847" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7210" PostId="4982" Score="0" Text="@DanHulme Since its a 2D Array Texture I just load in each mipmap level of a layer I want to overwrite. I have 50 or so layers of the 512 layer texture in use by the GPU and the layers which cause the odd performance issues when loading are right next to them since while finding new layers it wraps around from the end after walking through all the uninitalized layers back to the oldest &quot;free&quot; page in a list, which happens to be right next to that active range of texture layers. I thought it had something to do with that but the fact that it only happens the first time made me think otherwise." CreationDate="2017-04-12T16:26:47.467" UserId="4837" ContentLicense="CC BY-SA 3.0" />
  <row Id="7211" PostId="4982" Score="0" Text="Also I'm going to attempt to make a minimal example of this problem once I have some time to as I am making a lot of assumptions most likely about what is going on, not to mention all of the other things going on in this project which could be affecting this texture loading (there's a lot of places where I use `glClientWaitSync` and whatnot for synchronization but I doubt those would cause issues with something like this)." CreationDate="2017-04-12T16:34:47.830" UserId="4837" ContentLicense="CC BY-SA 3.0" />
  <row Id="7212" PostId="4987" Score="0" Text="The orientation trick from your first paragraph works perfectly as long as the object is closed and all faces have coherent orientation. It is my favorite solution." CreationDate="2017-04-12T17:32:23.597" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7213" PostId="4995" Score="3" Text="Why not just take the curves that Inkscape outputs and convert them to poly lines? It's fairly simple to convert a Bezier curve to a poly line." CreationDate="2017-04-13T05:34:17.240" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7217" PostId="4995" Score="0" Text="Why specifically a polyline? It's a much worse approximation to a curve than a Bezier spline. Regardless, I don't think you'll find a tool that will give you a &quot;clean&quot; polyline output without needing any manual tweaking." CreationDate="2017-04-13T07:16:57.893" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7218" PostId="4994" Score="0" Text="Nice answer. The OP also asked about Cook-Torrance importance sampling which this answer doesn't touch upon." CreationDate="2017-04-13T08:22:53.040" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="7219" PostId="4994" Score="6" Text="I updated the answer to add a section about Cook-Torrance" CreationDate="2017-04-13T14:25:05.987" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7220" PostId="4991" Score="0" Text="I think you can download SDK layer in &quot;Settings -&gt; Apps and Features -&gt; Manage optional features&quot;." CreationDate="2017-04-13T15:37:00.133" UserId="67" ContentLicense="CC BY-SA 3.0" />
  <row Id="7221" PostId="4985" Score="0" Text="You could also try to use PBOs to map memory client side from the driver. Then you'd fill up the PBOs, unmap the memory, and queue up a `glCompressedTexSubImage2D` to let the driver do it's own upload." CreationDate="2017-04-14T06:24:35.600" UserId="197" ContentLicense="CC BY-SA 3.0" />
  <row Id="7222" PostId="4985" Score="0" Text="@Mokosha Yeah I am probably going to do that for my next project, I was just not aware that was a thing at the time of making this and due to time constraints I can't exactly implement it now." CreationDate="2017-04-14T06:36:44.987" UserId="4837" ContentLicense="CC BY-SA 3.0" />
  <row Id="7224" PostId="4995" Score="0" Text="Section 4 of  the paper mentions that polylines can clearly define the contour segments. Those segments are used to compute the local Shape Context descriptors." CreationDate="2017-04-14T14:00:03.810" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7225" PostId="4995" Score="0" Text="I couldn't link the paper in my question because I am new. Here it is:&#xA;&#xA;https://www.cg.tuwien.ac.at/research/publications/2014/Guerrero-2014-TPS/" CreationDate="2017-04-14T14:00:29.533" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7226" PostId="4998" Score="0" Text="I understand that I need a matrix (9 numbers) for each color. So I will need a matrix for red, a matrix for green, and a matrix for blue. However you are telling me these 9 numbers has to be y0,y−11,y01,y11,y−22,y−12,y02,y12,y22 but I don't understand how to calculate these matrix values. I don't understand what you mean by&quot; nx3 matrix b with the color at that point?&quot; What is &quot;n&quot; here? Do you mean just RGB values?" CreationDate="2017-04-15T02:26:50.430" UserId="6282" ContentLicense="CC BY-SA 3.0" />
  <row Id="7227" PostId="4999" Score="1" Text="Can you post the resulting images? Is it a significant difference, or is it possible that the different ways of calculating the values lead to slight differences in rounding here and there and lead to images that are very similar, but not identical?" CreationDate="2017-04-15T03:16:21.960" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7228" PostId="4998" Score="0" Text="You don't need to do linear regression. Since SH form an orthonormal basis, each SH's coefficient will be equal to the inner product of the cubemap with the SH: the spherical integral of (cubemap * SH). Or in other words, sum (pixel color * SH evaluated at that pixel's direction vector * solid angle of the pixel) over all pixels in the cubemap." CreationDate="2017-04-15T05:45:21.223" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7229" PostId="4999" Score="0" Text="@user1118321 I've updated the question with many more details, please give a look at it" CreationDate="2017-04-15T11:05:04.900" UserId="6454" ContentLicense="CC BY-SA 3.0" />
  <row Id="7230" PostId="5002" Score="0" Text="Can you give an example of what you think isn't legible in regular 3D text rendering? There are certainly other techniques, but it's not clear what you're asking about." CreationDate="2017-04-15T15:29:24.370" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7231" PostId="5002" Score="0" Text="For example, the program [Virtual Desktop](https://www.oculus.com/experiences/rift/911715622255585/) shows all of your screens in VR space using the HTC Vive, Oculus Rift, etc. The text is barely legible in these environments (i.e., not good enough to use VR as a replacement for physical monitors)." CreationDate="2017-04-15T16:17:24.090" UserId="5225" ContentLicense="CC BY-SA 3.0" />
  <row Id="7232" PostId="5002" Score="0" Text="So you mean the 2D text rendered in a 3D space? The video on the page you linked to didn't strike me as looking terrible. When they were browsing the web, it was quite legible. (Of course I'm looking at it on my laptop, not on Vive, so maybe it's worse in real life?)" CreationDate="2017-04-15T16:22:11.580" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7233" PostId="5002" Score="0" Text="Yes -- much worse in real life. You can read text for a few minutes if you squint your eyes, but it's not good enough to fully replace a PC screen by any stretch." CreationDate="2017-04-15T18:24:55.240" UserId="5225" ContentLicense="CC BY-SA 3.0" />
  <row Id="7235" PostId="4995" Score="0" Text="@user1118321 No it's not? Can you elaborate on how to do that? How do you even parse the svg path attribute?" CreationDate="2017-04-17T05:50:49.660" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7236" PostId="5002" Score="0" Text="Super sampling could help. Applies to everything, not just text." CreationDate="2017-04-17T06:01:46.727" UserId="67" ContentLicense="CC BY-SA 3.0" />
  <row Id="7237" PostId="4995" Score="1" Text="I've added an answer explaining how it would work." CreationDate="2017-04-17T06:13:26.903" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7238" PostId="5005" Score="0" Text="Thank you! Could you also tell me if this is possible to do in MATLAB? If so, how?" CreationDate="2017-04-17T06:46:11.477" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7240" PostId="5005" Score="0" Text="I haven't used MATLAB in like 20 years, so I'm not really the one to ask. However, you can probably ask MATLAB questions on [Stack Overflow](http://stackoverflow.com/)." CreationDate="2017-04-17T16:11:46.790" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7241" PostId="5005" Score="0" Text="Ok. What platform is the SVG DOM interface implemented on? I can't decide what to use for this project.&#xA;&#xA;Also I don't think what you're suggesting will work for the kind of path I have. I have many curves and control points in one single d attribute." CreationDate="2017-04-17T17:11:15.853" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7242" PostId="5005" Score="0" Text="Sorry, I mean many curves and move to commands in a single d atrribute" CreationDate="2017-04-17T17:19:07.407" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7244" PostId="5001" Score="0" Text="Hi. Thank you for detailed instruction. It helps a lot to understand the paper. However I am wondering how you arrived to &quot;cosine lobe in SH&quot; are these numbers ratio given to you in the paper? If so I am not being able to find that section." CreationDate="2017-04-17T21:37:52.423" UserId="6282" ContentLicense="CC BY-SA 3.0" />
  <row Id="7245" PostId="4998" Score="0" Text="What do you mean by 9x3 matrix x of the unknown coefficients(9 for each color)? So there are three rows? And each row is all the same number that represents either color red or green or blue? Is my job then just multiplying matrix A to mat x to get matrix of 3x3 b?" CreationDate="2017-04-17T22:10:28.580" UserId="6282" ContentLicense="CC BY-SA 3.0" />
  <row Id="7246" PostId="5001" Score="0" Text="@BlueBug In diffuse lighting lambertian model ray is reflected at many angles rather than as in specular, in one direction. So, in order to approximate this and evaluate irradiance at given point the convolution on cosine lobe is performed - this means in case of SH - division by, for each of SH order 1, 2, 3 - [pi, (2/3)*pi, pi/4]. I hope it helps, check also [this](https://seblagarde.wordpress.com/2012/01/08/pi-or-not-to-pi-in-game-lighting-equation/) or [this](https://en.wikipedia.org/wiki/Lambert%27s_cosine_law)" CreationDate="2017-04-17T22:49:06.833" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7247" PostId="5001" Score="0" Text="I meant not divison but multiplication, sorry." CreationDate="2017-04-18T00:06:15.890" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7249" PostId="5001" Score="0" Text="After reading your answer I edited my question to ask you with some description. I don't understand why you don't do the green part of the equation. Are you already doing it but then I am not being able to see what you are doing? Or is it that there are more than one way to make SH map?" CreationDate="2017-04-18T00:59:47.847" UserId="6282" ContentLicense="CC BY-SA 3.0" />
  <row Id="7250" PostId="5005" Score="0" Text="It should work just fine for many curves and move to commands. Each bezier can be handled as above to turn it into poly lines. they will connect with the next one in the sequence. As for which platform to use, that's not something I can answer. I've done SVG parsing using a standard XML parser in Objective-C (Apple's NSXMLParser), but it could be done in any language for which you have an XML parsing library. Good luck!" CreationDate="2017-04-18T02:02:34.350" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7251" PostId="5005" Score="0" Text="I see. I am just going to try to get contour segments from a raster image and not bother with this. This seems way too complicated" CreationDate="2017-04-18T03:28:55.323" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7252" PostId="5006" Score="1" Text="try some values, the coefficient that is near 1 when the point inside is close to a vertex belongs to that vertex." CreationDate="2017-04-18T08:04:32.533" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7253" PostId="5006" Score="1" Text="@ratchetfreak I did that, but I'm also curious how to mathematically find the respective coefficient." CreationDate="2017-04-18T09:01:48.193" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="7255" PostId="5001" Score="1" Text="Instead of weighting each cube map sample equally, it would be better to weight by the solid angle of each pixel. Otherwise, you'll over-weight the corners and under-weight the face centers." CreationDate="2017-04-18T20:04:41.240" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7257" PostId="5003" Score="1" Text="I wouldn't worry so much about generating images directly. Rather, write an application that does what you need in whatever language feels most appropriate, then capture the rendered output to an image (e.g. similar to what happens when you click &quot;Print&quot; while viewing a web page). As far as what library you can or should use, I have no idea (nor is this the appropriate place to ask). The latter problem seems rather special purpose and I suspect you'll be mostly starting from scratch." CreationDate="2017-04-19T12:48:16.093" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="7264" PostId="4994" Score="0" Text="For example GGX, to sample spherical coordinates angle cos(θ) we use the importance sampled formula to calculate the angle and use that in GGX as usual right? Or does the formula replace GGX entirely?" CreationDate="2017-04-19T15:22:30.663" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7265" PostId="5005" Score="0" Text="@MrWarlock616 its far  less complicated than implementing your own tracer. is just text processing. but then you most likely will not need to implement this as such applications allready exist, like say inkscape." CreationDate="2017-04-19T21:10:26.157" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7266" PostId="4994" Score="3" Text="I added a section to help answer your questions. But, in short, your first method is correct. You use the sampling formula to generate a direction, then you use that new direction in the normal GGX formula *and* to get the pdf for the Monte Carlo formula." CreationDate="2017-04-19T21:26:38.637" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7267" PostId="5009" Score="0" Text="Awesome, I'm looking for this kind of explanation. BTW, there is small mistake, the $V_1$ should be $V_0$, $V_2$ should be $V_1$ and $V_3$ should the vector pointing to $T$. Hopefully you have the time to fix that error. Thanks once again, hopefully many people benefit from this." CreationDate="2017-04-20T05:04:12.603" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="7268" PostId="5009" Score="1" Text="Argh, @Bla... yes i will redraw the pictures. Anyway i am feeling that the variables in the code could have more understandable names. $V_0$ - $V_2$ offcourse being reasonable names but would be much better to name them with same nomenclature as your points." CreationDate="2017-04-20T05:34:08.433" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7269" PostId="5009" Score="0" Text="I just changed the variable names, hopefully now it becomes clearer." CreationDate="2017-04-20T06:32:07.133" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="7270" PostId="5014" Score="0" Text="You might already know this, but caustics can be rendered with regular old path tracing as well. It just takes a lot of samples to look decent." CreationDate="2017-04-20T06:41:32.280" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7271" PostId="5014" Score="0" Text="What are the benefits of using photon mapping as opposed to regular path tracing? Is it just render times or will I be getting a significant difference in terms of how good it looks?" CreationDate="2017-04-20T07:45:54.590" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7272" PostId="5001" Score="0" Text="@NathanReed Where? During accumulation to the buffer?" CreationDate="2017-04-20T08:58:27.287" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7273" PostId="5001" Score="0" Text="Right, when computing the SH coefficients from the cube map." CreationDate="2017-04-20T13:21:01.773" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7274" PostId="5012" Score="0" Text="I think you can also use [ID3D12Resource::ReadFromSubresource](https://msdn.microsoft.com/en-us/library/windows/desktop/dn914415(v=vs.85).aspx)&#xA; it is a little simpler." CreationDate="2017-04-20T13:57:11.980" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7275" PostId="5001" Score="0" Text="@NathanReed Myself I am using pretty uniform poisson sampling to avoid biasing samples." CreationDate="2017-04-20T14:17:59.650" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7276" PostId="5016" Score="1" Text="What happens when you run it? Can you provide a screenshot as a visual reference?" CreationDate="2017-04-20T14:24:06.153" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="7277" PostId="5016" Score="0" Text="@Dan I tried with the directional and positional lights turned off and only with the spotlight turned on and the cube appears to be all black, so a scene without lights, so I assumed that the spotlight doesn't work" CreationDate="2017-04-20T14:56:01.290" UserId="6401" ContentLicense="CC BY-SA 3.0" />
  <row Id="7278" PostId="4484" Score="1" Text="Just one question your gradient texture is rectangular texture . Just wanted to know in your application code you have not accidently enabled texture filtering modes .&#xA;Only min filter GL_TEXTURE_MIN_FILTER should be set to GL_NEAREST or GL_LINEAR . Also wrapping modes for each coordinate must be either GL_CLAMP_TO_EDGE or GL_CLAMP_TO_BORDER.&#xA;Also see mipmap is not turned on .&#xA;One more thing in your shader make sure texelFetch pos is not normalised." CreationDate="2017-04-19T19:19:22.147" UserId="6475" ContentLicense="CC BY-SA 3.0" />
  <row Id="7279" PostId="5012" Score="0" Text="But I am trying to read out of a compute buffer. I was under the impression that ReadFromSubresource is used primarily on textures and layouts." CreationDate="2017-04-20T16:37:28.903" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7280" PostId="1506" Score="0" Text="Great answer; thank you. I had never come across the distinction between bi- and covectors, but it clarifies things for me. Bivectors obviously have a direct geometric algebra representation; do you know if covectors also do?" CreationDate="2017-04-20T16:43:23.850" UserId="6478" ContentLicense="CC BY-SA 3.0" />
  <row Id="7281" PostId="5016" Score="2" Text="Unfortunately I don't have the time to debug your shader for you, but I would start by removing things from the shader code incrementally until it works and backtrack from there to find the problem. Bugs that cause nothing to render at all are the worst ones to find!" CreationDate="2017-04-20T17:10:14.440" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="7282" PostId="5005" Score="0" Text="@joojaa It's not that simple, really, think about it. I basically have to implement the SVG DOM in Matlab." CreationDate="2017-04-20T17:23:56.820" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7283" PostId="5005" Score="0" Text="@MrWarlock616 you dont need a DOM  (or a SAX)  to do this. It makes it is slightly more fragile without one but not really a big deal. In any case matlab has a XML DOM implementation (called **xmlread**) so your complaint is totally irrelevant. See [docs](https://se.mathworks.com/help/matlab/ref/xmlread.html)" CreationDate="2017-04-20T17:36:01.670" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7284" PostId="5016" Score="0" Text="I've tried to remove the other two lights but I can't find the bug!" CreationDate="2017-04-20T17:55:31.783" UserId="6401" ContentLicense="CC BY-SA 3.0" />
  <row Id="7285" PostId="5005" Score="0" Text="@joojaa I've seen the docs. I could retrieve the d attribute using xmlread and all that but I don't have the time to implement a parser for the d attribute. I just care about the point coordinates since I'm trying to replicate the results of a paper." CreationDate="2017-04-20T17:57:15.193" UserId="6435" ContentLicense="CC BY-SA 3.0" />
  <row Id="7286" PostId="1506" Score="0" Text="@PaulDuBois I think covectors are what geometric algebra terms a dual vector. In 3D GA, the dual of a vector is a bivector; it doesn't make the distinction in the algebra. But they still transform differently: one should use the outermorphism for bivectors, and the dual transform for dual vectors. (And again, in 3D I think those come out the same, up to a scale factor.)" CreationDate="2017-04-20T20:46:23.197" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7287" PostId="5016" Score="0" Text="Keep removing features (e.g. texturing), one at a time. Maybe try a very simple shader that just makes everything white, then start adding features back once you have something that renders." CreationDate="2017-04-20T21:04:53.473" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="7288" PostId="5012" Score="0" Text="I never used this with `UAV` but maybe it will work, just set `DstRowPitch` and `DstDepthPitch` to buffer size (in bytes) and you will see if its work. I don't see why it shouldn't work (but maybe it don't)" CreationDate="2017-04-20T21:18:10.633" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7289" PostId="5016" Score="0" Text="Yeah I did it, maybe I just need a good reference in order to understand better the use of a spotlight, it's not really clear in the online material that I've found." CreationDate="2017-04-20T21:40:42.403" UserId="6401" ContentLicense="CC BY-SA 3.0" />
  <row Id="7290" PostId="5012" Score="0" Text="I created the Input buffer with the following properties:&#xA;&#xA;ThrowIfFailed(m_device-&gt;CreateCommittedResource( &amp;defaultHeapProperties, D3D12_HEAP_FLAG_NONE, &amp;bufferDesc,&#xA;D3D12_RESOURCE_STATE_COPY_DEST, nullptr,&#xA;IID_PPV_ARGS(&amp;m_InputBuffer1[0])));&#xA;&#xA;and later on when I try to read it back as follows, the compiler throws an exception!&#xA;&#xA;UINT8* pDataBegin;&#xA;m_InputBuffer1[0]-&gt;ReadFromSubresource(reinterpret_cast&lt;void**&gt;(&amp;pDataBegin),5,5,0,0);&#xA;printf(&quot;The value pointed to is %d&quot;, *pDataBegin);&#xA;&#xA;any idea why this is happening ?" CreationDate="2017-04-21T00:43:15.963" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7292" PostId="5017" Score="1" Text="One approach can be to render the back faces of a mesh to a depth buffer to compute an 'optical depth' which won't work so well for highly concave meshes but looks decent enough. http://http.developer.nvidia.com/GPUGems/gpugems_ch16.html" CreationDate="2017-04-21T05:54:21.453" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7293" PostId="5012" Score="0" Text="`ID3D12Resource::ReadFromSubresource` doesn't allocate memory for you and it get pointer not pointer to pointer, so you need to allocate memory and cast `pDataBegin` to `void*` not `void**`. Remember to free allocated memory" CreationDate="2017-04-21T09:07:35.853" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7294" PostId="5012" Score="0" Text="Also i'm not sure about `D3D12_RESOURCE_STATE_COPY_DEST` but i don't see whole code so can't tell if its should be this resource state." CreationDate="2017-04-21T09:18:06.320" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7295" PostId="5018" Score="0" Text="It looks like you're stretching the texture rather than tiling. Tiling wouldn't deform the rounded corners, but it would give you multiple copies of the rounded corners, and you wouldn't be able to fix it the way you propose." CreationDate="2017-04-21T15:32:46.923" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7296" PostId="5018" Score="0" Text="@DanHulme Yeah my bad, I misspoke. Instead of increasing the tiling, I am lowering it, that's why I am having the stretch effect. But this is the expected effect behaviour. I just want to avoid the deformation caused by the stretching." CreationDate="2017-04-21T15:38:52.257" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="7297" PostId="5018" Score="0" Text="I edited my question. Sorry about that." CreationDate="2017-04-21T15:40:55.090" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="7298" PostId="5018" Score="1" Text="I realise your example textures are quite simple, but if you ever have to do something more complex and still want to avoid the &quot;distortion&quot;, [Seam Carving might be of interest:](http://www.win.tue.nl/~wstahw/edu/2IV05/seamcarving.pdf) When I saw it presented it almost seemed like magic." CreationDate="2017-04-21T17:01:04.310" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7299" PostId="5019" Score="3" Text="Another good one is [Pre-Integrated Skin Shading](http://advances.realtimerendering.com/s2011/Penner%20-%20Pre-Integrated%20Skin%20Rendering%20(Siggraph%202011%20Advances%20in%20Real-Time%20Rendering%20Course).pptx) from SIGGRAPH a few years ago." CreationDate="2017-04-21T20:27:42.410" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7302" PostId="5021" Score="0" Text="Could you expand on the requirements? When you say &quot;texture&quot;, does this require variation in light emission across the surface, or can it just be a uniformly glowing cube? Does it need to affect nearby objects or just to appear emissive alone? Writing down exactly what is required may also help you come up with ideas." CreationDate="2017-04-23T16:37:28.247" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7303" PostId="5021" Score="0" Text="@trichoplax this is the assignment: &quot;combine the lighting with a texture (256x256) that models the emission of light by the object. Generate this texture randomly with values between 0 and 128 for each component. The amount of light emitted by the object must be added to the computed lighting&quot;" CreationDate="2017-04-23T16:58:37.423" UserId="6401" ContentLicense="CC BY-SA 3.0" />
  <row Id="7304" PostId="5021" Score="0" Text="@trichoplax By the way, I think I have to apply a kind of &quot;glowing&quot; texture that emits light, so it should affect nearby objects" CreationDate="2017-04-23T18:54:41.777" UserId="6401" ContentLicense="CC BY-SA 3.0" />
  <row Id="7305" PostId="5021" Score="1" Text="If it's an assignment, I'd recommend double checking exactly what is required with your teacher/tutor/lecturer so you don't end up giving yourself a bigger task than is required." CreationDate="2017-04-23T19:40:57.910" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7306" PostId="5021" Score="0" Text="hm normally you'd do multiple passes, somehting like this: http://io7m.com/documents/glow-maps/s3.xhtml idk if you can render to a frame buffer object with webgl. I guess the question is: does webgl support frame buffer objects?" CreationDate="2017-04-24T00:14:10.500" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7307" PostId="5018" Score="2" Text="User interfaces like Android use something call 'nine patches' which breaks the texture into a 3x3 grid with different scaling rules such that the corners are preserved and only the interiors are scaled in 1 or both axis. Further reading here: http://en.miui.com/thread-26099-1-1.html" CreationDate="2017-04-24T05:24:39.583" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7308" PostId="5015" Score="0" Text="Note that you need the second edition of Physically Based Rendering: From Theory to Implementation, since the third does not contain or handle pure (like Jensen defined it) Photon Mapping." CreationDate="2017-04-24T06:26:09.390" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="7309" PostId="5014" Score="0" Text="@ArjanSingh There is a little more to it. Using an infinite number of rays, both path tracing (unbiased) and photon mapping (consistent) will result in the same fully converged image. Photon mapping, however, is biased and will not result in the converged image on average for a finite (practical) number of rays (like path tracing) and can contain additional artefacts due to its density estimation techniques (besides the typical Monte Carlo noise)." CreationDate="2017-04-24T06:28:31.697" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="7310" PostId="5016" Score="0" Text="UPDATE: if i delete the &quot;if(lEffect&gt;lCutOff)&quot;, the spotlight works as a positional light so I assume that with the &quot;if&quot; construct the code runs always the &quot;else&quot; part and it doesn't compute the part within the &quot;if&quot;. By the way, I can't still find what's wrong with the code :_(" CreationDate="2017-04-24T11:19:49.717" UserId="6401" ContentLicense="CC BY-SA 3.0" />
  <row Id="7311" PostId="5021" Score="0" Text="yeah, I checked what I have to do and I have to apply a glow map through textures, but I can't find a good reference that explains the procedure :(" CreationDate="2017-04-24T11:22:14.663" UserId="6401" ContentLicense="CC BY-SA 3.0" />
  <row Id="7312" PostId="5023" Score="0" Text="Do you mean that you have a set of 3D control points, C0, C1...Cn, and, I assume, some rotation matrix, P, such that, (P.Ci) [x] is strictly increasing, as are (P.Ci) [y] and (P.Ci) [z] ?" CreationDate="2017-04-24T11:56:14.690" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7313" PostId="5023" Score="0" Text="No, i mean in 3D a point is defined by (x, y, z) instead of (x, y)." CreationDate="2017-04-24T12:04:53.353" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="7314" PostId="5012" Score="0" Text="But wouldn't it be easier if i use the Map and Unmap pair?&#xA;&#xA;std::vector&lt;UINT8&gt; pDataBegin;&#xA;ThrowIfFailed(m_InputBuffer1[0]-&gt;Map(0, NULL, reinterpret_cast&lt;void**&gt;(&amp;pDataBegin)));&#xA;&#xA;but then again even this throws an error :(" CreationDate="2017-04-24T18:36:13.967" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7315" PostId="5012" Score="0" Text="What error do you get? Did you try `ReadFromSubresource`? Did you do steps pointed [on Microsoft page](https://msdn.microsoft.com/en-us/library/windows/desktop/dn899202(v=vs.85).aspx)?" CreationDate="2017-04-24T22:43:10.197" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7316" PostId="5012" Score="0" Text="I was not able to declare my heap as type D3D12_HEAP_TYPE_READBACK. Because the second I do that, I am not able to call CreateCommitResource and even create the buffer.&#xA;&#xA;If i keep the heap type DEFAULT and call Map, there is nothing returned into *pDataBegin" CreationDate="2017-04-24T23:06:21.883" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7317" PostId="5023" Score="0" Text="Your question is ambiguous as you don't specify monotonic in relation to what in 3D?" CreationDate="2017-04-25T13:10:58.360" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7318" PostId="5023" Score="0" Text="@JarkkoL You need to check the link that I provided." CreationDate="2017-04-25T15:27:48.523" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="7319" PostId="5023" Score="0" Text="It doesn't disambiguate how you want the function to be monotonic in 3D. If you want the function to be monotonic along y-axis like in the 2D case, it's trivial extension, but if you want it to be monotonic along each axis, it's different. So which is it? Please clarify your question." CreationDate="2017-04-25T15:58:37.033" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7320" PostId="5023" Score="0" Text="@JarkkoL Oh, I see.. How can you tell that the function in the site is monotonic along y-axis only? I think the important step is &quot;Compute the slopes of the secant lines between successive points:&quot;, but the site only provide the 2D case. I think if I can extend that to 3D than the rest are the same. Am I right?" CreationDate="2017-04-26T01:42:41.317" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="7321" PostId="5022" Score="2" Text="Can you post any more examples or links to similar stuff? It's not clear to me what the histogram on the right is showing, so it's difficult to say much about it." CreationDate="2017-04-26T02:42:13.947" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7324" PostId="5022" Score="0" Text="From further investigation I believe it's a scatterplot of gradient magnitude and intensity of the 3d texture. (The tooth) Im still having a little trouble visualizing it though." CreationDate="2017-04-27T03:40:36.103" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="7325" PostId="5028" Score="2" Text="Can we see some code for this? It sounds like you function continues recursing when it meets the RR condition so it's probably a coding problem rather than a problem with the algorithm. As a side note, it is possible implement  path tracing without needing recursion, GPU implementations rely on this." CreationDate="2017-04-27T04:24:29.463" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7326" PostId="5028" Score="1" Text="Have you stepped through it in a debugger to see whether the pseudo-random number is being generated the way you think? And that everything else you're doing is happening the way you think?" CreationDate="2017-04-27T04:47:58.113" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7329" PostId="5028" Score="1" Text="&quot;Unclear wht you're asking&quot; isn't very specific, but I'm closing this question because it can't be answered without the code that produces the error. If you [edit] your question and make it answerable, it can be reopened." CreationDate="2017-04-27T08:38:31.060" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7330" PostId="5028" Score="0" Text="Sorry! I've edited my question." CreationDate="2017-04-27T14:58:56.470" UserId="6515" ContentLicense="CC BY-SA 3.0" />
  <row Id="7331" PostId="5027" Score="0" Text="I'm hardly an expert, but when graphics articles start talking about light reflections based on dielectrics and metals, they are often talking about [physically based rendering.](https://learnopengl.com/#!PBR/Theory)  Hope that helps." CreationDate="2017-04-27T17:55:32.460" UserId="6521" ContentLicense="CC BY-SA 3.0" />
  <row Id="7332" PostId="5030" Score="0" Text="Sorry. I was experimenting with my code and forgot to edit the values. Then I copied and pasted and here is... I edited the question so now it has the correct values in the last set." CreationDate="2017-04-27T22:28:26.643" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="7333" PostId="5030" Score="0" Text="Okay—the result is the same, just scaled differently." CreationDate="2017-04-27T23:15:30.077" UserId="506" ContentLicense="CC BY-SA 3.0" />
  <row Id="7336" PostId="2366" Score="1" Text="Please don't post screenshots of code. It's not searchable and it prevents other people pasting your code into their own editor to test it." CreationDate="2017-04-28T08:16:17.977" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7337" PostId="5028" Score="0" Text="why did you comment out the depth test? it's what should prevent the stack overflow" CreationDate="2017-04-28T09:47:11.397" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7338" PostId="5030" Score="0" Text="But all the points inside the square results in values out of the range [ -1 ,1 ]. For example  (0.1, -0.5, 0.1) -&gt; (1, -5)" CreationDate="2017-04-28T14:23:40.237" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="7339" PostId="5035" Score="0" Text="My understanding is &quot;haze&quot; refers more to scattering due to water droplets or other small particles in the air, not the blue (Rayleigh scattering) effect from the air itself." CreationDate="2017-04-28T16:34:02.537" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7340" PostId="5030" Score="2" Text="@4dr14n31t0rTh3G4m3r The vertex shader only runs per vertex, not for every point in the square. So the four vertices are mapped to (±2, ±2) and then the GPU draws a polygon between those vertices, which does cross the viewport. The points interior to the polygon do not go through the `v.y = v.y/v.z` transform, so the result is not `(-inf,-2] U [2,inf)` after all." CreationDate="2017-04-28T16:44:32.887" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7341" PostId="5035" Score="1" Text="Meteorologically speaking, *haze* is specifically water, not other particles, so yes, there are multiple causes leading to the real-world effect. That's part of why it looks different in different situations, and why you hear so many words in connection with it." CreationDate="2017-04-28T16:46:50.333" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7342" PostId="5030" Score="0" Text="The problem was that I believed the vertex shader runs for every point in the square. Thanks a lot @Nathan and please post your comment as answer. I would like to accept it because you explained how vertex shaders run. However, now that I understand why does it happen, how could I fix this issue?" CreationDate="2017-04-28T16:49:48.593" UserId="5852" ContentLicense="CC BY-SA 3.0" />
  <row Id="7343" PostId="5028" Score="2" Text="@ratchetfreak It is, because Russian Roulette algorithm should stop the resursion randomly (to keep the estimator unbiased). As far as I know, I should be like that." CreationDate="2017-04-28T17:12:47.760" UserId="6515" ContentLicense="CC BY-SA 3.0" />
  <row Id="7344" PostId="5031" Score="0" Text="Thanks!  Wouldn't have guessed first quote referred to split-sum..." CreationDate="2017-04-28T21:53:20.780" UserDisplayName="user3412" ContentLicense="CC BY-SA 3.0" />
  <row Id="7345" PostId="5025" Score="0" Text="Thank you for you recommendation @Derag. However, the values in the readback buffer are incorrect. &#xA;Can this be because of the readback buffer description, that the numbers are not being interpreted correctly?" CreationDate="2017-04-28T23:53:15.670" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7347" PostId="5025" Score="0" Text="What values do you get? If you get only zeros you probably have some bug in code. If buffer you pass to `Map` have different type than `UAV` buffer that you read values can vary.&#xA;And I don't think that `D3D12_RESOURCE_DESC` should be problem here as long as `D3D12_RESOURCE_DESC::Width` is set to right size and `D3D12_RESOURCE_DESC::Format` is set to `DXGI_FORMAT_UNKNOWN`" CreationDate="2017-04-29T11:07:40.650" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7350" PostId="5025" Score="0" Text="The definition of the readback buffer is the same as what you suggested, the buffer which the shader actually calls (or in the example above &quot;bufferB) has the following properties:&#xA;D3D12_HEAP_PROPERTIES defaultHeapProperties = { D3D12_HEAP_TYPE_DEFAULT,D3D12_CPU_PAGE_PROPERTY_UNKNOWN,D3D12_MEMORY_POOL_UNKNOWN,0,0 };&#xA;ThrowIfFailed(m_device-&gt;CreateCommittedResource(&amp;defaultHeapProperties, D3D12_HEAP_FLAG_NONE, &amp;buffersDesc1, D3D12_RESOURCE_STATE_GENERIC_READ, nullptr, IID_PPV_ARGS(&amp;m_computeBuffer)));" CreationDate="2017-05-01T16:50:18.383" UserId="6349" ContentLicense="CC BY-SA 3.0" />
  <row Id="7351" PostId="5039" Score="0" Text="Have you tried stepping through in a debugger to see just where and how the values &quot;go crazy&quot;? If it happens only when cos &gt; 0.9999, that suggests a possible numerical precision problem. (Also, it sounds like you have two questions here—is your main question &quot;how to sample a half-vector distribution&quot; or &quot;why does this function go crazy&quot;? We can answer the former, but I don't know if it will help debug the problem you're having; for the latter, the relevant source code, example results, etc would be useful.)" CreationDate="2017-05-01T17:52:48.027" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7352" PostId="5039" Score="0" Text="I worked on this over the last 2 days, I will update my question with the results.  I have ruled out that the cosine angle is the issue.  It more looks like an issue where the spherical coordinates are rotating about the wrong axis.  I graphed the results of my half-vector samples and I will upload a link to the image viewing their distribution." CreationDate="2017-05-01T17:57:07.563" UserId="6534" ContentLicense="CC BY-SA 3.0" />
  <row Id="7353" PostId="5039" Score="0" Text="Also, thanks for the pointer - I updated the title accordingly." CreationDate="2017-05-01T18:19:17.813" UserId="6534" ContentLicense="CC BY-SA 3.0" />
  <row Id="7354" PostId="5045" Score="0" Text="Thank you for that.  I put together a live HTML/WebGL test that isolates only this problem.  I am essentially using a variation of the equation you posted (but following the PBRT code to be anisotropic instead of iso).  Something strange is happening as I move off of normal incidence to this function.  My half-vectors seem to be &quot;leaning&quot;.  Not sure how else to describe it.  I posted the HTML proof that has all the math/shader code inline and commented for readability." CreationDate="2017-05-01T20:27:37.517" UserId="6534" ContentLicense="CC BY-SA 3.0" />
  <row Id="7355" PostId="5045" Score="0" Text="Followed your advice and went back to the basics.  Implemented the cook torrence equation, verified basic inputs look right, and determined that sampling the distribution works fine, the problem must be elsewhere, outside of these basics.  Thanks for taking the time to read my question." CreationDate="2017-05-01T21:21:02.723" UserId="6534" ContentLicense="CC BY-SA 3.0" />
  <row Id="7356" PostId="5045" Score="0" Text="@Steve your WebGL example looks smooth here. Ugly yellow in the lower left corner, more grey in the upper right. Did you change it to the simpler distribution or is it still supposed to be the broken code? I'm using chromium on linux btw. Or is the yellow &quot;blob&quot; supposed to be centered? Either way,`wo *= -wo;` looks suspicious." CreationDate="2017-05-01T22:48:16.543" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7357" PostId="5045" Score="0" Text="I'm actually debugging that issue right now.  I called it &quot;solved&quot; because i made an error.  I just updated to &quot;correct&quot; code and my distribution problem is back again.  If you refresh the webpage you'll see that my &quot;crossed quadrants&quot; are back" CreationDate="2017-05-01T22:50:40.923" UserId="6534" ContentLicense="CC BY-SA 3.0" />
  <row Id="7358" PostId="5045" Score="0" Text="Currently, I am visualizing the half vectors as they come back raw from the distribution." CreationDate="2017-05-01T22:51:11.040" UserId="6534" ContentLicense="CC BY-SA 3.0" />
  <row Id="7359" PostId="5045" Score="0" Text="@Steve Ok, I see the mess now. Will try to analyze later." CreationDate="2017-05-01T22:51:57.387" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7360" PostId="4340" Score="0" Text="I'm voting to close this question as off-topic because it is an end-user question about using graphics software, not about graphics algorithms or research." CreationDate="2017-05-02T08:31:31.287" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7361" PostId="5025" Score="0" Text="What size of buffers did you set? And what types of data? (int, float...)" CreationDate="2017-05-02T09:20:15.083" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7362" PostId="5010" Score="1" Text="Can you not subdivide your world into a 3d grid and if any grid has &gt;= 1 point inside it, it can be made into a voxel?" CreationDate="2017-05-02T09:40:08.457" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7363" PostId="5049" Score="1" Text="IIRC if you integrate the PDF over the entire domain the result must be 1." CreationDate="2017-05-02T14:55:36.033" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7365" PostId="5059" Score="1" Text="Is your question about older hardware or recent hardware? The title says &quot;older&quot; but your actual question text seems like your asking more about more recent models. I suspect the answer will be different between the 2." CreationDate="2017-05-04T06:07:00.787" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7367" PostId="5059" Score="0" Text="I believe that the some of the required features (including compute! and coarse occlusion queries) and some of the required limits may be out of reach." CreationDate="2017-05-04T10:37:50.000" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7369" PostId="5059" Score="0" Text="@user1118321: I mostly mean the most recent GPUs that don't support Vulkan. By &quot;older&quot; I just meant &quot;old enough to not support it&quot;." CreationDate="2017-05-04T12:06:45.967" UserId="6567" ContentLicense="CC BY-SA 3.0" />
  <row Id="7370" PostId="5062" Score="1" Text="Pardon me for asking, but that looks very much like a patent diagram in which case, doesn't it explain how it's done?" CreationDate="2017-05-04T12:22:43.627" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7371" PostId="5062" Score="0" Text="@SimonF you are correct! Unfortunately, it does not go into detail as to how it should be achieved." CreationDate="2017-05-04T12:26:03.837" UserId="6572" ContentLicense="CC BY-SA 3.0" />
  <row Id="7372" PostId="5062" Score="2" Text="Hmm. Given a patent is supposed to be implementable by an &quot;average skilled&quot; person in the field, that doesn't sound promising.   Having said that, it looks as though the right edge of each piece is part of the convex hull. There are documented algorithms for finding that, if that's of use to you." CreationDate="2017-05-04T13:51:27.840" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7373" PostId="5059" Score="1" Text="@Dolda2000: I don't think the question is well-founded. You list three pieces of hardware, but you assume that they don't have Vulkan implementations for non-&quot;business reasons&quot;. Do you have a basis for such an assumption?" CreationDate="2017-05-04T14:45:22.047" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7374" PostId="5059" Score="0" Text="@NicolBolas: Sorry, it seems I made myself misunderstood. I really just intended to speak of whatever most recent hardware that technically cannot support Vulkan. Those three specific architectures were just my best guess on what those might be, but I'm entirely open to having guessed wrong." CreationDate="2017-05-04T17:37:15.060" UserId="6567" ContentLicense="CC BY-SA 3.0" />
  <row Id="7375" PostId="5059" Score="1" Text="@Dolda2000: But you already know why GL 3.x/D3D10-class hardware can't support Vulkan: because they don't have very obvious hardware features that Vulkan requires. So the only hardware you seem to be talking about is GL 4.x/D3D11-class hardware that doesn't have Vulkan implementations." CreationDate="2017-05-04T18:08:02.143" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7376" PostId="5028" Score="0" Text="I've added the pathtracing tag again, and I've left the raytracing tag too as they both seem relevant." CreationDate="2017-05-04T19:43:24.273" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7377" PostId="5056" Score="0" Text="Nice detective work. Have you fed this back to the author?" CreationDate="2017-05-04T19:55:23.870" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7378" PostId="5060" Score="0" Text="Great solution, which works well. Here's the (exaggerated) result of placing an additional virtual source behind the mirror: https://www.youtube.com/watch?v=A6fppB5jeAY" CreationDate="2017-05-04T20:01:09.090" UserId="6562" ContentLicense="CC BY-SA 3.0" />
  <row Id="7379" PostId="5059" Score="0" Text="@NicolBolas What are these &quot;very obvious hardware features&quot;? I think this is an excellent question. Also, I'm not sure I follow that GL 3.x can't support Vulkan since there exist GPUs that support Vulkan and can't support GL 3.2 devices because of lacking geometry shaders." CreationDate="2017-05-05T01:52:15.437" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="7380" PostId="5059" Score="2" Text="@aces: The &quot;very obvious features&quot; in question would be compute shaders and image load/store. Obvious due to the fact that it's right there in the Vulkan specification that these are not optional. The only GPUs that can provide these yet aren't GL 3.x capable would be *mobile* GPUs. But they don't support OpenGL of any version; they support OpenGL ***ES*** versions." CreationDate="2017-05-05T02:10:45.760" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7381" PostId="5059" Score="4" Text="@NicolBolas You should write this as an answer :), as well as explain what hardware changes are needed to support these." CreationDate="2017-05-05T02:31:46.450" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="7382" PostId="5063" Score="3" Text="Yes the 0 is for GL_TEXTURE0. Don't forget to bind the texture object to texture unit 0 also." CreationDate="2017-05-05T07:07:10.187" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7383" PostId="5050" Score="0" Text="I wasn't think in terms of area, so thanks a lot. I still have an unresolved paradox in my poor brain. Imagine a unit hemisphere and a hemisphere of radius 2, centred at the same point. The first has surface area $2\pi$ and the second $8\pi$. The probability of picking a particular direction over these hemispheres seems the same, but it's going to be $\frac{1}{2\pi}$ and $\frac{1}{8\pi}$ respectively. I can't get my head around why it's different mathematically but _seemingly_ the same theoretically. Put another way, why does the unit hemisphere get precedence over ones of other radii?" CreationDate="2017-05-05T10:06:20.270" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="7384" PostId="5064" Score="0" Text="Have you tried saturating the dot products before raising it to some power ? They should not go &lt; 0" CreationDate="2017-05-05T10:17:14.290" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7385" PostId="5064" Score="0" Text="@PaulHK Just tried it and it didn't work. But there is the possibility the specular values are less than 0 when raising it to a power." CreationDate="2017-05-05T10:26:43.137" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="7386" PostId="5059" Score="2" Text="It also raises the question in what way eg. Tesla GPUs don't support compute shaders. Since they support CUDA, why can they not support Vulkan compute shaders? Ie., what actual hardware feature do they lack that prevents them from running compute shaders." CreationDate="2017-05-05T11:57:47.993" UserId="6567" ContentLicense="CC BY-SA 3.0" />
  <row Id="7387" PostId="5063" Score="0" Text="This is all well-covered in the [first tutorial on texturing on that very site](https://learnopengl.com/#!Getting-started/Textures)." CreationDate="2017-05-05T15:11:09.260" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7388" PostId="5056" Score="0" Text="I'm actually the one who screwed up the code when I translated it to webgl - so yeah :-) I definitely let myself know. What I think I found most difficult about this problem is the lack of good information or graphics that actually visualize what the algorithms produce in graphics." CreationDate="2017-05-05T17:04:50.597" UserId="6534" ContentLicense="CC BY-SA 3.0" />
  <row Id="7390" PostId="5043" Score="0" Text="I am wondering where it would be better to put the code for Russian Roulette: in traceRay or in the indirectLighting function?" CreationDate="2017-05-05T18:02:42.320" UserId="6515" ContentLicense="CC BY-SA 3.0" />
  <row Id="7392" PostId="5043" Score="0" Text="That sounds like a potential new question..." CreationDate="2017-05-05T19:33:58.363" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7393" PostId="5024" Score="0" Text="Could you provide a screenshot of what it looks like?" CreationDate="2017-05-05T20:14:08.903" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="7396" PostId="5065" Score="0" Text="@NicolBolas https://open.gl/textures states that samplers in the fragment shader are bound to texture units so wouldn't it make sense to use GL_TEXTURE0 and so on to make the code more readable" CreationDate="2017-05-06T16:33:54.397" UserId="6546" ContentLicense="CC BY-SA 3.0" />
  <row Id="7399" PostId="5065" Score="0" Text="@NicolBolas Does my answer look correct now?" CreationDate="2017-05-06T17:34:26.883" UserId="6546" ContentLicense="CC BY-SA 3.0" />
  <row Id="7403" PostId="5067" Score="0" Text="Also, why are multiplying the light position by the w component of the position?" CreationDate="2017-05-08T04:08:37.453" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="7405" PostId="5067" Score="0" Text="I was about to comment the same thing. Division by W is typical for things that have had a perspective matrix performed on them. It should not be needed for the world positions of lights." CreationDate="2017-05-08T06:23:44.060" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7406" PostId="5067" Score="0" Text="wLiPos is a vec4, and I have to avoid dividing by zero, so I cross multiplied in the formula." CreationDate="2017-05-08T06:45:19.170" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="7407" PostId="5067" Score="0" Text="Now I multiple by the transpose of the inverse of the modelMatrix, but nothing changed.&#xA;wNormal = (vec4(vtxNorm, 0) * transpose(inverse(ModelMatrix))).xyz;" CreationDate="2017-05-08T06:46:51.617" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="7408" PostId="5050" Score="1" Text="I think Olivier's answer explains that well. Steradians and the unit sphere go together in the same way that radians and the unit circle go together. If you change units so that your sphere has a different radius, that radius becomes an extra factor in the integration. &quot;Choosing a direction&quot; and &quot;choosing a point on the surface of a unit sphere&quot; can be treated as the same thing." CreationDate="2017-05-08T08:56:28.300" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7409" PostId="5067" Score="0" Text="You don't need the W component for world transformations. You can truncate the vec4*mat4 result to a vec3." CreationDate="2017-05-08T09:25:20.640" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7412" PostId="5067" Score="0" Text="Oh yes, that's a typo. wLightPos is not in world, it's a vec4, so it can be in the infinity for example." CreationDate="2017-05-08T10:03:56.980" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="7413" PostId="5067" Score="0" Text="Or you mean that wPos? But if yes, then the w component is always 1, so that multiplication shouldn't change anything." CreationDate="2017-05-08T10:06:37.417" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="7415" PostId="5067" Score="0" Text="but if I just write eye - pos and LightPos - pos, everything stays the same." CreationDate="2017-05-08T10:34:28.327" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="7416" PostId="5042" Score="0" Text="Thank you for explaining this and sorry for the delayed answer, but I had to find the time to read this very carefully. I feel like the only paragraph I do not completely understand is the one where you talk about fragment shaders and derivatives. What exactly is derived in screen-space X and Y? Is it the texture coordinate value?" CreationDate="2017-05-08T13:15:54.893" UserId="6535" ContentLicense="CC BY-SA 3.0" />
  <row Id="7417" PostId="5042" Score="0" Text="@NicolaMasotti: [&quot;Derivative&quot; is a calculus term.](https://en.wikipedia.org/wiki/Derivative) In this case, it is the rate of change of the texture coordinate, across the surface of the triangle, in screen-space X or Y. If you don't know calculus, then I'm not going to be able to explain it to you in a single post." CreationDate="2017-05-08T15:52:40.840" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7421" PostId="5042" Score="0" Text="Fortunately I know what a derivative is. Is there any place I can look at for the exact math?" CreationDate="2017-05-08T17:24:17.720" UserId="6535" ContentLicense="CC BY-SA 3.0" />
  <row Id="7422" PostId="5042" Score="0" Text="Also, I would propose a couple of changes to your answer, i.e. &quot;_it depends on how the polygon is mapped to the texture_&quot; instead of &quot;_it depends on how the texture is mapped to the polygon_&quot;. Also, when you say: &quot;_using coverage to scale colors that are on the border_&quot; do you actually mean &quot;_weight colors that are on the border_&quot;?" CreationDate="2017-05-08T17:31:46.863" UserId="6535" ContentLicense="CC BY-SA 3.0" />
  <row Id="7425" PostId="5078" Score="0" Text="If you don't get an answer here, you can also try on the [Photography Stack Exchange](https://photo.stackexchange.com/)." CreationDate="2017-05-09T05:14:32.823" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7426" PostId="5067" Score="0" Text="How does it look if you remove the distance attenuation?  (*le[i] and / pow(length(wLight[i]), 2)  )" CreationDate="2017-05-09T06:12:25.200" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7427" PostId="5064" Score="0" Text="How about diffVal ? They also need saturating as they are used in your reflection calculation. I say this because it looks like your specular reflections are at N.L &lt; 0" CreationDate="2017-05-09T06:25:20.790" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7428" PostId="5021" Score="0" Text="It is important to note that glow is certainly not light emission! You should probably edit your question" CreationDate="2017-05-09T08:43:52.710" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7429" PostId="5067" Score="0" Text="Didn't fix it. https://i.gyazo.com/814598d0d3afea8d9e83b6c4abb84854.png" CreationDate="2017-05-09T09:21:26.297" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="7430" PostId="5067" Score="0" Text="If I comment out the kd part in the color: https://i.gyazo.com/0006479371f58a9931250b2d03dcd244.png&#xA;So the specular part causes it. With only ka and kd:&#xA;https://i.gyazo.com/e9630d77a2dc443aa17412a2d2cc486a.png" CreationDate="2017-05-09T09:28:14.450" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="7431" PostId="5067" Score="0" Text="It's something about the 'cosd'" CreationDate="2017-05-09T09:37:48.657" UserId="5609" ContentLicense="CC BY-SA 3.0" />
  <row Id="7432" PostId="5064" Score="0" Text="Have you tried regenerating your vertex normals to be sure they're accurate?" CreationDate="2017-05-09T13:43:17.977" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7433" PostId="5064" Score="0" Text="It was the diff value. Needed to be saturated before calculating the reflectance vector." CreationDate="2017-05-09T13:57:31.543" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="7434" PostId="5022" Score="1" Text="Is this paper publicly available? In that case it would be useful if you could provide us with a link to it." CreationDate="2017-05-09T14:10:30.250" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="7435" PostId="5085" Score="0" Text="I think it is possible but solution depends on definition of `light intensity`. So I would suggest to specify what do you mean by that" CreationDate="2017-05-09T14:59:46.400" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7437" PostId="5085" Score="0" Text="Are you sure? According to wiki luminous intensity is based on the luminosity function, `a standardized model of the sensitivity of the human eye`" CreationDate="2017-05-09T15:20:59.090" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="7438" PostId="5085" Score="0" Text="Yeah.. you're right again" CreationDate="2017-05-09T15:21:19.617" UserId="6603" ContentLicense="CC BY-SA 3.0" />
  <row Id="7441" PostId="5024" Score="0" Text="@flawr  The included image, the purple should take up the entire plot window" CreationDate="2017-05-09T18:53:17.833" UserId="6501" ContentLicense="CC BY-SA 3.0" />
  <row Id="7442" PostId="5024" Score="0" Text="I cannot reproduce your problem, when I try it the whole axis are filled. You can of course cut the axis using `xlim([a,b])` and `ylim([c,d])` or `axis([a,b,c,d])`." CreationDate="2017-05-09T19:09:43.983" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="7443" PostId="5086" Score="0" Text="Could you add a bit more about the wider context? Are the offset triangles to remain attached as in the original mesh? Does &quot;culled&quot; mean the original triangle is discarded, or just that the offsetting is abandoned, leaving the triangle at its original size?" CreationDate="2017-05-09T22:18:30.857" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7445" PostId="5086" Score="1" Text="So... how does this work with meshes? Because in a mesh, a vertex has more than 2 neighbors. Or is this just for individual triangles?" CreationDate="2017-05-09T22:32:41.903" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7446" PostId="5086" Score="0" Text="My implementation is such that all triangles are laid out in a continuous buffer:&#xA;[P1.x, P1.y, P1.z, P2.x, P2.y, P2.z ... Pn.x, Pn.y, Pn.z] with neighboring points also laid out explicitly at attributes. This way, each vertex of each face can be calculated and manipulated without affecting neighboring faces. Nicol Bolas, yes, dealing with each triangle separately." CreationDate="2017-05-09T23:02:06.493" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7447" PostId="4807" Score="0" Text="Not quite clear on what you're asking here. You can use a canvas as a texture for a WebGL context in another canvas. Typically, this &quot;texture&quot;-canvas would be off-screen. There's no need to worry about aspect. Best if the dimensions of the texture are powers of 2. &quot;256x256&quot;, &quot;128x64&quot;, and &quot;1024x4096&quot; are all fine. Affect your aspect ratio using mapping coordinates and geometry." CreationDate="2017-05-09T23:17:08.767" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7448" PostId="5085" Score="3" Text="I suppose you could multiply the R,G,B by the inverse wavelength (frequency) of said light to approximate its power, although this assumed R,G,B are 3 discrete peaks at 3 different wavelengths when in reality the power is spread across some overlapping range." CreationDate="2017-05-10T02:12:55.230" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7449" PostId="5087" Score="1" Text="First thing would be to check your texture coordinates are valid. This you can do by outputting the tex.xy to gl_FragColor,you should see a red/green gradient across your triangle. Once you confirm that is working you should also make sure your texture does not contain all 0 in its alpha as you're multiplying the A channel in your final colour, you can test this by not multiplying the alpha channel from the texture." CreationDate="2017-05-10T03:50:32.750" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7450" PostId="5087" Score="1" Text="Also texture coordinates are usually normalised (0...1). Your 'tex' array has values higher than 1 ?" CreationDate="2017-05-10T03:53:27.993" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7451" PostId="5087" Score="0" Text="Hi @PaulHK, thanks for responding. I'll try that experiment. I set the texture coordinates so that the texture repeats (I'm not sure what this is called but I know this is how you use a texture to tile an object.)" CreationDate="2017-05-10T03:55:12.423" UserId="5521" ContentLicense="CC BY-SA 3.0" />
  <row Id="7452" PostId="5087" Score="0" Text="I thought it may have been that, but I didn't see anything like glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT);   I think the default texture behaviour is clamping." CreationDate="2017-05-10T04:17:24.263" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7453" PostId="5087" Score="0" Text="I added the function call to set the GL_TEXTURE_WRAP=GL_REPEAT but I still see black. I also set the texture coordinates to 0, 0.5, and 1 (from 0,2, and 4) but this did not work either. Also, WebGL inspector is not working (won't turn red on a page) butt I think I've seen before that my texture loads but for some reason is not sampled." CreationDate="2017-05-10T05:12:40.250" UserId="5521" ContentLicense="CC BY-SA 3.0" />
  <row Id="7454" PostId="5088" Score="2" Text="Usually the normal in a BRDF calculation is the normal of the surface at the intersection point. the 'if (n.dot(r.d) &lt; 0) n = n*-1.0;' bit looks strange, what is that for ?" CreationDate="2017-05-10T09:05:34.723" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7455" PostId="5088" Score="0" Text="Thanks for the input, and that part was given to me by my professor, but as far as I know that ensures that the normal points out from the surface, or is properly oriented." CreationDate="2017-05-10T09:30:07.807" UserId="6606" ContentLicense="CC BY-SA 3.0" />
  <row Id="7457" PostId="5092" Score="0" Text="Not sure if I'm reading this right ? Are you proposing I switch from image-based texturing to polygonal mesh of the sphere and use perspective texturing rather ? The computational complexity (and related visual artifacts alone ) disqualify this approach altogether. Right now, until planet is close, I can stay around 60 fps. No chance with perspective texturing. That link looks like it allows to move the point of distortion up and down, so not sure how it can be used to remap the sphere points into the texture. Doesn't look like it." CreationDate="2017-05-10T14:15:25.677" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="7458" PostId="5092" Score="0" Text="No I mean changing the source image to minimize coordinate processing" CreationDate="2017-05-10T14:18:17.023" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7459" PostId="5092" Score="0" Text="But that would only help with the distortion around poles (which I'm not showing,  deliberately). The 3-D perspective (most visible on left/right edge)  is a real-time effect - it's essentially some nonlinear function (most probably it has a kind-of quadratic falloff, I'm guessing). Essentially, for each 2D point (x,y) on a known scanline y, we want to know the 3D point (x,y,z) - which I can easily linearly remap to the texture coordinates" CreationDate="2017-05-10T14:38:35.893" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="7460" PostId="5090" Score="1" Text="What kind of CPU/system can hold useful 2D textures but not a short lookup table for an `asin()` approximation? A dozen entries + linear interpolation should be good enough for your application." CreationDate="2017-05-10T20:10:03.743" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7461" PostId="5090" Score="1" Text="It's not about lack of main RAM but about slowing down the RISC chip which gets effectively locked till a read from RAM is finished. Transferring texels is about the extent of RAM access that is doable in 30+ fps.  The local cache is only 4 KB for both code and data. Problem is, it's 32-bit internally for read/write which either wastes a lot of precious cache or performance when you unpack single bytes from 32 bits. You mention dozen values for asin. Can you elaborate? 90 degrees/12 =~7 degree step, if I get you right." CreationDate="2017-05-10T20:48:52.767" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="7462" PostId="5090" Score="2" Text="Yes, that's what I meant, with 12 being a rough guess. You would still have some distortion compared to the real `asin()` but probably not that much. Can your CPU do multiplications in reasonable time? If so, a cubic spline would be an even better approximation (more quality for the same table size). I think this can be done with integer math but float would be easier, if you have it." CreationDate="2017-05-10T21:08:05.090" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7463" PostId="5090" Score="1" Text="Conversion from float to int is usually last step, when I confirm in excel that the numbers work- but is necessary as floats are not supported and I try to avoid fixed point as much as possible. The multiplication takes just 3 cycles like add or sub or bitshift. Cubic spline is a great idea. I completely forgot about those! So far I managed to avoid tables, but if there's no other way I could sacrifice , say , 100 bytes for 25 values of asin , at 4 degree resolution. Or more- it's easy to tweak table size when hunting for acceptable level of distortion." CreationDate="2017-05-10T21:49:14.470" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="7465" PostId="5088" Score="0" Text="how about 1/n.dot(i) in your eval() function  -- if n.dot(i) is in the 0..1 range your output is going to be 1 .. infinity" CreationDate="2017-05-11T02:43:50.617" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7468" PostId="5088" Score="0" Text="hmm I tried removing the dot product and just sending back the ks color and nothing changed. Also I've updated my code a bit, I had a mistake in the first brdf function calls in which I was sending the r.d vector instead of -r.d (or o) vector. This has given me a new stranger error now though where the specular sphere has a black ring now." CreationDate="2017-05-11T03:30:55.847" UserId="6606" ContentLicense="CC BY-SA 3.0" />
  <row Id="7469" PostId="2185" Score="0" Text="If the normals are varying between vertices, why should the tesselation be so important?" CreationDate="2017-05-11T04:36:00.023" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7470" PostId="1888" Score="0" Text="You could also specify a normal for each vertex as an attribute, not a uniform, and render the mesh all in one call." CreationDate="2017-05-11T04:49:18.350" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7471" PostId="2185" Score="0" Text="@Jackalope Tessellating more finely means the normals will be evaluated correctly at each tessellated vertex, and interpolated over a shorter distance between vertices, so the error due to using linear interpolation will be reduced." CreationDate="2017-05-11T04:53:13.783" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7472" PostId="5095" Score="0" Text="Nice demo! That is _exactly_ what I'm doing right now !  I noticed you kept the radius small to keep the framerate high ;) Yes, I'm very often experimenting with how far I can push the division vs interpolation, often times I end up adjusting the dataset so I can replace division with bitshift. Since we can mirror the right half of planet, we need to cover just 90 degrees with the LUT. But, is the AngleStep per on-screen pixel, constant for those 90 degrees?So, if for current camera,equator takes up 180 pixels, e.g. half takes 90 pixels,then each pixel would correspond to 90/90 =1 degree step?" CreationDate="2017-05-11T14:10:12.003" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="7473" PostId="5095" Score="0" Text="I think I just realized what I need to do: Create a sphere in 3D, place camera right in front of it, transform each point on equator, and note the X coordinate. That will give me the exact distribution that I need for the LUT creation. I'm not sure why I thought initially I could get away without doing that - I was hoping there would be some simple math trick,as it's a sphere. Actually, I can do it in excel, as I have a tab that I used for testing the integer version of the 3D transformations..." CreationDate="2017-05-11T14:13:44.383" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="7474" PostId="5095" Score="1" Text="I noticed you mentioned 4KB code/data cache &amp; memory latency issues in your target platform in comments. This is very cache friendly way of rasterizing the sphere so small LUT should remain in the cache of that size, and each entry in the LUT could be stored in 1 or 2 bytes. You also don't need 1 degree precision so the LUT can be smaller (linearly interpolate between the LUT entries), so the entire LUT could fit in just ~100 bytes or less. You can calculate the LUT easily with acos(), which maps horizontal position to the surface of the cylinder for the &quot;rounded&quot; texture mapping effect." CreationDate="2017-05-11T14:37:16.127" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7481" PostId="5095" Score="0" Text="Thanks, I'm still trying to wrap my head around the arccos, though. I get that its numerical distribution of values is nonlinear, and that's exactly what we need to cheat. I guess, when I implement it,it'll be obvious,but to get the index to the table, I can only remap it [from the horizontal position on the screen] linearly (e.g. a point that is horizontally in the middle between the left edge and circle center would have value of 0.5, arccos of which is 60 degrees, and that corresponds to texel at position of 0.66 x TextureSegmentWidth).It may not matter,as the end result is nonlinear,anyway" CreationDate="2017-05-11T17:30:43.553" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="7482" PostId="5086" Score="0" Text="trichoplax - &quot;Culled&quot; means thrown out, not rendered, as in a back-facing, single-sided primitive." CreationDate="2017-05-11T17:38:10.983" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7483" PostId="5095" Score="1" Text="You map the x-coordinate linearly to the table and fetch the u-coordinate you use to fetch from the texture. This is essentially:&#xA;for(unsigned i=0; i&lt;lut_suze; ++i) lut[i]=acos(1.0f-2.0f*float(i)/lut_size) / pi * 0.5f * tex_width;" CreationDate="2017-05-11T17:40:07.540" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7484" PostId="5086" Score="0" Text="Both of you seem to suggest that the GPU sees faces as &quot;tethered&quot; to other faces. As far as I know, this is not the case. The GPU sees faces as separate entities. Faces only appear to be connected because congruent vertices from neighboring faces typically have identical attributes. One common exception is face normals in a flat-shaded mesh." CreationDate="2017-05-11T17:42:38.940" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7485" PostId="5088" Score="0" Text="I'm not sure if this gives a clue, but the reflective sphere also seems to be lacking any shadow on the plane beneath it." CreationDate="2017-05-11T21:25:38.607" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7486" PostId="5095" Score="0" Text="Thanks a lot, examining the data closer reveals your equation provides data for whole width of the circle, not just for diameter, correct ? I watched your video again, but I find it hard to see any quantization artifacts there. Did you use full floats or fixed point ? I find it hard to believe that texturing would be integer-only!" CreationDate="2017-05-11T21:32:38.363" UserId="6063" ContentLicense="CC BY-SA 3.0" />
  <row Id="7488" PostId="5088" Score="0" Text="From the shadowed wall area's point of view, it will see the glowing ringed ball so it acts as a light source instead of an occluder. Energy is also not being conserved due to this problem so the scene ends up with too much light." CreationDate="2017-05-12T02:28:51.547" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7489" PostId="5088" Score="0" Text="One thing I would do is make a debug mode in your path tracer. In this mode you stop recursing at the first intersection and instead output the outgoing sampling direction as a colour (colour = rayDir*0.5+0.5). From there you can visualise if the out bound ray direction is being computed properly, the walls can act as a reference." CreationDate="2017-05-12T02:34:52.140" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7490" PostId="5088" Score="1" Text="I was finally able to solve this problem, I had a problem with the &quot;rad = rad + (receivedRadiance(y, depth + 1, true) * (PI)).mult(b) * (1 / p);&quot; statement. I changed it to this : &quot;rad = rad + receivedRadiance(y, depth + 1, flag).mult(brdf.eval(n, o, i))*(n.dot(i)/(pdf*p));&quot; I found this solution by just looking up similar projects and formulas online. In the diffuse BRDF I added &quot;pdf = i.dot(n) * (1 / PI)&quot; to account for this change, and cancel out the dot product. edit: oh I also needed my normal to be checked again o, not r.d." CreationDate="2017-05-12T06:29:34.743" UserId="6606" ContentLicense="CC BY-SA 3.0" />
  <row Id="7491" PostId="5099" Score="0" Text="Is image 2 somehow broken? Or are you in fact trying to figure out disparity to then operate on the second image somehow. Like making colors uniform or something." CreationDate="2017-05-12T14:59:03.447" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7492" PostId="5099" Score="0" Text="@joojaa, no, sorry for not being clearer. Image 2 is a perfect image/photo. What the aim of the algorithm is, is to take blocks of a totally different image (Image 1) - say for example comic book scans, and use that to re-draw image 2, purely for the artistic effect." CreationDate="2017-05-12T15:14:12.713" UserId="6618" ContentLicense="CC BY-SA 3.0" />
  <row Id="7493" PostId="5099" Score="0" Text="Added example images to clarify." CreationDate="2017-05-12T15:16:36.470" UserId="6618" ContentLicense="CC BY-SA 3.0" />
  <row Id="7494" PostId="5099" Score="0" Text="Not sure how much help this would be, but what you are after is, in effect, a version of [Vector Quantisation](https://en.wikipedia.org/wiki/Vector_quantization), i.e. VQ, except that the representative vectors aren't derived/synthesised from the source image but supplied separately. You thus don't have to do the training process." CreationDate="2017-05-12T16:17:16.493" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7495" PostId="5099" Score="2" Text="...Or you could just grab Mosaic program (maybe from http://www.brighthub.com/multimedia/photography/articles/34691.aspx ). The only tedious part would be manufacturing a set of images to corresponding to your blocks." CreationDate="2017-05-12T16:21:05.967" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7496" PostId="5086" Score="1" Text="@Jackalope: &quot;*Both of you seem to suggest that the GPU sees faces as &quot;tethered&quot; to other faces.*&quot; That's because, generally speaking, this is true. Most meshes don't merely have neighboring triangles use &quot;identical attributes&quot;; they reuse *the same vertices*. This could be through triangle lists that use the same index multiple times, or through triangle strips, or whatever. But generally speaking, meshes reuse neighboring vertices. Your meshes do not, but your specific case doesn't change the general case. That's why I asked for clarification." CreationDate="2017-05-12T16:44:45.493" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7497" PostId="5086" Score="0" Text="Thanks for that, Nicol Bolas. I was unclear about that. I'd thought that vertex data was usually expanded to explicit values before going to the GPU, ala STL-style rather than OBJ-style notation." CreationDate="2017-05-12T17:14:54.217" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7498" PostId="5095" Score="0" Text="Yes, it generates the LUT for full width of a scanline. Can't remember the details, but I'm pretty sure the LUT stored just integer pixel coordinates and I just point sampled it. You could do better with linear interpolation and fixed point." CreationDate="2017-05-12T19:23:22.237" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7499" PostId="5102" Score="0" Text="OpenGL ES is for [embedded platforms](https://www.khronos.org/opengles/) like cell phones. macOS definitely doesn't support it, and I'm pretty sure neither Linux nor Windows do either. They all support straight OpenGL, though." CreationDate="2017-05-13T03:34:23.830" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7500" PostId="5100" Score="1" Text="Nice work figuring this out! Really like the mathematical description at the beginning." CreationDate="2017-05-13T03:36:59.053" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7501" PostId="5102" Score="0" Text="Do you mean, there is no backdoor to display OpenGL ES content in a native app?" CreationDate="2017-05-13T18:12:50.370" UserId="6623" ContentLicense="CC BY-SA 3.0" />
  <row Id="7502" PostId="5102" Score="0" Text="No there's no such backdoor. OpenGL and OpenGL ES are fairly similar, and portions of your code may work on both, but usually there are some parts that will not. For example on macOS you have a `CGLContextObject` that represents the context, whereas on iOS you use an [`EAGLContext`](https://developer.apple.com/library/content/documentation/3DDrawing/Conceptual/OpenGLES_ProgrammingGuide/WorkingwithOpenGLESContexts/WorkingwithOpenGLESContexts.html)." CreationDate="2017-05-13T20:44:07.067" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7503" PostId="5104" Score="0" Text="Are you looking for a memory-efficient way of producing this exact effect (particles moving into their required positions), or are you looking for similar effects that don't look quite the same but are easier/more memory-efficient to implement? For example. fading in the final text whilst simultaneously fading out animated white noise would have a similar feel, but wouldn't really look like moving particles, so we need to know precisely what you require in order to be able to answer." CreationDate="2017-05-14T15:24:37.013" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7504" PostId="5104" Score="0" Text="The random dots dissolving into text doesn't look to me like the dots move at all. It looks like random noise which is then blended with the letters using the &quot;noise dissolve&quot; blend mode or transition." CreationDate="2017-05-14T15:45:34.570" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7505" PostId="5104" Score="0" Text="@trichoplax Same effect with &quot;moving&quot; binary pixels" CreationDate="2017-05-14T15:53:46.297" UserId="6628" ContentLicense="CC BY-SA 3.0" />
  <row Id="7506" PostId="5104" Score="0" Text="@user118321 I think I understand what you mean: Wouldn't that require non-random noise blend masks (to blend _all_ pixels within a few transition frames)? That would require full frame animated masks (and the respective memory)." CreationDate="2017-05-14T15:53:51.970" UserId="6628" ContentLicense="CC BY-SA 3.0" />
  <row Id="7507" PostId="5104" Score="0" Text="If you require individual pixels to be visibly following paths, would you be interested in solutions that save memory by using a limited number of distinct path patterns, so that some pixels are following the same path as each other, just offset by the distance between their final destinations?" CreationDate="2017-05-14T16:42:19.243" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7508" PostId="5104" Score="0" Text="Might work if the pixels come from the same letters, but if e.g. all set top-left pixels move in synch I suspect it will be noticable. Another question is whether it would work with proportional fonts." CreationDate="2017-05-14T18:17:49.537" UserId="6628" ContentLicense="CC BY-SA 3.0" />
  <row Id="7509" PostId="5106" Score="0" Text="Do you mean any integer or any unsigned integer?" CreationDate="2017-05-14T20:20:52.873" UserId="6546" ContentLicense="CC BY-SA 3.0" />
  <row Id="7510" PostId="5106" Score="2" Text="@Archmede The type of names is unsigned integer, so that's what I mean." CreationDate="2017-05-14T20:32:22.047" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7511" PostId="5085" Score="0" Text="0..255, or 0..1 equals Black to White, but 128 (or 0.5) is NOT the actual middle of value between 0 and 1 when it comes to display output. The middle is somewhere around 158, iirc." CreationDate="2017-05-15T11:45:56.110" UserId="6633" ContentLicense="CC BY-SA 3.0" />
  <row Id="7515" PostId="5104" Score="1" Text="@trichoplax If I understand correctly, that sounds good indeed - but it'll take some time for me to try. Thanks for now, I'll report back!" CreationDate="2017-05-15T19:01:37.580" UserId="6628" ContentLicense="CC BY-SA 3.0" />
  <row Id="7517" PostId="5104" Score="0" Text="@trichoplax Thanks, I'd like to get back to you once more questions come up." CreationDate="2017-05-15T19:12:50.243" UserId="6628" ContentLicense="CC BY-SA 3.0" />
  <row Id="7519" PostId="5112" Score="0" Text="My first attempt to [run this on shadertoy.com](https://www.shadertoy.com/view/lslBRH) is missing a few pieces. If you know the site, could you please have a look?" CreationDate="2017-05-16T06:47:01.240" UserId="6628" ContentLicense="CC BY-SA 3.0" />
  <row Id="7520" PostId="5010" Score="0" Text="You could use a fullscreen triangle and raymarch it. There's lots of tutorials about that. And iirc it's the preferred method." CreationDate="2017-05-16T07:06:15.170" UserId="6633" ContentLicense="CC BY-SA 3.0" />
  <row Id="7522" PostId="5116" Score="0" Text="One key point of the near and far planes is that they set the how the depth changes throughout the region between them." CreationDate="2017-05-16T15:08:56.053" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7523" PostId="5112" Score="0" Text="Yes, this wasn't intended to be a complete program. I was just trying to explain my method. In looking at it again, I think I'll need to revise it. I don't have time right now, but I'll try to get an example up &amp; running soon." CreationDate="2017-05-16T19:23:17.657" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7524" PostId="5112" Score="0" Text="Yes, of course. I'm just not able to fill in the gaps on my own without spending too much time with WebGL/Shadertoy, but I thought I'd give it a bit of a try. However, while I'm curious to see how this works and to get a glimpse at the result, it's not ultimately the approach I am looking for, since it requires complex graphics hardware and is way beyond a &quot;simple&quot; algorithm." CreationDate="2017-05-16T19:42:33.397" UserId="6628" ContentLicense="CC BY-SA 3.0" />
  <row Id="7529" PostId="5112" Score="0" Text="Actually, almost everybody has the hardware needed to view WebGL.&#xA;http://caniuse.com/#feat=webgl" CreationDate="2017-05-17T03:50:35.177" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7531" PostId="5119" Score="0" Text="I want to understand the purpose of the depth buffer. I've only attempted a wireframe renderer, so I didn't consider depth when plotting the 3D points to a 2D screen (I could plot the points and lines in any order and it would look right).&#xA;&#xA;Is the purpose of the depth buffer to maintain a record of the Z distance of each point once it's been projected to 2D so that solid polygons may be drawn in the correct order? Is this why I didn't require a near/far plane in my simple wireframe visualizer?" CreationDate="2017-05-17T18:01:32.443" UserId="6646" ContentLicense="CC BY-SA 3.0" />
  <row Id="7533" PostId="5119" Score="2" Text="@VilhelmGray Right, the depth buffer records the depth of the nearest surface at each pixel, so that when rasterizing a triangle, you can tell if it should be occluded by the previously rendered pixels or not. But if you don't care about depth sorting (because you're drawing wireframe, you've pre-sorted your triangles already, or some other reason) then there's no need for a depth buffer." CreationDate="2017-05-17T18:31:31.577" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7535" PostId="5120" Score="0" Text="More about the context: I need to use the billboards in **points** mode. So a billboard is attached in screen space from a single vertex position." CreationDate="2017-05-17T22:21:50.433" UserDisplayName="user4613" ContentLicense="CC BY-SA 3.0" />
  <row Id="7536" PostId="5122" Score="2" Text="In 2015 Pixar gave a great talk at SIGGRAPH about [Multi-Threading for Visual Effects](http://s2015.siggraph.org/attendees/courses/sessions/multi-threading-visual-effects.html). I'm not sure if the video is available, but if you can find it, it's worth watching. They break down their tests to do multi-threading several different ways: Per frame, per tile, per effect pass, etc. Really interesting and useful info!" CreationDate="2017-05-17T22:24:59.630" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7538" PostId="5124" Score="0" Text="If this site gets [stack snippets](https://computergraphics.meta.stackexchange.com/questions/26/do-we-want-stack-snippets) at some point, I'll replace the jsfiddles with stack snippets so it can all be viewed from the answer post." CreationDate="2017-05-18T00:30:02.877" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7539" PostId="5124" Score="1" Text="Thanks for this nice demo of your initial idea. I'll post a capture of the animation tomorrow for you to integrate into your post. I think this might be called a (pseudo) random-random walk? This might be extended with more directionality at the cost of additional state memory for each pixel (3 bits). But the effect is close enough to the original's presumed particle effect. I'll try to find out if there are any RNGs/LSFR whose state can be set without iteration (for the reverse operation) and then implement this on a microcontroller. Thanks again." CreationDate="2017-05-18T05:36:22.980" UserId="6628" ContentLicense="CC BY-SA 3.0" />
  <row Id="7540" PostId="5125" Score="0" Text="Is [wikipedia](https://en.wikipedia.org/wiki/Torus) not enough?" CreationDate="2017-05-18T15:37:40.340" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="7542" PostId="5099" Score="1" Text="For the case X=1 you might want to take a look here: https://codegolf.stackexchange.com/questions/33172/american-gothic-in-the-palette-of-mona-lisa-rearrange-the-pixels/33206#33206&#xA;&#xA;One solution you could think of is reducing each X by X square to one colour (e.g. by averaging) and then using the algorithm presented in the linked challenge." CreationDate="2017-05-18T22:09:42.617" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="7543" PostId="5124" Score="1" Text="I have to admit, this is the better answer. +1" CreationDate="2017-05-18T22:34:47.567" UserId="6596" ContentLicense="CC BY-SA 3.0" />
  <row Id="7544" PostId="422" Score="1" Text="Note that (0.3, 0.59, 0.11) are the [luma coefficients](https://en.wikipedia.org/wiki/Luma_(video)) for Rec. 601 (aka Standard Def. NTSC), whereas Rec. 709 (aka High Def. ATSC) would be (0.2126, 0.7152, 0.0722). And sRGB images (which is likely what you're dealing with) are closer to Rec. 709 than Rec. 601." CreationDate="2017-05-19T02:45:55.393" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7545" PostId="4611" Score="0" Text="PaulHK That's exactly what I am doing in my path tracer, russian roulette for each interface between layers, so, no branching at all. Unfortunatelly, my implementation is not finished yet, so I do not have information regarding the actual performance. I've based my implementation on the paper &quot;Arbitrarily Layered Micro-Facet Surfaces&quot; by Andrea Weidlich and Alexander Wilkie, which seems to be more limited than the framework of Wenzel Jakob (pointed out in the answer by Stefan), but which is capable of generating quite good results and is much simpler to implement." CreationDate="2017-05-19T04:31:42.510" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="7546" PostId="297" Score="0" Text="The path tracing example at the bottom is simulating total internal reflection also which adds a lot of complexity to the shading. Thin areas can appear both 'thicker' and more attenuated like the rims of the ears for example." CreationDate="2017-05-19T07:38:49.653" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7547" PostId="5125" Score="0" Text="Not really, because it describe everything in matter of 3+ dimensions, whereas I would like properties in the 2 dimension space. For example, I would like a formulae to calculate the distance minimal distance between two points.  &#xA;I found a way to do it but I'd like more mathematical insights, because I think it would be better and faster." CreationDate="2017-05-19T08:28:19.957" UserId="6655" ContentLicense="CC BY-SA 3.0" />
  <row Id="7548" PostId="5125" Score="0" Text="You just see the embeddings of a torroidal space in 3d, but the space itself is 2d. *In the field of topology, a torus is any topological space that is topologically equivalent to a torus.* So a rectangle with opposing sides &quot;glued&quot; together is a toroidal space." CreationDate="2017-05-19T08:42:09.070" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="7549" PostId="57" Score="0" Text="I do not quite see the difficulty: Couldn't you just start with any coarse initial mesh, and refine each triangle where e.g. the product of $curvature\times area$ (or e.g. the sum of the absolute angles to the neighbour triangles, or any other heuristic) exceeds some constant? In bthe basic adaptive FEM you basically try to estimate the error on each element (i.e. triangle) and refine it if the error is above some threshhold." CreationDate="2017-05-19T09:14:24.353" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="7550" PostId="5129" Score="1" Text="It's unclear what you mean by &quot;shaderpass&quot; here. Are you rendering quads to the texture? You say that each pass should render to different locations; how are those locations determined? Is your shader trying to read from the same texture it's writing to? Please be more explicit (or post some code) as to exactly what you're trying to do." CreationDate="2017-05-19T15:53:22.607" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7551" PostId="5129" Score="0" Text="Actually my post is super explicit and to the point. I'm not sure why you come up with so many things not related to the question. It tells you exactly what i do and need to know. You confuse me." CreationDate="2017-05-19T17:02:49.483" UserId="6633" ContentLicense="CC BY-SA 3.0" />
  <row Id="7552" PostId="5130" Score="0" Text="after rethinking... i'm not sure i follow. How would i blend one texture with itself? Read-modify-write in the shader isn't an option for me, it would kill performance. Remember, there is only one output texture.. and a dozen shaderpasses." CreationDate="2017-05-19T17:05:18.600" UserId="6633" ContentLicense="CC BY-SA 3.0" />
  <row Id="7553" PostId="5129" Score="1" Text="Well, the term &quot;shaderpass&quot; is not part of OpenGL or GLSL lexicon. I've heard it used in many different contexts for radically different things. It's been used to mean &quot;multipass rendering over the same mesh&quot;, &quot;full-screen quads over a scene&quot;, and other things. There is no one definition of &quot;shaderpass&quot;, so I'm trying to ask what *you* mean by this term. As well as the other things. And FYI, whether you're reading from the texture you're rendering to is *not* unrelated to the question." CreationDate="2017-05-19T17:05:35.920" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7554" PostId="5024" Score="0" Text="Cutting the axis works for cutting the axis, then the resulting image takes up a quarter of the new window.   If I enter graphics_toolkit(&quot;gnuplot&quot;) I get the entire axes to be filled with the purple squares like I want.  However, if I use the line( ) function to plot a line overtop I get the following error:     lot &quot;-&quot; binary format='%float64' record=2 using ($1):($2) axes x1y1 title &quot;&quot; with lines linestyle 10 ;&#xA;           ^&#xA;           line 0: invalid command" CreationDate="2017-05-19T17:22:25.030" UserId="6501" ContentLicense="CC BY-SA 3.0" />
  <row Id="7555" PostId="5024" Score="0" Text="If I don't use gnuplot, and instead use OpenGL (which is the default), I get the same quarter plot image of purple but plotting overtop works." CreationDate="2017-05-19T17:25:15.020" UserId="6501" ContentLicense="CC BY-SA 3.0" />
  <row Id="7556" PostId="5024" Score="0" Text="@flawr  Please see the edited post.  Hopefully it helps and thank you very much for helping me out." CreationDate="2017-05-19T17:48:07.310" UserId="6501" ContentLicense="CC BY-SA 3.0" />
  <row Id="7557" PostId="5129" Score="0" Text="I didn't mention that i'm reading from the output texture, therefore I'm not doing it. If I did that, I'd mention it. :/ well, a shader pass is when you have a shader doing things. The question implied that there are multiple passes Everything else is just &quot;flavour&quot; and changes nothing about it being multiple shader passes. Your attempt at accuracy is kind of appreciated, but not leading anywhere." CreationDate="2017-05-19T17:48:41.070" UserId="6633" ContentLicense="CC BY-SA 3.0" />
  <row Id="7558" PostId="5130" Score="0" Text="@z0rberg's: &quot;*How would i blend one texture with itself?*&quot; You don't blend *textures* at all. Blending happens between the current render target and the outputs of the fragment shader." CreationDate="2017-05-19T17:52:55.907" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7559" PostId="5130" Score="0" Text="I must have struck a nerve somewhere in your ego. You're not helpful and actually annoying. :) sure, you wanna be super accurate, but you don't see how that only serves yourself and no one else. Can you stop being annoying? Cheers! :) ... or wait. You can have the last word, as that will make you feel better... and i will simply ignore you. :)" CreationDate="2017-05-19T18:18:41.733" UserId="6633" ContentLicense="CC BY-SA 3.0" />
  <row Id="7560" PostId="5130" Score="0" Text="@z0rberg's: I did answer your question. How do you blend a texture with itself? You don't; you blend *the output of your rendering process* with the texture. You render with blending. You render again with blending. Both are rendered to the same image, with the latter blended on top of the former. If you find it &quot;annoying&quot; to use the right words to describe things, I really can't help you. But that's how you do what ratchet freak was talking about." CreationDate="2017-05-19T19:01:15.010" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7561" PostId="5129" Score="1" Text="&quot;*Everything else is just &quot;flavour&quot; and changes nothing about it being multiple shader passes*&quot; No, they're not &quot;flavour&quot;. For example, if your multiple &quot;shader passes&quot; are actually rendering *different geometry*, then I would suggest you construct the geometry for the separate passes to simply not overlap. But if they're rendering the *same* geometry, then you'd need to do something else. And that &quot;something else&quot; would be based on how each pass knows what it should and shouldn't render to. This is not sophistry; these details directly impact what my answer would be." CreationDate="2017-05-19T19:03:33.073" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7562" PostId="5120" Score="0" Text="Screenshots of the problem would have made this much easier to answer by more people FYI!" CreationDate="2017-05-19T21:15:12.377" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7563" PostId="5127" Score="0" Text="You're right, I found some good stuff with &quot;toroidal space&quot; after all. And thanks for the algorithm. I think I have an optimization I will test and confirm here. I'll accept your answer after that." CreationDate="2017-05-19T21:48:02.437" UserId="6655" ContentLicense="CC BY-SA 3.0" />
  <row Id="7564" PostId="5130" Score="3" Text="@z0rberg's Welcome to Computer Graphics Stack Exchange! Please understand that people are here to help you, and are not asking questions to annoy you. Narrowing down what you are aiming to do is part of helping you. All members of this community are expected to [Be Nice](https://computergraphics.stackexchange.com/help/be-nice). Please bear this in mind when responding to others." CreationDate="2017-05-20T00:02:37.117" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7565" PostId="5131" Score="0" Text="Do you mean you want the object to move directly towards or away from the eye?" CreationDate="2017-05-20T00:31:11.103" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7566" PostId="5131" Score="0" Text="Yeah along the direction of the eye" CreationDate="2017-05-20T10:06:27.693" UserId="6401" ContentLicense="CC BY-SA 3.0" />
  <row Id="7567" PostId="5108" Score="0" Text="Do you know anything more about the structure? E.g. are there just a handful of planes, or are there many planes but just a few points per plane? Are the planes equidistant? Are the points within a plane closer to eachother than to the points of the other planes?" CreationDate="2017-05-20T13:03:04.303" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="7568" PostId="5108" Score="0" Text="Also do you have an upper bound of the number of planes?" CreationDate="2017-05-20T14:53:16.570" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="7569" PostId="5132" Score="0" Text="It means their values depend on two directions (usually incoming and outgoing light). With a direction being a direction in 3-space, of course, not something like &quot;up-down&quot;." CreationDate="2017-05-21T16:37:01.117" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="7570" PostId="5133" Score="2" Text="So the &quot;two directions&quot; are simply the incidence ray and the reflection/transmission ray? :)" CreationDate="2017-05-21T19:08:44.187" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="7571" PostId="5129" Score="2" Text="@z0rberg's Please reconsider your attitude toward people you are hoping to get free help from. And please do add more details—including screenshots and relevant rendering code—to your question. We can't read your mind and it is far from clear what exactly the problem you're trying to solve is." CreationDate="2017-05-22T03:33:21.743" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7572" PostId="5134" Score="0" Text="Some things to consider, while you wait for the answer. How would you get a unsorted dataset in order if not by sorting? So nearly every algorithm stars by organising data to suit their need. Where would you store that data? Suddenly you have tables and sorting on everything you do." CreationDate="2017-05-22T05:37:29.730" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7574" PostId="5137" Score="0" Text="The Lumen application posted above suggests it can emulate &gt;60hz frequencies by using either horizontal clock or pixel clock simulation." CreationDate="2017-05-22T06:53:11.987" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7575" PostId="5137" Score="0" Text="@PaulHK yes but then you need to average frames. Most systems have vertical sync  on by default. I mean i can sort of do this on my monitor since i have sync turned off but you couldn't depend on a web application to do this since it wouldnt work for 99% of users, and you have no control over the setting." CreationDate="2017-05-22T06:59:04.463" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7576" PostId="5137" Score="0" Text="Could you not simulate the time difference from the top-left corner ? This is (roughly) how video game emulators handle raster effects for instance. For example, the middle scanline of the frame would be roughly +8ms from the top of the frame, assuming a 16.6ms frame." CreationDate="2017-05-22T07:03:15.623" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7577" PostId="5137" Score="0" Text="@PaulHK Not on the platform you use in a browser, the hardware does multi buffering and you can not escape the sync even if you wanted to, as far as i understand." CreationDate="2017-05-22T07:04:29.473" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7578" PostId="5135" Score="0" Text="Oow, thanks! That's a really nice explanation! :)" CreationDate="2017-05-22T07:36:30.463" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="7584" PostId="5124" Score="0" Text="@Jackalope thank you! My approach is less sophisticated and I can't imagine why the memory would need to be so restricted, but assuming that it is, this was the simplest I could think of." CreationDate="2017-05-22T12:11:42.673" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7585" PostId="5137" Score="0" Text="I guess the core of my question is what sort of calculations are being performed in order to emulate a video oscillation of &gt;60hz? Since both Lumen and my browser are running on the same display with the same refresh rate, albeit one is a native app which might mean that it is doing things that are limited in the browser..." CreationDate="2017-05-22T17:57:59.377" UserId="6674" ContentLicense="CC BY-SA 3.0" />
  <row Id="7586" PostId="5137" Score="0" Text="@Corey well the local program has more access to the system. In any case your question does not really specify that your asking about teh emulation. Your just asking why you have problems below 60Hz. But the real answer is to actually calculate the color as it would be. But its just horribly complicated to get it right. multi sample or integrate the result of your data end run it in color correct environment that should do the trick. Only a browser can not know the colorspace." CreationDate="2017-05-22T18:01:37.387" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7589" PostId="5140" Score="3" Text="As long as all duplicates of a vertex have the same normal, you should get smooth shading. Check that (i) the duplicated normals are indeed identical, and (b) the shader is actually doing Gouraud shading and not, say, flat shading (incorrect setting of [`glShadeModel`](https://www.khronos.org/registry/OpenGL-Refpages/gl2.1/xhtml/glShadeModel.xml) or [interpolation qualifier](https://www.khronos.org/opengl/wiki/Type_Qualifier_(GLSL)#Interpolation_qualifiers))." CreationDate="2017-05-23T02:06:24.997" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7590" PostId="5137" Score="0" Text="The Lumen application is showing results for one frame rather than integrating all possible phases, so we should expect some kind of rolling effect for each frame. Check the red gifs in the link under the 'Frequency ranges' heading." CreationDate="2017-05-23T04:05:25.160" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7593" PostId="5140" Score="0" Text="If you just want to know whether this is possible, this seems ready to answer, and @Rahul's comment contains a good start on such an answer. If you want to know why your specific code isn't working, we'd need to see it in order to investigate it." CreationDate="2017-05-23T10:17:36.590" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7594" PostId="5140" Score="0" Text="@Rahul Thank you for your hints. I check the normals and the glShadeModel and both are set correctly. I also wrote the model into an Wavefront obj file and this also results in strange lightning. Here is an image of the model http://imgur.com/a/JjLxp." CreationDate="2017-05-23T12:11:48.953" UserId="6682" ContentLicense="CC BY-SA 3.0" />
  <row Id="7595" PostId="5144" Score="1" Text="Can you please add a link to the paper or book you're referring to, and a relevant excerpt from the code you're talking about?" CreationDate="2017-05-23T14:35:16.307" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7596" PostId="5144" Score="0" Text="http://www.dgp.toronto.edu/people/stam/reality/Research/pdf/ns.pdf this is it I thought the links are banned or something" CreationDate="2017-05-23T14:46:40.623" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7601" PostId="4900" Score="0" Text="A surface of revolution of a line segment would be a disk, cylinder, or cone, no? I can't see how you would get a hyperboloid; it should be a surface of revolution of a hyperbola. I'm guessing the hyperbola is fit to pass through the two points, or something like that." CreationDate="2017-05-23T15:13:40.263" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7602" PostId="5140" Score="0" Text="Oh..could it be that you are  seeing  &quot;Mach band-ish&quot; related&quot; artefacts that appear when you get a discontinuity in the 1st derivative of the shading? Basically, the human visual system amplifies these discontinuities (probably to save our ancestors being eaten by lions)." CreationDate="2017-05-23T16:12:31.970" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7603" PostId="5146" Score="0" Text="Make sure you got clamp (vs wrap) texture address mode enabled when fetching from shadow map." CreationDate="2017-05-23T16:59:55.583" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7604" PostId="4900" Score="2" Text="@NathanReed It's not intuitive at all but it is possible. Look at the diagrams on the right side of the [wikipedia entry](https://en.wikipedia.org/wiki/Hyperboloid#Parametric_representations)." CreationDate="2017-05-23T22:31:48.730" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7605" PostId="5140" Score="2" Text="Going by your image, it looks like you have Gouraud shading working perfectly well. The problem is a combination of skinny triangles and poorly estimated normals, making it *look* like the shading is not smooth when really it is just varying rapidly over a narrow region." CreationDate="2017-05-24T05:06:42.087" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7606" PostId="5147" Score="0" Text="What about if b is not 1 and 2? Another thing, where is the Neuman boundaries? Those are slip or no-slip? Is it a solid boundaries? If i  use the PCG, will something change in the boundaries? Sorry for all of these stupid questions but I want to understand. Thanks cheers!" CreationDate="2017-05-24T11:42:32.357" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7607" PostId="5147" Score="0" Text="@AnasAlaa if you have a new question you can post it as a question rather than a comment." CreationDate="2017-05-24T20:14:15.433" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7608" PostId="5147" Score="0" Text="I am sorry but it's really the same question but in another form. And it's a complete of his answer too." CreationDate="2017-05-24T22:05:10.613" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7609" PostId="5150" Score="0" Text="Thanks! That first part was the important information for me. I missed this information in the Tutorials. Since I use textures, I only use color to &quot;tint&quot; Quads (Sprites), if you know what I mean, or to alter transparency via the alpha channel. So using a uniform was a good hunch. I am still stuck on matrices and how to do them properly on the GPU, but I am getting there." CreationDate="2017-05-24T22:08:58.997" UserId="6679" ContentLicense="CC BY-SA 3.0" />
  <row Id="7610" PostId="5147" Score="0" Text="There's a little mistake in my previous answer, and I've editted the answer.  About Neuman boundaries, this demo code didn't implement it." CreationDate="2017-05-25T01:49:01.350" UserId="6691" ContentLicense="CC BY-SA 3.0" />
  <row Id="7614" PostId="5153" Score="0" Text="Thank you for the clarifying answer! I understand that if we were to use a path tracer without explicit light sampling, we would never hit a point light source. So, we can basically add its contribution. On the other hand, if we sample an area light source, we have to make sure we should not hit it again with the indirect lighting in order to avoid double dip" CreationDate="2017-05-25T15:37:02.323" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7615" PostId="5153" Score="0" Text="Exactly! Is there any part that you need clarification on? Or there isn't enough detail?" CreationDate="2017-05-25T15:39:24.237" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7616" PostId="5153" Score="0" Text="Also, is multiple importance sampling used only for direct lighting calculation? Maybe I missed but I didn't see another example of it. If I shoot just one ray per bounce in my path tracer, it seems that I cannot do it for the indirect lighting calculation." CreationDate="2017-05-25T15:43:01.690" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7617" PostId="5153" Score="2" Text="Multiple Importance Sampling can be applied anywhere you use importance sampling. The power of multiple importance sampling is that we can combine the benefits of multiple sampling techniques. For example, in some cases, light importance sampling will be better than BSDF sampling. In other cases, vice versa. MIS will combine the best of both worlds. However, if BSDF sampling will be better 100% of the time, there is no reason to add the complexity of MIS. I added some sections to the answer to expand upon this point" CreationDate="2017-05-25T18:53:35.473" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7619" PostId="5153" Score="1" Text="It seems we separated incoming radiance sources into two parts as direct and indirect. We sample lights explicitly for the direct part and while sampling this part, it is reasonable to importance sample the lights as well as BSDFs. For the indirect part, however, we have no idea about which direction may potentially give us higher radiance values since it is the problem itself that we want to solve. However, we can say which direction can contribute more according to the cosine term and BSDF. This is what I understand. Correct me if I'm wrong and thank you for your awesome answer." CreationDate="2017-05-25T19:50:22.677" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7620" PostId="5153" Score="0" Text="That's correct! Very good summary" CreationDate="2017-05-25T19:52:58.853" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7621" PostId="5147" Score="0" Text="So, if I just want to solve the pressure poisson equations with the PCG method, can I just use these boundaries in the code?" CreationDate="2017-05-25T23:07:02.193" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7622" PostId="5147" Score="0" Text="Yes, I think so." CreationDate="2017-05-26T01:50:29.490" UserId="6691" ContentLicense="CC BY-SA 3.0" />
  <row Id="7623" PostId="5147" Score="0" Text="I think the answer is not correct. The two options are either homogenous Neumann boundary conditions (if the value is copied) or homogenous dirichlet boundary condition as negating the value represents a zero value in between the interface. Furthermore I doubt that you can use this function for a cg solver. To which variables do you want apply it? For a cg solver I would recommend to check Robert bridsons book fluid simulation for computer graphics, although he uses a slightly modified (superior) discretization (staggered grid)." CreationDate="2017-05-26T19:18:48.873" UserId="2695" ContentLicense="CC BY-SA 3.0" />
  <row Id="7624" PostId="5147" Score="0" Text="I don't understand how the boundaries will be dirchlet and neuman at the same time. Bridson book is good but he has no code to implement what he said about boundaries and PCG... etc" CreationDate="2017-05-27T03:05:56.473" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7625" PostId="5147" Score="0" Text="Boundary conditions are either dirichlet or Neumann, I don't know from which part of the code you see this. Neumann  conditions are realized by modifying the matrix. You might want to check a finite difference book for details. This function from stam emulates a modified matrix for a gauss seidel iteration.  I fear you have to do the transition from text / equations to code. You can't just copy and paste everything from different sources..." CreationDate="2017-05-27T13:15:16.483" UserId="2695" ContentLicense="CC BY-SA 3.0" />
  <row Id="7626" PostId="5156" Score="0" Text="Can it be assumed that the orientation of all instances of a character are identical? That is, that there is zero rotation?" CreationDate="2017-05-27T13:32:50.193" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7627" PostId="5156" Score="0" Text="Are you able to share the raw data for people to experiment?" CreationDate="2017-05-27T13:33:37.863" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7628" PostId="5156" Score="1" Text="@trichoplax There is a very slight consistent rotation clockwise as a scanning artifact. I've accounted for it when segmenting. There is no variation of rotation of individual characters.&#xA;&#xA;[Initial image](https://www.dropbox.com/s/01ygrkzlhylu612/ACPU-128.jpg?dl=0) [Charset encoding](http://en.wikipedia.org/wiki/GOST_10859) [Segmented](https://www.dropbox.com/s/doc2bzh155mey26/tiles.zip?dl=0)" CreationDate="2017-05-27T16:44:44.147" UserId="6705" ContentLicense="CC BY-SA 3.0" />
  <row Id="7630" PostId="5153" Score="0" Text="Hi Richie, I enjoyed reading your answer here. While ago I posted a question on MIS but didn't get answer. I wonder if I could have your thoughts as well, thanks. https://computergraphics.stackexchange.com/questions/4868/efficiently-sampling-specific-surfaces-using-mis-in-path-tracing" CreationDate="2017-05-27T23:24:40.693" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7631" PostId="5147" Score="0" Text="I understand you, in the code, the boundaries are dirichlet or neuman in the code. But I doubt if I can just use the same boundaries for cg solver you can see the full code here" CreationDate="2017-05-27T23:55:11.180" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7632" PostId="5147" Score="0" Text="https://www.autodeskresearch.com/sites/default/files/TheArtOfFluidAnimationCode.zip&#xA;&#xA;&#xA;Here is the code and you just use the set_bnd for div in the project function to whom I want just to replace the gauss seidel with PCG." CreationDate="2017-05-28T02:42:20.940" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7634" PostId="4789" Score="0" Text="Why do you use the vector A(1, 1, 1) instead of the VUP vector(0,0,1) to work out the first row of the rotation matrix?" CreationDate="2017-05-27T18:09:33.243" UserId="6716" ContentLicense="CC BY-SA 3.0" />
  <row Id="7635" PostId="4789" Score="0" Text="isnt transformation matrix M = R.T , why are you saying to transform it again. I think the question just asked for the transformation matrix" CreationDate="2017-05-27T19:51:36.523" UserId="6718" ContentLicense="CC BY-SA 3.0" />
  <row Id="7636" PostId="4789" Score="0" Text="@link in object(camera) coordinate space, the z-axis from the eye to a point where the eye looks at. yes, it's point A, `VRP - A` equals to the coordinate of z-axis of object space in world space." CreationDate="2017-05-28T08:07:57.280" UserId="6140" ContentLicense="CC BY-SA 3.0" />
  <row Id="7637" PostId="4789" Score="0" Text="@fahadchowdhury The question starter wants a matrix which can convert world coordinate to object coordinate, this matrix consist of two matrices, one is rotation matrix, another is translation matrix, only if the camera is looking at positive z-axis, the rotation matrix would be an identity matrix." CreationDate="2017-05-28T08:22:11.757" UserId="6140" ContentLicense="CC BY-SA 3.0" />
  <row Id="7640" PostId="4342" Score="0" Text="How do I calculate the light direction for use in a Ray Tracer to first calculate the Direct Lighting though? Do I even need to do that? I am little confused here." CreationDate="2017-05-28T12:53:09.150" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7641" PostId="5167" Score="0" Text="Will try this method! Although, would you have any knowledge of how I can do it without approximating it with point lights?" CreationDate="2017-05-28T14:10:01.077" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7642" PostId="4342" Score="0" Text="You don't need to do that, no.  You just shoot random rays in the hemisphere, and some of them will hit the light, some of them won't.  There are techniques to shoot rays at the lights more directly while still accounting for the probability of you having chosen them randomly (to make the image look the same), but it's not required, no." CreationDate="2017-05-28T14:20:07.700" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7643" PostId="4342" Score="0" Text="I'm still really confused (I'm sorry), but wouldn't this only work for Indirect lighting and not Direct Lighting? Could you add some code to your answer to illustrate this clearer? I looked at the source for path tracer in the blog post on GitHub but I couldn't understand what was going on." CreationDate="2017-05-28T14:31:47.553" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7644" PostId="4342" Score="0" Text="Also, does this mean I can remove the direct lighting raytracer component from my Renderer entirely since the path tracer can handle this?" CreationDate="2017-05-28T14:35:09.933" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7645" PostId="4342" Score="0" Text="The light leaving a surface in a specific direction is made up of two components: One is the $L_e$ term which is the emissive light, or how much the object glows.  That is the direct lighting. The other component is the rest of the integration, which is how much light is reflected from other lighting sources (direct and indirect lighting sources), this is indirect lighting.  Those two components are added together as light leaving the surface.  Path tracing as I described finds them both because they are treated the same (they are summed and treated as &quot;lighting&quot; not direct or indirect)." CreationDate="2017-05-28T14:38:01.137" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7646" PostId="4342" Score="0" Text="Yes, you can remove the direct lighting code from your path tracer if you want to.  You will still get accurate results.  However, as I explained from the first couple sentences of the answer, it will take longer to make a good looking image if you do so.  It takes more complex math to have &quot;direct lighting&quot; in your path tracer, and have the results come out correctly, but it does speed things up to have that. You get better results in fewer samples.  But yeah, &quot;direct lighting&quot; code is totally optional, and path tracing works fine without it." CreationDate="2017-05-28T14:40:26.307" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7647" PostId="5167" Score="0" Text="Unfortunately, I do not know any method for arbitrary surfaces which works with simple raytracing because it's hard to tell if an point can see the plane or not, but depending on your case you might want to try a Monte Carlo method such as Distribution Ray Tracing." CreationDate="2017-05-28T17:00:09.253" UserId="6682" ContentLicense="CC BY-SA 3.0" />
  <row Id="7648" PostId="5168" Score="2" Text="CG has already passed the uncanny valley, as can be shown by the rising ubiquity of completely digital doubles in Hollywood blockbusters of recent years. I would imagine giving critique of a particular artwork is somewhat out of the scope of this website, but for me it's mostly the lack of detail on the clothes and the skin shader. The hair is also a bit off but hard to say exactly how." CreationDate="2017-05-28T18:10:52.757" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="7649" PostId="5168" Score="3" Text="Sometimes people geet hooked on knowing that it is cg. They then say they can spot CG. However when you then give them a mixture of images which are real and which are CG they suddenly can not spot the difference." CreationDate="2017-05-28T20:53:00.597" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7650" PostId="5168" Score="0" Text="I would say for things to improve, the tie and shirt don't quite look right but it would take a lot of observation and the intent to find rendering issues to observe those things. In terms of the interaction of light, it has been nailed but what will need to happen next (for CG in general) will be an accurate simulation of smoke and other fluids to perfect CGI in most scenarios. This also brings up the question for me of where rendering goes next? Other than speeding up renders what can be done to improve? Is there a dead end soon approaching?" CreationDate="2017-05-28T21:03:42.723" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7651" PostId="5140" Score="0" Text="Does your algorithm produce a stream of triangles that you need to process immediately, or do you have the full model at hand before sending it to the visualization software? In the second case, could some processing allow you to create an indexed model (with averaged normals on each vertex)?" CreationDate="2017-05-29T09:07:09.723" UserId="6725" ContentLicense="CC BY-SA 3.0" />
  <row Id="7653" PostId="5168" Score="0" Text="See also [this very similar question](https://computergraphics.stackexchange.com/questions/2556/what-physical-properties-are-lacking-to-keep-this-3d-scene-from-looking-like-a). I'm not closing this as a duplicate because I'm treating it as a review of the image for realism, rather than a general question about the state of the art. I've edited the title to reflect this." CreationDate="2017-05-29T12:28:13.727" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7654" PostId="5168" Score="0" Text="@trichoplax much appreciated" CreationDate="2017-05-29T13:44:22.840" UserId="6722" ContentLicense="CC BY-SA 3.0" />
  <row Id="7655" PostId="5140" Score="0" Text="The problem here is, that is a custom made visualisation software, which gets an input stream in a custom format from a server software, which could work on multiple computing nodes (made for high performance computing). This format doesn't support indices and the model have to be outputed as a triangle stream. But for processing it's possible to use indices, with the restriction that the whole mesh may be distributed across the server nodes." CreationDate="2017-05-29T13:49:40.380" UserId="6682" ContentLicense="CC BY-SA 3.0" />
  <row Id="7656" PostId="5167" Score="0" Text="Maybe you could also try to sample some randomly distributed points on the plane of the area light and then check it against your surface point." CreationDate="2017-05-29T13:52:32.210" UserId="6682" ContentLicense="CC BY-SA 3.0" />
  <row Id="7658" PostId="4536" Score="0" Text="So in terms of innovation (excluding render times and performance) there isn't much left to do for offline rendering for light transport, does this mean offline rendering has hit a dead end? Would you happen to know of any of the challenges people who work on path tracers have? Light physics is what really interests me and the fact that the focus is moving towards animation really sucks :(..." CreationDate="2017-05-29T17:06:34.440" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7660" PostId="5144" Score="0" Text="https://www.autodeskresearch.com/sites/default/files/TheArtOfFluidAnimationCode.zip Here is the code and you just use the set_bnd for div in the project function to whom I want just to replace the gauss seidel with PCG." CreationDate="2017-05-27T23:57:41.090" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7662" PostId="5156" Score="0" Text="Interesting indeed. just an idea- since you make own program, how is about setting different threshold by averaging, depending on which pixel row it is - say the top pixel row contributes pixels which are &gt;95% black, second slightly more, say, all pixels which are &gt;94% black ... and so on." CreationDate="2017-05-29T21:58:10.173" UserId="6160" ContentLicense="CC BY-SA 3.0" />
  <row Id="7663" PostId="5156" Score="0" Text="@Mikhail That's a good idea, but it definitely requires tuning." CreationDate="2017-05-30T03:02:40.507" UserId="6705" ContentLicense="CC BY-SA 3.0" />
  <row Id="7664" PostId="5168" Score="0" Text="The dead eyes, for me." CreationDate="2017-05-30T05:01:19.737" UserId="6679" ContentLicense="CC BY-SA 3.0" />
  <row Id="7666" PostId="5168" Score="0" Text="Looks to clean. Doesn't suffer the artefacts which many cameras/lenses create. E.g. grain. Lighting also looks too perfectly distributed." CreationDate="2017-05-30T11:38:27.237" UserId="3331" ContentLicense="CC BY-SA 3.0" />
  <row Id="7667" PostId="4536" Score="1" Text="Well actually I think there is more to figure out about basic lighting.  Give this a read (Disney's PBR paper from 2012) to see some open problems with basic things: https://disney-animation.s3.amazonaws.com/library/s2012_pbs_disney_brdf_notes_v2.pdf" CreationDate="2017-05-30T13:36:12.027" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7668" PostId="5147" Score="0" Text="Where are you? I need help" CreationDate="2017-05-30T17:47:36.870" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7669" PostId="4536" Score="0" Text="The book &quot;physically based rendering from theory to practice&quot; also describes a lot of open problems, like better ways to describe a color spectrum or a brdf." CreationDate="2017-05-30T18:17:09.297" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7670" PostId="4536" Score="0" Text="That actually is true, a lot of the exercises in that book point out many unsolved challenges. While they may not be the low hanging fruit I was expecting it does show a lot of the problems left with path tracers which go very deep. Is there a book like PBRT for simulations? Simulations are cool but for a novice, it's nearly impossible to learn at the moment due to the lack of resources that go step-by-step." CreationDate="2017-05-30T19:14:37.050" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7671" PostId="4536" Score="0" Text="I think it depends a lot on the type of simulation you want to make, but I'm not sure of any intro resources to eg fluid or smoke sim. I'd like to know of some!" CreationDate="2017-05-30T19:16:57.733" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7672" PostId="5181" Score="0" Text="Yes there is a good tutorial called [12 steps to CFD](http://lorenabarba.com/blog/cfd-python-12-steps-to-navier-stokes/)." CreationDate="2017-05-30T19:56:35.823" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7674" PostId="5181" Score="0" Text="This seems like a really broad question. (And I'm not sure if recommendations are allowed on this stack exchange. They're usually frowned upon.) Can you narrow down what you're having trouble understanding with fluid sims?" CreationDate="2017-05-31T05:22:55.350" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7675" PostId="5181" Score="0" Text="See Bridson and Müller-Fischer's notes from their SIGGRAPH 2007 course, *[Fluid Simulation for Computer Animation](http://www.cs.ubc.ca/~rbridson/fluidsimulation/)*. You should also consider picking up [Bridson's book](https://www.crcpress.com/Fluid-Simulation-for-Computer-Graphics-Second-Edition/Bridson/9781482232837) which has more detail." CreationDate="2017-05-31T08:38:52.177" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7677" PostId="5156" Score="0" Text="Maybe it is worth posting an image with _one_ letter (before/after filtering) in native resolution. Then probably you get more chances for real proposals. Higher resolution is better. In general removing smear can become a complex task, if you want a pure programmatical way. And I suppose it would need anyway row-by-row filtering, and to filter all chars it will need vertical aligning the glyphs or finding boundaries for each glyph." CreationDate="2017-05-31T21:14:39.480" UserId="6160" ContentLicense="CC BY-SA 3.0" />
  <row Id="7678" PostId="5156" Score="0" Text="So if your examples is the highest possible resoultion, then I'd say it is too small for post-processing.  And if something, you can always retouche the results with hand. Retouching the whole set can take a pair of evenings, but developing fully automatical software can take weeks." CreationDate="2017-05-31T21:30:00.330" UserId="6160" ContentLicense="CC BY-SA 3.0" />
  <row Id="7679" PostId="5156" Score="0" Text="@MikhailV Retouching is always an option, if everything else fails. Now I'm doing simple averaging after estimating the offset in whole pixels. I was hoping for some kind of a smarter algorithm for offset estimation and/or averaging." CreationDate="2017-05-31T21:38:51.010" UserId="6705" ContentLicense="CC BY-SA 3.0" />
  <row Id="7682" PostId="5184" Score="3" Text="That's kind of a big question to ask. If you're hoping for a big block of code you can just paste in, you're going to be disappointed. Perhaps you could narrow the question down to a particular part you're having trouble with. Would a clearer description of the maths help? Would a pseudocode descrption of the main formulae help?" CreationDate="2017-06-01T09:11:23.853" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7683" PostId="5184" Score="1" Text="Yeah, you're right I should be doing the coding myself. A clearer explanation of the mathematics of what does what (also the symbols which confuse the hell out of me) will help and pseudo code will work just fine. I'm updating the question to be more specific." CreationDate="2017-06-01T12:31:39.520" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7684" PostId="5192" Score="0" Text="Isn't a second texture a more dense sampling with additive blending of samples?" CreationDate="2017-06-01T15:38:16.353" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7685" PostId="5189" Score="0" Text="I have red that part of the article, but I fail to see the connection between that equation and the one I showed above. Perhaps my math skills arn't good enough to work that out." CreationDate="2017-06-01T15:44:30.417" UserId="2736" ContentLicense="CC BY-SA 3.0" />
  <row Id="7686" PostId="5184" Score="0" Text="Is there a beginner friendly book that covers this though? I find this extremely tough to grasp and I honestly need a complete explanation of everything." CreationDate="2017-06-01T17:03:01.447" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7687" PostId="5192" Score="0" Text="No, but there is a similar way to generate blue noise textures to what you describe.  Basically you place a point and then low pass filter (blur), then put a point in the lowest valued pixel and blur again. Rinse and repeat. That's how I've heard it described but I think there must be more to it, to keep the points sharp where you placed them." CreationDate="2017-06-01T17:16:23.317" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7688" PostId="5193" Score="2" Text="Another possible reason for the confusion is that Ambient Occlusion is most of the times rendered out into a separate pass and added in later. Making it seem like it could be a post processing effect." CreationDate="2017-06-01T18:41:24.950" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="7689" PostId="5184" Score="1" Text="There's an index of the symbols on page 2 of the paper. I'm afraid there's no simple way to understand that method. IIRC, there are slower and more accurate methods which are simpler to understand because they actually trace multiple bounces instead of simulating them with a complex formula. You could perhaps start there." CreationDate="2017-06-01T19:29:26.297" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7690" PostId="5184" Score="0" Text="If there are other simpler and accurate techniques I would definitely want to start there :D! Could you link the resources here? Also, does Path Tracing already render subsurface scattering?" CreationDate="2017-06-01T19:31:49.147" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7691" PostId="5193" Score="0" Text="@bram0101 does that apply to Ambient Occlusion that is not using the Screen Space Ambient Occlusion approximation?" CreationDate="2017-06-01T19:48:27.437" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7692" PostId="5186" Score="2" Text="&quot;It seems to just select the longer pair of lines over the shorter pair.&quot; What? All the red lines in the figure are the same length." CreationDate="2017-06-01T21:51:14.903" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7693" PostId="5192" Score="2" Text="The &quot;blue noise texture&quot; is from [this page](http://momentsingraphics.de/?p=127), which also explains the relationship between blue noise sampling and the texture." CreationDate="2017-06-01T21:53:55.977" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7694" PostId="5193" Score="2" Text="@trichoplax Yes, for example with Voxel-Cone Traced AO. In that method, the AO is added as a separate pass" CreationDate="2017-06-01T22:10:59.830" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7695" PostId="5193" Score="1" Text="@trichoplax with offline render engines it is done a lot, especially with VFX or general 3d animation. There could be a few reasons for rendering it out separately. 1. Most shaders and lights do not have a button to turn on ao meaning you have to create a render pass for it. 2. Passes can be faster to render or easier to set up. 3. In VFX or some animation productions, they render out in a bunch of different passes to then add them together in post for more control.  I am talking about ao from offline render engines so it is ray traced ao." CreationDate="2017-06-02T05:46:31.273" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="7696" PostId="5193" Score="0" Text="These comments suggest it is not as simple as I described, and there is a much better answer waiting to be posted..." CreationDate="2017-06-02T09:35:01.070" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7699" PostId="5197" Score="0" Text="If you already have the ray-casting algorithm, why not cast a ray from a vertex along its edge and find whether the first intersection is before the distance to the opposite vertex? Then you can find the intersections on each edge." CreationDate="2017-06-02T14:34:25.393" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7700" PostId="5197" Score="0" Text="That doesn't help you where a finger of the mesh pokes through the polygon, though. Perhaps you're better off starting by classifying the mesh's vertices by which side of the plane of the polygon they're on." CreationDate="2017-06-02T14:47:06.050" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7701" PostId="5199" Score="0" Text="I'm afraid I don't what any of what you just said means. I have almost no background in computer graphics. I'm learning this as I go over the last year or so. I'm writing this code from scratch in C++ for a computational physics application. For reasons I won't go into, this has to run in an environment with almost nothing available. Imagine that this has to be done on a non-network-connected machine in 1997. So straight up from scratch with no special libraries." CreationDate="2017-06-02T15:06:52.907" UserId="6762" ContentLicense="CC BY-SA 3.0" />
  <row Id="7702" PostId="5197" Score="0" Text="@DanHulme Once I've determined which mesh vertices are on which side of the plane, what would I do with that information?" CreationDate="2017-06-02T15:11:36.973" UserId="6762" ContentLicense="CC BY-SA 3.0" />
  <row Id="7703" PostId="5197" Score="0" Text="Use it to find mesh edges that cross the plane and intersect *them* with the plane to find all the crossing points. You can use the crossing points to make a new polygon which is the section of the plane and the polyhedron. (You need to use the topology of the polyhedron to know what order to connect the edges together, because this new polygon is non-convex.) Now just intersect the two polygons in the usual way. I'm making this up as I go along but it sounds plausible." CreationDate="2017-06-02T17:06:40.873" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7704" PostId="5199" Score="0" Text="It comes down to the idea you suggested about projecting the mesh into the plane of the polygon. The near clipping plane part cuts off the parts of the mesh &quot;in front of&quot; the plane. Only drawing the inside faces means you'll only see the inside of the mesh where this cut has made a hole, giving you an image of the inside of the polyhedron. Finally, the stencil buffer intersects that image with your polygon. If you had a 3D library available it would be easy to write (because you don't need to do all the maths yourself) and fast (run on the GPU)." CreationDate="2017-06-02T17:10:44.303" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7705" PostId="5199" Score="0" Text="What is &quot;the near clipping plane part&quot;? What do you mean when you say &quot;inside faces&quot;? Does that mean triangles which partially overlap the polygon in projection? What is the &quot;stencil buffer&quot;? As to using a 3D library... no can do. Some networks are VERY selective about what code can and can't be stored on it. I don't mind doing the math of implementing an algorithm. I've had a lot of practice at that. I just need to understand the geometry of the algorithm." CreationDate="2017-06-02T17:27:06.017" UserId="6762" ContentLicense="CC BY-SA 3.0" />
  <row Id="7706" PostId="5197" Score="0" Text="@DanHulme So it seems to me that there are two kinds of crossing points: Where an edge of the polygon crosses a triangle, and where an edge of a triangle crosses a polygon. Both kinds can be found by some kind of exhaustive search. I guess the next step would be to throw away vertices of the original polygon that were known to be outside the polyhedron. How do I use the topology of the polyhedron to order the resulting bunch of points?" CreationDate="2017-06-02T17:31:45.100" UserId="6762" ContentLicense="CC BY-SA 3.0" />
  <row Id="7707" PostId="5185" Score="1" Text="Something that might help you out is that &quot;specular&quot; means &quot;mirror like&quot;.  So, when talking about specular and diffuse reflections, specular means mirror like reflections, what people normally call reflection.  Diffuse is just &quot;things lighting up&quot;.  They are reflecting light too, but they are reflecting from many different directions, so the result isn't anything that you can recognize an image in.  There is a scale between specular and diffuse reflection in real objects, and this is part of what the &quot;roughness&quot; parameter of PBR controls." CreationDate="2017-06-02T22:06:48.067" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7708" PostId="5197" Score="0" Text="If I knew that off the top of my head I'd have posted an answer instead of drip-feeding suggestions one at a time!" CreationDate="2017-06-03T10:31:53.727" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7709" PostId="5200" Score="3" Text="What output are you looking for? You say you're going *from* some proportions of subtractive primaries, but what colour space are you trying to convert it *to*? And if the proportions have to add to 100%, how do you express the lightness of the colour?" CreationDate="2017-06-03T10:44:03.387" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7710" PostId="5198" Score="0" Text="My issue was that my normals were in world space. I thought otherwise, but realized when you posted this. Also, You don't necessarily need 2.0 - 1.0 in the normal retrieval section if your normals are signed." CreationDate="2017-06-03T23:15:01.397" UserId="5026" ContentLicense="CC BY-SA 3.0" />
  <row Id="7711" PostId="5202" Score="0" Text="Really thanks for this detailed answer. Can you read Bridson paper who has a different way of boundaries from Stam's one and I can't relate them. Another thing, can we just evaluate the values of the velocity and pressure at the boundaries then put the values directly in the matrix?" CreationDate="2017-06-04T21:12:34.727" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7712" PostId="5188" Score="1" Text="It should be noted that this is true for all versions of GLSL, all the way back to 1.10 and even the ES versions." CreationDate="2017-06-04T23:24:27.413" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7713" PostId="5202" Score="0" Text="Why can't I use the same technique in the PCG Method? I mean the set_bnd" CreationDate="2017-06-05T01:30:43.107" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7714" PostId="5202" Score="0" Text="I edited my answer w.r.t. the equivalence between stam and bridson" CreationDate="2017-06-05T01:48:09.417" UserId="2695" ContentLicense="CC BY-SA 3.0" />
  <row Id="7715" PostId="5202" Score="0" Text="... If you have a more detailed question, please add page numbers / equation numbers, so I can find the right spots." CreationDate="2017-06-05T01:49:28.723" UserId="2695" ContentLicense="CC BY-SA 3.0" />
  <row Id="7716" PostId="5202" Score="0" Text="Did you try the set bnd method for pcg? I think it is unclear to which variables it needs to be applied to. You can give it a try and report about your results." CreationDate="2017-06-05T01:52:59.430" UserId="2695" ContentLicense="CC BY-SA 3.0" />
  <row Id="7717" PostId="5202" Score="0" Text="Firstly, thank you for your detailed answer. Stam applies a no slip boundary condition for the velocity and Bridson applies slip boundary condition. I don't have a problem with the velocity right now. I am sorry I didn't put the code because i thought it's in the paper. The problem is at the pressure boundary condition which is indeed a neuman (the normal derivative of p is just zero) and it's applied in the projection step but stam applies it in a different way from Bridson. If you see the code of stam (I will modify the question to add it),  TBC in the next comment" CreationDate="2017-06-05T01:58:31.000" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7718" PostId="5202" Score="0" Text="If you see Stam code, you will see that he applied the boundaries as mentioned here in GPU GEMS Equation 17 and 18.    http://developer.download.nvidia.com/books/HTML/gpugems/gpugems_ch38.html.    But Bridson derives a formula to calculate the pressure of the boundary cells dependant on the velocity which is in the SIGGRAPH paper i mentioned above in chapter 4 equation number 4.10" CreationDate="2017-06-05T01:59:46.137" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7719" PostId="5202" Score="0" Text="Actually stam doesn't imply the boundaries in the matrix of the pcg like the math. He just finds the values of the pressure and the velocity that applies the boundaries and put them in the matrix. Actually I see not the code of the no slip of Stam which I will try to understand (the set_bnd and how it works according to the paper of cpu gems) but I didn't find a way for the free slip or no stick condition in which the fluid moves freely in the tangential direction. Thanks for your answers again and again. I would like to chat with you at any time." CreationDate="2017-06-05T02:05:46.487" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7720" PostId="5202" Score="0" Text="Sorry, Bridson changes the equations so that they fit the boundary conditions for the pressure and the velocity. Now i understand this. But how can we implement the free slip boundaries?" CreationDate="2017-06-05T02:19:41.760" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7721" PostId="5202" Score="0" Text="The gpu gems code is implementing the boundary conditions the same way as stam but again &quot;only&quot; for Jacobi iterations,  not for a cg solver." CreationDate="2017-06-05T10:25:14.187" UserId="2695" ContentLicense="CC BY-SA 3.0" />
  <row Id="7722" PostId="5202" Score="0" Text="I edited my answer" CreationDate="2017-06-05T10:37:03.210" UserId="2695" ContentLicense="CC BY-SA 3.0" />
  <row Id="7723" PostId="5108" Score="0" Text="If the points are noisy and some are not in a plane, could you clarify how much deviation from a plane due to noise is permitted, before a point is no longer regarded as being in that plane? For an approximate solution you would need to explain what your priorities are." CreationDate="2017-06-05T12:08:48.423" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7724" PostId="5202" Score="0" Text="Now I understand how stam realizes the velocity boundary condition on an allocation grid. Can we use his implementation in a staggered grid? I still don't understand the relation between forcing the normal derivative to be zero in the way stam is doing and the equation that Bridson derives of the pressure at boundaries. Bridson says that when we subtract the pressure gradient with the velocity of the slip boundary conditions it will just enforce the pressure boundary but he didn't explain why. One time I asked someone and he told me that it's prescribed in the momentum and I didn't understand" CreationDate="2017-06-05T14:00:47.980" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7725" PostId="5202" Score="0" Text="Also there is no code implementing the slip boundaries of velocity like the code of stam is implementing the no slip. I added this in an answer by mistake lol" CreationDate="2017-06-05T14:01:50.823" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7727" PostId="5202" Score="0" Text="Another thing please, Stam says that the normal derivative of pressure should be zero in stick condition but Bridson says a difference thing in equ in the secondary edition book you told me about. Is the normal derivative of pressure different in the two cases? I would like to add you on Facebook or something" CreationDate="2017-06-05T15:05:28.133" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7728" PostId="5200" Score="0" Text="It can not work. Because that would mean tye primaries are not pure. Although mathematically you could have some fudge factor but no ink can actually work like this." CreationDate="2017-06-05T15:14:11.893" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7729" PostId="5200" Score="0" Text="What do you mean it can't work? If you take magenta paint, cyan paint and yellow paint, you can mix the colour wheel. This is exactly what I'm talking about, you take some proportion of primary colours and mix them together.&#xA;&#xA;one part yellow to 1 part magenta and you get red and so on" CreationDate="2017-06-05T15:40:37.397" UserId="6770" ContentLicense="CC BY-SA 3.0" />
  <row Id="7730" PostId="5207" Score="0" Text="I have edited my question, thanks for your input." CreationDate="2017-06-05T17:03:40.647" UserId="6770" ContentLicense="CC BY-SA 3.0" />
  <row Id="7731" PostId="5200" Score="0" Text="It wont work because to make red you need 100% cyan + 100% yellow. Otherwise you get a red otherwise your formula no longer adds up on any other concentrations. As that would mean you would need a darker mixture for red than from black. So best not think percentages but parts of color mixture." CreationDate="2017-06-05T17:18:09.107" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7732" PostId="5200" Score="0" Text="That would be true if it was represented in cmyk, but I'm not in the colour model cmyk, I need to take some ratio of primary paints and return some rgb colour code. &#xA;&#xA;The cmyk colour model as an intermediary seems unwise given that it is 0-100% for each component c m y and k and goes from white to pure , and i have no white.&#xA;&#xA;What I want is a colour wheel that covers hues and chroma. If you want red you mix an equal proportion of magenta and yellow to get secondary colour red. if you mix blue and red you're mixing all 3 cmy in some proportion, that's the definition of a primary." CreationDate="2017-06-05T17:22:46.870" UserId="6770" ContentLicense="CC BY-SA 3.0" />
  <row Id="7733" PostId="5195" Score="0" Text="okay, that's a bit more clear but it still doesn't explain what do I do with my &quot;vec3 reflectionColour&quot; that I get from re-launching a ray in raytracing/marching or using SSR/cubemap. It's like the equations only taking into account a pointlight/spotlight but no idea for area lights or reflections. It also seems that the &quot;specular&quot; is only related to the &quot;white dot&quot; instead of reflection because of that which doesn't make sense =/" CreationDate="2017-06-05T23:57:17.883" UserId="5506" ContentLicense="CC BY-SA 3.0" />
  <row Id="7734" PostId="5195" Score="0" Text="@newin In a PBR model, specular and reflection are different names for the same thing. It's a specular highlight when the ray hits a light, a reflection when it hits another object (or an environment). As for your reflectionColour, it depends how the ray is generated. Are you sampling direct lighting? Area lights? Integrating a BRDF against and environment map? Try reading [Progressive Path Tracing with Explicit Light Sampling](https://computergraphics.stackexchange.com/questions/5152), there's a long answer which might clear a few things up about the process." CreationDate="2017-06-06T01:04:41.147" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7735" PostId="5202" Score="0" Text="I still don't understand everything. Why can't I use set_bnd in staggered grids? What's the tangential component and how can I code some function like set_bnd to enforce the slip boundary? (yes or no) Does the velocity boundary condition whether it's slip or no slip affects the normal derivative of pressure? I mean that if slip, the normal derivative will be like Bridson stated and in no slip, it will be just zero?" CreationDate="2017-06-06T02:48:18.047" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7736" PostId="5202" Score="0" Text="Where can I read more about this? I don't find any paper say something clear about what is a boundary condition!!!" CreationDate="2017-06-06T02:51:40.587" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7737" PostId="5202" Score="0" Text="Why stam uses the normal derivative equals zero and Bridson uses the normal derivative of pressure to be different in page 83 in his book" CreationDate="2017-06-06T02:54:52.727" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7738" PostId="5202" Score="0" Text="I really want to contact you in private even Skype chat or anything you want please. I want to hit a wall with my confused head." CreationDate="2017-06-06T02:57:49.383" UserId="4978" ContentLicense="CC BY-SA 3.0" />
  <row Id="7739" PostId="5185" Score="0" Text="You can try to read the original Cook-Torrance paper: http://inst.cs.berkeley.edu/~cs294-13/fa09/lectures/cookpaper.pdf - be sure to check the references listed in the end too, when possible (most are available online). It will take some time, but it is a sure way to understand &quot;in depth&quot; what is happening." CreationDate="2017-06-06T06:00:40.910" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="7741" PostId="5195" Score="0" Text="So if I understand correctly that &quot;vec3 reflectionColour&quot; should be treated as a light and if I want to make mesh emitting light I should just put a value &gt; 1 in the albedo and that's it ?&#xA;(I'm trying to integrate PBR in to different scenario the first is a raymarcher where I want meshes emitting light, the other is a game engine that use SSR and probably will use cubemaps)" CreationDate="2017-06-06T12:16:28.103" UserId="5506" ContentLicense="CC BY-SA 3.0" />
  <row Id="7742" PostId="5210" Score="1" Text="Since this is a VBA-specific question, and there aren't a lot of VBA-using graphics researchers, I doubt you'll get good answers here; you might do better on [so]." CreationDate="2017-06-06T13:47:03.337" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7743" PostId="5210" Score="0" Text="Thanks! I was able to find what I needed with some digging anyway, but I'll be sure to post all future followup questions there." CreationDate="2017-06-06T13:51:20.090" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="7744" PostId="5195" Score="0" Text="@newin emitted light is just added on top of everything else. It's unrelated to BRDFs. If your mesh does nothing but emit light, you don't even need to trace any further rays from it. A large albedo will amplify light (physically wrong), not create it." CreationDate="2017-06-06T14:57:19.783" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="7745" PostId="5212" Score="0" Text="You can slerp a quaternion by repeatedly nlerping to 0.5 and binary searching. So if you can expand out the math of conversion to quaternion, the nlerp and the conversion back, then it should be possible" CreationDate="2017-06-07T13:41:56.787" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7746" PostId="5210" Score="0" Text="If you feel the answer you discovered for yourself might be relevant to others working with computer graphics in future, you could add a self answer to this question." CreationDate="2017-06-07T14:52:21.797" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7747" PostId="5211" Score="1" Text="If your Constructive Solid Geometry method allows intersection as well as union, then you can use this to find the internal area, and subtract that from the total to give the external area. Would that make a useful answer?" CreationDate="2017-06-07T15:01:15.320" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7748" PostId="5211" Score="1" Text="This would depend on all the initial meshes having no internal triangles. If you can guarantee this then the method will work. For example, a hollow ball constructed from two concentric spheres would not give correct results when intersected with another sphere. However, if the construction is using only unions, then intersections can be used to subtract the area of the internal triangles." CreationDate="2017-06-07T15:08:42.777" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7749" PostId="5212" Score="0" Text="More generally, if you can lerp from the identity to any rotation $r$, i.e. find the rotation $r^t$ with the same axis and $t$ times the angle, then you can lerp from $a$ to $b$ via $(ba^{-1})^ta$." CreationDate="2017-06-07T15:31:39.147" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7750" PostId="5210" Score="0" Text="Will do. Once I get off this train! :-)" CreationDate="2017-06-07T17:17:12.593" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="7751" PostId="5211" Score="0" Text="I think that this could be the answer, but for example, if one of the meshes is inside the other the one that's inside should be completely ignored. Would that be the case? I tried to use intersection as stated and found the same results." CreationDate="2017-06-07T19:50:23.823" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7752" PostId="5213" Score="4" Text="With or without the cooperation of the app getting injected into? :) If you have control of both apps, you could use shared memory to efficiently communicate mesh data between the processes. Otherwise, you're looking at something like shimming the graphics API." CreationDate="2017-06-07T23:01:54.360" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7753" PostId="5213" Score="0" Text="Perhaps an ironic question as, no doubt, some effort has to be put into the GPU drivers to prevent parallel applications interfering on the shared resource." CreationDate="2017-06-08T08:24:30.840" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7754" PostId="5211" Score="1" Text="If your intersection method returns the surface that encloses the volume enclosed by both the first mesh and the second mesh, then the intersection of the two meshes you describe will simply be the entire inner mesh, which is precisely the triangles that you wish to exclude. So it sounds like either your intersection method works differently, or there is a separate problem causing overcounting. Could you include your code so we can analyse it?" CreationDate="2017-06-08T09:15:06.203" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7755" PostId="5213" Score="1" Text="Yes its called inter process communication. Not in any way specific to graphics programming. Do you mean directly on the GPU?" CreationDate="2017-06-08T11:07:00.347" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7756" PostId="5211" Score="0" Text="Sure! Currently I'm using [Net3dBool](https://github.com/Arakis/Net3dBool) to perform the CSG operations. The code that performs the intersection is on the file [BooleanModeller.cs](https://github.com/Arakis/Net3dBool/blob/master/src/Net3dBool/BooleanModeller.cs), on the **getIntersection** method, wich calls the **composeSolid** method. Thanks!" CreationDate="2017-06-08T12:13:17.157" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7757" PostId="1993" Score="0" Text="clinfo did not work, but sudo clinfo did for me." CreationDate="2017-06-08T14:20:30.920" UserId="6798" ContentLicense="CC BY-SA 3.0" />
  <row Id="7758" PostId="5217" Score="1" Text="I think &quot;window of 20x20 pixels&quot; applies to &quot;energy&quot; rather than to &quot;image Laplacian&quot;, i.e. you compute the Laplacian as usual, the compute its total energy (i.e. sum of squared values) on a window of 20x20 pixels. But you should add a link or reference to the research paper to be sure." CreationDate="2017-06-08T15:36:52.227" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7759" PostId="5213" Score="1" Text="If you know the graphics API the other application is using, you can use the concept of dll hooking to inject code. For example, you could hook Present() and use the context to draw your triangles before calling the real Present(). For example: https://www.microsoft.com/en-us/research/project/detours/ or http://easyhook.github.io/ Hooking is how applications like OBS/Fraps do frame overlays and captures." CreationDate="2017-06-08T17:13:23.453" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7760" PostId="5214" Score="1" Text="Are you looking to estimate subsurface scattering in your path tracer, or evaluate it exactly using full-volumetric scattering?" CreationDate="2017-06-08T17:17:30.390" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7761" PostId="5215" Score="1" Text="I thought [you said](https://stackoverflow.com/q/44431751/734069), &quot;I will avoid asking more than one question once time later.&quot; That rule doesn't only apply to SO; it's all of our sites. Also, all of the other questions asked about that question still apply." CreationDate="2017-06-08T17:26:16.717" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7762" PostId="5214" Score="0" Text="I read that simulating full-volumetric scattering is really slow. If it is not that slow I would like to implement it. Although diffusion approximations work faster, they assume lots of things or they need ugly preprocessing steps. Have you done it before? Any suggestion would be great!" CreationDate="2017-06-08T19:22:55.860" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7763" PostId="5214" Score="2" Text="It can be slow, depending on the implementation. I have implemented brute-force, isotropic scattering media in my hobby path tracer. Example image: http://imgur.com/a/oy9F8. (The FPS / total frame times are in the upper left corner). Using a backscattering phase function like Henyey-Greenstein might make it converge faster for more opaque materials, but I'm not sure since I haven't tried it. If nothing else, implementing full volumetric scattering is a great way to learn what the approximations are trying to approximate." CreationDate="2017-06-08T19:57:59.250" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7764" PostId="5214" Score="0" Text="I'll try to do a write up later tonight when I get home. In the mean time, here's the link to my path tracer. https://github.com/RichieSams/lantern It's not production ready at all. Rather, it's just for helping me learn the math behind everything. That said, it's open source under Apache 2.0, so feel free to copy and learn from it yourself." CreationDate="2017-06-08T19:59:38.570" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7765" PostId="5214" Score="1" Text="Lastly, here are two questions that I asked here about this topic: https://computergraphics.stackexchange.com/questions/2482/choosing-reflection-or-refraction-in-path-tracing https://computergraphics.stackexchange.com/questions/2563/full-monte-carlo-volumetric-scattering" CreationDate="2017-06-08T20:01:22.773" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7766" PostId="5215" Score="0" Text="Actually, this posting is before that comment I replied.@NicolBolas" CreationDate="2017-06-09T02:56:03.560" UserId="6660" ContentLicense="CC BY-SA 3.0" />
  <row Id="7767" PostId="5214" Score="0" Text="Thank you! I implemented perfect refractive and reflective surfaces already. By the way, the answers to both questions are enlightening for me but I would prefer to see something you wrote up." CreationDate="2017-06-09T08:08:28.930" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7768" PostId="5212" Score="0" Text="Thank you for your valuable comments, Ratchet Freak and Rahul! I will take your advice and answer this question." CreationDate="2017-06-09T09:16:09.300" UserId="6791" ContentLicense="CC BY-SA 3.0" />
  <row Id="7770" PostId="5217" Score="0" Text="That would make sense, maybe this is it. I can't tell. But that's a good idea and I will try to implement it to see.&#xA;&#xA;You're right for the link, I will add it in the post." CreationDate="2017-06-09T09:53:16.497" UserId="6662" ContentLicense="CC BY-SA 3.0" />
  <row Id="7772" PostId="5192" Score="0" Text="Yeah that's where I got the image.  It doesn't give the information I'm looking for.  For instance, if you DFT the first data set, the frequency spectrum should look much like the DFT of the second, but how would you even DFT the first one?  How are these two things &quot;duals&quot; of each other in frequency space?  And can you take concepts from each and apply them to the other?" CreationDate="2017-06-09T20:34:20.627" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7773" PostId="5222" Score="0" Text="What if there's rotation around a different axis than the normal?" CreationDate="2017-06-10T04:54:02.550" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7774" PostId="5222" Score="0" Text="The normal changes then." CreationDate="2017-06-10T16:11:26.143" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7775" PostId="5224" Score="0" Text="Very interesting. Now one question that I have is how does the transformation matrices look like (both for translation and rotation)? A matrix that if applied to all the origin points will give me the new object." CreationDate="2017-06-10T16:18:04.190" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7776" PostId="5224" Score="1" Text="That could be the beginning of a new question... We also have many questions tagged [transformations](https://computergraphics.stackexchange.com/questions/tagged/transformations)" CreationDate="2017-06-11T00:17:59.123" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7777" PostId="5220" Score="0" Text="Subsurface scattering won't necessarily require ray marching if it is in a homogeneous medium. Are you specifically looking to model inhomogeneous media (fire and smoke) rather than homogeneous media (milk or glass)?" CreationDate="2017-06-11T00:25:44.353" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7778" PostId="5226" Score="0" Text="You go buy a colorimeter from the store (or lend one... from the library), make profile and do a profile to profile conversion. Note this is NOT a linear transform. Also your srgb mode needs to be calibrated since color is only accurate if you have calibrated the monitor in place taking into account surrounding color conditions so factory cslibration is allways off." CreationDate="2017-06-11T06:49:50.697" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7779" PostId="5226" Score="0" Text="@joojaa That's not needed for what the question asks, which is merely converting values from one color space to another using a formula. Whether the color appears correctly to the human eye on a particular monitor is not the question." CreationDate="2017-06-11T08:15:51.373" UserId="6814" ContentLicense="CC BY-SA 3.0" />
  <row Id="7780" PostId="5226" Score="0" Text="No, it wont work." CreationDate="2017-06-11T09:01:11.767" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7781" PostId="5226" Score="0" Text="Why not? It may not be a linear transform, but whatever the formula, we should be able to apply it to an input value to get an output." CreationDate="2017-06-11T09:14:48.137" UserId="6814" ContentLicense="CC BY-SA 3.0" />
  <row Id="7782" PostId="5226" Score="0" Text="Because the profile is the formula and only way to obtain one is to measure that individual panel! But its not a big deal colorimeters meantbfor profile buidlding are cheap." CreationDate="2017-06-11T09:53:10.753" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7783" PostId="5214" Score="0" Text="@RichieSams I read physically based rendering and got the intuition. There is only one part that confuses me. We cannot sample the light sources directly due to refraction at the boundaries, right? If so, do we have to wait until the ray we are tracing exit the object because otherwise there would be lots of black pixels on the image?" CreationDate="2017-06-11T13:40:01.567" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7784" PostId="5214" Score="0" Text="Sorry, I got busy this week. I'm writing up the answer now. But yes, you are correct. Light sampling is almost impossible from within media with traditional path tracing. Photon mapping works quite well in this area, though. Vertex Connection and Merging combines the best of both worlds: http://www.smallvcm.com/" CreationDate="2017-06-11T15:24:57.320" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7786" PostId="5226" Score="0" Text="Again, I didn't ask for anything related to an individual panel, but about the color space itself in an ideal sense. I'm not interested in buying hardware. Please answer the question originally asked." CreationDate="2017-06-12T02:54:06.823" UserId="6814" ContentLicense="CC BY-SA 3.0" />
  <row Id="7788" PostId="5229" Score="0" Text="Thank you for this amazing answer! The problem is that no matter what I try for the coefficients, my outputs look like a glass. Does this mean I am doing scattering part wrong? Here is the sequence of outputs: http://imgur.com/a/gg2u1&#xA;I only changed the scattering coefficient while keeping absorption the same" CreationDate="2017-06-12T07:04:07.920" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7790" PostId="5229" Score="1" Text="Are you sure that you're not accidentally setting the coefficient equal to the scattering distance? It should be scatteringCoefficient = (1 / scatteringDistance). Also absorptionCoefficient = -log(absorptionColor) / absorptionAtDistance. In the end, it doesn't really matter how you come up with the coefficient. That said, a lower coefficient will mean less absorption / scattering. A higher coefficient will mean less. For artists, it's a bit easier to think of it in terms of distances, so we back-calculate the coefficients from distances." CreationDate="2017-06-12T13:25:28.770" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7791" PostId="5223" Score="0" Text="If I remember correctly, MLT usually starts with a frame from general path tracing (one way, or bi-directional). They use this image to find places with high luminance, and explore more in that area." CreationDate="2017-06-12T13:32:29.433" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7792" PostId="5224" Score="1" Text="@trichoplax ok. My new question is posted here: https://computergraphics.stackexchange.com/questions/5230" CreationDate="2017-06-12T19:00:53.457" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7793" PostId="5223" Score="0" Text="Oh? I thought it starts randomly, but the probability of moving to a new location favors locations that are brighter. So I do multiple rounds from multiple initial locations to avoid the sampling getting stuck in bright areas.&#xA;&#xA;Also, I'm a student in this so I don't know a lot, but I just learned while researching about this that path tracing is not the same as ray tracing. Does that have anything to do with my problem above? Do you know if I can even do MLT using ray tracing?" CreationDate="2017-06-12T20:18:17.980" UserId="6811" ContentLicense="CC BY-SA 3.0" />
  <row Id="7796" PostId="4994" Score="0" Text="For GGX how would I calculate/sample `wi`? I understand how to sample the spherical coordinates angle θ but for the actual direction vector how is that done?" CreationDate="2017-06-13T14:59:53.260" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="7798" PostId="5237" Score="1" Text="the [video](https://www.youtube.com/watch?v=jdMSEJwbCCY) linked to the wiki page looks really interesting" CreationDate="2017-06-13T18:35:14.433" UserId="6829" ContentLicense="CC BY-SA 3.0" />
  <row Id="7799" PostId="5234" Score="0" Text="Thank you so much for sharing your knowledge. Please read my clarification edit. Thanks again." CreationDate="2017-06-13T19:43:24.940" UserId="6824" ContentLicense="CC BY-SA 3.0" />
  <row Id="7800" PostId="5232" Score="0" Text="You can actually try it and see if it results in a expected result or not. Note that matrix multiplication is only valid for the column vector comig after the matrix and row coming before due to the way matrix multiplication is defined. But you can allways transpose the vector resulting in the same thing (if the matrix chain is computed first)" CreationDate="2017-06-13T19:47:52.683" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7801" PostId="5240" Score="0" Text="is it possible to generate a height map using 3d software such as maya or blender rather than trying to create one manually using python? and does the generated map store the 3d coordinates in each channel like you have described?" CreationDate="2017-06-14T01:59:17.187" UserId="6834" ContentLicense="CC BY-SA 3.0" />
  <row Id="7802" PostId="5240" Score="1" Text="I haven't done it that way before, but I believe you can. (I'm usually generating them in code.) There is a [Blender Stack Exchange](https://blender.stackexchange.com) where you could ask about that. The [Video Production Stack Exchange](https://video.stackexchange.com) may have info on Maya or other apps. In my experience height maps are usually just grayscale images where brightness represents height rather than putting all 3 coordinates into the r, g, and b channels, so I'd expect most apps to output that way." CreationDate="2017-06-14T02:33:49.793" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7803" PostId="5240" Score="0" Text="@MarkMarkrowaveCharlton Yes all you have to do is choose a format that allows writing the zbuffer and check ot under camera settings. If really need this in a rgb image you can just write the distance to the pixel value using a node (sampler info in maya)" CreationDate="2017-06-14T05:36:24.007" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7804" PostId="5237" Score="2" Text="Yes lenses that do this are called pericentric." CreationDate="2017-06-14T05:42:01.220" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7805" PostId="5232" Score="0" Text="Also note that for a pure rotation matrix the inverse is a transpose." CreationDate="2017-06-14T05:56:31.980" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7806" PostId="5232" Score="0" Text="Rather than the 3x3 matrix, I think it'd be more instructive to use a 4x4 so as to include a translation part. That way you should get a clearer understanding of ordering etc." CreationDate="2017-06-14T10:56:24.973" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7807" PostId="5238" Score="0" Text="PBRT implementation seems like photon beam diffusion. Are photon beam diffusion and the what Solid Angle does the same? I ask because I thought PBD also take point samples on the geometry as a preprocessing step but Solid Angle slides don't do it." CreationDate="2017-06-14T12:17:21.757" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7809" PostId="4994" Score="0" Text="GGX is assumed to be isotropic so $\phi = \xi_{2}$. With $\phi$ and $\theta$, you can transform to cartesian coordinates to get $w_i$ http://tutorial.math.lamar.edu/Classes/CalcIII/SphericalCoords.aspx" CreationDate="2017-06-14T15:05:38.347" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7810" PostId="5243" Score="0" Text="Any reason you're limiting yourself to a single texture? I've seen it done with multiple textures in shipping apps. (Not to say it can't be done with a single texture, but my guess is that it's easier with more than 1 texture.)" CreationDate="2017-06-15T01:44:10.347" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="7811" PostId="5243" Score="0" Text="It's easier to get a single texture. And, I'm curious :)" CreationDate="2017-06-15T02:34:37.913" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7812" PostId="5243" Score="0" Text="@DanielKareh: &quot;*But how does one go about taking a single texture and getting a decent normal map out of it?*&quot; What makes you think that there's a way to convert an image into a normal map? What is the image of, exactly?" CreationDate="2017-06-15T02:42:07.420" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7813" PostId="5243" Score="0" Text="try reading this: https://fenix.tecnico.ulisboa.pt/downloadFile/845043405449073/Tangent%20Space%20Calculation.pdf" CreationDate="2017-06-15T05:47:50.190" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7814" PostId="5229" Score="0" Text="No, parameter setting part is the same. I couldn't solve the problem but if I solve it, I will comment here." CreationDate="2017-06-15T07:19:48.460" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="7818" PostId="5243" Score="0" Text="@NicolBolas: Well, software like CrazyBump seems capable of generating normal maps of many, many surfaces from a single texture." CreationDate="2017-06-15T13:05:47.993" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7819" PostId="5243" Score="0" Text="@trichoplax: Sorry if my question was confusing. I'm looking for a way to generate a decent normal map from a single texture, if that's possible." CreationDate="2017-06-15T13:06:35.593" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7820" PostId="5243" Score="0" Text="@Charlie: I already know how to use a normal map and apply it to a surface. I'm trying to learn how to get the normal map in the first place." CreationDate="2017-06-15T13:08:59.300" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7822" PostId="5243" Score="0" Text="There are several  different ways to do this, easiest being converting a bump map into a normal map." CreationDate="2017-06-15T13:46:57.660" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7823" PostId="5243" Score="0" Text="@joojaa: Alright. Any ideas on how to get a bump map from an image?" CreationDate="2017-06-15T14:16:10.877" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7824" PostId="5243" Score="0" Text="If the image is a bump map then the normal map is just the derivate of the bump." CreationDate="2017-06-15T14:39:06.967" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7825" PostId="5243" Score="0" Text="@DanielKareh aha, I just use mudbox myself. You generate a normal map from a bump map." CreationDate="2017-06-15T14:43:17.323" UserId="6255" ContentLicense="CC BY-SA 3.0" />
  <row Id="7826" PostId="5243" Score="0" Text="But, how do I get the bump map from the image. Like, if I have an image texture of say, a rock wall, how can I get a decent bump map from it, which I can then convert into a normal map." CreationDate="2017-06-15T15:00:28.350" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7828" PostId="5243" Score="0" Text="@trichoplax: Yes, that is correct. I'll edit the question." CreationDate="2017-06-15T17:29:21.393" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7829" PostId="5243" Score="0" Text="Hopefully someone can give an overview explanation as an answer here, but as a starting point there is a free open source tool called [AwesomeBump](https://github.com/kmkolasinski/AwesomeBump) if you (or anyone looking to write an answer) wants to see example code." CreationDate="2017-06-15T17:29:58.677" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7830" PostId="5243" Score="0" Text="Thank you! Do you happen to know which file the generator is in, or will I have to look around?" CreationDate="2017-06-15T18:00:20.633" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7831" PostId="5230" Score="1" Text="For near minimal you store a vector for the translation and a quaternion for the rotation, so 7 elems to store.  So the matrix is simply the quaternion to rotation matrix + translation." CreationDate="2017-06-15T20:05:24.093" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="7832" PostId="5230" Score="0" Text="Yes, that I know. But can you post how that rotation matrix really looks like? The rows and columns." CreationDate="2017-06-15T21:25:25.903" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7833" PostId="5246" Score="3" Text="In general it's about 10 times harder to get your code correct on the GPU than on the CPU, so regardless of where it's faster, I'd recommend you start by getting everything working on the CPU." CreationDate="2017-06-16T09:07:05.487" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7834" PostId="5247" Score="0" Text="Thanks. What was `Q*`?" CreationDate="2017-06-16T14:34:55.197" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7835" PostId="5247" Score="1" Text="It's the conjugate which is the same as inverse when unit magnitude." CreationDate="2017-06-16T16:12:08.617" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="7836" PostId="398" Score="0" Text="&quot;and anyway there's unusually high-quality pseudocode at that link&quot;, I disagree. That pseudo code is likely not a proper implementation of Wu's algorithm even though it seems to be what was used in countless places around the web. Wu's original algorithm drew from both ends inward towards the center and was actually *faster* than Bresenham's because it performs about half as many operations even though it writes to more pixels. I am talking about Wu's actual algorithm *not* the one posted in the linked wikipedia article." CreationDate="2017-06-16T16:37:17.097" UserId="20" ContentLicense="CC BY-SA 3.0" />
  <row Id="7837" PostId="5246" Score="0" Text="Thanks, Dan, you are quite right, I do not have enough experience writing for GPU, I just want to understand general performance ideas." CreationDate="2017-06-16T16:37:37.597" UserId="6841" ContentLicense="CC BY-SA 3.0" />
  <row Id="7838" PostId="5246" Score="0" Text="It really depends on what your goals are.  Is this something you want to do in real time? During runtime of your game? During &quot;real time&quot; editing in the editor?  How complex are the meshes you intend on splitting?" CreationDate="2017-06-16T21:57:33.577" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7841" PostId="5220" Score="0" Text="Are you trying to do real time or offline rendering? SSS is a surface based algorithm that uses what's called a BSSRDF. Usually the implementation only accepts the data of a few layers past the surface. Usually just a few extra texture maps. Volume rendering can do the job of SSS but it models the entire inside of a volume. Often it does this with a 3d texture (uniform grid). But sometimes it's done with other datasets like tetrahedral grids. TLDR: Volume rendering isn't used for SSS although the concepts of emission, absorption, in/out scattering are the same." CreationDate="2017-06-16T23:47:03.353" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="7842" PostId="5243" Score="0" Text="I'm afraid I'm not familiar with the software, just stumbled upon it while searching." CreationDate="2017-06-17T09:06:02.857" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7845" PostId="5249" Score="0" Text="I'll make sure to check out the mentioned neural network approach. I guess I'll have to go back to messing around with custom image filters, considering you mentioned that those applications use artistic approaches rather than super accurate models of light transfer." CreationDate="2017-06-17T16:09:59.873" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7846" PostId="5243" Score="0" Text="That's okay. I'll make sure to do some searching through the files." CreationDate="2017-06-17T16:11:04.613" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="7847" PostId="3947" Score="1" Text="Cannot imagine a normal with negative Z in screen (view) space. It would require the culling turned off which itself is a bad idea from the performance perspective." CreationDate="2017-06-17T16:57:08.867" UserId="6846" ContentLicense="CC BY-SA 3.0" />
  <row Id="7848" PostId="5247" Score="1" Text="There was a seemly deleted comment along the lines of &quot;not useful to have the matrix&quot;.  This is true, you work with the rotation translation pair..and when transforming points you don't actually create a matrix (don't need to at least).  The matrix is to show the sub-expressions that need to be computed.  And this is assuming that the average number of points is (somewhere) greater than one.  Otherwise you're better off performing the rotation directly from the quaternion expression." CreationDate="2017-06-17T18:15:48.150" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="7849" PostId="5247" Score="1" Text="It's more efficient to convert to a matrix instead of using quaternion directly if you transform more than one vector with the same quaternion. Also for more compact representation you don't need to store w since you are dealing with unit quaternions: w=sqrt(1-x^2-y^2-z^2)." CreationDate="2017-06-18T04:25:12.007" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7850" PostId="5247" Score="1" Text="I was attempting to say the matrix doesn't need to be stored..it's on the stack.  Dropping the scalar doesn't work very well: http://marc-b-reynolds.github.io/quaternions/2017/05/02/QuatQuantPart1.html" CreationDate="2017-06-18T07:13:17.563" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="7851" PostId="5253" Score="1" Text="1. try update your system. and run `ldconfig`&#xA;2. how did you compile your code? what arguments are you using?" CreationDate="2017-06-18T17:28:50.323" UserId="2448" ContentLicense="CC BY-SA 3.0" />
  <row Id="7852" PostId="398" Score="0" Text="@Octopus [Expresses vague skepticism, especially on the faster bit, but lacks context to refute or confirm—if this is so, sources, corrections, and edits are of course welcome.]" CreationDate="2017-06-18T23:31:25.497" UserId="523" ContentLicense="CC BY-SA 3.0" />
  <row Id="7855" PostId="5256" Score="0" Text="I would think part of it has to do with the layout of 2d elements on the screen.  If you allow the user to freely resize the screen then the elements could overlap." CreationDate="2017-06-19T13:24:42.943" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="7856" PostId="5256" Score="0" Text="Someone who can't build a GUI that's scaling resistant shouldn't be allowed to build GUIs at all. Also, scaling is possible, but only in steps. So a little window where things might overlap is still possible, so why not sizes in between?" CreationDate="2017-06-19T13:28:07.610" UserId="6092" ContentLicense="CC BY-SA 3.0" />
  <row Id="7857" PostId="5256" Score="1" Text="Also, the 2d elements are often textures that are designed to be the right size for a certain screen resolution.  If the user arbitrarily blows up the screen size, those textures will stretch and look bad.  Instead, there are different textures for each supported resolution." CreationDate="2017-06-19T13:30:45.260" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="7858" PostId="5256" Score="2" Text="This question is probably more appropriate for gamedev.stackexchange.com.  See https://gamedev.stackexchange.com/a/125316/26734" CreationDate="2017-06-19T13:34:15.813" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="7859" PostId="5256" Score="0" Text="I posted it here because I think this is mostly a graphical issue that just happens to appear mostly in games, but you might be right. Also, the answer on that question on gamdev isn't really satisfying." CreationDate="2017-06-19T13:56:13.407" UserId="6092" ContentLicense="CC BY-SA 3.0" />
  <row Id="7860" PostId="5256" Score="0" Text="See also: https://gamedev.stackexchange.com/a/56952" CreationDate="2017-06-19T13:58:53.293" UserId="6092" ContentLicense="CC BY-SA 3.0" />
  <row Id="7861" PostId="5256" Score="2" Text="Not quite enough info for an answer, but we query the driver to ask for what resolutions will work in full screen mode.  The specific list of resolutions you give in dark souls 2 will vary from machine to machine. There are hardware and software (driver) limitations to what is available." CreationDate="2017-06-19T14:14:28.783" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7862" PostId="5256" Score="0" Text="But why would they matter in windowed mode? And if the game takes all the resolution the system throws at them, does it mean the game in fact *is* resolution independent and the restriction is purely artificial?" CreationDate="2017-06-19T14:29:40.100" UserId="6092" ContentLicense="CC BY-SA 3.0" />
  <row Id="7863" PostId="5253" Score="0" Text="I used following commands: &quot;g++ pgm1.cpp -lglut&quot;" CreationDate="2017-06-19T14:52:37.567" UserId="5099" ContentLicense="CC BY-SA 3.0" />
  <row Id="7864" PostId="5255" Score="0" Text="I did try adding -lgl to the compilation command on seeing this suggestion on one of the internet sites. But it does not help" CreationDate="2017-06-19T14:53:41.643" UserId="5099" ContentLicense="CC BY-SA 3.0" />
  <row Id="7865" PostId="5253" Score="0" Text="I tried running ldconfig command. I am getting a message &quot;Cant create temporary cache file/etc/ld.so.cache~: Permission denied&quot;" CreationDate="2017-06-19T15:13:12.047" UserId="5099" ContentLicense="CC BY-SA 3.0" />
  <row Id="7867" PostId="5253" Score="0" Text="try `sudo ldconfig`. When you see Permission denied. adding sudo to the command will fix that 99% of the time(not the best habit though)" CreationDate="2017-06-19T18:23:33.920" UserId="2448" ContentLicense="CC BY-SA 3.0" />
  <row Id="7868" PostId="5256" Score="2" Text="If you find the linked answer unsatisfying, you might be more likely to get the answer you seek if you edit the question to explain why you are not satisfied with the reasons given. That way you can avoid getting the same answer here." CreationDate="2017-06-19T18:25:28.247" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7869" PostId="5252" Score="0" Text="Great answer, and thanks for a thorough explanation. I believe this is exactly what I'm looking for, though I'll need to reread your answer several more times to get it through in my head haha. But please fix your original M matrix to include z. Thank you again, good sir!" CreationDate="2017-06-19T18:28:43.440" UserId="6824" ContentLicense="CC BY-SA 3.0" />
  <row Id="7870" PostId="5258" Score="0" Text="One way to generate a triangulated sphere is to use an isosphere: https://schneide.wordpress.com/2016/07/15/generating-an-icosphere-in-c/&#xA;&#xA;What do you mean by pre-defined adjacency? What's the purpose?" CreationDate="2017-06-19T19:39:27.857" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7872" PostId="5258" Score="0" Text="I need a 3D sphere consistent with a given 3D mesh in terms of the number of vertices and face-connectivity. The given mesh is genus 1, so I guess that is possible. I have looked into icosahedron division method but the issue is face-connectivity." CreationDate="2017-06-20T04:47:45.107" UserId="6854" ContentLicense="CC BY-SA 3.0" />
  <row Id="7873" PostId="5261" Score="0" Text="Interesting point. Does this mean Dark Souls will run distorted on screens with different aspect ratios (e. g. an old 800x600)? I think the fact that they all have the same ratio might be due to what @Alan Wolfe said in the comments, that these are only those supported by my hardware on full screen mode." CreationDate="2017-06-20T07:57:02.083" UserId="6092" ContentLicense="CC BY-SA 3.0" />
  <row Id="7874" PostId="5263" Score="2" Text="I think you need to provide some more background to your question." CreationDate="2017-06-20T09:20:53.817" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7875" PostId="5263" Score="0" Text="Please narrow down the question with more detail on what you are looking to achieve. It might also help to describe what information you already have about the mesh, perhaps with an example." CreationDate="2017-06-20T11:42:03.770" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7876" PostId="5261" Score="2" Text="As a specific example, in the 2d game [slither.io](http://slither.io/) there is a max distance you can see. In portrait, you can see this distance vertically, with your horizontal view restricted. In landscape, you can see this distance horizontally, with your vertical view restricted. If you resize the window to be perfectly square, then you can see the max distance both vertically and horizontally, giving a significant advantage over other players. In addition to the time and money required to adjust to different aspect ratios, it can also become a potentially undesirable meta game." CreationDate="2017-06-20T11:50:27.963" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7877" PostId="5211" Score="0" Text="@trichoplax I think that you're right, and the library that I'm using is computing something wrong on the CSG operations. Just for testing I tried the method powered by Carve used in Blender and it gave the results that I expected, so I'll try to use another library (maybe Carve itself) to do the operations and will answer the question acordingly if it works. Thanks!" CreationDate="2017-06-20T13:03:33.967" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7878" PostId="5251" Score="0" Text="Can you give a chapter or page number where $W(x,\theta) $ is described in PBR?" CreationDate="2017-06-20T13:34:06.133" UserId="6608" ContentLicense="CC BY-SA 3.0" />
  <row Id="7879" PostId="5265" Score="0" Text="So, reading this it seems to  me you do not know how the standard 3D camera projections work. Simply you transform vectors by the transformation matrix of the projection." CreationDate="2017-06-20T14:40:53.557" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7880" PostId="5265" Score="0" Text="Thanks for the answer! But wouldn't this give me a view-dependent projection? I'm interested in knowing exactly how to define a plane on which the mesh should be projected." CreationDate="2017-06-20T14:52:38.187" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7881" PostId="5266" Score="0" Text="One can backace cull the data and only consider the edges that adjoin a backface culled member." CreationDate="2017-06-20T15:11:31.440" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7882" PostId="5266" Score="0" Text="@joojaa that's an equivalent way of generating perimeter candidates." CreationDate="2017-06-20T15:13:27.907" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7883" PostId="5265" Score="0" Text="A view is a transform just like anything else. Nothing states that it has to be the same view as your viewport. So i take it you do not know how the camera works." CreationDate="2017-06-20T15:14:21.520" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7884" PostId="5266" Score="0" Text="Thanks! I'll try your suggestions out and accept the answer if that is the case. One question though: which method should I use to find the normal of this arbitrary plane? For example, if I have a wall, I need to use its larger face as the arbitrary plane, so how should I compute it's normal?" CreationDate="2017-06-20T15:14:21.873" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7885" PostId="5266" Score="1" Text="@LiordinoNeto cross product" CreationDate="2017-06-20T15:15:28.177" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7887" PostId="5266" Score="0" Text="@ratchetfreak this? &#xA;`meshNormal = (0,0,0)&#xA;foreach vertexNormal in meshNormals&#xA;    meshNormal = meshNormal x vertexNormal`" CreationDate="2017-06-20T15:20:35.003" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7888" PostId="5266" Score="0" Text="Yes but you get the area with those edges too." CreationDate="2017-06-20T15:28:22.260" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7889" PostId="5266" Score="0" Text="@joojaa sorry, but I think I didn't understand. Are you talking about the arbitrary plane's normal calculation with the cross product? What edges exactly?" CreationDate="2017-06-20T15:32:00.527" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7890" PostId="5252" Score="0" Text="Thanks @MatthewL for noticing that error. Just fixed it now!" CreationDate="2017-06-20T15:54:23.963" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="7891" PostId="5266" Score="0" Text="@LiordinoNeto I wasnt talking to you no handle means i am talking to ratchett. Anyway face normal is different from vertex normals. Face normal is formed out of 2 triangle edges. Note this is why i asked about you knowing how the 3D view rendering works since now we are in fact telling you how the viewport rendering works in a roudabout way as the operations needed are exactly the same." CreationDate="2017-06-20T16:01:37.917" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7892" PostId="5266" Score="0" Text="@joojaa no problem, thanks for the explanation!" CreationDate="2017-06-20T16:34:53.243" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7893" PostId="5211" Score="0" Text="Thanks for the update, and a self answer if it works would be great!" CreationDate="2017-06-20T18:38:29.417" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7894" PostId="5265" Score="1" Text="For the area, why don't you just add up areas of the triangles in the original 3D mesh? What you need the projection for? The way you describe it sounds like an overcomplicated solution to a fairly straightforward problem. Or is it because you don't know how to calculate the area of a triangle in 3D?" CreationDate="2017-06-21T02:25:00.510" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7896" PostId="5251" Score="0" Text="I'am using a book 'Advanced Global Illumination 2ed' (page 89 chapter 4.2)  and referring to &quot;dspace5.zcu.cz/bitstream/11025/15417/1/Willems_94.pdf&quot; (page 4, 5 the potential equation)" CreationDate="2017-06-21T07:38:58.917" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="7897" PostId="5238" Score="1" Text="There are two things to distinguish here: The function to integrate (BSSRDF) and the method of integration. The first one are functions like the dipole, multipole, photon beam diffusion, etc. The second are methods like Jensen's hierarchical octree or Solid Angle's importance sampled ray tracing." CreationDate="2017-06-21T08:36:52.163" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="7898" PostId="5263" Score="1" Text="Sorry to be a pain, but what defines &quot;good topology&quot; vs &quot;bad topology&quot;? Are you after the effectiveness of each polygon, or how regular the mesh is?" CreationDate="2017-06-21T09:09:53.740" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7899" PostId="5265" Score="0" Text="@JarkkoL thanks for the suggestion. Summing up the areas of all the triangles of the original 3D mesh surface gives me the **surface area**, which I'm already calculating correctly. I don't know exactly how this is called, but what I need is a way to compute the planar area filled by this same mesh to apply in an architectural context." CreationDate="2017-06-21T12:12:47.627" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7900" PostId="5265" Score="1" Text="Ok, so your question really is &quot;how do you calculate the projected area of a 3D object on an arbitrary 2D plane&quot;? If the 3D object is convex, then it's quite simple (ignore backfacing triangles and project triangles using dot-product), but it's much more difficult for concave objects because you have to account for (partial) occlusion to avoid &quot;double counting&quot; areas." CreationDate="2017-06-21T12:59:22.987" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="7901" PostId="5265" Score="0" Text="@JarkkoL yes, you got it right. I'm working now on the backface culling with dot-product. Thanks!" CreationDate="2017-06-21T14:07:24.833" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7902" PostId="5265" Score="0" Text="Just did the backface culling and now it's giving me results near what I expect. I suspect that the difference may be because I'm not accounting yet to partial occlusion. Will check on it to see what can be done. Thanks for the insights, everyone!" CreationDate="2017-06-21T17:50:19.067" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7904" PostId="5266" Score="0" Text="I didn't implemented this yet, but indeed it seems like the most straightforward approach to be taken and would give the expected results, so I'm accepting your answer." CreationDate="2017-06-21T17:51:54.900" UserId="6788" ContentLicense="CC BY-SA 3.0" />
  <row Id="7905" PostId="5263" Score="0" Text="good topology is a low polygon mesh with faces in the right location such that when animated the mesh deforms properly. i want to generate a low poly mesh FROM the high poly mesh with good topology" CreationDate="2017-06-21T20:12:51.150" UserId="6834" ContentLicense="CC BY-SA 3.0" />
  <row Id="7906" PostId="5263" Score="0" Text="&quot;Suitable for animation&quot; is much more meaningful than &quot;good&quot;, and different things can be good in different contexts. If you edit the question to describe more about the task you are facing, including that info from your comment, it will be much easier to know what type of answers you are seeking." CreationDate="2017-06-21T23:33:32.357" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7907" PostId="5274" Score="1" Text="Note that which faces are hidden depends on the whole camera projection, not just the position and normal. Say you look at one face of a cube, but deform the cube into a frustum where the near face is *slightly* smaller than the far face. The other four faces might be visible in an orthographic projection but not in a perspective projection." CreationDate="2017-06-22T16:03:43.193" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7908" PostId="5274" Score="0" Text="@DanHulme OK. Let's assume we have the camera projection. Usually given by the 3D graphics API. Remember, we have points. Not meshes which makes the problem simpler." CreationDate="2017-06-22T16:08:11.617" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7909" PostId="5274" Score="1" Text="I don't think it does make the problem simpler. If you don't know the connectivity of the points, you don't know where the faces are, so you can't test for occlusion at all. I think mesh reconstruction needs to be the first step of your algorithm, unless an approximation is good enough (e.g. if a point is within a half-degree of another from the camera's pov, one occludes the other)" CreationDate="2017-06-22T16:12:34.783" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7910" PostId="5274" Score="0" Text="@DanHulme Right. Probably we need some mathematical algorithms playing around with the XYZ coordinates and understand if a point is behind others. Or some other approach." CreationDate="2017-06-22T16:18:19.620" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7911" PostId="5269" Score="0" Text="The MSE will be a random variable. You can solve for $ \sigma $ by pre-defining the expected value of the PSNR rather than it's exact value. Of course, there are things you can do if you want to prescribe MSE on the nose, for example you can add a constant, but how you do it depends on what properties you need for the noise." CreationDate="2017-06-22T22:43:57.977" UserId="6608" ContentLicense="CC BY-SA 3.0" />
  <row Id="7912" PostId="5274" Score="1" Text="Seems like you might want to do a sort of software render. You can create a buffer and initialize it with max z value. For each vertex apply MVP to find the x, y coord in the buffer. perform depth testing and if passes store new z and vertexID. If you have a buffer the size of your screen then you have perfect precision, if you create a buffer less than your screen size you improve performance at accurracy expense. You can probably clusterize and compute with a compute shader." CreationDate="2017-06-22T23:17:17.703" UserId="250" ContentLicense="CC BY-SA 3.0" />
  <row Id="7913" PostId="5270" Score="0" Text="Are you more interested in post processing to disguise the noise, or ways of speeding up convergence so that less noise is present?" CreationDate="2017-06-22T23:31:02.323" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7915" PostId="5277" Score="0" Text="`/*layout (location=1)*/` Why did you comment out important declarations like that?" CreationDate="2017-06-23T01:35:19.970" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7916" PostId="5272" Score="0" Text="Is there any image or picture with all of these components working together?" CreationDate="2017-06-23T04:02:54.570" UserId="6868" ContentLicense="CC BY-SA 3.0" />
  <row Id="7917" PostId="5278" Score="0" Text="Thank you! Updating it to have one color per vertex worked!" CreationDate="2017-06-23T05:18:09.657" UserId="6874" ContentLicense="CC BY-SA 3.0" />
  <row Id="7918" PostId="5278" Score="0" Text="If I may ask for furthur optimization and advice, Currently in order to generate those quadrilateral cells, I am using 8 vertices for each cell. This then means each time I change the colour of a cell, I send 3*4*8 bytes each time. I changed it to send a third of the data by creating the colour in the shader itself, and using an int to determine which colour to use. Is there a more efficient method than the one I just described?" CreationDate="2017-06-23T05:21:01.093" UserId="6874" ContentLicense="CC BY-SA 3.0" />
  <row Id="7919" PostId="5277" Score="0" Text="I had a bug where I was unable to obtain attribute indices that I resolved. The indices that I declared (and commented out) just so happened to be the same ones that the compiler chose. Already changed it back to query the attribute index based on the name!" CreationDate="2017-06-23T05:22:18.383" UserId="6874" ContentLicense="CC BY-SA 3.0" />
  <row Id="7920" PostId="5263" Score="0" Text="Still need to be more specific—what info about the topology do you want, precisely? &quot;What direction it is flowing&quot; doesn't help much." CreationDate="2017-06-23T06:43:02.260" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="7921" PostId="5278" Score="2" Text="Since the original question was about getting the code to work correctly, you could post a separate question about efficiency." CreationDate="2017-06-23T08:06:29.767" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7923" PostId="5274" Score="2" Text="@TinaJ A mathematical point can't occlude another because they have zero size. Only the surface can occlude. To take your diagram, what if the orange object were a doughnut? How would we know we can see through the middle of it? You need more information about the nature of this point cloud to decide what surfaces the points represent. You probably have that if you know what the real-world objects are, but you haven't told us anything about that." CreationDate="2017-06-23T08:21:33.280" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7924" PostId="5272" Score="0" Text="@mrigendra Every frame displayed on your Android phone shows these components working together! Seriously, if you mean a diagram, I don't know of one." CreationDate="2017-06-23T08:22:41.297" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7925" PostId="5270" Score="1" Text="FWIW, Benedikt Bitterli recently released the following https://twitter.com/tunabrain/status/872174108385136640   based on his denoising paper." CreationDate="2017-06-23T08:58:25.933" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7926" PostId="5270" Score="0" Text="In postprocessing area, there is nice algorithm called bilateral filter https://www.shadertoy.com/view/4dfGDH" CreationDate="2017-06-23T09:10:57.343" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="7927" PostId="5274" Score="0" Text="@FelipeLira That's interesting. I didn't know it can be as complicated. Is there any reference/source code to that?" CreationDate="2017-06-23T10:08:34.623" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7928" PostId="5274" Score="0" Text="@DanHulme Well, you are right in general. But even with my simple mathematical assumption, the middle of donut is empty, no points there. So we should see whatever behind. It is like sending a beam to every single pixel visible from my projection. Then if two points are in the same line, we have to remove whatever behind." CreationDate="2017-06-23T10:11:01.427" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7929" PostId="5274" Score="1" Text="@TinaJ The point is, how big does the gap between points have to be before you think it's a doughnut hole? We can't know that (or even if there is a value that makes sense for that) without knowing something about what the points represent in the real world." CreationDate="2017-06-23T10:16:04.993" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7930" PostId="5274" Score="0" Text="@DanHulme Yes the size of points in a point cloud matter to know if anything is visible in the behind or not. We can start from the simplest approach and assumptions independent. I've updated the question with a test model." CreationDate="2017-06-23T10:40:41.423" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7931" PostId="5277" Score="0" Text="Why query the attribute index when you can just *specify it* in the shader?" CreationDate="2017-06-23T13:35:15.483" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="7932" PostId="5274" Score="1" Text="I think you're a bit optimistic if you think people will go download your file and run tests on it before they can answer your question usefully. You're most likely to get good answers when all the relevant information is in the question." CreationDate="2017-06-23T13:58:47.280" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7933" PostId="5281" Score="0" Text="can you reference an implementation of a contour detector for which you can specify shape?  I was using find_contours (http://scikit-image.org/docs/dev/api/skimage.measure.html#find-contours) which apparently does no support that" CreationDate="2017-06-23T14:48:35.437" UserId="6875" ContentLicense="CC BY-SA 3.0" />
  <row Id="7934" PostId="5269" Score="0" Text="I realized that because the expected value of the random noise is 0, I can estimate the sum of the squares of the noise values for each pixel as the sum of $\sigma^2$ itself. That makes the whole thing a lot easier to solve when I have a predefined PSNR" CreationDate="2017-06-23T15:01:41.913" UserId="6865" ContentLicense="CC BY-SA 3.0" />
  <row Id="7935" PostId="5274" Score="0" Text="The test file is to show what I mean of point cloud and the format for those who are not familiar. Yes, most important for me is the description of the approach." CreationDate="2017-06-23T15:06:20.720" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7936" PostId="5274" Score="1" Text="A point (in the mathematical sense generally used in computer graphics) has zero size, as mentioned in previous comments. If you want an approach that will work with points that *do* have a size, we'd need to know what size the points are and how their size works. For example, does a point have some defined size in 3D space, or does it have a fixed 2D size independent of distance from the camera (maybe 4 pixels per point)? Using a non-standard definition of &quot;point&quot; is fine, but we can't answer without knowing what that new definition is." CreationDate="2017-06-23T17:32:31.623" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7937" PostId="5284" Score="0" Text="Are you talking about diffuse hemisphere lighting from a point light?" CreationDate="2017-06-23T17:38:31.773" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7938" PostId="5284" Score="1" Text="By hemisphere lighting I mean that the top and bottom (hemispheres) of the &quot;world&quot; both emit light of a differing color, e.g. the top may emit blue light (sky) while the bottom may emit a dim brown (earth). You can think of it as a simplified environment map." CreationDate="2017-06-23T17:42:48.957" UserId="6876" ContentLicense="CC BY-SA 3.0" />
  <row Id="7939" PostId="5281" Score="0" Text="I can't think of an existing one; when I wrote the answer I was thinking you'd have to do it yourself. For an arbitrary shape that might be a lot of work but you're only looking for four lines." CreationDate="2017-06-23T18:43:29.123" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7940" PostId="5274" Score="1" Text="@trichoplax yes In point clouds, points have a size. So let's assume they have a size, say 5 pixel radius just for modeling purpose. Updated the question." CreationDate="2017-06-23T21:36:32.440" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7941" PostId="5274" Score="0" Text="I see. So just to be explicit, points further away don't look any smaller, still 5 pixels?" CreationDate="2017-06-23T22:27:29.640" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7942" PostId="5274" Score="1" Text="It might be worth changing the example picture if you are interesting in points occluded by other points, rather than by polygonal faces. A picture showing points might make for less confusion." CreationDate="2017-06-23T22:29:10.057" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7943" PostId="5274" Score="1" Text="@trichoplax Sure. I updated the figure. It should be clear what I mean." CreationDate="2017-06-23T23:23:41.737" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7944" PostId="5263" Score="0" Text="i want information about the shape of the mesh so i can create a new mesh from that information with different geometry (but geometry with specific edges in specific places according to the shape of the mesh)&#xA;&#xA;So in other words, i want geometry that follows the form of the mesh" CreationDate="2017-06-24T01:11:30.323" UserId="6834" ContentLicense="CC BY-SA 3.0" />
  <row Id="7945" PostId="5263" Score="3" Text="Sounds like you want to perform curvature-aligned remeshing, see e.g. https://hal.inria.fr/inria-00099624, http://users.cs.cf.ac.uk/Yukun.Lai/papers/remesh_cad.pdf" CreationDate="2017-06-24T06:20:18.920" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="7946" PostId="5281" Score="0" Text="actually after using find_contours I am indeed looking for rectangle myself, but I thought that if I can make find_contours produce better contours, my final result will be more accurate and less prone to lighting conditions." CreationDate="2017-06-24T06:24:04.033" UserId="6875" ContentLicense="CC BY-SA 3.0" />
  <row Id="7949" PostId="5267" Score="0" Text="@DanHulme Do you have any answer for this post ?" CreationDate="2017-06-24T11:08:27.120" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="7950" PostId="5263" Score="0" Text="Thanks Rahul! this is the first reply that has something to do with what i am looking for!" CreationDate="2017-06-24T23:05:13.590" UserId="6834" ContentLicense="CC BY-SA 3.0" />
  <row Id="7951" PostId="5288" Score="0" Text="Thank you for the answer, I understand my problem.&#xA;But , in my opinion, the second image should be changed, the direction vector should not be changed as I know." CreationDate="2017-06-25T00:12:08.467" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="7952" PostId="5288" Score="0" Text="In the second image I swapped the light source and the camera, this means that the incoming light is coming from another direction than previously. It is not like the camera suddenly emits light. That is why I changed Wo and Wi with each other. I was a bit lazy and I did not swap the arrow shapes themselves, but I thought it was enough to convey the point." CreationDate="2017-06-25T08:57:10.393" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="7953" PostId="5274" Score="1" Text="There is no simple answer for this question simply because it depends on your interpretation of things and as such has many answers and none at the same time. Since you are not describing your qualitative border constraints. So the question might as well be how to create a mesh fom a pointcloud." CreationDate="2017-06-25T09:27:15.360" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7956" PostId="5267" Score="0" Text="Note that the @username notifications only work for users who have already commented on this particular post. It's just coincidence that the person mentioned happens to have posted an answer..." CreationDate="2017-06-25T18:43:13.147" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7957" PostId="5294" Score="0" Text="Wow, this is exactly what I've been looking for.  I started reading a Linear Algebra textbook...but what kind of mathematics do I need to know for physically based rendering?" CreationDate="2017-06-25T20:06:11.133" UserId="6025" ContentLicense="CC BY-SA 3.0" />
  <row Id="7958" PostId="5294" Score="2" Text="Wait, did you meant &quot;Because $x_{proj}$ doesn't vary from $0 \rightarrow width$&quot;?" CreationDate="2017-06-25T20:42:15.407" UserId="6025" ContentLicense="CC BY-SA 3.0" />
  <row Id="7959" PostId="5294" Score="0" Text="For openGL it needs to project to normalised device coordinates (-1 to 1) so it scales from -Width/2 to Width/2" CreationDate="2017-06-26T02:19:54.297" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7961" PostId="5287" Score="0" Text="Correct. This is called spectral rendering. The easiest implementation is to assign a wavelength to each starting ray and then reflect, refract, etc. as per normal. The BRDF / fresnel would then take the wavelength as an additional parameter. This would automatically handle Iridescence." CreationDate="2017-06-26T13:18:26.693" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7964" PostId="5294" Score="0" Text="You should ask another question about PBR, but here is a good resource: https://learnopengl.com/#!PBR/Theory" CreationDate="2017-06-26T17:56:01.663" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="7965" PostId="5294" Score="0" Text="@mikeglaz I did, thank you. I'd say if you know some undergrad linear algebra and high-school optics, you're fine. Any particulars you don't already understand, you'll already have the grounding to understand by reading the appropriate Wikipedia page." CreationDate="2017-06-26T19:22:05.960" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7966" PostId="5264" Score="0" Text="Well, yeah, you are quite right. I'd better do it on CPU and if I am not happy with performance, I'll try to find different ways. Thank you!" CreationDate="2017-06-27T08:51:19.307" UserId="6841" ContentLicense="CC BY-SA 3.0" />
  <row Id="7967" PostId="5295" Score="1" Text="Part 1 is the sticky point, it is in fact the problem. This makes the question subjective as the mesh creation in fact has same definition problem as the question in general. We can not give qualitative advice because we dont know what the data is supposed to mean. Only the original author can nknow what the broder constraints should be. And in this case it seems OP does not so we can not really help. It affects the choice of disk size in method 2 too." CreationDate="2017-06-27T08:55:30.997" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="7968" PostId="5296" Score="1" Text="It sounds like your triangles are being transformed so they can have Z &lt; 0 when rotated into eye space. Usually you need to clip the triangle against a near plane to avoid this." CreationDate="2017-06-27T12:28:33.567" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="7969" PostId="5296" Score="2" Text="@PaulHK You should post that as an answer." CreationDate="2017-06-27T12:47:00.513" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7970" PostId="5297" Score="0" Text="My suggestion would be a screenshot, or if you want a higher resolution, print directly to PNG. Perhaps with something like: https://sourceforge.net/projects/pdfcreator/ I've never used it before, but it seems to get pretty good reviews. And it's open source." CreationDate="2017-06-27T13:51:24.437" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="7971" PostId="5295" Score="0" Text="Thanks. It is a good description. But we don't know if they work unless we test them. Are you aware of any implementation, source code, or anything?" CreationDate="2017-06-27T14:33:04.553" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="7972" PostId="5296" Score="0" Text="@PaulHK that should indeed be right. The clipping should happen before the perspective divide and after the projection transformation. So, I would indeed post that as an answer as Dan said." CreationDate="2017-06-27T16:24:03.327" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="7975" PostId="5295" Score="0" Text="At that point you'd have to start experimenting. _C_ should be trivial to implement, _B_ is simple but a little more involved, and as said, the triangulation for _A_ is really problematic. Not only it is an open problem, but I'd expect existing solutions to be tedious to implement. I would recommend finding a library that does a half decent job at it." CreationDate="2017-06-28T07:32:23.457" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="7979" PostId="5290" Score="0" Text="In your equation, you transform from integration in cartesian coordinates to integration in sphere coordinates. Shouldn't you add *sin(ϕ)* there to account for this coordinate transformation? Alternatively, you might define *p(θ, ϕ)=sin(ϕ)p(sin(ϕ)cos(θ), sin(ϕ)sin(θ), cos(ϕ))*, but I think it will be more clear if the *sin(ϕ)* factor is distinct from *p(θ, ϕ)*." CreationDate="2017-06-28T10:07:11.750" UserId="6876" ContentLicense="CC BY-SA 3.0" />
  <row Id="7980" PostId="5267" Score="0" Text="@trichoplax It's not a complete coincidence. I think I did get notified, because I'd previously edited the question, and the comment reminded me that I was intending to come back and post an answer when I got more time." CreationDate="2017-06-28T10:55:16.287" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7981" PostId="5290" Score="1" Text="@FlorianR. The question already describes that *sin(θ)* is inside *p(θ, ϕ)*. The questioner didn't seem to have trouble with it and it's a bit of a side issue, so I didn't really pay any attention to it." CreationDate="2017-06-28T11:32:59.357" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="7983" PostId="5299" Score="1" Text="Wouldn't the cartesian bounds of the rectangle with h=1 and w=2 be equal to the bounds of the rectangle with h=2 and w=1?" CreationDate="2017-06-28T13:15:11.423" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="7984" PostId="5299" Score="0" Text="@ratchetfreak h=2 and w=1 on isometric space?&#xA;&#xA;If so, I think not, the image that I posted has the same w and h on isometric space with 2:1 Cartesian bounds, but we could fit different objects with different isometric dimensions on the same Cartesian bounds." CreationDate="2017-06-28T15:13:22.633" UserId="6908" ContentLicense="CC BY-SA 3.0" />
  <row Id="7985" PostId="5299" Score="0" Text="Thinking again, my algorithm should take circles into account." CreationDate="2017-06-28T15:15:41.127" UserId="6908" ContentLicense="CC BY-SA 3.0" />
  <row Id="7986" PostId="5267" Score="0" Text="Interesting. I didn't realise that edits did that. It makes sense though... Thanks for letting me know." CreationDate="2017-06-28T21:43:24.287" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="7990" PostId="5300" Score="0" Text="OK, made the edit." CreationDate="2017-06-29T06:24:35.323" UserId="2040" ContentLicense="CC BY-SA 3.0" />
  <row Id="7992" PostId="5300" Score="0" Text="Is it understandable now?" CreationDate="2017-06-29T08:58:03.953" UserId="2040" ContentLicense="CC BY-SA 3.0" />
  <row Id="7994" PostId="5300" Score="1" Text="Yes, it's way more clear." CreationDate="2017-06-29T14:11:30.473" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="7995" PostId="5301" Score="1" Text="Dan, I've added an image describing my expectation as an &quot;answer&quot;." CreationDate="2017-06-29T16:28:03.307" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="7996" PostId="5301" Score="1" Text="You probably want to search for CSG (Constructive Solid Geometry) and Stencils.  For example, this https://www.usenix.org/legacy/events/usenix05/tech/freenix/full_papers/kirsch/kirsch.pdf might allow you to achieve the desired results.  Similarly, perhaps look at http://www.opencsg.org/ ?&#xA;&#xA;Actually, I might add this as a possible answer if no other answers appear" CreationDate="2017-06-29T16:43:57.417" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="7997" PostId="5301" Score="1" Text="Thanks! In fact, I was reading that paper last night, but was hoping that OpenGL would have provided a way of accomplishing this with just basic stencil functions. It looks like quite an involved task!" CreationDate="2017-06-29T16:50:45.123" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="7998" PostId="5301" Score="1" Text="Simon, one of the issues that I'd have with using the EXCELLENT CSG resources is that I'm accessing OpenGL via Visual Basic modules linked to an Excel spreadsheet from which I drive my application. I do not know whether or not the project files for MSVC6 will include extensions for VBA." CreationDate="2017-06-29T17:49:04.300" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="8000" PostId="5267" Score="0" Text="@trichoplax I just tried my luck" CreationDate="2017-06-29T21:27:39.423" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="8004" PostId="5290" Score="0" Text="@DanHulme Thanks for your answer.&#xA;Ok it's good to see p(w) as an abstract representation.&#xA;In my question, I didn't understand how to compute $p(\theta, \phi)$. According to your answer, to calculate $p(\theta, \phi)$ you state that _integrals equality_ involves _integrands equality_ so $p(\omega)d\omega$ = $p(\theta, \phi)d\theta d\phi$, am I right ?&#xA;But is it still right if in this case integration domains are not the same for the two integrals?" CreationDate="2017-06-29T21:58:27.563" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="8005" PostId="5290" Score="0" Text="@FlorianR do you state that the transformation from $p(\omega)$ to $p(\theta, \phi)$ is the transformation from cartesian coordinates to spherical coordinates ?" CreationDate="2017-06-29T22:03:19.413" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="8007" PostId="5307" Score="4" Text="If you know how diffuse reflection works (ie not a specular mirror like reflection), what that does with reflected rays, you do with refracted rays." CreationDate="2017-06-30T04:40:05.140" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8008" PostId="5290" Score="2" Text="@Yoo Rather the transformation from integrating over *ω* to integrating over *θ* and *ϕ* is the transformation from integrating in cartesian to integrating in spherical coordinates. While you already define *p(ω) = sin(θ)p(θ,ϕ)*, I'd rather not change *p* at all, using *ω=(sin(ϕ)cos(θ), sin(ϕ)sin(θ), cos(ϕ))* and using *dω = sin(θ)dθdϕ*. I think that should make it clear that *p* stays the same, we are just transforming the coordinate system of the integration. This is, however, subjective and if you redefine *p* to include *sin(θ)* like you did, the resulting equation will be the exact same." CreationDate="2017-06-30T04:41:08.773" UserId="6876" ContentLicense="CC BY-SA 3.0" />
  <row Id="8009" PostId="5307" Score="1" Text="Keep the same parameters, but increase the roughness." CreationDate="2017-06-30T04:48:36.567" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8011" PostId="5309" Score="1" Text="If you compute du/dx etc can you not check if the bounds of the pixel (including required line widths) include the required U value and set the colour accordingly?" CreationDate="2017-06-30T09:02:17.050" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8012" PostId="5290" Score="0" Text="@FlorianR You are right to precise that p is the same thing no matter its starting set ($\omega$, or {\theta, \phi}). It's the probability density function, used to find the relative probability of a direction, am I right ?&#xA;&#xA;I think you are misunderstanding what I understand and what I don't (my bad, my question can be confusing).&#xA;**Even though I state that $p(\omega) = \sin(\theta)p(\theta, \phi)$, i don't know why it is true.** (That's why I asked the third question in my post), and because I don't know why is true, I can't plainly understand what is the transformation." CreationDate="2017-06-30T11:40:23.130" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="8013" PostId="5290" Score="0" Text="@Yoo Is my edit helpful? At first I thought I had understood what you understand and what you don't, but I'm not so sure now." CreationDate="2017-06-30T12:21:03.047" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8014" PostId="5290" Score="1" Text="@Yoo Ah, I'm sorry. Dan just added an explanation and maybe to help you visualize it, think of [a map using latitude/longitude](https://en.wikipedia.org/wiki/Equirectangular_projection). The south pole appears enormous, and if you were to integrate something along latitude and longitude, the poles would be overrepresented. The sin(θ) factor removes this bias towards the poles. Also, you might want to read up on integration by substitution for the general case." CreationDate="2017-06-30T12:25:31.683" UserId="6876" ContentLicense="CC BY-SA 3.0" />
  <row Id="8015" PostId="5290" Score="0" Text="@FlorianR. Oh, that's a good way of describing it. I might use that one in future." CreationDate="2017-06-30T12:29:44.127" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8017" PostId="5312" Score="0" Text="Thanks for your answer. I was checking more for the formula to get to know which points are inside my rotated rectangle, based on X, Y position, width and height and also rotation angle (centered). Thx" CreationDate="2017-06-30T14:50:06.800" UserId="6844" ContentLicense="CC BY-SA 3.0" />
  <row Id="8019" PostId="5312" Score="0" Text="I've just done it. Thx" CreationDate="2017-06-30T21:00:45.583" UserId="6844" ContentLicense="CC BY-SA 3.0" />
  <row Id="8021" PostId="5290" Score="0" Text="@DanHulme yes your edit is helpful, it is always good to restart from scratch to be sure that everything is understood." CreationDate="2017-07-01T14:43:01.353" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="8022" PostId="5290" Score="0" Text="@FlorianR.Thanks, it's useful to have a &quot;viewable&quot; way of what the integral means." CreationDate="2017-07-01T14:44:09.660" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="8023" PostId="5290" Score="0" Text="@DanHulme&#xA;&#xA;However, Florian and you explained why $d(\omega) = \sin(\theta)d(\theta)d(\phi)$, am I right ?&#xA;&#xA;Let me ask the question:&#xA;**Why can we state that $p(\omega)d(\omega) = p(\theta,\phi)d(\theta)d(\phi)$ ? An equality that is in the book.** This is the equality I want to prove, because it would infer that $p(\omega) = p(\theta, \phi) \sin(\theta)$, the questions I asked in my post.&#xA;&#xA;**Is it because the _integrals_ (in the Dan's answer) are equals so their _integrands_ are equals ?**&#xA;I'm looking for a concrete, and rigorous proof of the equality." CreationDate="2017-07-01T14:56:50.787" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="8024" PostId="5290" Score="0" Text="@DanHulme To be clear, in th book they state that:&#xA;&#xA;$$p(\omega)d(\omega) = p(\theta, \phi)d(\theta)d(\phi)$$ so &#xA;$$p(\omega)\sin(\theta)d(\theta)d(\phi) = p(\theta, \phi)d(\theta)d(\phi)$$ so&#xA;$$p(\omega)\sin(\theta) = p(\theta, \phi)$$.&#xA;&#xA;And I can't rigorously demonstrate the first equality." CreationDate="2017-07-01T15:06:02.820" UserId="6862" ContentLicense="CC BY-SA 3.0" />
  <row Id="8025" PostId="5312" Score="0" Text="Can you edit your answer please?" CreationDate="2017-07-01T15:16:57.853" UserId="6844" ContentLicense="CC BY-SA 3.0" />
  <row Id="8026" PostId="5290" Score="1" Text="That wouldn't generally be true, but it is true here because of the extra constraint that the distribution is uniform. *Any* probability distribution has to integrate to 1, but this one also has the same value everywhere. (Or to put it another way, the two integrals have to be equal for *every* region you integrate over.)" CreationDate="2017-07-01T18:22:54.607" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8028" PostId="5311" Score="1" Text="Could you clarify what is meant by &quot;subpixel&quot; in a greyscale context? Is this just where a new section overlaps with a non-integer number of old pixels, and a proportional value is returned?" CreationDate="2017-07-02T08:01:11.137" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8029" PostId="5311" Score="0" Text="It seems worth checking whether you are only interested in this approach specifically, or whether you are just looking for the best approach to resizing. There are other approaches, and which is best will depend on the purpose of resizing and your priorities." CreationDate="2017-07-02T08:05:26.210" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8031" PostId="5315" Score="0" Text="That doesn't seem to work for me. In this [example](https://pastebin.com/dKC8xz4V), I tried to use these identifiers but with no success. I also added the source .mtl file and the code I was trying to run with the output for clarification." CreationDate="2017-07-03T06:35:01.510" UserId="2138" ContentLicense="CC BY-SA 3.0" />
  <row Id="8032" PostId="5315" Score="1" Text="Unfortunately I didn't test with a .mtl file, the information was from browsing the header files, the 2 macros I posted are the only references to those material parameters I could find so this could possibly be a incomplete feature in Assimp." CreationDate="2017-07-03T07:35:30.067" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8033" PostId="5317" Score="0" Text="I've no idea what that tool uses, but one approach might be to use seam carving. A quick internet search, for example, turned up this project report:  https://inst.eecs.berkeley.edu/~cs194-26/fa15/upload/files/projFinalUndergrad/cs194-dm/Alex_Liu_Report.pdf   Another possibility is to build textures via texture synthesis." CreationDate="2017-07-03T07:38:21.600" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8034" PostId="5315" Score="0" Text="That might very well be the case. I tried to find some document specifying which parameters are supported and which aren't, but no luck so far.&#xA;Would you know of any alternative way to define and load a material in an external file?" CreationDate="2017-07-03T07:43:53.253" UserId="2138" ContentLicense="CC BY-SA 3.0" />
  <row Id="8035" PostId="5311" Score="2" Text="Your description sounds like box filtering which while easy is not very good in terms of quality" CreationDate="2017-07-03T08:16:22.590" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8037" PostId="5311" Score="0" Text="@trichoplax: Your &quot;subpixel&quot; assumption is correct. *_new is not necessarily an integer quotient of *_old. And it's not just about resizing - I know there are better algorithms for that. Let's say that in my case each pixel value represents an amount of energy, and I want to know how much energy each pixel of a downsized image must emit when the sum of energy shall be constant." CreationDate="2017-07-03T11:57:31.477" UserId="6346" ContentLicense="CC BY-SA 3.0" />
  <row Id="8038" PostId="5312" Score="0" Text="Edited following the edit to the question." CreationDate="2017-07-03T12:26:08.583" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8040" PostId="5311" Score="0" Text="Unless you are only interested in this specific approach, it might help to edit the question to describe your underlying objectives, like the energy conservation you mentioned." CreationDate="2017-07-03T12:29:14.297" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8041" PostId="5311" Score="0" Text="+1 to joojaa's comment.  Also, if your grey scale data has come from a captured image, there's a good chance it may be non-linear, e.g. sRGB (Search also for Charles Poynton's gamma FAQ).  If it is, you should first linearise it, filter, and then de-linearise again." CreationDate="2017-07-03T12:33:20.120" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8042" PostId="5312" Score="1" Text="That's an hell of an edit. I wish I could upvote more." CreationDate="2017-07-03T12:34:43.807" UserId="6844" ContentLicense="CC BY-SA 3.0" />
  <row Id="8043" PostId="5317" Score="0" Text="That looks like a very interesting report. I'll make sure to check that out." CreationDate="2017-07-03T16:01:55.743" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8044" PostId="5311" Score="0" Text="@joojaa: I played a little with box filtering in Gimp, but have no idea what filter matrix could do the job. Actually it would have to sum all (sub)pixel grayscale values in every section, followed by maybe a simple nearest neighbor resize to maintain that values in the final image. The problem is that summing up needs different filter matrix values for each pixel, e.g. with resize factor 1/2,33: Horizontally: p_new[0] = p_old[0] + p_old[1] + 0.33*p_old[2],  p_new[1] = 0.67*p_old[2] + p_old[3] + 0.67*p_old[4],  p_new[3] = 0.33*p_old[4] + p_old[5] + p_old[6]" CreationDate="2017-07-04T07:33:24.757" UserId="6346" ContentLicense="CC BY-SA 3.0" />
  <row Id="8045" PostId="5311" Score="0" Text="Just do a usual resizing then divide by the area ratio." CreationDate="2017-07-04T10:24:59.870" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="8046" PostId="5319" Score="0" Text="Can you show us your shader?" CreationDate="2017-07-04T12:02:44.047" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8047" PostId="5319" Score="0" Text="Well, since the base colour of all the quads (or triangles) is white upon which I overlay the cloud feature, I didn't think I needed to do anything specific with shading. For the light model I've used GL_SMOOTH, if hat helps." CreationDate="2017-07-04T12:27:28.457" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="8048" PostId="5319" Score="0" Text="Well, there shouldn't be a light model at all. You're using the obsolete fixed-function shading, aren't you?" CreationDate="2017-07-04T12:52:03.037" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8049" PostId="5319" Score="0" Text="I *think* I may have figured out the problem. For common vertices between adjacent quads, I'm using different normals depending on the quad being rendered. I calculate these vertex normals  as the local cross product of the unit tangent vectors that follow from the bilinear parametrisation of each quad. If I average out these normals, then that *should* address the issue. I'll need to do some coding." CreationDate="2017-07-04T12:52:05.600" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="8050" PostId="5319" Score="0" Text="if you average them out, you'll get something that actually looks like an ellipsoid, but that shouldn't be necessary anyway for your use case." CreationDate="2017-07-04T12:53:30.110" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8051" PostId="5319" Score="0" Text="True,  but it would (hopefully!) eliminate the visible boundaries between adjacent quads due to the light model interacting with different normals at the common vertices of adjacent quads. What's encouraging is that when I disabled lighting, I confirmed that the quad boundaries were no longer visible, so back to coding it is for me!" CreationDate="2017-07-04T13:32:46.747" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="8052" PostId="5319" Score="0" Text="Well yes, disabling lighting is the right answer. It'll fix this problem as well as bias added by the lighting, which you haven't yet noticed. You should post that as an answer and then not worry about the normals at all." CreationDate="2017-07-04T13:35:42.290" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8053" PostId="5322" Score="1" Text="This projection is a 2d projection, with the Z axis pointing into your screen. When both vertices with the same X&amp;Y are projected to screen space they will occupy the same screen coordinate so your line will either be 1 pixel or invisible." CreationDate="2017-07-05T03:20:35.283" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8055" PostId="70" Score="1" Text="A cheap way to do this is simply use Gaussian blur on your final scene and store two of them. One for foreground and one for background. They can have the same blur level or different and it is up to you. Upon finishing the blur, simply lerp from depth 0 to some determined (and tweak-able) depth (say 0.10). What you are learning is this blurred scene to the perfectly in-focused scene. For the background blur you can simply do the same except you are blurring from perfect in-focus to the blurred image. Leave some room for the middle to create a perfectly in focus region." CreationDate="2017-07-05T03:55:46.300" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="8056" PostId="5026" Score="0" Text="In the link provided above it's Monotone cubic Hermite interpolation. The idea is to maintain the direction of tangent vectors. Any idea how to extend it for 3D case?" CreationDate="2017-07-05T06:40:47.247" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="8057" PostId="5026" Score="0" Text="@Bla...  So you dont actually want a monotone curve at all then? As you ar not defining what your going to be monotone about. Try a catmul-rom spline." CreationDate="2017-07-05T06:52:38.407" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8059" PostId="5311" Score="0" Text="@Rahul: What resize algorithm would you suggest? A &quot;usual&quot; resizing will calculate average values from neighboring pixels, but not sum-up." CreationDate="2017-07-05T07:41:53.563" UserId="6346" ContentLicense="CC BY-SA 3.0" />
  <row Id="8060" PostId="5322" Score="0" Text="@PaulHK You should write that as an answer. Please avoid writing answers as comments, because they might get lost. Comments are for temporary messages only." CreationDate="2017-07-05T07:52:22.293" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8061" PostId="5324" Score="0" Text="&quot;the dot product of this blur vector&quot; with what? dot product has two operands. For your question, don't forget the possibility that an error in the code itself is what's making the effect too strong." CreationDate="2017-07-05T07:54:32.630" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8063" PostId="5297" Score="0" Text="Thank you for your comment. I have installed the PDFcreator virtual printer, as you suggested. I have tried to print the 3D PDF directly to PNG, and it works well. The only drawback is that I have to give the 3D PDF image very large dimensions (height and width) to obtain a high resolution of the print. But it does work, and it allows me to obtain the PNG-images that I want." CreationDate="2017-07-05T09:07:48.270" UserId="6902" ContentLicense="CC BY-SA 3.0" />
  <row Id="8064" PostId="5327" Score="0" Text="Yes, 3 points in any dimension above 2 all lie on the same plane unless they are degenerate and lie on a line" CreationDate="2017-07-05T10:04:28.607" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8065" PostId="5328" Score="0" Text="Please check my maths, people. I haven't done this since I was an undergrad. :-)" CreationDate="2017-07-05T10:33:35.887" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8066" PostId="5328" Score="0" Text="Well yes i would just construct a affine matrix with a crossed and normalized 3 axis does much the same but with less thinking, and then just matrix multiply. Anyway your missing one normalize." CreationDate="2017-07-05T10:37:05.617" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8067" PostId="5328" Score="0" Text="Yeah, doing it by a matrix would have been simpler but maybe harder to understand. Thinking about each vector separately lets you approach the problem a piece at a time instead of all at once." CreationDate="2017-07-05T10:44:14.790" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8068" PostId="5327" Score="0" Text="Or even an infinite number of lines for the most degenerate case. (I'll show myself out)" CreationDate="2017-07-05T10:50:31.123" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8069" PostId="5328" Score="0" Text="I would say its easier to understand as it has a more straightforward visual interpretation. Assuming you know how affine matrices work" CreationDate="2017-07-05T10:57:24.770" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8070" PostId="5327" Score="1" Text="@SimonF and planes ;)" CreationDate="2017-07-05T11:00:05.600" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8071" PostId="5329" Score="0" Text="Then, how do you get the point A?" CreationDate="2017-07-05T11:14:45.263" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="8072" PostId="5328" Score="0" Text="$a$ is the point? I haven't fully understand it. So, what's next after I know the three basis vectors?" CreationDate="2017-07-05T11:20:49.957" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="8073" PostId="5328" Score="0" Text="Just to elaborate, initially I have the info for all four points. My goal is to find where point $A$ is when the other three points changing position and orientation." CreationDate="2017-07-05T11:23:34.937" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="8074" PostId="5329" Score="0" Text="Just to elaborate, initially I have the info for all four points. My goal is to find where point $A$ is when the other three points changing position and orientation." CreationDate="2017-07-05T11:23:40.433" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="8075" PostId="5324" Score="0" Text="Are you happy to show your code?" CreationDate="2017-07-05T11:38:47.997" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8076" PostId="5328" Score="0" Text="Ah, I think I understand your solution.." CreationDate="2017-07-05T11:39:59.103" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="8077" PostId="5329" Score="0" Text="Ah I think I understand your point." CreationDate="2017-07-05T11:40:22.087" UserId="2687" ContentLicense="CC BY-SA 3.0" />
  <row Id="8078" PostId="5332" Score="0" Text="Thank you! Both your answer and Michal's answer explained it to me very well. I'll have to remember this next time a code a shader." CreationDate="2017-07-05T16:23:00.687" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8079" PostId="5331" Score="1" Text="Thank you! Your answer as well as Dan's answer covered my question very well. I'll make sure to use this new piece of knowledge." CreationDate="2017-07-05T16:24:00.770" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8080" PostId="5324" Score="0" Text="@DanHulme, I'm using the dot product of the blur vector, so basically dot(blurVector, blurVector). This is basically the length squared to see if the blur vector is large enough." CreationDate="2017-07-05T17:09:16.700" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="8081" PostId="5330" Score="0" Text="Note that later in the article, wikipedia says &quot;When the color is represented as RGB values, as often is the case in computer graphics, this equation is typically modeled separately for R, G and B intensities, allowing different reflections constants k a , {\displaystyle k_{\text{a}},} k_{\text{a}}, k d {\displaystyle k_{\text{d}}} k_{\text{d}} and k s {\displaystyle k_{\text{s}}} k_{\text{s}} for the different color channels.&quot;" CreationDate="2017-07-06T00:46:04.340" UserId="6947" ContentLicense="CC BY-SA 3.0" />
  <row Id="8082" PostId="5334" Score="0" Text="Can you show what you've tried so far, what the results were, and what you were expecting? Would the above image be three or four lines, for example? (Or something else?)" CreationDate="2017-07-06T04:05:07.850" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8083" PostId="5335" Score="2" Text="Does it have to be a procedural texture? A photo of a shrink-wrapped black box would be the obvious solution." CreationDate="2017-07-06T08:48:18.147" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8084" PostId="5335" Score="0" Text="Yes, because I would like to generate thousands of unique textures. Thanks for the remark, I updated my question accordingly." CreationDate="2017-07-06T08:51:28.177" UserId="6951" ContentLicense="CC BY-SA 3.0" />
  <row Id="8085" PostId="5334" Score="0" Text="Points 1 and 2 together imply that no line will have a slope greater than 45 degrees above or below horizontal. Is this correct or do the definitions need to be adjusted?" CreationDate="2017-07-06T09:02:16.327" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8086" PostId="5334" Score="0" Text="What kind of noise does point 3 refer to? Does this only add points, or can points be removed/moved (which would then sometimes leave gaps between points in the line)?" CreationDate="2017-07-06T09:03:38.063" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8087" PostId="5334" Score="0" Text="We won't know for certain until you have fully specified what you are looking for, but it sounds likely that this is a far from trivial task, rather than you overcomplicating it." CreationDate="2017-07-06T09:09:27.053" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8088" PostId="5335" Score="1" Text="You'll want to generate a bump map/normal and use that for the reflective highlights. Getting the tension streaks to look realistic on the geometry is going to be challenging though." CreationDate="2017-07-06T09:24:07.760" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="8089" PostId="5335" Score="1" Text="I wonder if you took something like your original plasma texture and stretched it along a random axis and used the result as ratchet freak suggests above, if that would look decent?" CreationDate="2017-07-07T01:19:34.250" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8090" PostId="5200" Score="0" Text="From your additional description, it sounds like you just want `r = 1-c`, `g = 1-m` and `b = 1-y`, then work in RGB." CreationDate="2017-07-07T01:24:35.750" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8091" PostId="5324" Score="0" Text="Is the 'oval like artefact' because the blur is sampled in device coordinate space?" CreationDate="2017-07-07T06:01:10.907" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8093" PostId="5336" Score="1" Text="I notice that the example output is not minimal according to the definition (there exists an arrangement of 5 convex polygons that covers the mesh). Is this because it's just an example, or are there further requirements that prevent the 5 polygon output being valid? For instance, are the output polygons required to be in vertical strips of fixed width or is that just for the example image?" CreationDate="2017-07-07T11:18:53.900" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8095" PostId="5336" Score="0" Text="Thanks trichoplax. I've added the requested additional information to the question. Hope they clarify your questions." CreationDate="2017-07-07T11:41:53.843" UserId="4558" ContentLicense="CC BY-SA 3.0" />
  <row Id="8096" PostId="5336" Score="0" Text="I've identified the difference between the example output and the minimal one I was thinking of: Mine covers the area using convex polygons that use only the existing vertices, but do not use the existing triangles. The example output shows the minimal number of convex polygons if they are required to be constructed from the existing triangles. Is this a requirement?" CreationDate="2017-07-07T15:13:02.573" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8097" PostId="5336" Score="1" Text="No this is no requirement. The resulting polygons can be generated with any means available. The only requirement is, that the resulting shape is exactly the same as with triangles." CreationDate="2017-07-07T19:23:04.757" UserId="4558" ContentLicense="CC BY-SA 3.0" />
  <row Id="8098" PostId="5324" Score="0" Text="@PaulHK, what do you mean? NDC? I'm drawing the screen as a post-processing effect, meaning I'm drawing a square that's the size of the screen and with texture coordinate (0,0) being top left and (1, 1) being bottom right. Therefore the UV will be interpolated from top left to bottom right." CreationDate="2017-07-07T20:30:36.610" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="8101" PostId="5340" Score="0" Text="I have tried glFinish or glTextureBarrier, but still can't fix my problem.I think I should post this into AMD's community. Anyway, Thanks.@Nicol Bolas" CreationDate="2017-07-08T01:15:54.817" UserId="6660" ContentLicense="CC BY-SA 3.0" />
  <row Id="8102" PostId="5336" Score="0" Text="This problem is known as *convex decomposition*. See https://gamedev.stackexchange.com/q/53142, http://doc.cgal.org/latest/Partition_2/index.html#title2, http://masc.cs.gmu.edu/wiki/ACD" CreationDate="2017-07-08T05:02:54.800" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="8104" PostId="5324" Score="0" Text="Yes that's what I was thinking. The problem is that if you go, say, 0.01 in any direction, it will be stretched horizontally because it is not corrected for aspect. When sampling for a blur, you want to be in pixel space, or at least a aspect corrected version of NDC, e.g. vec2 samplePos = ndcPos + blurOffset * vec2(aspectRatio, 1);" CreationDate="2017-07-09T08:03:40.343" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8105" PostId="5307" Score="0" Text="Thanks for the comments. it shed light on how I got it wrong first." CreationDate="2017-07-09T09:22:15.863" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8106" PostId="5071" Score="0" Text="The same question was asked on Math. SE, and answered, here https://math.stackexchange.com/questions/2269849/join-two-bezier-curves-so-that-the-result-is-two-times-continuously-differentiab/2271518#2271518" CreationDate="2017-07-09T11:06:57.020" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8109" PostId="5335" Score="1" Text="I would suggest using the &quot;ridged&quot; version of FBM/Perlin noise to mimic the crests and branchiness you get with cling film. This can do done using something like 1-abs(FBM). visualisation of ridge FBM here :  http://accidentalintricacy.blogspot.hk/2009/04/base-landmass.html" CreationDate="2017-07-10T03:23:30.473" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8111" PostId="5348" Score="0" Text="Nice survey. Alexei Efros has some nice [slides on image blending](http://graphics.cs.cmu.edu/courses/15-463/2005_fall/www/Lectures/Pyramids.pdf) as well from his computational photography course. He mentions classical Burt-Adelson laplacian pyramid blending as well as gradient domain methods and graph cuts." CreationDate="2017-07-10T06:02:46.910" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="8115" PostId="5348" Score="0" Text="This is a very good answer, and I'm glad you took the time to go over multiple algorithms, rather than just one. I'll make sure to look at those links." CreationDate="2017-07-10T14:40:03.727" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8117" PostId="5343" Score="0" Text="I am only interested in description in analytic solutions though. I dont really care how complex they become i just need to see how pthers have approached the problem. It is easy to say it becomes hard but thats just waving hands." CreationDate="2017-07-10T17:13:35.593" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8118" PostId="5324" Score="0" Text="@PaulHK I will try that but I doubt it will change much. I'm already calculating everything in pixel space ... so I reconstruct the 3D position from uv and depth texture, then multiply by inverse view-projection matrix. I then do perspective divide myself after multiplying by previous view-projection matrix. This coordinate is then converted to texture space [0, 1]. I then use this to subtract uv in fragment shader to get the delta uv." CreationDate="2017-07-10T17:37:35.277" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="8119" PostId="5349" Score="0" Text="What are you requirements? Gaussian kernels are designed to blur without artifacts. A different kernel that blurs but isn't Gaussian is the box filter. It will produce artifacts however. I suppose more importantly, why isn't Gaussian good enough?" CreationDate="2017-07-10T20:26:09.630" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="8120" PostId="5309" Score="0" Text="Can't you simply convert the thickness into texture coordinates? I assume you know the dimension of your planar polygon and you know your starting point in world space? If so, simply offset from that point given your thickness, than transform it into UV coordinate. You can either multiple view-projection matrix and then divide by w (perspective divide) and then convert [-1, 1] to [0, 1] by times 0.5 and then add 0.5. This should get you a UV offset from the line start. You now have your thickness. You might want to do half the thickness and offset from center (the start)." CreationDate="2017-07-10T20:50:04.140" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="8121" PostId="5343" Score="0" Text="The hand-waving was meant to convince you that seeking analytic solutions is hopeless without making you read a lot of justification. But, I added some justification, anyway." CreationDate="2017-07-11T00:22:59.217" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8122" PostId="5343" Score="0" Text="Yes well now it is better however I was thinking one could solve this from the derivate." CreationDate="2017-07-11T06:37:20.880" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8123" PostId="5343" Score="0" Text="See a torus is much more difficult than what i am asking here. As its perfectly fine to eliminate all curves that self occlude." CreationDate="2017-07-11T07:07:57.153" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8124" PostId="5349" Score="1" Text="What kind of aliasing is present? Is it a synthetic image with staircasing/jaggies? Is it a texture with spurious low-frequency components (like a high-frequency grating or checkerboard)? Is it a photo that's been badly upscaled?" CreationDate="2017-07-11T11:07:24.513" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8125" PostId="5349" Score="0" Text="The image consist of Jaggies mostly ... like for instance, i have an image filled with Grass and I want to make sure that the grass blades are not too blurred." CreationDate="2017-07-11T11:22:44.190" UserId="6974" ContentLicense="CC BY-SA 3.0" />
  <row Id="8126" PostId="5343" Score="0" Text="Your example could easily be the inside region of a torus. Occluding or not occluding doesn't change the basic silhouette equation." CreationDate="2017-07-11T12:19:51.173" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8127" PostId="1984" Score="0" Text="I'm sure SolidWorks (or any system based on the Parasolid kernel) can do this correctly." CreationDate="2017-07-11T12:24:25.863" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8128" PostId="1983" Score="0" Text="@linguisticturn Have you tried contacting Hartley &amp;/or Zisserman for the additional details you require? A quick search brought up contact info for both." CreationDate="2017-07-11T18:24:20.763" UserId="5349" ContentLicense="CC BY-SA 3.0" />
  <row Id="8129" PostId="5354" Score="1" Text="You dont actually need a rotation at all, it is practical but not needed. Also singularities in euler angles is only a problem if you intend to interpolate stuff." CreationDate="2017-07-12T13:44:37.653" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8130" PostId="5354" Score="0" Text="Apparently we have different understandings of the word &quot;rotation&quot;. So I changed to use &quot;orientation&quot;, instead. Clearly you **do** need an orientation if you want to orient objects in 3D." CreationDate="2017-07-13T00:26:26.117" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8131" PostId="5356" Score="0" Text="I thought of edge detection, but when observing the same effect ingame in the terrain (which has rather large triangles), it is possible to see the same transition between a &quot;wide&quot; rim and the constant-width as the camera turns to a narrower angle. Also, what kind of edge detection could be used here? I actually tried using Sobel on depth, but then I have no idea how to adjust the &quot;width&quot; without blurring internal lines. (Though I imagine this should be a separate question)" CreationDate="2017-07-13T12:31:43.727" UserId="6986" ContentLicense="CC BY-SA 3.0" />
  <row Id="8132" PostId="5356" Score="0" Text="They probably distinguish characters and terrain (I thought the effect was absent on terrain, but maybe I just missed it). For Sobel, just scale the kernel to the thickness you want: instead of comparing the.adjacent pixels, compare the ones a bit further." CreationDate="2017-07-13T12:58:54.767" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8133" PostId="5356" Score="0" Text="Hmm I'll take another dab at Sobel, and report back" CreationDate="2017-07-13T23:07:59.447" UserId="6986" ContentLicense="CC BY-SA 3.0" />
  <row Id="8134" PostId="5355" Score="0" Text="Frankly, I like the look you've achieved better!" CreationDate="2017-07-14T01:16:29.610" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8135" PostId="5355" Score="0" Text="@user1118321 I would be satisfied too, if not for the fact that it silhouettes perfectly every minor problem with the models it is applied to." CreationDate="2017-07-14T01:21:51.130" UserId="6986" ContentLicense="CC BY-SA 3.0" />
  <row Id="8137" PostId="5358" Score="1" Text="I think you might get a better answer on [physics.se]. They have people who actually work with carbon nanotubes, and probably understand the optics better." CreationDate="2017-07-14T11:13:17.473" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8138" PostId="5358" Score="0" Text="That is possibly right, in fact, I initially wanted to post it there, but thought it was too CG specific. You are however welcome to migrate the question there." CreationDate="2017-07-14T11:15:21.050" UserId="6991" ContentLicense="CC BY-SA 3.0" />
  <row Id="8139" PostId="5358" Score="1" Text="I think it's actually on-topic for us, so I'm not keen to migrate it, but if other people vote to close it I will definitely migrate rather than leave it closed here." CreationDate="2017-07-14T12:40:15.320" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8140" PostId="5359" Score="1" Text="There isn't necessarily a conversion factor afterwards. If your `M` is a projection or a non-uniform scale, then the distance between a pair of points depends on where it was in the original space." CreationDate="2017-07-14T14:02:39.433" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8141" PostId="5361" Score="0" Text="Thank you Dan.. is there any link that you can suggest for me to look further in to this (like a code or paper ) ?" CreationDate="2017-07-14T18:18:29.553" UserId="6974" ContentLicense="CC BY-SA 3.0" />
  <row Id="8142" PostId="5358" Score="2" Text="I think &quot;what it looks like&quot; would depend highly on how the nanotubes are put together into a macroscopic material—like how long are the nanotubes? Are they aligned with each other, or tangled/braided, or just randomly oriented? What treatments have been done to the surface of the material, like polishing or coating, etc.?" CreationDate="2017-07-14T21:01:09.227" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8143" PostId="5361" Score="0" Text="This link explains how to get the frequency information of an image, which you could use to know how much aliasing there would be if you downsized the image: https://blog.demofox.org/2016/07/28/fourier-transform-and-inverse-of-images/" CreationDate="2017-07-14T22:44:27.483" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8144" PostId="5349" Score="0" Text="a sinc filter can downsample &quot;perfectly&quot; such that any details that would cause aliasing are removed, while other details are left perfectly alone.&#xA;https://en.wikipedia.org/wiki/Sinc_filter" CreationDate="2017-07-15T00:34:15.260" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8145" PostId="5362" Score="0" Text="Sounds like marketing mumbo-jumbo to me. I would assume &quot;slope-based&quot; means based on the gradient. Usually the gradient is generated from the luminance, though, so who knows?" CreationDate="2017-07-15T01:29:25.130" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8146" PostId="4099" Score="0" Text="Opinion based?  Puzzling. I don't think any of the items in my answer are &quot;opinions&quot;, except perhaps the statement in the last line." CreationDate="2017-07-15T03:17:03.337" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8147" PostId="5363" Score="0" Text="Yes, but the best way depends on what the particular constraint is, so you need to be more specific. What's the actual problem you're trying to solve?" CreationDate="2017-07-15T12:27:07.363" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8148" PostId="5363" Score="0" Text="Thanks @DanHulme, let's say, I would like to have it for first constrain. I am a novice in this area so probably, not even sure if I am looking in the right direction." CreationDate="2017-07-15T12:47:52.530" UserId="6994" ContentLicense="CC BY-SA 3.0" />
  <row Id="8149" PostId="5362" Score="0" Text="You make a good point. I wouldn't be surprised if what you said is true. I'll keep this question open just to see if anybody else has an idea of what it means." CreationDate="2017-07-15T14:13:38.683" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8150" PostId="5367" Score="0" Text="This question might be better on the [graphic design stack exchange](https://graphicdesign.stackexchange.com) (not positive though, so be sure to read their help section to see)." CreationDate="2017-07-16T05:04:01.300" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8151" PostId="5367" Score="0" Text="@user1118321 yeah it would fit on GD but im sure it can be here too." CreationDate="2017-07-16T10:43:18.293" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8152" PostId="5363" Score="0" Text="If you edit the question to show just a single question, it will be better received and it will be easier for people to write good answers. You can then post the other points as separate questions later." CreationDate="2017-07-16T16:12:35.407" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8154" PostId="5369" Score="0" Text="Size of file can be also quite expensive imagine your image gets downloaded a 10 million times due to getting popular then the difference between a 2 mega file and a 400kb file is 1700 \$ vs 340 \$ (using aws pricing)" CreationDate="2017-07-16T17:44:39.957" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8156" PostId="5369" Score="0" Text="S3 is currently ~\$0.023/GB, so \$450 vs \$90.. Not that I would still want to pay \$360 for nothing (:" CreationDate="2017-07-16T19:49:56.567" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="8157" PostId="5358" Score="0" Text="In some configuration CNT is used for Vantablack, so it would be quite a simple shader (:" CreationDate="2017-07-16T20:36:52.697" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="8158" PostId="5371" Score="0" Text="Great to see you active on this new Stack Exchange site dedicated to graphics." CreationDate="2017-07-16T20:37:42.823" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="8161" PostId="4099" Score="0" Text="@bubba The first question about places to learn is very much opinion-based as it can differ per person. The second question about open problems is very broad." CreationDate="2017-07-17T03:42:47.867" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="8162" PostId="5359" Score="0" Text="Just to clarify the original data is 3d imaging data in imager coordinates and the transformed data is in world coordinates. If there were no rotation, only scaling and translation I could work out the new pix/vox-&gt;mm conversion but because the volume is also rotated it makes it more difficult..." CreationDate="2017-07-17T09:36:08.893" UserId="6990" ContentLicense="CC BY-SA 3.0" />
  <row Id="8166" PostId="5378" Score="0" Text="Thanks for the answer and reference paper. I will try to implement the method proposed." CreationDate="2017-07-18T12:55:37.787" UserId="7013" ContentLicense="CC BY-SA 3.0" />
  <row Id="8167" PostId="5377" Score="2" Text="Assuming that barycentric approach does what I think it does it would be quite slow with large sets. Imagine a set of 9 million vertices with only 9 vertices in the desired set. Why iterate the entire set when v1, v2, and v3 give you all the information you need. The flood fill answer would be the fastest flexible solution. Although unflexible, if you can assume you have lines like you do now in the geometry then scanline would be the fastest approach." CreationDate="2017-07-19T05:25:46.620" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8168" PostId="5380" Score="0" Text="I'm not familiar with flood filling algorithm. Your explanation seems a little complicated to me. Could you please provide a decent reference to look at? Thanks." CreationDate="2017-07-19T05:47:27.480" UserId="7013" ContentLicense="CC BY-SA 3.0" />
  <row Id="8169" PostId="5377" Score="0" Text="You're absolutely right about performance issues. I'd like to use this approach in large meshes, so what I'm looking for is an efficient method. Actually I'm not familiar with neither flood filling nor scan fill algorithms, I'll take a look at them. Thanks." CreationDate="2017-07-19T05:52:26.147" UserId="7013" ContentLicense="CC BY-SA 3.0" />
  <row Id="8170" PostId="5377" Score="3" Text="A flood fill with a graph would start at a node, visit every neighbor node if boundary condition is met and not visited, mark it as visited, and repeat (recursion). Alteration: mark every node on path as visited and start from a node inside the set. Then simply use the visitation check as the boundary condition." CreationDate="2017-07-19T07:00:57.967" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8172" PostId="5377" Score="0" Text="Thanks for the detailed explanation. I find flood filling algo more reasonable, but I want to implement both flood fill and scan line, then compare the performances." CreationDate="2017-07-19T08:09:43.763" UserId="7013" ContentLicense="CC BY-SA 3.0" />
  <row Id="8173" PostId="5380" Score="0" Text="I got the solution by reading some. Thanks." CreationDate="2017-07-19T08:12:40.590" UserId="7013" ContentLicense="CC BY-SA 3.0" />
  <row Id="8178" PostId="5389" Score="0" Text="How are you currently accessing memory? Can you share code for that?" CreationDate="2017-07-20T03:58:55.177" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="8179" PostId="5389" Score="0" Text="This topic is big enough that books have been written on it. To get a good answer, you need to [edit] your question and make it more specific. But your software rasterizer will always be orders of magnitude slower than using OpenGL or DirectX." CreationDate="2017-07-20T08:21:47.120" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8181" PostId="5390" Score="2" Text="Sorry, how are you getting these values? Are you simply reading the depth buffer (which is a linear function of the inverse value of Z) value for each pixel?  If so, then for a plane, all the points should still lie in a plane if you map them (correctly) back into 3D.  (Yes the distance from the camera for each point will vary and you would get a curve, but you shouldn't be using that)" CreationDate="2017-07-20T08:36:33.043" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8182" PostId="5390" Score="1" Text="I'm exporting custom render passes out of the Sequencer. The specific pass is 'Scene Depth World Units'. The standard depth pass ('Scene Depth') doesn't give actual world Z position in correct units." CreationDate="2017-07-20T09:17:22.253" UserId="7023" ContentLicense="CC BY-SA 3.0" />
  <row Id="8184" PostId="5392" Score="3" Text="Mip mapping is also a good strategy to improve cache coherence in texture mapping." CreationDate="2017-07-20T13:26:09.640" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="8186" PostId="5389" Score="0" Text="@aces The backbuffer I write to is the buffer of the screen. It is represented as 3 chars for RGB in sequential order. I call it &quot;ibuffer&quot;. The texture RGB values are stored in an integer array where each integer is the chars RGBA.&#xA;&#xA;[code]&#xA;*ibuffer = mesh-&gt;texture-&gt;intbuffer[(int)textureX + mesh-&gt;texture-&gt;width * (int)textureY];&#xA;[\code]&#xA;&#xA;The integer I copy from the texture to the backbuffer overwrites the R value of the next pixel in the back buffer. I do this so as to not use bitwise operations used to extract the RGB values from texture memory and store in backbuffer." CreationDate="2017-07-20T16:05:47.067" UserId="7016" ContentLicense="CC BY-SA 3.0" />
  <row Id="8188" PostId="5389" Score="0" Text="A minor optimisation would be to use powers of 2 for your texture size, so you can compute the offset using a logical shift. E.g. if you have a 256 pix wide texture you can compute the texel offset with &quot;U + V &lt;&lt; 8&quot;. Also pad your RGB structure to 4 bytes instead of 3 so the compiler can apply a similar optimisation to the array indirection. You can also apply texture wrapping trivially if power of 2 size e.g.  (U &amp; 255) + (V &amp; 255) &lt;&lt; 8" CreationDate="2017-07-21T03:55:40.950" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8189" PostId="5392" Score="0" Text="@Stefan Indeed. Texture aliasing is painful in multiple ways :-).&#xA;&#xA;There's a bit more on the subject here: https://computergraphics.stackexchange.com/questions/357/is-using-many-texture-maps-bad-for-caching/419#419" CreationDate="2017-07-21T08:42:05.620" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8190" PostId="5396" Score="0" Text="Can you not just replace the &quot;Standard&quot; shader a copy in which the hack is fixed?" CreationDate="2017-07-21T10:35:33.103" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8191" PostId="5397" Score="0" Text="What OpenGL version are you using, and is it WebGL or a real program?" CreationDate="2017-07-21T14:13:34.777" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8192" PostId="5397" Score="1" Text="I'm using SDL2 to create a context for OpenGL 4.1. It's the regular version of OpenGL, not ES or WebGL" CreationDate="2017-07-21T15:00:07.990" UserId="7029" ContentLicense="CC BY-SA 3.0" />
  <row Id="8193" PostId="5389" Score="0" Text="@PaulHK I will try that Paul. Compared to my original statement, I found if I access the same texel for all interpolated pixels (*ibuffer = mesh-&gt;texture-&gt;intbuffer[10000]), the program framerate increases by 3 to 4 times. Do you have any idea why?" CreationDate="2017-07-21T16:42:22.820" UserId="7016" ContentLicense="CC BY-SA 3.0" />
  <row Id="8194" PostId="5398" Score="2" Text="Considering (0, 0) as the center of the screen. If we divide any point we want to place on that screen by z it will get infinitely closer to (0, 0). This creates that vanishing effect. Farther back it goes, the larger the z, the more it vanishes. This is essentially what perspective projection is doing despite all the confusing notation, matrices, and what not. It also handles all the other transformations. Certain scalings and translations must be applied to get a point into screen coordinates." CreationDate="2017-07-21T21:29:26.703" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8195" PostId="5397" Score="2" Text="In your `kernel` array you divide `float` by `int`. I suspect that for some reason your compiler cast this to `int`instead of `float`  so information is lost and then to `float`." CreationDate="2017-07-21T23:21:30.023" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="8196" PostId="5399" Score="0" Text="Indeed.  A simple operator to make HDR colors to LDR is a simplified reinhard operator which is this:  colorOut = colorIn / (colorIn + 1.0).  It's not the best, but it's easy.  It makes it so the color will never quite reach 1.0 no matter how large a number it is.  For more information, check this out: https://imdoingitwrong.wordpress.com/2010/08/19/why-reinhard-desaturates-my-blacks-3/" CreationDate="2017-07-22T00:34:48.260" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8197" PostId="5397" Score="0" Text="Why don't you use the numerators from the kernel and divide the whole thing by 16.0 at the end just to see if it works?" CreationDate="2017-07-22T01:56:57.510" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8198" PostId="5396" Score="0" Text="It seems like they multiply the BRDF by pi to maintain backwards compatibility with existing shaders using Blinn-Phong and their existing spherical harmonics system. With a bit more research I'm not sure this is the problem, more as Stefan says below, highlights will realistically be much brighter than diffuse light, and LDR just doesn't have the bandwidth for it. Looking at GGX renders with various material values on Google images, it seems to be an issue that goes beyond unity." CreationDate="2017-07-22T07:11:37.453" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8199" PostId="5399" Score="0" Text="Thanks for the suggestions guys. I guess what I need is something like a soft-knee compressor as used in audio processing - linear below a threshold and with a strong reinhard-like rolloff above it. Not sure what such a function looks like mathematically though. I'm gonna play around with it over the weekend and see what I can come up with." CreationDate="2017-07-22T07:16:09.427" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8200" PostId="5397" Score="0" Text="I've tried both adding `.0` to the divisions by 16 and removing the division altogether, and even changing the kernel to another post-proc effect, but it still gives me the same results.&#xA;For now, my solution is to just unravel the loop and write all the steps manually. Would that be a good approach or a bad practice in a professional shader?" CreationDate="2017-07-22T08:55:11.543" UserId="7029" ContentLicense="CC BY-SA 3.0" />
  <row Id="8201" PostId="5389" Score="0" Text="Using a constant index makes the address calculation a lot simpler, the compiler will calculate 1 pointer outside of your loops as opposed to recalculating it each pixel because it is using a none-constant index. Also accessing the same memory location will ensure it's in the CPU cache so you will get massive performance boost from that." CreationDate="2017-07-22T10:12:09.687" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8202" PostId="5400" Score="0" Text="Thanks for the answer. tell me If I am wrong here that we map frustum ranges to [-1,1] because OpenGL expects them to be in that range(because OpenGL performs clipping) so that clipping can occur properly. Am I correct?" CreationDate="2017-07-22T11:19:35.910" UserId="2096" ContentLicense="CC BY-SA 3.0" />
  <row Id="8203" PostId="5400" Score="1" Text="That's about right. The clipping occurs in automatic hardware on the GPU,  so it's broadly the same across different graphics APIs. Some specify -1 for the near plane and some specify 0. 0 is actually better for depth precision, for reasons Nathan Reed explains at http://reedbeta.com/blog/depth-precision-visualized/" CreationDate="2017-07-22T13:37:08.827" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8204" PostId="5403" Score="0" Text="Excellent answer.  After commenting out all of the enabled attributes, I am able to call `glDrawArrays` with no buffer / enabled attributes.  However, I can see that I *do* still need a VAO bound when I call glDrawArrays. Why is this? My current VAO has no attributes enabled at all so it seems like it wouldn't be necessary." CreationDate="2017-07-23T20:55:40.293" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="8205" PostId="5403" Score="2" Text="@Startec: Because that's the rule; you need a VAO bound for that state to exist. Even if the state is the empty default state, you still need vertex array state in order to render." CreationDate="2017-07-23T21:19:10.353" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8207" PostId="5404" Score="0" Text="https://math.stackexchange.com/questions/187107/calculate-coordinates-of-3rd-point-vertex-of-a-scalene-triangle-if-angles-and?rq=1" CreationDate="2017-07-24T09:12:59.793" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="8208" PostId="5404" Score="0" Text="I tried to implement this to my inputs but not succeed." CreationDate="2017-07-24T10:40:51.883" UserId="7039" ContentLicense="CC BY-SA 3.0" />
  <row Id="8209" PostId="5408" Score="1" Text="I think this question might do better on Stack Overflow than here, as your questions are very specific to using VBA in Excel and not much to do with graphics algorithms." CreationDate="2017-07-24T13:48:41.613" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8210" PostId="5408" Score="0" Text="Ah, I see you asked an earlier, less detailed version of the question there. You should probably edit *that* question to give as much detail as this one. But you should brace yourself for the possibility that Excel just can't do what you're asking." CreationDate="2017-07-24T13:51:07.137" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8211" PostId="5408" Score="0" Text="Yes, and in fact I've almost given up on a simple solution from within glut as I was hoping. The workarounds suggested all involve things like &quot;VB script worker threads&quot;  and the like that will take me on a gargantuan sidetrack from my key objectives, so very well, I suppose ..." CreationDate="2017-07-24T13:55:45.150" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="8212" PostId="5389" Score="0" Text="@PaulHK In my innermost for loop I am iterating over the pixels in the triangle and fetching the appropriate texels in a 1D array and storing in the back buffer:&#xA;for all pixels in triangle&#xA;     *ibuffer = mesh.texture.intbuffer[texcoord];&#xA;&#xA;If my texture is allocated on the heap, and the texture is being accessed vertically... I do not understand how the cache is filling up with unecessary values. I suppose if I tiled my texture, the cache would guess the next values and store them in registers and since it's tiled, those values would more likely be the correct ones to use." CreationDate="2017-07-24T22:41:16.973" UserId="7016" ContentLicense="CC BY-SA 3.0" />
  <row Id="8213" PostId="5397" Score="0" Text="I would find that odd, but if it works where a loop doesn't, go for it! It could be a bug in the glsl compiler or something. I'd add a comment explaining why you're doing." CreationDate="2017-07-25T04:23:04.013" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8214" PostId="5409" Score="1" Text="From a quick glance it looks like your application of Beer's law is correct. The image also looks reasonable to me. Can you clarify what you think is wrong, and why? Have you compared the same scene in another path tracer so you can tell if there are subtle differences?" CreationDate="2017-07-25T18:26:07.953" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8215" PostId="2450" Score="0" Text="joojaa do you have any more information on this technique? What is done in case of double curved free form surfaces? I'm also not sure what happens when you have a cone or a cylinder that is being cut/trimmed by something free form. https://stackoverflow.com/questions/43795262/silhouette-rendering-with-webgl-opengl" CreationDate="2017-07-25T20:35:13.490" UserId="5005" ContentLicense="CC BY-SA 3.0" />
  <row Id="8216" PostId="5411" Score="2" Text="I don't think there's a &quot;right&quot; way to &quot;strengthen&quot; normals. It's not clear what your intent is behind it. What kind of visual effect are you trying to accomplish here?" CreationDate="2017-07-26T14:24:40.757" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8217" PostId="5411" Score="0" Text="The desired  'effect' is a stronger bump.What?There are no standard techniques in CG to do that?" CreationDate="2017-07-26T14:26:41.970" UserId="213" ContentLicense="CC BY-SA 3.0" />
  <row Id="8218" PostId="5411" Score="1" Text="What do you mean by a &quot;stronger&quot; bump? Again, what do you want it to look like? Be specific. Generally speaking, bump mapping only effectively works for short bumps (relative to the viewing distance). For taller bumps, you need geometry to reproduce it convincingly. Or at the very least, parallax normal mapping." CreationDate="2017-07-26T14:31:58.400" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8219" PostId="5411" Score="0" Text="@NicolBolas I need an ability to adjust the strength similar to how it is done in any typical game engine.If you want an example, open Unity,drop a normal map on a surface and tweak its strength in the inspector.That's the effect. My code currently seems to produce similar effect. But,I wanted to know if there is a standard technique to do that because mine is based on trials&amp;errors." CreationDate="2017-07-26T14:35:09.953" UserId="213" ContentLicense="CC BY-SA 3.0" />
  <row Id="8220" PostId="5404" Score="0" Text="Please show what you tried to implement. Are you familiar with all the necessary mathematics used here? Can you solve this problem by hand?" CreationDate="2017-07-26T17:04:18.477" UserId="7055" ContentLicense="CC BY-SA 3.0" />
  <row Id="8221" PostId="5413" Score="0" Text="If you solved the problem yourself, please undelete the question and explain what fixed it. That will help anyone else with a similar problem." CreationDate="2017-07-27T08:58:46.333" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8222" PostId="5415" Score="0" Text="You probably need to use a separate pass to do this kind of effect." CreationDate="2017-07-27T09:07:36.637" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8227" PostId="5412" Score="2" Text="BTW, you don't need to normalize twice. :)" CreationDate="2017-07-27T20:27:41.710" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8228" PostId="5412" Score="0" Text="@Nathan: sure, but that's what the OP did. I would use a signed RG texture too, or a quternion based pipeline." CreationDate="2017-07-27T20:43:53.617" UserId="7057" ContentLicense="CC BY-SA 3.0" />
  <row Id="8229" PostId="5422" Score="0" Text="You went exactly to the point! I did not realize that the decision was being made on top of a different probability (i.e. 0.25 + 0.5 * Re). Now the scaling makes total sense. Actually, this framework seems to be quite flexible since the decision of which path to follow can be made from a probability that can be completely detached from the original one (e.g. P could be a fixed value) since RP and TP are recomputed to account for it. Obviously, some choices for P may have better justifications and may work better than others (as the one you have discussed above). Thank you Stefan!" CreationDate="2017-07-27T23:16:55.510" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="8231" PostId="5412" Score="0" Text="@ybungalobill but can you point out if there is a problem with my approach because it seems to me like that's pretty similar to what I do.Also,performance wise,I would prefer doing less ALU operations in the fragment shader(the target is mobile platform)" CreationDate="2017-07-28T10:15:26.930" UserId="213" ContentLicense="CC BY-SA 3.0" />
  <row Id="8232" PostId="5412" Score="1" Text="Oh, I missed that part. See, in principle it's the same. However, to get correct results you need to reorthogonalize the TBN per-fragment. Even without doing it, you can still improve the code performance wise. First, scale the normal maps so that z=1 and use `GL_RG8_SNORM` to store only the xy components. This already saves you half the memory bandwidth and the `n=2n-1` calculation in the shader. Then your function becomes simply `vec3 GetNormal(){ return v_TBN*normalize(vec3(texture(normalMap, uv).xy, C)); }`, and that's already with correct scaling." CreationDate="2017-07-28T10:27:01.087" UserId="7057" ContentLicense="CC BY-SA 3.0" />
  <row Id="8233" PostId="5429" Score="0" Text="There is a lack of `360 panorama`, `cubemap` (!), `codec` and other relevant tags on computergraphics.stackexchange.com." CreationDate="2017-07-28T13:40:33.397" UserId="5430" ContentLicense="CC BY-SA 3.0" />
  <row Id="8234" PostId="3928" Score="0" Text="Using  trigonometric functions has some costs. You could avoid using  trigonometric functions by directing multiplying complex numbers." CreationDate="2017-07-28T14:33:15.843" UserId="7066" ContentLicense="CC BY-SA 3.0" />
  <row Id="8235" PostId="5429" Score="0" Text="If you do not use the mobile interface then you can make new tags if you wish." CreationDate="2017-07-28T14:43:01.220" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8236" PostId="5432" Score="2" Text="How are you currently storing your mesh data? For example do you have a  half-edge data structure, or just vertex/index buffers, or something else?" CreationDate="2017-07-28T15:28:12.563" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8238" PostId="5430" Score="1" Text="Non-pyramid view frustum is not supported by rasterisators, but we use ray-tracer and it may have sense in our case. Also pixel density can be adjusted to be one-to-one on resulting flat video, but still pixel density is not uniform in case of octamap." CreationDate="2017-07-28T17:11:07.130" UserId="5430" ContentLicense="CC BY-SA 3.0" />
  <row Id="8239" PostId="5430" Score="0" Text="Currently &quot;3x2-unfolded&quot; video is used. The video looks seamless. No edges, no distortion on the first (and second) glance. Only four edges are glued and ten are onto the outer boundary. Currently used codec is randomly chosen and it is a luck, that there is no notable seams. (OpenGL supported) cubemap is a way to avoid geometry and filtering on glued and outer edges at all. It is interesting to try octamap, but presumably it have more distortion on the one hand and less glued and outer edges OTOH. Pixel to pixel mapping is impossible in case of non-cube Platonic solids." CreationDate="2017-07-28T17:17:19.707" UserId="5430" ContentLicense="CC BY-SA 3.0" />
  <row Id="8240" PostId="3928" Score="0" Text="@CroCo I don't think there are any cases where complex numbers save you from trig. You still need trig functions to get a unit complex number corresponding to a given angle, for instance." CreationDate="2017-07-28T17:54:24.460" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8241" PostId="5434" Score="0" Text="So, when writing to the texture, WebGL internally does something like `floor(x*255)` to map the [0,1] float value _x_ to [0,255]? And then later, when reading from that texture again, _x_ will probably deviate from the original value. Is that correct?&#xA;And in case of a `gl.FLOAT` texture, no mapping to [0,255] occurs?" CreationDate="2017-07-28T20:14:28.357" UserId="5406" ContentLicense="CC BY-SA 3.0" />
  <row Id="8242" PostId="5434" Score="2" Text="@Muad It would be `round(x*255)` instead of `floor`, but yes. And yeah, when you read it back, you'll likely get a slightly different value due to the rounding. In a float texture, it just stores and retrieves the exact value you write, no remapping (and no clamping to [0, 1])." CreationDate="2017-07-28T22:08:37.137" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8243" PostId="5431" Score="1" Text="You can also make rotation matrices for special angles like these by plugging values directly into the matrix. For example the matrix $\begin{bmatrix}1 &amp; -1 \\ 1 &amp; 1\end{bmatrix}$ implements the same rotation and scaling as the complex number $1 + 1i$. For general angles you still need trig functions, with either matrices or with complex numbers." CreationDate="2017-07-29T04:28:44.200" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8245" PostId="5436" Score="0" Text="What about when 2 vertices are outside the clipping area and the other 2 are inside? You can end up with a pentagon or hexagon inside. Or when 2 are outside one corner and the other 2 are outside the opposite corner?" CreationDate="2017-07-29T14:33:41.383" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8246" PostId="5436" Score="0" Text="Good point... I guess I have to cover these edge cases." CreationDate="2017-07-29T20:42:38.913" UserId="7068" ContentLicense="CC BY-SA 3.0" />
  <row Id="8247" PostId="5439" Score="0" Text="Thanks Clabe45 for the excellent suggestion. I'll give this a shot and revert if I run into issues or have more questions" CreationDate="2017-07-30T00:12:54.133" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="8248" PostId="5439" Score="0" Text="No problem. I apologize if I didn't answer directly since I don't really understand list objects." CreationDate="2017-07-30T00:37:55.227" UserId="6883" ContentLicense="CC BY-SA 3.0" />
  <row Id="8250" PostId="5443" Score="1" Text="You don't need GLU for that. You can just use straight OpenGL. According to [wikipedia](https://en.wikipedia.org/wiki/OpenGL_Utility_Library): &quot;The GLU specification was last updated in 1998, and it depends on features which were deprecated with the release of OpenGL 3.1 in 2009.&quot;" CreationDate="2017-07-31T05:24:23.717" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8251" PostId="5432" Score="0" Text="There is a similar question in stackoverflow which could help you: https://stackoverflow.com/questions/14108553/get-border-edges-of-mesh-in-winding-order" CreationDate="2017-07-31T07:04:36.717" UserId="7059" ContentLicense="CC BY-SA 3.0" />
  <row Id="8252" PostId="5436" Score="0" Text="@user1118321 although if you do the planes one by one you will just split the original quad and have a standard solution again." CreationDate="2017-08-01T13:45:28.613" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8253" PostId="5451" Score="0" Text="Alright, that makes sense. So post processing should usually be applied before gamma correction, so that the operations happen in linear space. Got it." CreationDate="2017-08-02T01:52:35.867" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8254" PostId="5440" Score="0" Text="I think it's because if you look at just the error it looks like noise, whereas error could be just a positive bias for instance." CreationDate="2017-08-02T02:42:21.190" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8255" PostId="5442" Score="0" Text="Could you maybe add an illustration or a reference picture of the effect you are trying to achieve?" CreationDate="2017-08-02T02:55:58.363" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8256" PostId="5295" Score="0" Text="@JulienGuertault Hi Julien. On A.2, what do you mean of &quot;render the mesh busing a depth buffer but not color buffer&quot;? Is that a known technique? Any references?" CreationDate="2017-08-02T17:11:06.080" UserId="4678" ContentLicense="CC BY-SA 3.0" />
  <row Id="8257" PostId="5436" Score="0" Text="@joojaa I'm not sure what you mean. What do you mean by standard solution?" CreationDate="2017-08-03T01:12:35.420" UserId="7068" ContentLicense="CC BY-SA 3.0" />
  <row Id="8258" PostId="5459" Score="0" Text="For instancing would that mean to use `glDrawElementsInstanced`?" CreationDate="2017-08-03T16:08:40.673" UserId="6883" ContentLicense="CC BY-SA 3.0" />
  <row Id="8259" PostId="5459" Score="0" Text="@clabe45 yeah or glDrawArraysInstanced" CreationDate="2017-08-03T16:10:56.333" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="8260" PostId="5451" Score="0" Text="@DanielKareh it depends on how you developed the tone map. If you developed it say in Photoshop from a  screen capture then it will obviously go last" CreationDate="2017-08-03T19:00:17.437" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8261" PostId="5295" Score="1" Text="@TinaJ Before rendering the mesh, you just disable writing to the color buffer. The depth will be written but the image will be left untouched. It's pretty standard, although the details will depend on the API or software you are using." CreationDate="2017-08-04T12:14:51.710" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8262" PostId="5462" Score="4" Text="Hi and welcome. Why would you use ray casting? Why not just dump the triangles to opengl and let it render with a rasterizer? Its not like you need reflections or have complex transparency sorting. Even your geometry is not very complex." CreationDate="2017-08-04T18:39:55.130" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8263" PostId="5462" Score="0" Text="Hi joojaa. I'm not very experienced with coding (I'm a mechanical engineer by trade); I know the basics to Java coding, but outside of using Java swing, I have no experience with computer graphics. What I'm getting about opengl is that it's to do with writing graphics code that takes better of advantage of the screen's hardware. If you could point me in the write direction, I'd really appreciate that. :) (I've been getting by using Java with Eclipse)" CreationDate="2017-08-04T19:42:56.700" UserId="7108" ContentLicense="CC BY-SA 3.0" />
  <row Id="8264" PostId="5462" Score="3" Text="So am I, a mechanical engineer that is. You should really take your time to go through some modern OpenGL (but not legacy opengl) tutorial to understand the concepts." CreationDate="2017-08-04T21:18:23.617" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8265" PostId="5456" Score="1" Text="Thank you for the long explanation. &quot;try it in your scene and if you then think it's too bright, lower the values until you're satisfied&quot;: The whole purpose of PBR is to not just look good, but follow the laws of physic to get the result. I am looking for (linear) RGB values and intensities, which I can use to display a scene with daylight, or with lamps. But at the moment I can not put lights in that scene, because I have no idea, which color and intensity values would be realistic in the `radiance = lightColor * intensity * attenuation` equation" CreationDate="2017-08-04T21:42:26.160" UserId="7028" ContentLicense="CC BY-SA 3.0" />
  <row Id="8267" PostId="5462" Score="3" Text="I second joojaa's comments. Give triangles to OpenGL. You're right that ray casting would be way to computationally expensive." CreationDate="2017-08-04T21:49:33.047" UserId="2941" ContentLicense="CC BY-SA 3.0" />
  <row Id="8268" PostId="5465" Score="0" Text="A triangle is simpler math wise. All 3 points are coplanar so all the points inside the are in the same plane which makes interpolation simpler. For quads I wouldn't know why to prefer that, unless it's to do with LoD." CreationDate="2017-08-04T22:12:19.133" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="8270" PostId="5464" Score="1" Text="To be clear—is your question how to _compute the coordinates_ of the intersection box without an if statement?" CreationDate="2017-08-05T00:10:06.090" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8271" PostId="5464" Score="0" Text="Also are you on cpu or gpu? What language? It can vary but min and max functions are probably the key component to answering your question, so long as they are branchless where you are using them. They are branchless in hlsl/glsl for what it's worth." CreationDate="2017-08-05T00:21:54.560" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8272" PostId="5467" Score="0" Text="You lost me somewhere. In the first paragraph, you mention finding the left-most, right-most, top and bottom *squares*. Aren't there only 2 squares? Then, in the second paragraph, you're talking about `(right.x, bottom.y)`, which sound like points, not rectangles. Can you clarify? I'm not understanding how the top-left position is `(right.x, bottom.y)` given what you've written above that." CreationDate="2017-08-05T04:05:18.163" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8273" PostId="5467" Score="0" Text="Okay, `top`, `bottom`, `left`, and `right` are references; `top` just references whatever one of the two is higher, and `bottom` the lower one. `left` references whatever one is to the left, and `right` the one to the right. So, for instance, `top` could be rectangle A, and `right` could be rectangle A also. I'll edit the answer to clarify about the coordinates." CreationDate="2017-08-05T12:02:56.740" UserId="6883" ContentLicense="CC BY-SA 3.0" />
  <row Id="8274" PostId="5471" Score="0" Text="I believe you're looking for the [Nyquist rate](https://en.wikipedia.org/wiki/Nyquist_rate). Also, he is very much attempting to rebuild a signal. That signal being a radial sine wave. This is very much a case of aliasing: the digital sampling of an analog signal with fewer samples than required to reconstruct it." CreationDate="2017-08-05T15:09:50.207" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8275" PostId="5471" Score="0" Text="Wow I never realized the nyquist limit applied to graphics (I'm an ex-sound engineer turned programmer) :) Can I ask more details (or a link) when you refer to shifting points toward the gradient maximum?" CreationDate="2017-08-05T15:35:13.690" UserId="5978" ContentLicense="CC BY-SA 3.0" />
  <row Id="8276" PostId="5471" Score="0" Text="@NicolBolas I meant he is just linear interpolating points not recosntructing the signal with a higher order fuction." CreationDate="2017-08-05T17:19:52.283" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8277" PostId="5471" Score="2" Text="@scx points in space are signals. Shader shards are signals. Antialiasing is exactly the same in sound as in images... etc. A lot of signal processing applies to images. Scaling, rotating etc. same things." CreationDate="2017-08-05T17:23:59.407" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8278" PostId="5460" Score="1" Text="Could you either include a link to the source of this formula, or define what the terms in it refer to?" CreationDate="2017-08-05T21:05:01.283" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8279" PostId="4917" Score="0" Text="As this question is being closed as a duplicate of the [first half](https://computergraphics.stackexchange.com/questions/4737/building-view-transform-matrices), I've copied and pasted the wording from here to there, with minor adjustment to make it fit as a second half. You may need to make minor edits if I've accidentally changed your intention." CreationDate="2017-08-05T21:33:42.117" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8280" PostId="4926" Score="0" Text="As this question is being closed as a duplicate of the [first half](https://computergraphics.stackexchange.com/questions/4737/building-view-transform-matrices), I've copied and pasted this answer from here to there, with minor adjustment to make it fit as a second half of your original answer there. You may need to make minor edits if I've accidentally changed your intention." CreationDate="2017-08-05T21:34:31.123" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8281" PostId="5464" Score="0" Text="Yes, how to compute (/find) the coordinates." CreationDate="2017-08-06T06:18:59.177" UserId="7111" ContentLicense="CC BY-SA 3.0" />
  <row Id="8282" PostId="5451" Score="0" Text="@DanielKareh Note though that even in programs like Photoshop you can change the color space" CreationDate="2017-08-06T08:30:24.187" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8283" PostId="5456" Score="0" Text="@IterAtor, sorry, I assumed you were looking to understand the rendering method rather than a translation of the real world to simulation (or as close as possible).&#xA;&#xA;As far as I can see it, the color (in PBR) is usually determined by the luminous intensity and the temperature. Chapter 4 in the Frostbite notes explains this. Note that this is perception based.&#xA;Another way to get the color is with CIE color matching and the spectral emission of the light.&#xA;&#xA;I really advice to look into the Frostbite notes, I don't think I can give you a better explanation than they delivered already." CreationDate="2017-08-06T08:45:06.650" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8284" PostId="5464" Score="0" Text="@AlanWolfe I'm using JavaScript in the web browser, so CPU. I had not heard of branchless min/max functions, but that seems to be the right direction to go in. Thanks." CreationDate="2017-08-06T09:27:17.337" UserId="7111" ContentLicense="CC BY-SA 3.0" />
  <row Id="8285" PostId="5467" Score="0" Text="Thanks for the breakdown, @clabe45. I'm using JavaScript, so now I'm researching if JavaScript's Math.max and Math.min functions are branchless." CreationDate="2017-08-06T09:29:59.600" UserId="7111" ContentLicense="CC BY-SA 3.0" />
  <row Id="8286" PostId="5470" Score="0" Text="glad loads function pointers. GLU includes utility functions for drawing. They're not comparable." CreationDate="2017-08-06T10:19:16.740" UserId="67" ContentLicense="CC BY-SA 3.0" />
  <row Id="8288" PostId="5465" Score="0" Text="@ratchetfreak if you begin form a parametric curve and then extend it into a planar definition you end up with something that has a four sided topology, Now as time has gone forward the same problem is still there." CreationDate="2017-08-06T14:00:40.690" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8289" PostId="5467" Score="2" Text="I'm curious why you care about it being branchless in JavaScript.  If you were doing this in a shader I can see why you'd want that (for efficiency), but branching is less of a problem on the CPU and JavaScript has much bigger perf issues than worrying about your code being branchless." CreationDate="2017-08-07T15:47:49.010" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8290" PostId="5479" Score="0" Text="Can you give some more context for what kind of correlation you're asking about? To define correlation you need two variables (eg &quot;correlation between $x$ and $y$&quot;). So—Monte Carlo samples with correlation between what and what?" CreationDate="2017-08-07T20:26:40.733" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8291" PostId="5479" Score="0" Text="For example in BDPT, I've read that there is path correlation between the different paths which are generated by connecting eye and light paths at different points, since the paths share vertices." CreationDate="2017-08-07T20:51:08.173" UserId="6367" ContentLicense="CC BY-SA 3.0" />
  <row Id="8292" PostId="5479" Score="0" Text="I'm curious about this too. I *think* it's when you reuse the same random number for two different things, or derive a new &quot;random number&quot; from an old one. It starts to make patterns that using independent random numbers wouldn't have. I'd like a more precise / formal explanation though too." CreationDate="2017-08-08T17:51:37.950" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8293" PostId="5480" Score="1" Text="Can you clarify your question? Just saying &quot;I'm confused&quot; doesn't give us much to go on. What do you understand already, what don't you understand, what specific questions do you have?" CreationDate="2017-08-08T22:22:48.520" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8294" PostId="5480" Score="0" Text="I don't understand what the question is." CreationDate="2017-08-09T01:11:22.820" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8295" PostId="5480" Score="0" Text="@NathanReed Question updated." CreationDate="2017-08-09T13:46:44.183" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="8296" PostId="5480" Score="1" Text="@JulienGuertault Question updated." CreationDate="2017-08-09T13:48:20.757" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="8297" PostId="5485" Score="0" Text="I think CGPP uses &quot;specular&quot; for sharp, mirror-like reflection due to a perfectly smooth surface, and &quot;glossy&quot; for blurred reflection due to a rough surface. I don't think that distinction has anything to do with the specular color or energy conservation. It sounds like you might be mixing this up with metallic vs dielectric materials (metallic = high specular with no diffuse; dielectric = low specular + diffuse)." CreationDate="2017-08-09T17:13:25.293" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8298" PostId="5485" Score="0" Text="no i know that, i wrote all that on the basis of CGPP's assumption. That's one way to think of it. what you said &quot;dielectric = low specular + diffuse&quot; that's what cgpp is referring to as glossy." CreationDate="2017-08-09T18:14:20.863" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8299" PostId="5485" Score="2" Text="No, dielectric != glossy. Glossy just means a blurry reflection due to a rough surface. Metals can have glossy or sharp reflection, and dielectrics can have glossy or sharp reflection. They're two independent axes of variation." CreationDate="2017-08-09T18:20:32.777" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8300" PostId="5485" Score="0" Text="I think i am bad at explaining and you are getting me wrong. I never said dielectric == glossy. What i mean is, dielectrics can be glossy if they have a specular componenet like you mentioned. I don't understand what do you mean by metals having &quot;sharp or glossy&quot; reflections. Reflection is either diffuse or specular or atleast that's what i have read in all different places. Glossiness or specular highlights is an effect of specular reflection." CreationDate="2017-08-09T19:29:56.147" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8301" PostId="5484" Score="0" Text="What's the difference between the two different phenomena - &quot;glossy reflection&quot; and &quot;specular reflection&quot;?" CreationDate="2017-08-10T03:56:54.830" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="8302" PostId="5484" Score="0" Text="I found another image." CreationDate="2017-08-10T03:58:40.957" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="8303" PostId="5485" Score="0" Text="https://en.wikipedia.org/wiki/Gloss_(optics)" CreationDate="2017-08-10T06:14:19.213" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="8304" PostId="5485" Score="0" Text="@chaosink - ? like i am saying there is no such thing as glossy reflection. Gloss is an effect due to specular reflection. Specular reflection is the correct word. The other type is the diffuse reflection. You can think of gloss as how much the light is reflected specularly. The higher the amount the more glossy the surface." CreationDate="2017-08-10T06:54:17.083" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8305" PostId="5484" Score="0" Text="There **aren't** two different phenomena. That's exactly what I just explained. Your third image is no different from the original two: it's just showing what effect the glossiness parameter has on the specular highlights." CreationDate="2017-08-10T08:42:29.773" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8306" PostId="5489" Score="2" Text="I've only seen something like this when someone has interpreted the bottom 8 bits of a 16- or 24-bit depth buffer as an image channel. How did you generate this image?" CreationDate="2017-08-10T10:38:25.367" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8307" PostId="5489" Score="0" Text="@DanHulme With the buffer visualiser: https://docs.unrealengine.com/latest/INT/Engine/UI/LevelEditor/Viewports/ViewModes/#buffervisualization" CreationDate="2017-08-10T10:44:15.077" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="8308" PostId="5484" Score="0" Text="I misunderstood your words..." CreationDate="2017-08-10T13:02:48.353" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="8309" PostId="5489" Score="0" Text="It could be a visualisation aid, instead of showing the depth as 1 gradient you can scale it to fit inside 3 steeper gradients. This is especially helpful for objects near the camera which have less variation in terms of depth." CreationDate="2017-08-11T02:43:03.517" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8310" PostId="5487" Score="0" Text="I haven't looked at it in years, but it might be worth looking through [ILM's source for OpenEXR](http://openexr.org/)." CreationDate="2017-08-11T05:12:00.790" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8311" PostId="5491" Score="0" Text="Are you doing a first-person-shooter style camera? Most games that implement this kind of camera restrict the x-axis rotation so you can never look straight up or down (clamped at maybe 85 degrees) to avoid exactly this problem." CreationDate="2017-08-11T07:06:21.977" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8312" PostId="5491" Score="0" Text="@russ its a CAD camera in my case, so it does need every view angle." CreationDate="2017-08-11T07:09:15.890" UserId="5646" ContentLicense="CC BY-SA 3.0" />
  <row Id="8313" PostId="5491" Score="0" Text="hmmm ok, then i guess storing a right vector to handle the edge cases would be one way to do it. either that or use a quaternion for the camera orientation and extract the matrix from that." CreationDate="2017-08-11T07:19:16.197" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8314" PostId="5491" Score="0" Text="@russ yeah i think quaternion is a nice solution. If i get how to do euler rotation on quaternion done, i will answer this question myself. THX" CreationDate="2017-08-11T07:24:06.873" UserId="5646" ContentLicense="CC BY-SA 3.0" />
  <row Id="8315" PostId="5489" Score="0" Text="@PaulHK yeah it could be, I mean, it actually repeats more than 3 times, it repeats indefinitely!" CreationDate="2017-08-11T08:32:20.533" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="8316" PostId="5489" Score="0" Text="the upper 2 bytes would have a resolution of 256*256 = 65536. If the depth buffer is 32 bits then that increases to 16777216. That may seem indefinite at first glance." CreationDate="2017-08-11T11:12:28.520" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="8317" PostId="5491" Score="0" Text="An alternate to quaternions is to describe the camera's rotation in spherical coordinates: one angle represents rotation around the vertical axis (0 to 360 degrees) and another angle represents how far you are looking down or up (-90 to +90).  If going that route you may be interested in this which talks about how to transform between spherical and cartesian coordinates so you can get vectors out: https://blog.demofox.org/2013/10/12/converting-to-and-from-polar-spherical-coordinates-made-easy/" CreationDate="2017-08-11T18:15:39.527" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8321" PostId="5494" Score="1" Text="&quot;*Since were close to hitting photorealism now it seems like the only main challenge for the next decade is performance.*&quot; Because God forbid that you would want a graphical effect that photorealism doesn't cover." CreationDate="2017-08-13T17:19:26.413" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8322" PostId="5496" Score="1" Text="Generally our customers decide what needs to be improved. They write up bugs or blog posts, or buy competitors products when ours don't meet their needs. Marketing figures out what the problems are from user feedback and passes it on to design. Design then decides how that should translate into product features. They discuss it with engineering who decides how it will be implemented." CreationDate="2017-08-13T18:52:28.907" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8323" PostId="5496" Score="0" Text="At this point in time what are some of the common issues if you don't mind me asking?" CreationDate="2017-08-13T19:14:53.880" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="8324" PostId="5496" Score="0" Text="Unfortunately, I can't go into details about not-yet-released features. But as mentioned above, we are in the process of moving from OpenGL to Metal, and we're always trying to take advantage of new hardware features." CreationDate="2017-08-13T22:17:46.917" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8325" PostId="5495" Score="0" Text="i can't get why this formula will change the bad situation. If w equals l, then c will still be zero vector? Because when these two vectors are parallel, there will only parallel part, and no orthogonal part." CreationDate="2017-08-14T06:41:19.960" UserId="5646" ContentLicense="CC BY-SA 3.0" />
  <row Id="8326" PostId="5495" Score="0" Text="Yes that's why i said, you have to check for this. If that's the case then you'll have to take another arbitrary vector. For example if &quot;j&quot; is the world up and it's parallel with look at, take &quot;i' or &quot;k&quot;. What i wanted to point out is, `world up` and `front` vector won't always be orthogonal. To make an orthogonal vector close to `world up` you have to use that formula" CreationDate="2017-08-14T07:23:56.220" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8327" PostId="5495" Score="0" Text="if just switch j to i, the the RIGHT vector will probably change a lot. And cross(front, worldUp) is used to compute right vector, the worldUp don't need to be orthogonal to front, just not parallel." CreationDate="2017-08-14T07:29:23.927" UserId="5646" ContentLicense="CC BY-SA 3.0" />
  <row Id="8328" PostId="5495" Score="0" Text="i think the world up needs to be orthogonal to the front, else you won't have an orthogonal basis for the camera?" CreationDate="2017-08-14T07:41:52.023" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8329" PostId="5495" Score="0" Text="for usual, figure out RIGHT by cross(front, woldUp) first, and then get real up by cross(front, right). That's why it's called world up but not camera up. Your idea make sense maybe, but i think it's not related to the problem i met here." CreationDate="2017-08-14T07:45:21.660" UserId="5646" ContentLicense="CC BY-SA 3.0" />
  <row Id="8330" PostId="5495" Score="0" Text="Well from what i have read in books either you choose an arbitrary vector to solve this issue or store the starting orientation in a matrix then apply a rotation matrix to change the orientation." CreationDate="2017-08-14T08:10:41.437" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8331" PostId="5497" Score="0" Text="Without even a stack trace of the crash it's unlikely that anyone will be able to help you." CreationDate="2017-08-14T10:34:56.990" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8332" PostId="5497" Score="0" Text="You are right, see edit." CreationDate="2017-08-14T11:28:22.650" UserId="7151" ContentLicense="CC BY-SA 3.0" />
  <row Id="8333" PostId="5497" Score="0" Text="Just checking: do you have the latest graphics drivers installed? Some googling suggests that this error is common with very long-running GPU compute jobs, and it may be (speculating) that there's some other timeout or bug inside the driver. Breaking up your job into smaller pieces may help (and would avoid the TDR problem too)." CreationDate="2017-08-14T21:13:46.577" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8334" PostId="5498" Score="0" Text="Would like to add that normal map is about bump mapping, it wouldn't be &quot;lighting&quot; the surface. Lighting equation does that and so check that if you aren't getting them lit." CreationDate="2017-08-15T06:29:18.053" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="8335" PostId="5500" Score="0" Text="Think of it like this which one looks more sharp one that is blurred more versus one that is blurred less." CreationDate="2017-08-15T07:47:49.067" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8336" PostId="5497" Score="0" Text="Yes, I have the latest driver installed. I thought about breaking it up into smaller parts but it cannot be easily done. Also, I would really like to solve this problem for its own sake. I have it on two computers using two different Nvidia cards. TDR is deactivated on both and I always get error code 2. I did a Google search myself and tried everything I found - nothing works (setting energy options to maximum, playing around with the other TDR registry keys). I would like to have the computation run for months in the end." CreationDate="2017-08-15T07:59:47.700" UserId="7151" ContentLicense="CC BY-SA 3.0" />
  <row Id="8337" PostId="5499" Score="0" Text="Thank you! I knew derivatives were the answer, just couldn't figure it out by myself. The vector way is really elegant and clever." CreationDate="2017-08-15T14:26:26.903" UserId="2064" ContentLicense="CC BY-SA 3.0" />
  <row Id="8338" PostId="5503" Score="0" Text="Yes, I thought of this after posting the question. And I'm trying to calculate $k_s$." CreationDate="2017-08-16T09:20:58.933" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="8339" PostId="5503" Score="0" Text="Wow, the link you give is amazing!" CreationDate="2017-08-16T09:22:03.830" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="8340" PostId="5501" Score="0" Text="Which filters do most monitors use?" CreationDate="2017-08-17T01:09:59.227" UserId="7154" ContentLicense="CC BY-SA 3.0" />
  <row Id="8341" PostId="5496" Score="0" Text="Which software are you making?" CreationDate="2017-08-17T12:48:22.863" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="8342" PostId="5496" Score="1" Text="I'd rather not say publicly, as I like to keep my personal and work lives a little separate." CreationDate="2017-08-18T01:00:02.323" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8343" PostId="5506" Score="2" Text="What kind of laptop does not have a GPU?" CreationDate="2017-08-18T04:33:08.273" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8344" PostId="5506" Score="3" Text="You need to profile your code to see where the problem is. Drawing 10,000 points should be no problem for even a low-end GPU. Are you actually uploading the data on every draw call? If so, don't do that. You're using `GL_STATIC_DRAW`, so it looks like your data is not changing. If that's the case, upload it once and draw it on every frame." CreationDate="2017-08-18T05:15:08.030" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8345" PostId="5506" Score="2" Text="Are you creating a new vertex buffer each frame? If so it would be more efficient to create it only once, then update its content when it changes. Nowadays, even a low end laptop with an integrated chipset can draw hundreds of thousands of points at a good framerate." CreationDate="2017-08-18T07:49:07.503" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8346" PostId="5506" Score="0" Text="@joojaa I assume he means it has an Intel integrated GPU instead of a beefier off-chip one." CreationDate="2017-08-18T08:27:09.030" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8347" PostId="5498" Score="0" Text="Couldn't you get the tangent by just doing a cross product between the normal and the main axis of your spherical coordinate system?" CreationDate="2017-08-18T08:57:32.860" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8348" PostId="5494" Score="0" Text="Disney's rendering team recently built a brand-new path tracing renderer from the ground up to make 'Big Hero 6' while the artists were already well underway animating the movie. See here https://www.disneyanimation.com/technology/innovations/hyperion" CreationDate="2017-08-18T09:19:43.433" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8349" PostId="5506" Score="1" Text="@DanHulme Thats my point its still a GPU! You will have hard timefinding laptops with really no GPU." CreationDate="2017-08-18T09:56:26.003" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8350" PostId="5498" Score="0" Text="@russ Yes, Nathan proposed exactly that on the accepted answer." CreationDate="2017-08-18T11:47:54.863" UserId="2064" ContentLicense="CC BY-SA 3.0" />
  <row Id="8353" PostId="5510" Score="0" Text="The image is loaded with Java's ImageIO.read, and it'll correctly load the transparency. I've attached pictures." CreationDate="2017-08-19T11:46:28.083" UserId="7177" ContentLicense="CC BY-SA 3.0" />
  <row Id="8355" PostId="5484" Score="0" Text="_The amount or brightness of the specular reflection_ - this is also called metalness, right?" CreationDate="2017-08-21T14:27:55.467" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="8356" PostId="5484" Score="0" Text="@narthex No, that's something different. You should ask a new question." CreationDate="2017-08-22T07:36:59.323" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8357" PostId="5498" Score="0" Text="whoops so he did, i didn't read that far down ;)" CreationDate="2017-08-23T05:13:09.937" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8358" PostId="5520" Score="0" Text="Without reading the code and just from the picture, my intuition would be to double check the integration. It looks like too much light is absorbed along the ray. I would first check that in the first implementation the color stays the same when changing the number of steps (just to make sure it's a good reference image), then I would double check the absorption equation used in the MT implementation." CreationDate="2017-08-23T07:22:37.037" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8361" PostId="5523" Score="0" Text="Nevermind. all 3d patch solutions are done by sphere tracking." CreationDate="2017-08-24T15:53:26.563" UserId="7199" ContentLicense="CC BY-SA 3.0" />
  <row Id="8362" PostId="5523" Score="0" Text="it just significantly increases performance and precision when the patch is simpler. to a point where a patch of seamess heavensines gets you very far in a few iterations:" CreationDate="2017-08-24T15:56:34.310" UserId="7199" ContentLicense="CC BY-SA 3.0" />
  <row Id="8363" PostId="5523" Score="0" Text="https://www.shadertoy.com/view/MtsXzl" CreationDate="2017-08-24T15:56:48.017" UserId="7199" ContentLicense="CC BY-SA 3.0" />
  <row Id="8365" PostId="5528" Score="3" Text="[Bilinear interpolation](http://reedbeta.com/blog/quadrilateral-interpolation-part-2/)?" CreationDate="2017-08-25T15:52:36.583" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="8366" PostId="5528" Score="0" Text="It does look like it from that link. In fact it's the first term that popped into my head but the initial Google hits seemed to be mostly about something that looked different to me." CreationDate="2017-08-25T16:11:49.413" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="8367" PostId="5506" Score="0" Text="@joojaa &quot;GPU&quot; is very common colloquial nomenclature for a dedicated graphics card, excluding integrated graphics processors. Don't be pedantic. :P" CreationDate="2017-08-25T17:28:24.590" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="8368" PostId="5506" Score="0" Text="@Dan well, most GPUs on laptops tend to be integrated to the motherboard. So does that mean laptops do not have them?" CreationDate="2017-08-25T18:01:34.797" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8369" PostId="5506" Score="0" Text="@joojaa It's vague, depends on the model of the GPU whether or not I would consider it worthy of being called a &quot;GPU&quot; in such a colloquial way as to imply that it e.g. has drivers which support OpenGL &gt; 1.1." CreationDate="2017-08-25T18:17:31.340" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="8370" PostId="5529" Score="0" Text="I'm not sure if anti-aliasing techniques would work for me. As far as I understood AA it aims to smooth these so called jaggies, so it works small-scale. But that's not my concern. My goal is to eliminate this 'large-scale' blocky appearance so that a great amount of neighboring pixels will change their color in order to manipulate the big picture." CreationDate="2017-08-25T21:58:52.317" UserId="5406" ContentLicense="CC BY-SA 3.0" />
  <row Id="8371" PostId="5524" Score="0" Text="Would blurring the image do what you want? It would produce a gradual transition between colors. Or do you want to retain the sharp edge between the colors, but remove the noisy small-scale details?" CreationDate="2017-08-26T00:19:39.417" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8372" PostId="5520" Score="0" Text="Thanks for your comment. Still not working and I do not understand why. I sample a point uniformly along the ray, instead of dividing it to segment, and the pdf is simply 1/dist. But for some reason it doesn't converge to same result." CreationDate="2017-08-26T09:21:48.233" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8373" PostId="5524" Score="0" Text="It's more the latter that I want. E.g., a highest 'degree of smoothing' could produce proper circles. So what I want is the boundaries between the different colored zones to be more straight or linear, respectively. Hope this helps clarify. If not, I will probably have to sketch it..." CreationDate="2017-08-26T10:04:15.380" UserId="5406" ContentLicense="CC BY-SA 3.0" />
  <row Id="8375" PostId="5524" Score="0" Text="If you want to preserve the discrete zone values while smoothing their boundaries, you could look into morphological opening and closing (faster and easier to implement) or level set curvature flow (more expensive but will converge to circles)." CreationDate="2017-08-27T18:23:24.117" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="8376" PostId="5529" Score="0" Text="@Muad Gaussian blur?" CreationDate="2017-08-27T19:01:48.067" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8379" PostId="5536" Score="0" Text="Welcome to the Computer Graphics Stack Exchange site! I am not familiar with Vulkan but it seems using an UBO would be more efficient in your case (you can try to compare the performances of both approaches in a sample program). Which graphic APIs are you familiar with? Also you might be interested by this GDC 2012 presentation: [Don't Throw it all Away: Efficient Buffer Management](http://gamedevs.org/uploads/efficient-buffer-management.pdf)" CreationDate="2017-08-28T13:29:55.270" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="8380" PostId="5536" Score="0" Text="From [Vulkan documentation](https://www.khronos.org/registry/vulkan/specs/1.0/html/vkspec.html#vkCmdPushConstants): while an UBO allocates a block of video memory on the GPU (that you can update at a later timing), the Push Constant does not use video memory (which is why it must be provided during every draw/compute call, or else the shader would not know which value to use). I guess it is stored in an other short-term, fast storage area on the GPU, though the details might depend on your GPU vendor." CreationDate="2017-08-28T13:44:31.957" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="8384" PostId="5538" Score="0" Text="The three visible faces aren't necessarily closer because the projection biases to +z." CreationDate="2017-08-29T13:36:19.227" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8387" PostId="5524" Score="0" Text="a reasonable approach would be blurring and then limit the amount of colours using k-means or a pallete" CreationDate="2017-08-30T06:21:11.183" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="8388" PostId="5518" Score="0" Text="Did you produce the black dot from the yellow dot image using thresholding? Looks pretty spot on to me? Or is that what you want to achieve? You could consider some sort of region growing technique? Like active contours?" CreationDate="2017-08-30T06:34:34.017" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8389" PostId="5518" Score="0" Text="I achieved this by manual thresholding-median filtering-noise reduction on ImageJ...I have thousands of these images and would like to get a generalized image processing procedure to implement a batch algorithm" CreationDate="2017-08-30T06:54:03.630" UserId="7185" ContentLicense="CC BY-SA 3.0" />
  <row Id="8390" PostId="5538" Score="0" Text="The visible faces are the faces that have cos(theta) &lt; 0 where theta is the angle between the faces normal and cameras direction. The dot product. You can use the cross product to get the faces normal. That doesn't tell you anything about how close the faces are to the cameras plane. You can use the GJK algorithm to find the minimum distance between the plane and cube." CreationDate="2017-08-30T07:08:09.553" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8394" PostId="5542" Score="0" Text="Thanks! I had seen that along the way, but thought it was for something else. One question though, is a stencil buffer all or nothing, or would it be possible to have some parts be more/less visible based on ie: distance from light source, overlapping light sources." CreationDate="2017-08-30T07:50:13.947" UserId="7202" ContentLicense="CC BY-SA 3.0" />
  <row Id="8395" PostId="5518" Score="0" Text="Theres a lot of papers on automatically thresholding images. A quick look at the images histogram should make it obvious where you should threshold." CreationDate="2017-08-30T08:14:14.963" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8396" PostId="5542" Score="1" Text="The value (in case of a 8bit stencil) can be anything between 0-255. There are also glStencilOp's like `GL_INCR` and `GL_DECR`. If 0=hidden, 1=half-visible, 2=visible, then you can draw a black quad on the screen with `GL_EQUAL` and `0`, and a half-transparent gray quad with `GL_EQUAL` and `1`. If you would like to draw continuous shadow gradients, then you have to use a shadow texture. But stencil buffers are still really useful even if you have a shadow texture, because the stencil test is much cheaper than a discard in the fragment shader" CreationDate="2017-08-30T09:00:48.577" UserId="7028" ContentLicense="CC BY-SA 3.0" />
  <row Id="8409" PostId="5453" Score="0" Text="The Colour of a 60w light source can be any colour, it depends on its wavelengths. For a white colour light source &quot;it cannot consist solely of the green light to which the eye's image-forming visual photoreceptors are most sensitive, but must include a generous mixture of red and blue wavelengths, to which they are much less sensitive.&quot;" CreationDate="2017-08-30T20:56:05.283" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8410" PostId="5542" Score="0" Text="Hm, so sounds like I definitely want a stencil buffer to mark off everything that's not lit up at all. However I would definitely like to be able to have gradient shadows or possibly even tinted shadows for various effects. So then I'd need a texture buffer for that? Or a frame buffer? Or are those the same thing?" CreationDate="2017-08-31T00:27:05.500" UserId="7202" ContentLicense="CC BY-SA 3.0" />
  <row Id="8412" PostId="5542" Score="0" Text="If you render to texture, you will use a frame buffer with only one color attachment. A framebuffer is an object, which contains depth+stencil buffer and color attachment(s). You already use the default framebuffer to render to the screen. But if you would like to render offscreen (to a texture) you have to create a new one for it. The texture is the color attachment in that case. I advise you to implement it with stencil buffers only, and if everything looks as intended, start experimenting with textures" CreationDate="2017-08-31T09:10:01.070" UserId="7028" ContentLicense="CC BY-SA 3.0" />
  <row Id="8415" PostId="5540" Score="0" Text="this is somewhat the same [question or atleast the answer is the same as this](https://computergraphics.stackexchange.com/questions/1887/how-to-implement-a-realtime-2d-light-renderer-with-fog-colored-light-on-the-gpu). the approach is essentially the inverse of the answer you got. Apparently the examples have undergone link rot so i need to see if i can find an alternate solution for this." CreationDate="2017-08-31T17:49:49.197" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8416" PostId="5520" Score="0" Text="The comment was directed toward people willing to propose answers. But thanks, it makes the question more interesting too. :)" CreationDate="2017-09-01T00:29:41.157" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8417" PostId="5539" Score="2" Text="It sounds vaguely like using [signed distance fields](https://www.youtube.com/watch?v=s8nFqwOho-s) to generate geometry. However I've never heard of &quot;folding&quot; in this context. I would guess it's a specific technique invented by the guy who did the talk, and if he hasn't published any more details then it may not be feasible to reproduce his results closely." CreationDate="2017-09-01T04:37:30.143" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8418" PostId="5547" Score="0" Text="Are you able to share your code?" CreationDate="2017-09-01T06:43:21.457" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8419" PostId="5547" Score="0" Text="I will try to make it readable later and then share it (or at least the important parts). It is (or should be) what they do in Physically Based Rendering Third (p.324), if by chance you have that book." CreationDate="2017-09-01T07:57:55.250" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8421" PostId="5547" Score="0" Text="No the result is probably not grayscale. What happens is there is a very sophisticated white balance in the system. I mean put tinted glasses on and pretty soon you nolonger see the tint." CreationDate="2017-09-01T17:19:56.747" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8422" PostId="5540" Score="0" Text="With one lightsource they may be similar but that approach wouldn't work for multiple light sources, which is something I want to be able to do." CreationDate="2017-09-01T18:22:14.967" UserId="7202" ContentLicense="CC BY-SA 3.0" />
  <row Id="8423" PostId="5549" Score="0" Text="But what exactly is the smoothstep doing? How will it change the outcome?" CreationDate="2017-09-01T19:25:41.983" UserId="6546" ContentLicense="CC BY-SA 3.0" />
  <row Id="8424" PostId="5549" Score="1" Text="Smoothstep is just a hermite curve (a 3rd degree polynomial) that smoothly varies between 0 and 1. Rather than a straight line, it ramps up slowly at first, then faster, then eases up at the end. So it looks sort of like an integral symbol if you map it from 0 to 1. Like this: ∫. You can read [the wikipedia article](https://en.wikipedia.org/wiki/Smoothstep), which has a graph." CreationDate="2017-09-01T20:38:00.733" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8425" PostId="5540" Score="0" Text="It would work with as many light sources as you like. Just render multiple times." CreationDate="2017-09-01T21:57:46.740" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8426" PostId="5557" Score="4" Text="It looks to me like the RGB data from the first image is being interpreted as Y'CbCr data in the second image. Since the Y' value goes from -0.5 to +0.5, 0.0 is a greenish color rather than black." CreationDate="2017-09-02T01:59:57.313" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8427" PostId="5542" Score="0" Text="Manged to get the stencil version up and running, everything looks pretty good. Added checks for winding on the line segments so you can see the object casting the shadow as well, which makes for a nice effect. How do I tell OpenGL which frame buffer to draw to? Do I need a different shader program for that, or do I just use the same one like for stencil drawing? And for stencil drawing, would it be more efficient to use a specific program that doesn't take color data for that draw call, or does that not matter?" CreationDate="2017-09-02T03:59:07.033" UserId="7202" ContentLicense="CC BY-SA 3.0" />
  <row Id="8428" PostId="5542" Score="0" Text="You don't have to change the shaders if you render to texture instead of &quot;directly&quot; to the screen. Just tell OpenGL before drawing that instead of the default framebuffer, use the one you created. Read a tutorial about rendering to textures, and If you have further questions about that, please open a new question." CreationDate="2017-09-02T08:58:02.757" UserId="7028" ContentLicense="CC BY-SA 3.0" />
  <row Id="8429" PostId="5547" Score="0" Text="@joojaa thanks but I was looking more for a proof, i.e. someone who has a working spectrum-&gt;rgb converter who would calculate the rgb response you get for using an $ior = 1.52 \iff reflection intensity = 0.04257999496$ on the wavelengths $390-780nm$" CreationDate="2017-09-02T13:46:49.693" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8432" PostId="5513" Score="0" Text="Why AB instead of BA?  Presumably because the polygon is being defined in anticlockwise order and so all edges are labelled accordingly. In fact, it is important to keep things consistent, especially when creating the edge equations. This is to avoid 'holes' or 'double filling' of pixels when handling &quot;tie-break&quot; cases when the centre of a pixel lies 'exactly' on an edge." CreationDate="2017-09-04T08:31:27.517" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8434" PostId="5567" Score="0" Text="That's a good suggestion for clarity. Even so, if they misuse terms or write obscurely, it'll hurt their credibility in the paper, so it's wise to check even if they include the diagram." CreationDate="2017-09-05T07:46:21.357" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8435" PostId="5567" Score="0" Text="I see your point" CreationDate="2017-09-05T08:18:29.610" UserId="7250" ContentLicense="CC BY-SA 3.0" />
  <row Id="8436" PostId="5539" Score="0" Text="I have seen the TED talk now, and I really want to know how it is done. This must be a known techinque?" CreationDate="2017-09-05T08:20:13.873" UserId="7250" ContentLicense="CC BY-SA 3.0" />
  <row Id="8437" PostId="5571" Score="2" Text="I guess the author actually computes *inverse* orientation in `orientation` because he calls this `orientation` in `Camera::view` and then `Camera::matrix` without transpose or inversion of it. And because `glfwGetCursorPos` will give you flipped Y coordinates, the `orientation` does compute the inverse of orientation coincidentally. However X coordinates are not flipped, which should cause problem but I cannot explain why it does not." CreationDate="2017-09-05T17:02:46.930" UserId="120" ContentLicense="CC BY-SA 3.0" />
  <row Id="8438" PostId="5572" Score="0" Text="Great answer, thank you. I work on games but using Unity so no source code :( I was curious about what goes on under the hood. Sounds like just as much of a PITA as I was imagining with the older APIs" CreationDate="2017-09-06T01:06:21.503" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8441" PostId="5568" Score="0" Text="Using a transform matrix for UVs is super clever! I'll have to refactor my system a little, since I was (happily) using only 1 uv, but your answer seems like a good solution. Thank you!" CreationDate="2017-09-06T01:33:06.573" UserId="5978" ContentLicense="CC BY-SA 3.0" />
  <row Id="8445" PostId="5575" Score="2" Text="The code make a lot of sense now that I've read your answer. I had a hunch that this is it but couldn't quite figure it out. Thank you!&#xA;&#xA;While looking into the issue I have also stumbled across this article that explains in greater detail what you wrote: https://www.3dgep.com/understanding-the-view-matrix/. Maybe someone else will find it useful as well." CreationDate="2017-09-06T16:02:37.553" UserId="7255" ContentLicense="CC BY-SA 3.0" />
  <row Id="8448" PostId="5578" Score="1" Text="The incoming direction woW(coming from eye) is filipped as in pbrt &quot;the incoming and outgoing vectors direction are outward facing, after being transformed into local coordinate system, at the surface&quot;. It's just pbrt convention to ease the calculation." CreationDate="2017-09-07T03:54:43.683" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8449" PostId="5578" Score="0" Text="@ali Please post that as an answer. Comments are for seeking clarification, not for answering the question." CreationDate="2017-09-07T08:44:10.873" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8456" PostId="5585" Score="0" Text="I hadn't heard the term light tracing before.  For other folks who haven't either, it seems to just be path tracing from the light towards the screen, vs the normal screen towards light.  Similar to photon mapping, but without the photon?" CreationDate="2017-09-09T00:24:43.157" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8457" PostId="5585" Score="0" Text="Yes it is forward ray tracing as opposed to backward ray tracing. It traces importance, rather than radiance, from the light source, bouncing it around the scene and connecting each vertex to camera." CreationDate="2017-09-09T00:28:23.307" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8458" PostId="5585" Score="2" Text="Have you considered using fresnel to figure out how much light is reflected vs transmitted, then using that value as a percent chance to do diffuse or specular lighting?  If specular, since you are asking about mirror like specular only, you just reflect the vector against the surface normal.  If that is making sense, if wanting to go beyond mirror like specular reflection, you could use eg a microfacet specular BRDF instead later." CreationDate="2017-09-09T00:36:18.123" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8459" PostId="5585" Score="0" Text="Thanks but I guess it's impossible to simulate this sort of path in forward ray tracing; since an incoming ray from light to a specular surface point it has zero probability of going out towards the camera as it has a delta function." CreationDate="2017-09-09T01:43:43.203" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8460" PostId="5585" Score="2" Text="Darn, hrm. Maybe the problem is you can't have pure mirror specular reflection, but need a specular lobe that isn't a delta function spike." CreationDate="2017-09-09T03:42:22.727" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8461" PostId="5587" Score="0" Text="How did you make the diagram?" CreationDate="2017-09-09T18:34:01.570" UserId="7272" ContentLicense="CC BY-SA 3.0" />
  <row Id="8462" PostId="5587" Score="0" Text="I used Processing, good for quick and dirty sketches like this :)" CreationDate="2017-09-10T10:33:54.303" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8463" PostId="5587" Score="0" Text="Athough nearly anything could have worked. You could have used almost anykind of toolset to do this in less than 5 minutes. You could have used a editor like inkscape or illustrator, you could have written postscript, svg, TeX or javascript in a browser even webGL in that time." CreationDate="2017-09-10T13:31:55.677" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8464" PostId="5585" Score="2" Text="This will only work if your camera is not a pinhole but simulates a non-zero sized aperture and a film. Then you can continue the reflect light path through the aperture and record it once it hits the film plane." CreationDate="2017-09-11T06:30:36.190" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="8466" PostId="5594" Score="1" Text="So  what is your question?" CreationDate="2017-09-11T15:20:39.057" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8467" PostId="5594" Score="0" Text="Mainly, I'm wondering if I'm missing something obvious in this scenario. Also, I'm quite surprised that cheating way looks much better than proper way." CreationDate="2017-09-11T16:10:43.727" UserId="7282" ContentLicense="CC BY-SA 3.0" />
  <row Id="8470" PostId="5600" Score="0" Text="You should probably include what your using as reference as its hard to tell." CreationDate="2017-09-13T04:42:12.373" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8474" PostId="5600" Score="0" Text="In my experience, i,j,k typically imply mutually orthogonal unit *vectors*." CreationDate="2017-09-13T12:38:58.807" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8475" PostId="5600" Score="0" Text="@SimonF they do not have to be though there areplenty of reasons to transform them." CreationDate="2017-09-13T13:09:51.340" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8477" PostId="2003" Score="0" Text="I'm looking for the same thing. Did you get what you wanted?" CreationDate="2017-09-13T14:47:50.217" UserId="7299" ContentLicense="CC BY-SA 3.0" />
  <row Id="8478" PostId="5301" Score="1" Text="I´d treat the cylinder as a &quot;light source&quot;, putting its geometry as a shader uniform and test fragment position against the cylinder, clipping fragments inside. Not sure if it is feasible for you or even interesting but in case you or others are stuck I´m just saying there is a way :-)" CreationDate="2017-09-13T18:08:29.897" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8479" PostId="5301" Score="1" Text="FYI: you have not enabled depth test" CreationDate="2017-09-13T18:10:15.700" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8485" PostId="5607" Score="0" Text="That could work. IMO it will require too much extra work and maybe the performance won't be that great! Reversed depth FTW :3" CreationDate="2017-09-14T13:53:26.893" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="8486" PostId="5600" Score="0" Text="I am thinking mainly about 3d space for games. I have the feeling that the basis vectors are the vectors which show the orientation of the space. On wikipedia, the 2 images on the right seems to show this. But still, I am not sure that it is the main purpose of basis vectors. Also, does orthogonal mean at 90 degrees? As an example, in 2d, does (1,0) and (0,1) are orthogonals? And (0.7, 0) and (0, 1) are not? The wiki page : https://en.wikipedia.org/wiki/Basis_(linear_algebra)" CreationDate="2017-09-14T23:18:29.780" UserId="7291" ContentLicense="CC BY-SA 3.0" />
  <row Id="8490" PostId="5619" Score="0" Text="Also you can get near n, or atleast less than n log n, with bucket sort and it is trivial to do. Even better if you begin with a BSP/occ-tree you can even get it to be less than N as you get cameara occlusion culling for free." CreationDate="2017-09-15T13:03:41.230" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8491" PostId="5619" Score="0" Text="BSP and octrees are kinda overrated for real-time graphics work. All the pointer chasing required is detrimental for cache performance." CreationDate="2017-09-15T13:13:26.673" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="8492" PostId="5619" Score="0" Text="Not necceserily if your buckets are bigish then you divide the work, but still have the benfits of lists with very few jumps" CreationDate="2017-09-15T13:25:01.780" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8493" PostId="5619" Score="0" Text="For me, the &quot;ubershader&quot; is the magic sauce here and is a good point. Thanks!" CreationDate="2017-09-15T13:42:43.687" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8495" PostId="5619" Score="1" Text="If you have a huge amount of particles you can also sort them on the GPU with a compute shader using bitonic sort. It's n log n (i think) but parallelized so super fast. There's a working implementation in the old DirectX 2010 SDK." CreationDate="2017-09-16T11:54:40.797" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8496" PostId="5625" Score="1" Text="Personally I don't think you need a better explanation for Raytracing, you need one for rasterization because it's somewhat opposite to what happens naturally. Realistically speaking Raytracing is how we see objects, Just instead of rays coming into our eyes we shoot them from the eyes outward ( if we are talking about backward raytracing) and apply all the physics of light etc. Hence one of the reason we get so realistic images" CreationDate="2017-09-16T16:36:02.807" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8497" PostId="5625" Score="6" Text="Use pictures. Thing is raytracing may be one of the simplest concepts to get. With good enough sequence of pictures it becomes clear enough." CreationDate="2017-09-16T17:42:29.927" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8498" PostId="5630" Score="0" Text="My problem solved by disabling texture2d before drawing colored triangles. Also I did use GL.Finish() after reading pixels." CreationDate="2017-09-17T10:21:43.073" UserId="7298" ContentLicense="CC BY-SA 3.0" />
  <row Id="8499" PostId="5628" Score="0" Text="If I understand correctly what you are trying to do, then, it is possible to do with FBO. Just create rendertexture instead of renderbuffer for color attachment (and call appropriate bind function). Then you'll be able to use this render texture as shader input." CreationDate="2017-09-18T10:47:51.960" UserId="43" ContentLicense="CC BY-SA 3.0" />
  <row Id="8500" PostId="5594" Score="0" Text="What if you try to normalize also before passing your calculated normals to glNormalXXX(). As documentation says: &quot;normal vectors are normalized to unit length AFTER transformation and before lighting&quot;. So in cases when interpolated normal has really small length (may happen during interpolation if C and K normals look in opposite directions) it may be distorted even more after transformation. This may happen in both interpolation schemes, but could be that effect is more visible if using quadratic interpolation" CreationDate="2017-09-18T11:02:10.743" UserId="43" ContentLicense="CC BY-SA 3.0" />
  <row Id="8501" PostId="5628" Score="0" Text="No i was thinking that i could use the FBO to display the texture image on the window. Like i was thinking if the texture image is already set in the compute shader, then the FBO it's attached to would just display it in the window if i make it default, but i don't think there's any command for that." CreationDate="2017-09-18T13:10:48.140" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8503" PostId="5628" Score="0" Text="I guess internally opengl keeps a texture buffer for the default framebuffer. It's just the API does not allow you to consider it as a texture, so a draw/copy operation is needed." CreationDate="2017-09-19T07:28:56.473" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8504" PostId="5632" Score="2" Text="Hello, welcome to CGSE. It's difficult to infer the context of your question when it is just referring to someone's answer. Could you please quote and/or link the relevant discussion in your question?" CreationDate="2017-09-19T08:39:26.140" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8505" PostId="5632" Score="0" Text="please kindly click this link:  https://computergraphics.stackexchange.com/questions/1718/what-is-the-simplest-way-to-compute-principal-curvature-for-a-mesh-triangle/1719#1719?newreg=1059615fd16d40dcbcbc73980ae075de" CreationDate="2017-09-19T10:52:46.210" UserId="7335" ContentLicense="CC BY-SA 3.0" />
  <row Id="8506" PostId="5633" Score="0" Text="Because the question is old  I´ll ask you instead :-) For what I understand simply having adjencency information for each vertex was not good enough for russ?" CreationDate="2017-09-19T15:34:17.740" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8509" PostId="5628" Score="0" Text="Yes that's what i was thinking. I thought opengl provided the functionality to use the FBO as a default one to display the texture directly attached but it doesn't i guess. They provided FBO for offscreen rendering only i guess for help with things like getting the reflection texture etc." CreationDate="2017-09-19T19:07:02.510" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8510" PostId="5634" Score="0" Text="Thank you! (I solved this a while ago and posted the solution just now when I saw your comment.)" CreationDate="2017-09-20T11:41:34.707" UserId="4714" ContentLicense="CC BY-SA 3.0" />
  <row Id="8511" PostId="5632" Score="1" Text="I've extended my original answer with more explanation about the derivation of the formula. I think this question can be closed." CreationDate="2017-09-21T05:34:08.210" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8513" PostId="5639" Score="0" Text="Thanks this is very helpful! Great point about the non-object state values too! I do agree that giving the client more responsibility over the rendering process might be beneficial as they would be able to adapt to each situation more properly. But how would I handle things like depth sorting without having one system that manages/contains all the renderable instances? (the throw-all-things-at-a-wall approach)" CreationDate="2017-09-21T09:14:00.470" UserId="5960" ContentLicense="CC BY-SA 3.0" />
  <row Id="8515" PostId="5638" Score="0" Text="I'll make an answer with details specific to this question after i get it working, but from what i've heard, the answer is to use a high pass filter.  More info regarding high pass filters in graphical applications here: https://www.gamasutra.com/view/feature/3073/the_power_of_the_high_pass_filter.php?print=1" CreationDate="2017-09-22T20:07:46.780" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8516" PostId="5640" Score="0" Text="Can you explain a few things? What's your color scale? Are you using the full hue range? (For example 0° = red, 60° = yellow, 120° = green, 180° = cyan, 360° = red?) Also, what makes you think what you're seeing isn't correct or on the right track? Do you know how many particles were used in the &quot;goal&quot; picture? Given what you said about the colors for small `N`, it seems feasible that by using a larger `N` you might get coloring more like the goal. But I'm just speculating." CreationDate="2017-09-23T16:18:26.253" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8517" PostId="5640" Score="0" Text="I am using HSV colors where each parameter is between 0 and 1. I fix S and V to be 1 and H corresponds to the magnitude of the force. Then I convert HSV colors to RGB and multiply each parameter by 255 because pygame display only accepts RGB colors in this format." CreationDate="2017-09-23T19:12:45.643" UserId="7363" ContentLicense="CC BY-SA 3.0" />
  <row Id="8518" PostId="5640" Score="0" Text="What I mean is are you using the full Hue range from 0° to 360°? If so, you might want to limit it to either 0-180° or 0-270°. Otherwise, things with very large force look the same as things with very little force, since the hues wrap around at 360°." CreationDate="2017-09-23T19:15:19.577" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8519" PostId="5640" Score="0" Text="@user1118321: you might be right that the colors in my sim look weird because I don't have the same conditions as in the &quot;goal&quot; sim. We are both using 10,000 particles, but my initial conditions are randomized positions and velocities, which will never produce a galaxy like the one shown above. We need to give the system some initial angular momentum to get a rotating galaxy. So I will see what happens by using proper initial conditions. I was just wondering if there was anything else I should be considering w.r.t the color scaling, or anything I'm doing that is obviously wrong." CreationDate="2017-09-23T19:15:21.003" UserId="7363" ContentLicense="CC BY-SA 3.0" />
  <row Id="8520" PostId="5638" Score="0" Text="two related links:&#xA;http://hhoppe.com/proj/hatching/&#xA;and&#xA;http://www.floored.com/blog/2014sketch-rendering/" CreationDate="2017-09-24T14:38:01.087" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8521" PostId="5640" Score="0" Text="@user1118321: as I said, I use HSV colors where each parameter is from 0-1 instead of 0-360. If I set S and V to 1, and let H vary between 0 and 1, there is not this problem of wrapping around. H=0 gives red while H=1 gives green." CreationDate="2017-09-24T17:53:39.803" UserId="7363" ContentLicense="CC BY-SA 3.0" />
  <row Id="8522" PostId="5640" Score="0" Text="OK cool. I'm used to positive Hue going from red through yellow, to green, but I saw you had cyan, blue, and magenta in your image, so I was confused. But I think I get it now." CreationDate="2017-09-24T18:24:25.960" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8523" PostId="5643" Score="2" Text="&quot;I think these are NaNs, will double check, but if you know common scenarios when this happens it'd be appreciated.&quot; The common scenario is that you're using floating-point and you have a bug in your code :-) You have to expect that you'll get NaNs during development, so you should either (a) make sure they get filtered out instead of propagated by the filter, or (b) show all NaNs as bright pink so it's immediately obvious when they're there." CreationDate="2017-09-25T15:26:35.457" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8524" PostId="5633" Score="0" Text="This was for my thesis project last year, I ended up just going with the dumb  way and using integer atomic adds, scaled way up to maximize precision, then normalizing to float vectors. Couldn't figure out a way to list faces round each vertex without allocating worst-case space and using atomic counters to build the lists anyway. It's probably inefficient as hell but I still got a couple of orders of magnitude speedup from the CPU version and a first-class mark so I was happy enough with it :)" CreationDate="2017-09-26T07:23:48.047" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8525" PostId="5647" Score="2" Text="TBH, the time needed to save out an image file is insignificant compared to 5 seconds. So, I'd just save out the images and stitch them together into a video later, if that's the easiest thing to do. Another possibility is to [stream the frames out to ffmpeg](https://computergraphics.stackexchange.com/a/5627/48) (answer is for C++, but the same idea should be adaptable to Python)." CreationDate="2017-09-27T01:19:36.940" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8527" PostId="5645" Score="0" Text="Unfortunately in my case, there is no ground truth image available. We cannot get these images due to some issues related to Field of View of the images. &#xA;**Is there any other way we can use the results and do the comparisons against each other ?**" CreationDate="2017-09-27T08:45:25.103" UserId="6974" ContentLicense="CC BY-SA 3.0" />
  <row Id="8528" PostId="5645" Score="0" Text="WRT to generating the Ground Truth, make sure you don't down sample with a box filter. You could to 10k samples per pixel and still have aliasing." CreationDate="2017-09-27T09:33:39.870" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8529" PostId="5640" Score="0" Text="What do you mean by &quot;properly&quot; or &quot;realistic&quot;? It's not at all realistic to colour things according to the force acting on them. There are many choices of colour map and the answer depends on what your goal is for using colour." CreationDate="2017-09-27T10:12:09.300" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8530" PostId="5647" Score="0" Text="@NathanReed You should post that as an answer." CreationDate="2017-09-27T10:14:08.267" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8531" PostId="5640" Score="0" Text="@DanHulme: By realistic I mean a realistic representation of the forces acting on the bodies. I want to be able to see some sort of gradation where the force gradually increases as we move closer to the center of the galaxy, like the second image shown." CreationDate="2017-09-27T10:54:45.613" UserId="7363" ContentLicense="CC BY-SA 3.0" />
  <row Id="8532" PostId="5645" Score="0" Text="There may be some way but I don't know of one. The problem with aliasing is that information is lost, which is hard to detect after the fact." CreationDate="2017-09-27T14:51:14.017" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8533" PostId="5645" Score="0" Text="I'm surprised you can't get a ground truth due to a field of view setting.  Can you excplain?  Anyways, another idea that may or may not work out is to use the image you have as a super sampled ground truth image: shrink it down to like 1/4 size or smaller using good filtering to minimize aliasing.  Next, take the source image and shrink it down to the same size using bad filtering that causes aliasing. next use your AA techniques and compare them against the ground truth you have.  Another option that may work is to use a different image to test AA on, one you can get a ground truth for." CreationDate="2017-09-27T18:05:07.690" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8534" PostId="5647" Score="0" Text="Possible duplicate of [OpenGL animation - turn into mp4 movie](https://computergraphics.stackexchange.com/questions/5606/opengl-animation-turn-into-mp4-movie)" CreationDate="2017-09-27T18:55:02.617" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8535" PostId="5647" Score="0" Text="@DanHulme TBH, I think this question can probably be marked as a dupe of the one I linked. :)" CreationDate="2017-09-27T18:55:25.780" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8536" PostId="5647" Score="1" Text="@NathanReed I'm inclined not. The other question seems to be about turning an apitrace recording into a video. That doesn't have the same concern about keeping a fixed time-step. Also, this questioner has some misconceptions about performance, which need addressing directly." CreationDate="2017-09-28T07:28:53.187" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8537" PostId="5653" Score="0" Text="Can you elaborate on this a bit? As far as i can tell, the GL methods for loading data to buffers (glBufferData and glBufferSubData) just copy a contiguous block of bytes across. Is it possible to somehow get a pointer into GPU memory and write your own copy routine?" CreationDate="2017-09-28T07:48:35.363" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8538" PostId="5653" Score="0" Text="Ahh of course, you can map the buffer then copy directly. Derp :-/" CreationDate="2017-09-28T08:54:21.313" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8539" PostId="5599" Score="0" Text="You could try using the latest version of DirectXMath available at [Github](https://github.com/Microsoft/DirectXMath) and see if the problem persists." CreationDate="2017-09-29T16:37:51.593" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8540" PostId="5599" Score="0" Text="Is there actually some info at compile or runtime about the crash?" CreationDate="2017-09-29T16:39:27.067" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8541" PostId="5658" Score="0" Text="Initially, I thought the same. However, for images larger than 2x2 this doesn't hold. This is because of the condition −1≤xsi,ysi≤1−1≤xis,yis≤1. I have added an example at the end of my post to clarify this." CreationDate="2017-09-30T11:45:22.007" UserId="7390" ContentLicense="CC BY-SA 3.0" />
  <row Id="8543" PostId="5397" Score="0" Text="Minor optimization unrelated to your problem (but also circumventing it): your specific kernel can as well be achieved with just 4 texture accesses and an average of those 4 values. Just use a linearly filtered sampler and move the four texture coordinates by half a texel into the four corners of the current texel. You'll find that with normal linear filtering this will give you exactly the weighting specified in `kernel`." CreationDate="2017-09-30T15:52:44.333" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8546" PostId="5596" Score="0" Text="Cross x and y and you will get your z axis assuming x and y are perpendicular" CreationDate="2017-09-30T18:56:15.260" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="8551" PostId="5657" Score="0" Text="Where does this -0.833 come from in your example? Obviously if you sample outside the original image only the two border pixels will be closest. You have to express the sample points in the same co-ordinate system you're using for the pixels in the old image." CreationDate="2017-10-01T10:42:52.390" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8554" PostId="369" Score="0" Text="As it happens I just recently implemented this algorithm (in C++) and it's really not that difficult, I found a much better short explanation than the one on Wikipedia. I'll see if I can come up with a pseudocode answer that hopefully adresses your concerns. But then again, I also had the luxury of one of the polygons being a triangle." CreationDate="2017-10-01T14:25:48.753" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8555" PostId="1995" Score="1" Text="If you figured it out yourself, you might want to answer you own question then." CreationDate="2017-10-01T16:16:01.787" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8556" PostId="5303" Score="0" Text="Are you sure that the algorithm *needs* a tight exact AABB rather than just being satisfied with an approximation of triangles by boxes? I only gave it a short glimpse, but it seem that function is only used for finding the optimal splitting plane. It might not necessarily require a perfectly tight triangle box for that when the worst that could happen would just be a less than optimal splitting plane." CreationDate="2017-10-01T16:33:47.813" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8557" PostId="5059" Score="0" Text="@Dolda2000 But don't they in turn (theoretically) lack all the other things *apart* from compute shaders, like all the features related to rasterization and whatever graphics stuff?" CreationDate="2017-10-01T18:06:38.220" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8558" PostId="5010" Score="0" Text="Why do you *need* voxels to begin with? Can't you just render the points themselves (possibly with some adaptive point size)?" CreationDate="2017-10-01T20:00:14.843" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8559" PostId="4345" Score="0" Text="If you solved your problem, feel free to answer your own question." CreationDate="2017-10-01T20:09:36.783" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8560" PostId="5659" Score="1" Text="You can, however, make smart pointers work by specializing `std::allocator` for any aligned data type. This will also allow you to put your stuff into standard library containers, which is not a particularly rare use-case. Basically whenever you overload `new/delete` you should always remember to make your own allocator (or better, specialize the standard allocator). But Solution 2 might still be the most headache-free anyway." CreationDate="2017-10-01T20:37:04.210" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8562" PostId="5059" Score="0" Text="@ChristianRau: I don't know, that's why I'm asking. :) That being said, though, I'm not aware of any rasterization features that Vulkan requires which haven't been in OpenGL for a long time." CreationDate="2017-10-02T00:54:25.097" UserId="6567" ContentLicense="CC BY-SA 3.0" />
  <row Id="8563" PostId="5657" Score="0" Text="I am sorry for the incomplete addition to my question. Given that I am new, I cannot add another image to explain the values. You can view the image I was planning on putting in the following link https://www.dropbox.com/s/4jmcangptz1lftu/bilint_derivation11.png?dl=0. I will add this image to the question when my reputation is sufficient to include it." CreationDate="2017-10-02T07:14:33.637" UserId="7390" ContentLicense="CC BY-SA 3.0" />
  <row Id="8564" PostId="5657" Score="0" Text="The $x_s$ and $y_s$ in that image are clearly in [-1,1] co-ordinates, not [0,W) co-ordinates. Obviously to make the subtraction work they both have to be in the same co-ordinate system." CreationDate="2017-10-02T09:21:43.913" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8566" PostId="5657" Score="0" Text="In both cases the equation doesn't add up." CreationDate="2017-10-02T15:10:12.387" UserId="7390" ContentLicense="CC BY-SA 3.0" />
  <row Id="8572" PostId="1502" Score="0" Text="If the model matrix is made of translation, rotation and scale, you don't need to do inverse transpose to calculate normal matrix. Simply divide the normal by squared scale and multiply by model matrix and we are done. You can extend that to any matrix with perpendicular axes, just calculate squared scale for each axes of the matrix you are using instead. I wrote the details in my blog: https://lxjk.github.io/2017/10/01/Stop-Using-Normal-Matrix.html" CreationDate="2017-10-01T23:52:53.853" UserId="7400" ContentLicense="CC BY-SA 3.0" />
  <row Id="8585" PostId="4345" Score="0" Text="Thinking it over, doing this correctly would have required a full 3D model and a physical model of the weight distribution. Doing it manually by rotating the model in 3D-space until it looked straight was easier." CreationDate="2017-10-03T07:41:26.037" UserId="5603" ContentLicense="CC BY-SA 3.0" />
  <row Id="8589" PostId="5665" Score="1" Text="Here's an interesting extension to storing look up tables in textures. It (ab)uses N-linear texture interpolation to get higher order interpolation (aka better than linear) of data points, surfaces, volumes, and hypervolumes.&#xA;https://blog.demofox.org/2016/02/22/gpu-texture-sampler-bezier-curve-evaluation/" CreationDate="2017-10-03T17:14:18.673" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8591" PostId="5676" Score="0" Text="So you want to place a bunch of small shapes of one type (same size?) inside a larger shape? Its not possible to fill the shapes in the images with circles completely with no overlap." CreationDate="2017-10-04T03:18:11.033" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8592" PostId="5676" Score="0" Text="Same sized polygon/circle, there'd be a fixed distance between them, and it's fine to have partial shapes near the boundary." CreationDate="2017-10-04T03:49:54.190" UserId="7413" ContentLicense="CC BY-SA 3.0" />
  <row Id="8593" PostId="5676" Score="1" Text="For circles you'd hexagonally pack them to get them as tight as can be. Then clip ones that lie on boundaries. This wouldn't vary with regions or circle size. If all you want to do is fill these shapes with color then forget the rectangles/circles and just scanline them in." CreationDate="2017-10-04T03:57:30.467" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8594" PostId="1503" Score="0" Text="Normals are also so-called pseudovectors. As a generalization and rule of thumb, everything resulting from a cross product (e.g. planes) will be transformed in a similar fashion." CreationDate="2017-10-04T17:03:53.123" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8595" PostId="2246" Score="0" Text="Related: [Image downscale algorithm](https://stackoverflow.com/questions/31873215/image-downscale-algorithm)" CreationDate="2017-10-04T20:10:50.367" UserId="7421" ContentLicense="CC BY-SA 3.0" />
  <row Id="8596" PostId="3576" Score="2" Text="@MobyDisk - actually, the C++ analogy is right on, though &quot;C vs Java&quot; or &quot;C vs C#&quot; might be a better comparison.  Vulkan is a **lower-level** API than OpenGL, not a next-gen replacement. According to Khronos, they are &quot;complementary&quot;.  Eventually I expect we'll see an OpenGL-inspired layer on top of Vulkan. **That** would be a next-gen successor, if it successfully combined performance-gains with higher-level API calls for programmer convenience." CreationDate="2017-10-04T20:52:22.547" UserId="7422" ContentLicense="CC BY-SA 3.0" />
  <row Id="8597" PostId="3579" Score="0" Text="@DanHulme - I'm surprised by your comment &quot;GL encourages you to think in an immediate-mode style&quot;. I thought that was only true of *early* OpenGL versions?" CreationDate="2017-10-04T21:07:04.937" UserId="7422" ContentLicense="CC BY-SA 3.0" />
  <row Id="8600" PostId="5681" Score="1" Text="That could be a nice approach, I think we don't have inconsistencies on that aspect, but as I said, there are quite a few allocs and deallocs. And yeah, I have to start doing some profiling but I really think that this could be a potential improvement :). Thanks!" CreationDate="2017-10-05T08:40:44.750" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="8601" PostId="3579" Score="1" Text="@ToolmakerSteve The OpenGL WG has put a lot of work into adding concurrency and batched writes, so you can express your program in a way that's suitable for tiling/deferred implementations. But it's still a foundation that was set in the immediate-mode age, which is why those people decided a fresh start in Vulkan was needed. And I still see a lot of new users starting with GL and forming an immediate-mode mental model of how implementations work." CreationDate="2017-10-05T09:06:28.180" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8602" PostId="5687" Score="0" Text="How does this technique compare to the very easy to generate [Halton sequence](https://en.wikipedia.org/wiki/Halton_sequence)?" CreationDate="2017-10-05T22:05:30.717" UserId="3470" ContentLicense="CC BY-SA 3.0" />
  <row Id="8603" PostId="5687" Score="1" Text="It's a lot closer to ideal blue noise. Worth another question probably!" CreationDate="2017-10-05T22:52:08.610" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8604" PostId="5684" Score="1" Text="It's not entirely clear from your description and image what you're asking. Can you clarify? What does the image show? The sun's rays will hit every surface of (the outside of) the house on any given day, most likely. (Unless I suppose if a side is exactly parallel to the path of the sun, or the building is at an extreme latitude.) What do you want to color?" CreationDate="2017-10-06T02:57:12.357" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8611" PostId="5684" Score="2" Text="There's two ways i can think of going about this. The first is that this case is basically a ray vs tube test where the tube follows the 3d path of the sun. Maybe you can look for (or ask for) ray vs &quot;tube&quot; intersections. The second idea is to calculate the closest point on the ray to the path of the sun (the line), and if that is closer than the radius of the sun, count it as a hit, and do whatever else you need to do from there." CreationDate="2017-10-06T16:35:59.667" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8612" PostId="5684" Score="0" Text="Thanks for your comment. Could you please elaborate the second idea. Couldn't grasb it. Bear in mind that there are not one sun but many. Also regarding the first solution, do you have any idea what the equation would be for the tube like surface?" CreationDate="2017-10-06T20:34:32.773" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8613" PostId="5684" Score="1" Text="I don't have any equations in mind for the first thing, no sorry. The second thing is this: the sun is traveling on a path. Imagine that path as a line (it bends around and curves, but it doesn't have any width).  If you can find a way to find the closest point on that curve to a line (the line that the ray is part of), you can get the distance from the ray to that path. If it gets closer than the radius of the sun, that means it MUST hit the sun. Then from there need to figure out when / where etc, so more work needed, but it's a start." CreationDate="2017-10-06T21:02:02.397" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8614" PostId="5692" Score="0" Text="Have you tried out_s = col_s and out_v = in_v?" CreationDate="2017-10-07T15:15:37.360" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="8615" PostId="5692" Score="0" Text="@SebastiánMestre Yes, this was my first thought. Unfortunately it does not replicate the Colorize behavior." CreationDate="2017-10-07T16:53:14.073" UserId="7434" ContentLicense="CC BY-SA 3.0" />
  <row Id="8616" PostId="5692" Score="0" Text="Isn't the colorize function kinda operating on the HSL space instead of HSV? What if you convert directly into HSL then what the above person said would work?" CreationDate="2017-10-07T23:04:59.933" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8617" PostId="5695" Score="0" Text="I'm getting close. Using the source I was able to write a shader that approximates the intended results. See here:  &#xA;  &#xA;https://imgur.com/a/kWGce  &#xA;  &#xA;Here is my GLSL fragment shader code:  &#xA;  &#xA;https://gist.github.com/anonymous/aacbe1aafe2fac75216b113bacbb8a0f&#xA;    &#xA;And here is the GimpColorizeOperation source code I'm drawing from:  &#xA;  &#xA;https://github.com/GNOME/gimp/blob/master/app/operations/gimpoperationcolorize.c  &#xA;  &#xA;Still some ways off.." CreationDate="2017-10-09T01:29:59.570" UserId="7434" ContentLicense="CC BY-SA 3.0" />
  <row Id="8619" PostId="5701" Score="0" Text="it makes sense and thanks for the info.  Are you able to speak to what for instance &quot;uniform&quot; would look like in the progressive (non binary) texture setup by chance?" CreationDate="2017-10-10T00:24:48.670" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8622" PostId="5704" Score="6" Text="Yes if each kernel is just a convolution. Sharpening algorithms may or may not be, median also usually is not. Read [this](https://math.stackexchange.com/questions/1023984/combining-two-convolution-kernels)" CreationDate="2017-10-10T15:27:06.027" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8628" PostId="5704" Score="0" Text="It might depend on the actual *kernel*. Can you include the specific kernels you're talking about in the question?" CreationDate="2017-10-11T18:05:31.253" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8635" PostId="5704" Score="0" Text="&quot;Kernel&quot; is not a synonym for &quot;filter&quot;. There is no such thing as a median kernel." CreationDate="2017-10-12T18:23:34.120" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="8637" PostId="5680" Score="4" Text="Remember that disney's diffuse BRDF is not a physical correct one. They are fully aware that their BRDF is not energy conserving, but they found that it looked better (probably because it makes up for interreflections) and their artists liked it. &#xA;Also note, that it is based on a physical BRDF model, which takes an integral over all microfacet normals (for this, see [Earl Hammon Jr's Diffuse GGX Lighting Slides][1]). That integral is not solvable, thus **it is  only an approximation**.&#xA;&#xA;&#xA;  [1]: https://twvideo01.ubm-us.net/o1/vault/gdc2017/Presentations/Hammon_Earl_PBR_Diffuse_Lighting.pdf" CreationDate="2017-10-13T08:41:49.670" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8639" PostId="2498" Score="0" Text="You have to bother about the cosine Term. The thing with $E(\omega_i)$ is that it is defined as $E = \frac{\Phi}{A}$ and $A$ is in this case orthogonal to the light direction, i.e. $A$ is projected from the original surface onto a plane orthogonal to the light direction.&#xA;&#xA;In short, you can't ignore $cos$, but in the equation it is implicitly handled." CreationDate="2017-10-13T09:51:21.800" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8640" PostId="5718" Score="0" Text="The name of the guy is Hooke threfore it is Hooke's law" CreationDate="2017-10-13T09:54:58.607" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8641" PostId="5656" Score="0" Text="Blinn-Phong does not use $\alpha \equiv roughness^2$. it has an arbitrary &quot;roughness&quot; parameter. Also, the $\alpha$ in Beckmann is not the same as the $\alpha$ in GGX. In Beckmann, $\alpha \in [0, \infty)$ and in GGX $\alpha \in [0, 1]$ (although both describe the RMS Slope)." CreationDate="2017-10-13T12:46:27.970" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8642" PostId="5656" Score="1" Text="@Tare For Blinn-Phong you need to use a derived version which derives alpha from the specular exponent. See http://graphicrants.blogspot.be/2013/08/specular-brdf-reference.html" CreationDate="2017-10-13T13:21:11.003" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8643" PostId="5656" Score="1" Text="Okay, you didn't mention that in your post, so I assumed you were using the original form." CreationDate="2017-10-13T14:16:40.890" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8645" PostId="5720" Score="0" Text="I don't understand how having other types of translucent objects makes it impossible to keep using sorted alpha blending. Could you elaborate?" CreationDate="2017-10-13T14:53:56.987" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8648" PostId="5718" Score="0" Text="Oh, sorry about that! I edited my typos." CreationDate="2017-10-13T16:28:04.613" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="8649" PostId="5719" Score="0" Text="I add an example could you please see that?" CreationDate="2017-10-13T17:04:32.547" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="8650" PostId="5720" Score="0" Text="If you have 10,000 particles in a single draw call, you can sort them and do a single draw call.  If you then add 5 other transparent objects, you then have up to something like 12 draw calls since you need to draw all the objects in the correct sort order.  It's possible but painful and likely costly!" CreationDate="2017-10-13T17:14:06.493" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8651" PostId="5720" Score="0" Text="related question: https://computergraphics.stackexchange.com/questions/5618/are-there-tricks-for-getting-proper-sort-ordering-on-particle-systems" CreationDate="2017-10-13T17:14:43.160" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8652" PostId="5720" Score="0" Text="Ah I see what you mean. I suppose you could partition Z space to reduce the error due to local incorrect order." CreationDate="2017-10-13T17:21:47.443" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8653" PostId="5720" Score="0" Text="Vylsain you might be interested in &quot;Order Independent Transparency&quot; or OIT.  It's an active area of research with a few different leading solutions that may or may not be appropriate for your specific usage case.  It allows you to render transparency without sorting, but get the correct results anyways (or close enough to correct, or correct most of the time)." CreationDate="2017-10-13T17:51:12.667" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8654" PostId="5719" Score="0" Text="@shashack obviously they model a  different thing. The other models springs that can be any size but equally elastic and the other only works for springs that you haev separately tested. All is as i explained and expected." CreationDate="2017-10-13T17:56:46.187" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8656" PostId="5233" Score="0" Text="ORB means ORB features and BF means Brute Force Matching. In step 2, for each candidate region, calculate the ORB features and use bruteforce matching to compare it with the template pattern. If there are enough(by number and ratio) matches(corresponding feature points in the template and current region), then this candidate region is considered a target." CreationDate="2017-10-11T01:55:57.957" UserId="2131" ContentLicense="CC BY-SA 3.0" />
  <row Id="8657" PostId="5233" Score="0" Text="I cannot give you raw data because I'm off this case now. Kalman filter was once an option, but we solved the problem with an easier solution. Use opencv subpixel. That simply solves it. I have also tried upsampling and then downsampling. Now I cannot remember the result of the experiments." CreationDate="2017-10-11T02:02:00.447" UserId="2131" ContentLicense="CC BY-SA 3.0" />
  <row Id="8670" PostId="5722" Score="0" Text="I think I know what flip you're talking about. Most people just clamp it and stop the camera. You want your camera to be upside down after going over +-90 degrees? If not then there'd have to be a twist to the camera. Like once one of your spherical angles hits +-90 degrees the other angle does a 180." CreationDate="2017-10-14T01:18:31.690" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8671" PostId="5233" Score="0" Text="There are two reasons we didn't do the smoothing. First, we were processing the images on embedded systems. Resources are really limited and something like Gaussian filter costs a lot. The situation is, even with image resolution 640*480, the numbers of frames processed can drop a lot with Gaussian smoothing. Second, with smoothing, there is a chance that  the target zone being connected with outer zone." CreationDate="2017-10-14T03:41:16.393" UserId="2131" ContentLicense="CC BY-SA 3.0" />
  <row Id="8672" PostId="5722" Score="0" Text="The OP is having a different problem. Their code protects the pitch from getting to high or low, but they are having problems with yaw rotation." CreationDate="2017-10-14T15:39:04.530" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8673" PostId="5722" Score="1" Text="@AlanWolfe I've just uploaded a video of explaining the issue.&#xA;&#xA;I think the yaw(rotation around y axis) works, but pitch(around x) doesn't, if I'm not mistaking." CreationDate="2017-10-14T16:19:06.053" UserId="6908" ContentLicense="CC BY-SA 3.0" />
  <row Id="8674" PostId="5722" Score="1" Text="@AlanWolfe Also, sorry I think I've uploaded the wrong code on the original post, just updated it. I'm simply trying to rotate the camera around the center. Around y axis works, but around x it acts strange, and I think it's because the rotation on x axis is limited to 180 degrees and if we go over it, it probably has to rotate the y axis first and then x to achieve the desired rotation point?" CreationDate="2017-10-14T16:25:12.277" UserId="6908" ContentLicense="CC BY-SA 3.0" />
  <row Id="8675" PostId="5724" Score="0" Text="It is important to note that the Rasterization stage before the optional Fragment Shader performs fragment attribute interpolation **and** viewport transformation for the `gl_FragCoord` attribute." CreationDate="2017-10-14T16:29:17.517" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8678" PostId="5732" Score="0" Text="How to calculate angle and phi when rotate around all 3 axes? I've read wikipedia you linked, and try to calculate it... but still don't know how to calculate those with rotation angle used on glRotate(angle, rotate_x, rotate_y, rotate_z);" CreationDate="2017-10-16T05:22:54.443" UserId="7482" ContentLicense="CC BY-SA 3.0" />
  <row Id="8679" PostId="5720" Score="0" Text="Hi and thanks for your help. That's right, sorted alpha blending is not efficient if I constantly have to bind different shaders and split my particle system draw call. About Order Independant Transparency, that's what I've tried as I said in the first post. But it seems there's way too many particles and overdraw involved in my case to have correct or efficient results. I wonder how this is handled with a ray-marching approach..." CreationDate="2017-10-16T07:18:12.930" UserId="7473" ContentLicense="CC BY-SA 3.0" />
  <row Id="8681" PostId="5720" Score="1" Text="Have you thought of using volume rendering for this? It seems to me like a more sensible approach than your current one" CreationDate="2017-10-16T12:57:39.703" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="8682" PostId="5716" Score="0" Text="I'm not familiar with the Lane Resienfield algorithm but I think you need to be more specific as to what you don't understand." CreationDate="2017-10-16T13:57:06.903" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8683" PostId="5720" Score="0" Text="Yes, I'm thinking about it but I don't know what to expect with transparency using this kind of technique... If I render my clouds using ray marching, what happens when a ray encounters a semi transparent object ?" CreationDate="2017-10-16T14:10:41.890" UserId="7473" ContentLicense="CC BY-SA 3.0" />
  <row Id="8684" PostId="5732" Score="0" Text="The easiest way is to take a unit vector, like `&lt;0, 0, 1&gt;`, run it through the Euler rotation matrix, then convert the point at the end of the resulting vector to spherical coordinates. You can use [this formula](https://en.wikipedia.org/wiki/Rotation_matrix#Rotation_matrix_from_axis_and_angle) to get the cartesian rotation matrix from your Euler angles." CreationDate="2017-10-16T14:55:12.863" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8685" PostId="5734" Score="0" Text="While it is not common in games it does exist in applications" CreationDate="2017-10-16T15:28:56.423" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8686" PostId="5735" Score="0" Text="Does this need to be fast or can it be done offline in advance?" CreationDate="2017-10-16T19:59:20.483" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8687" PostId="5735" Score="0" Text="It doesn't need to be especially fast." CreationDate="2017-10-16T20:00:43.267" UserId="7490" ContentLicense="CC BY-SA 3.0" />
  <row Id="8688" PostId="5735" Score="0" Text="Do the segments need to be exactly equal length (within limitations of floating point etc) or can they be approximately equal? Like equal within some epsilon?" CreationDate="2017-10-16T20:02:08.053" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8689" PostId="5735" Score="0" Text="Interesting point. Some approximation is probably okay, but I'd prefer to maximise precision." CreationDate="2017-10-16T20:04:26.210" UserId="7490" ContentLicense="CC BY-SA 3.0" />
  <row Id="8690" PostId="5716" Score="0" Text="The actual algorithm in a geometrical sense. Like, with Castlejeaus you can explain it by taking fractions of the straight lines (1/3 of the line between A and B for example), recursively.&#xA;&#xA;I need something similar but for lane resienfield" CreationDate="2017-10-16T22:28:31.423" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="8691" PostId="5735" Score="0" Text="Do you want equal length segments or segments that represent equal arc lengths?" CreationDate="2017-10-17T00:44:23.520" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="8692" PostId="5695" Score="0" Text="Maybe use hsl instead of hsv/hsb?" CreationDate="2017-10-17T00:47:47.027" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="8693" PostId="5732" Score="0" Text="Thx. This works perfectly good. However, I'm still curious that how to calculate 'angle' and 'phi' that for spherical coordinates with Euler angle." CreationDate="2017-10-17T07:32:22.290" UserId="7482" ContentLicense="CC BY-SA 3.0" />
  <row Id="8694" PostId="5738" Score="2" Text="What's the context?" CreationDate="2017-10-17T09:40:51.630" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8696" PostId="5735" Score="0" Text="That makes sufficiently little difference to me that I'm happy with whichever is easiest." CreationDate="2017-10-17T10:30:03.067" UserId="7490" ContentLicense="CC BY-SA 3.0" />
  <row Id="8697" PostId="5737" Score="0" Text="That being said, this approach does mean that the value of a influences both line length and turn rate. I really need those to be separate terms." CreationDate="2017-10-17T11:01:00.420" UserId="7490" ContentLicense="CC BY-SA 3.0" />
  <row Id="8698" PostId="5732" Score="0" Text="A quick search didn't turn up any obvious answers, and my linear algebra is somewhat limited, so unfortunately, I don't know of another way." CreationDate="2017-10-17T16:37:05.477" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8699" PostId="5741" Score="0" Text="thank you for your quick answer. I didn't get the results I hoped, but you answered the question." CreationDate="2017-10-17T22:31:49.533" UserId="7492" ContentLicense="CC BY-SA 3.0" />
  <row Id="8700" PostId="5737" Score="0" Text="@SolarGranulation I just added that, and updated the JSFiddle and Desmos examples to reflect that." CreationDate="2017-10-18T00:10:01.723" UserId="7431" ContentLicense="CC BY-SA 3.0" />
  <row Id="8702" PostId="5720" Score="0" Text="What types of other objects are you expecting to draw besides clouds? I would think you could partition the objects by elevation." CreationDate="2017-10-18T03:52:41.317" UserId="2707" ContentLicense="CC BY-SA 3.0" />
  <row Id="8704" PostId="5743" Score="0" Text="You could have a backup dynamic viewport pipeline object for use during resize until the finalized pipeline object with fixed viewport is ready." CreationDate="2017-10-18T11:14:24.923" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="8705" PostId="5720" Score="0" Text="Other particles drawn with different shader and blending mode (additive), buildings with windows, aircrafts... The cloud ceiling altitude can be adjusted and I have case where a transparent object is in the middle of a cloud. For example, my test scene is a wind turbine equipped with lights in the middle of a cloud." CreationDate="2017-10-18T12:36:28.030" UserId="7473" ContentLicense="CC BY-SA 3.0" />
  <row Id="8711" PostId="5748" Score="0" Text="Explicitly thanks for the clarification &quot;Some information is lost, though: the sign of $w$, which corresponds to whether a point was behind or in front of the camera.&quot;" CreationDate="2017-10-19T08:17:07.697" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8712" PostId="4948" Score="1" Text="This is not right. OP wants to build a 3d model from mri scans. Not from photographs" CreationDate="2017-10-19T10:52:46.910" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="8713" PostId="4948" Score="0" Text="I'm pretty sure photogrammetry is not tied to photographs. If the source is multiple 2-dimensional images of any kind it should apply. But perhaps MRI scans are fundamentally different from things like X-ray images and are not 2 dimensional images as I expect." CreationDate="2017-10-20T02:36:19.480" UserId="1647" ContentLicense="CC BY-SA 3.0" />
  <row Id="8714" PostId="5754" Score="0" Text="What do you expect to happen for #1? Your assumption is correct. If you're scaling the accumulation texture but not the particles, then it's going to get larger (or smaller) than the particle. What is the problem with #3? I don't see anything odd about it." CreationDate="2017-10-20T06:01:17.083" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8715" PostId="5754" Score="0" Text="in #3 the original particle is not moving. So, when panning translation is applied, I expect both textures to have synchronized transformation, and no fading trail should appear. Yet you can see when translation is very small at the end - there is a fading trail around particle, as if transformations slightly misaligned" CreationDate="2017-10-20T06:27:20.710" UserId="7512" ContentLicense="CC BY-SA 3.0" />
  <row Id="8720" PostId="5757" Score="0" Text="Thank you! I'm actually computing position of a particle on a GPU and encode it into texture. The formulas to find next particle position are quite involved, and I want to support 100k - 1,000k particles. If I'd have to recompute positions for last N steps, I'm worried that would impact frame rate a lot." CreationDate="2017-10-20T16:42:36.333" UserId="7512" ContentLicense="CC BY-SA 3.0" />
  <row Id="8721" PostId="5760" Score="0" Text="See [What is glVertexAttrib (vs. glVertexAttribPointer) used for?](https://stackoverflow.com/questions/7718976/what-is-glvertexattrib-versus-glvertexattribpointer-used-for#7719060). Essentially, they're the same thing, but `glVertexAttrib` is for the older immediate mode." CreationDate="2017-10-21T05:17:51.297" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8722" PostId="5763" Score="0" Text="Thanks. I suspected such thing because I recently experimented with cloud raymarchind sample code form here: https://www.gamedev.net/forums/topic/680832-horizonzero-dawn-cloud-system/?page=3&amp;tab=comments#comment-5319203 It probably has corrected approach. I don't exacly understand how to implement this formula though. It looks like when putting _S_ before the formula, what was left in between parenthesis is transmittance? I need to put it in `output.w` for further reconstruction." CreationDate="2017-10-22T09:12:09.553" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="8723" PostId="5760" Score="0" Text="Thanks for the response! I think I understand the difference now. One sets a constant value, and the other links the values to a buffer." CreationDate="2017-10-22T20:21:24.750" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8724" PostId="5763" Score="0" Text="@narthex I added to the answer with my guess as to what the code should look like with the modified integration." CreationDate="2017-10-22T22:11:14.583" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8731" PostId="5757" Score="0" Text="I was assuming you'd save the positions each frame rather than recomputing. An array of particle positions is much less state to pass around than a framebuffer." CreationDate="2017-10-24T08:18:04.140" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8732" PostId="5773" Score="1" Text="Well, I am not getting a compiler error at all, all that happens is that OpenGL throws an INVALID_OPERATION error.&#xA;&#xA;So the syntax is correct, but some operation is wrong" CreationDate="2017-10-24T10:10:35.750" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="8733" PostId="5773" Score="1" Text="The error comes from my face type array being too big, but now I don't know how to pass the face type information into the shader, since I know I need 2048 elements at most" CreationDate="2017-10-24T10:27:24.317" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="8734" PostId="2532" Score="0" Text="In my experience using an integrated graphics card should be good enough for most OpenGL development on Windows, just make sure it supports a recent OpenGL version." CreationDate="2017-10-24T14:13:05.860" UserId="5462" ContentLicense="CC BY-SA 3.0" />
  <row Id="8739" PostId="5778" Score="0" Text="Note: I've asked the same question in the comp sci Stack Exchange, but received no answers: https://cs.stackexchange.com/questions/81169/how-to-prevent-moire-artifacts-in-this-light-casting-algorithm" CreationDate="2017-10-25T20:34:04.183" UserId="2437" ContentLicense="CC BY-SA 3.0" />
  <row Id="8740" PostId="5779" Score="1" Text="If you solved your problem yourself, you can answer your own question. Deleting it just means the next person with a similar problem won't get the benefit of your experience." CreationDate="2017-10-25T21:55:34.467" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8741" PostId="5781" Score="0" Text="Check out the answer to [this question](https://computergraphics.stackexchange.com/questions/3742/how-to-unproject-quadrilateral-into-rectangle). You can take any 4 points and transform the quadrilateral between them into a unit square. From there you can simply scale in x and y to the size and shape you want." CreationDate="2017-10-26T04:57:37.453" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8742" PostId="5781" Score="0" Text="Does the transformation need to be perspective correct ?" CreationDate="2017-10-26T12:27:22.553" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8743" PostId="5778" Score="0" Text="I don't understand, never heard of light casting before. Isn't what you are trying to do similar to ray-tracing? Just shoot rays from the origin, for every single pixel. That way there won't be any uncolored pixel." CreationDate="2017-10-26T13:02:25.377" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8744" PostId="5782" Score="0" Text="Got any links how to do this in a 2D engine? I've tried to write it in a &quot;gather&quot; fashion in a fragment shader - its unbearably slow since it does so many duplicate calculations." CreationDate="2017-10-26T13:45:47.913" UserId="2437" ContentLicense="CC BY-SA 3.0" />
  <row Id="8745" PostId="5781" Score="0" Text="@user1118321 thanks for the link, that's perfect!" CreationDate="2017-10-26T15:38:06.850" UserId="7541" ContentLicense="CC BY-SA 3.0" />
  <row Id="8746" PostId="5781" Score="0" Text="@PaulHK what do you mean by perspective correct?" CreationDate="2017-10-26T15:38:09.833" UserId="7541" ContentLicense="CC BY-SA 3.0" />
  <row Id="8748" PostId="5757" Score="0" Text="Dan, but I don't think I'm passing a framebuffer. I just use two textures and constantly swapping them. Does this mean I'm passing a framebuffer? Also I tried to re-render last 10 -15 steps and take into account transforms - performance was not acceptable.. Maybe I'm doing something wrong" CreationDate="2017-10-26T21:44:18.610" UserId="7512" ContentLicense="CC BY-SA 3.0" />
  <row Id="8749" PostId="5781" Score="0" Text="For example, if the floor was a huge chess board, we would see squares vanish into the distance. When transforming back to 2d we need to take that perspective into account." CreationDate="2017-10-27T02:40:05.170" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8750" PostId="2004" Score="0" Text="I haven't read the paper, but this &quot;maximum mipmaps&quot; sounds very similar to the technique used for _Cone Step Mapping_ (which improves over parallax occlusion mapping by skipping large areas thanks to cones)." CreationDate="2017-10-27T03:35:22.803" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8751" PostId="5780" Score="0" Text="You should also write to them the same way you write to `gl_Position`: before calling `EmitVertex()`." CreationDate="2017-10-27T03:51:01.957" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8752" PostId="5763" Score="0" Text="This is my comparison - standard vs corrected (above code): https://imgur.com/a/POTaD. But differences only get visible with very high scattering. Also correction leads to division by zero so small epsilon is needed." CreationDate="2017-10-27T14:02:14.517" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="8753" PostId="5784" Score="0" Text="what about CMYK?" CreationDate="2017-10-27T17:17:54.027" UserId="7551" ContentLicense="CC BY-SA 3.0" />
  <row Id="8755" PostId="5784" Score="0" Text="CMYK is a process by which we can print colors onto a substrate (and a much smaller selection of colors than we can see) but the way we see is RGB." CreationDate="2017-10-27T23:12:42.657" UserId="7555" ContentLicense="CC BY-SA 3.0" />
  <row Id="8756" PostId="5784" Score="12" Text="They don't make up *all* the colors. They just make up a sufficient range of them that most scenes can be represented with acceptable fidelty." CreationDate="2017-10-28T00:08:27.377" UserId="2653" ContentLicense="CC BY-SA 3.0" />
  <row Id="8758" PostId="5786" Score="5" Text="&quot;Thus, with only three colors, we can reconstruct all the colors we can see.&quot; This sentence is incorrect. Starting from three primaries, you can only reconstruct certain colors. The range of colors that can be reconstructed is called the &quot;gamut&quot;. You can search for &quot;sRGB gamut&quot; and find pictures that show a triangle inside a larger parabola. The triangle represents the colors that we can make from the sRGB primaries, and the parabola is all the colors we can see. From this it's clear that *any* triangle inside the parabola will be smaller than it." CreationDate="2017-10-28T02:20:25.410" UserId="7557" ContentLicense="CC BY-SA 3.0" />
  <row Id="8759" PostId="5784" Score="6" Text="Because humans have red, green and blue receivers in their eyes." CreationDate="2017-10-28T02:43:44.590" UserId="2316" ContentLicense="CC BY-SA 3.0" />
  <row Id="8760" PostId="5786" Score="0" Text="woops, you're right. I've replaced &quot;all&quot; with &quot;most&quot; and will try to think of an explanation for the remaining visible colors." CreationDate="2017-10-28T07:38:27.817" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8761" PostId="5786" Score="4" Text="Also the concept of white light is governed by our really fancy  white balance system it dont mich matter what the color is it will be precieved as white. Incandescent lightbulbs are orange but if we are inside the house we precieve them as white. As for the extra colors, if you integrate the energies of your color distribution multiplied by curves ratchet freaks show you youll notice that sometimes  you get unique signals because the overlap is different." CreationDate="2017-10-28T07:45:14.310" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8762" PostId="5789" Score="0" Text="_The region outside of the triangle cannot be shown on your screen._ Well, I can see this region on screen with colors. So how can it be more precisely stated?" CreationDate="2017-10-28T12:14:57.297" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="8763" PostId="5789" Score="2" Text="@narthex: Thanks for the comment. I updated the answer. Is it any better now?" CreationDate="2017-10-28T12:28:30.443" UserId="7560" ContentLicense="CC BY-SA 3.0" />
  <row Id="8764" PostId="5789" Score="0" Text="Ok, thats fine. But what acually the values on each axis mean? This is never explained." CreationDate="2017-10-28T12:39:43.350" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="8765" PostId="5789" Score="0" Text="@narthex see https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_xy_chromaticity_diagram_and_the_CIE_xyY_color_space" CreationDate="2017-10-28T12:57:57.177" UserId="7560" ContentLicense="CC BY-SA 3.0" />
  <row Id="8766" PostId="5599" Score="0" Text="Did my answer solved your question?" CreationDate="2017-10-28T13:26:26.260" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8767" PostId="5789" Score="1" Text="And also, (staring at that last image), the red circle dances around. Fun" CreationDate="2017-10-28T16:47:20.703" UserDisplayName="user7566" ContentLicense="CC BY-SA 3.0" />
  <row Id="8768" PostId="5789" Score="4" Text="The problem with CIE colorspace plots is that they are very hard to understand, hell we dont even know if some of the areas in the graph happen to make metamers.  Also the reason why you simply can not make a bigger triangle is not apparent (hint there is nothing outside the shape)." CreationDate="2017-10-28T18:33:18.013" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8769" PostId="5784" Score="2" Text="This would be better for biology stack exchange (if there is one) because it is more a question about the human visual system than one of computer graphics." CreationDate="2017-10-28T18:57:16.603" UserId="7567" ContentLicense="CC BY-SA 3.0" />
  <row Id="8770" PostId="5784" Score="2" Text="@mathreadler https://biology.stackexchange.com" CreationDate="2017-10-28T19:06:34.587" UserId="7568" ContentLicense="CC BY-SA 3.0" />
  <row Id="8771" PostId="5784" Score="0" Text="@briantist wow cool, who knew." CreationDate="2017-10-28T19:09:28.190" UserId="7567" ContentLicense="CC BY-SA 3.0" />
  <row Id="8772" PostId="5784" Score="0" Text="Good answers here and good answers on Biology.SE will give different perspectives, so I'm glad the question has been asked on both sites." CreationDate="2017-10-28T19:18:57.623" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8774" PostId="5789" Score="2" Text="@joojaa: https://xkcd.com/1882/" CreationDate="2017-10-28T19:24:20.387" UserId="7560" ContentLicense="CC BY-SA 3.0" />
  <row Id="8775" PostId="5784" Score="0" Text="[Similar/related questions on Biology.SE for comparison](https://biology.stackexchange.com/search?tab=votes&amp;q=red%20green%20blue%20is%3aq)" CreationDate="2017-10-28T19:29:09.067" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="8776" PostId="5789" Score="1" Text="@narthex The screen can show a _diagram_ that _represents_ the CIE color space.  You don't even need a color screen for that:  You can just have text labels &quot;red&quot;, &quot;orange&quot;', &quot;yellow&quot;,...  What the screen can't show is all of the actual colors that exist in that space." CreationDate="2017-10-28T19:33:24.810" UserId="7569" ContentLicense="CC BY-SA 3.0" />
  <row Id="8777" PostId="5790" Score="0" Text="Depends what you mean by &quot;color.&quot;  In many contexts, it makes a lot of sense to say that if nobody can see the difference between two different patches on a surface, then both patches must be the same &quot;color.&quot;  On the other hand, when a painter says &quot;color,&quot; she or he is talking about the physical substance into which he/she dips a brush.  In that case, see https://en.wikipedia.org/wiki/Metamerism_(color)#Metameric_failure" CreationDate="2017-10-28T19:38:32.483" UserId="7569" ContentLicense="CC BY-SA 3.0" />
  <row Id="8778" PostId="5784" Score="0" Text="@mathreadler Actually this is better for [cognitive sciences.se](https://cogsci.stackexchange.com/). More accurately the place that mixes all that's needed. But its fine here as well." CreationDate="2017-10-28T19:53:42.347" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8779" PostId="5790" Score="1" Text="@jameslarge: It really doesn't. Just because they look the same under one light source doesn't mean they'll look the same under a different one, even if both light sources look identical on a white surface." CreationDate="2017-10-28T21:02:00.480" UserId="3411" ContentLicense="CC BY-SA 3.0" />
  <row Id="8780" PostId="5789" Score="5" Text="Great, now I have a cyan dot in the middle of my vision :-(" CreationDate="2017-10-29T00:32:15.310" UserId="7576" ContentLicense="CC BY-SA 3.0" />
  <row Id="8781" PostId="5784" Score="4" Text="Apparently there is at least one tetrachromat woman (see https://en.wikipedia.org/wiki/Tetrachromacy) that is able to distinguish more colours than those of us who are trichromat." CreationDate="2017-10-29T03:53:59.353" UserId="7579" ContentLicense="CC BY-SA 3.0" />
  <row Id="8782" PostId="5793" Score="0" Text="What are &quot;original lines&quot;?" CreationDate="2017-10-29T04:08:29.697" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8783" PostId="5793" Score="0" Text="Sorry for not being clear, I fixed that now. I hope its clear now" CreationDate="2017-10-29T04:35:41.817" UserId="7578" ContentLicense="CC BY-SA 3.0" />
  <row Id="8784" PostId="5793" Score="3" Text="This diagram simply shows how in perspective projection the parallel lines are not parallel in the resulting image (image plane). They would only be parallel in orthogonal projection. The author of the diagram could project any lines he wanted to on the image plane. They don't have to be parallel with the line going through the vanishing point." CreationDate="2017-10-29T04:47:28.343" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8785" PostId="5784" Score="0" Text="@immibis that is a gross over-simplification and really quite inaccurate." CreationDate="2017-10-29T05:54:20.360" UserId="7581" ContentLicense="CC BY-SA 3.0" />
  <row Id="8786" PostId="5790" Score="0" Text="I don't think this answers the question in any way.  It also applies to any colours - not just violet and purple.  Monochromatic light of any hue from red through to violet won't get split by a prism, and any mixed light will get split." CreationDate="2017-10-29T05:59:17.800" UserId="7581" ContentLicense="CC BY-SA 3.0" />
  <row Id="8787" PostId="5784" Score="1" Text="@BillBell and probably its digital counterpart: [Quattron](https://en.wikipedia.org/wiki/Quattron), adding yellow color subpixels on Sharp Aquos LCD TV." CreationDate="2017-10-29T07:51:34.757" UserId="2170" ContentLicense="CC BY-SA 3.0" />
  <row Id="8788" PostId="5791" Score="1" Text="One common example for that is the quasi-monochromatic sodium vapor lamps, which are commonly used for city lamps and look always different in reality than on photos." CreationDate="2017-10-29T09:03:15.753" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8789" PostId="5795" Score="0" Text="Yeah i looked at the later code now after giving up and it was definitly a missprint. It works out with (A-C) = y as you said." CreationDate="2017-10-29T14:20:49.803" UserId="7577" ContentLicense="CC BY-SA 3.0" />
  <row Id="8790" PostId="5784" Score="1" Text="@AndrewT.: I expect that a lot of this is lost on people like me that can't see the difference between 'Saddle Brown' and 'Sienna'. :)" CreationDate="2017-10-29T16:16:11.667" UserId="7579" ContentLicense="CC BY-SA 3.0" />
  <row Id="8791" PostId="5782" Score="0" Text="2D is no different than 3D. check out https://youtu.be/0YdPHNJjSxI. In convential CPU-style thinking you'll tell yourself `for all lights: modify affected pixels`. In GPU thinking your start point is already inside the `for all pixel` loop. So you need the `all lights` loop &quot;nested&quot; in your fragment code. You must have a structured buffer (or texture, or constant buffer) that tells you lights information so you access `light[index].position` and `SV_Position` being the fragment pos, you have the light vector. from this  you can do a usual lambert &amp; attenuation calculation" CreationDate="2017-10-30T02:09:31.583" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="8792" PostId="5791" Score="0" Text="but those are fringe issues, I would consider very advanced. The issue doesn't materialize in most cases, RGB is just a fourier encoding with 3 harmonics of some signal that happens to be enough for most cases." CreationDate="2017-10-30T02:14:26.250" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="8793" PostId="5793" Score="1" Text="@AndrewWilson, absolutely. And the OP phrase `to the parallel lines of the image` is invalid, because there is no such thing that &quot;THE&quot; parallel lines. It's only meant to mean that some cubic looking, well aligned geometry, like architecture and streets, will tend to display lots of parallel lines that &quot;engenders&quot; the presence of a virtual vanishing point. Do anybody speaks of vanishing points in image of trees and ponds ?" CreationDate="2017-10-30T02:23:11.547" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="8794" PostId="2325" Score="0" Text="honestly I'm not completely sure about `to be completely unbiased it must be random`. I think you can still get math-ok results by using fractional weigthing of samples, rather than the binary pass/drop that the russian roulette imposes, it's just that the roulette will converge faster because it's operating a perfect importance sampling." CreationDate="2017-10-30T02:36:58.093" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="8795" PostId="5791" Score="0" Text="@JulienGuertault: While that's a nice example, I don't think it's quite an example of what my answer is pointing out - as long as your photosensor's/film's responses to the light in its 3 components matches the human eye's well enough, it should faithfully represent what a human would see. Where RGB (or any other model that lumps whole ranges of the frequency spectrum together) is insufficient is for actually modelling surfaces and light sources in a way that you can predict the perceived color of a light on a surface." CreationDate="2017-10-30T02:40:58.363" UserId="3411" ContentLicense="CC BY-SA 3.0" />
  <row Id="8796" PostId="2004" Score="0" Text="@JulienGuertault I would say this is plain HiZ tracing. it's a safe method to be sure of what you hit. But not very fast compared to unsafe methods like binary search." CreationDate="2017-10-30T02:46:17.023" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="8797" PostId="5791" Score="0" Text="@v.oddou: &quot;I don't care, it looks good enough&quot; is a reasonable position to take, but there really is a difference. You won't be able to model the way the color of a wall looks different under daylight vs incandescent light vs led light that's supposed to be the same color temp as one or the other." CreationDate="2017-10-30T02:52:23.420" UserId="3411" ContentLicense="CC BY-SA 3.0" />
  <row Id="8799" PostId="5793" Score="0" Text="This question seems nonsensical as it currently stands. Perhaps a photo showing which parallel lines you mean would clarify?" CreationDate="2017-10-30T03:28:54.167" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8800" PostId="5791" Score="0" Text="hmm, I might have misunderstood. Do you have a concrete example of the limitation you are referring to?" CreationDate="2017-10-30T03:42:51.897" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="8802" PostId="5789" Score="0" Text="It's worth noting that the CIE 1931 diagrams are somewhat misleading in the sense that the distance between two arbitrary points within it does not correspond to the perceived difference in colour. In other words, the area within and without the sRGB triangle is not representative of the perceived gamut: the difference is not that dramatic. (On the 1931 diagram, sRGB occupies less than half of the area). The 1976 version is much better in this regard." CreationDate="2017-10-30T05:01:06.800" UserId="7594" ContentLicense="CC BY-SA 3.0" />
  <row Id="8806" PostId="5791" Score="0" Text="I've seen plenty of visual phenomena that *seem to be* examples of what I'm saying, but haven't done experiments or anything to confirm. The canonical example would be something like a narrow-band yellow light source on a material that reflects most light but absorbs a band around the particular yellow of the light source. In an RGB model, the material's color would be close to white (1.0,1.0,1.0), the light's color would be something near (1.0,1.0,0.0), and a rendering would show a yellow-lit surface whereas in reality the surface would be barely-lit." CreationDate="2017-10-30T16:22:51.063" UserId="3411" ContentLicense="CC BY-SA 3.0" />
  <row Id="8807" PostId="5782" Score="0" Text="You mean something like this? https://github.com/matyasf/sparrow-game/blob/master/SparrowGame.Shared/CustomMesh/FragShader.frag This is sadly dog slow, I can render ~5-10 lights. The compute shader above can render ~200 with the same FPS." CreationDate="2017-10-30T23:16:17.257" UserId="2437" ContentLicense="CC BY-SA 3.0" />
  <row Id="8809" PostId="5782" Score="0" Text="It's slow because it is doing step marching. marching is necessary only for volumetric effects of complex intersection routines. why do you need this ?" CreationDate="2017-10-31T02:59:19.233" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="8810" PostId="5599" Score="0" Text="Yeah thanks, I've got it working now. I upgraded to the latest version of directx math and copied the source directly from Rastertek (second series). Still confused about what the bug actually was... Not alignment, AFAICT, since compiling 64-bit forces 16-byte heap alignment. And the other matrix methods worked fine and produced valid results. Possibly a subtle bug in my own copy of the code. Going forward, I think I'll work with Microsoft's own DirectX tutorials since these seem to work well out of the box with a minimum of hassle." CreationDate="2017-10-31T04:06:20.703" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8811" PostId="5782" Score="0" Text="Because I want volumetric effects, like my original algorithm :) This is not just a simple shadow cast, I want something new. It would be able to render nice god rays, light trough smoke, fog, coloured glass, etc. My aim is to make something that looks a bit different than everything else out there." CreationDate="2017-10-31T08:25:02.957" UserId="2437" ContentLicense="CC BY-SA 3.0" />
  <row Id="8812" PostId="360" Score="0" Text="&quot;adjust the knots so that they lie on top of each other. This is essentially a bit like having several control points on top of each other&quot; -- Well maybe it's a bit similar, but not much. Making control points coincident will cause derivative vectors to be zero, which leads to all sorts of problems. Making knots coincident is benign." CreationDate="2017-10-31T12:24:18.713" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8813" PostId="360" Score="0" Text="&gt; you end up with a cusp -- you have the *potential* for forming a cusp, but you won't necessarily get a cusp." CreationDate="2017-10-31T12:28:55.533" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8816" PostId="360" Score="0" Text="@bubba yes its possible to fit on the other side, and yes if you have multiple knots on top you loose some of the interpolant to no movement, anything outside this area is same." CreationDate="2017-10-31T14:21:26.733" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8817" PostId="5782" Score="0" Text="Ok, then in my book there are only 2 ways : either you pay the simulation price, either you find an analytical solution (or a bunch of separated solutions, like Antoine Bouthors did in his 2008 paper for cloud rendering). I can't help you much beyond that" CreationDate="2017-11-01T01:50:09.557" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="8818" PostId="5778" Score="0" Text="@wandering-warrior That would be waay too slow, this is for a game. To save GPU time I am shooting rays only to the edge of the circle, these colour the path they trace. Just they don't do it evenly, so the artifacts." CreationDate="2017-11-01T01:56:59.837" UserId="2437" ContentLicense="CC BY-SA 3.0" />
  <row Id="8819" PostId="5802" Score="0" Text="Powerpoint is actually quite expensive considering things. Its ok, after all drawing skill comes from you. But atleast tikz, inkscape, blrnder and postscript are way cheaper than powerpoint." CreationDate="2017-11-01T05:38:45.533" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8820" PostId="5803" Score="0" Text="If you can pass the coordinates of the original line to the fragment shader, you can try to find the distance from the fragment to the original relevant line segment, and then discard any fragments which are further away than your line thickness." CreationDate="2017-11-01T09:03:18.037" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="8821" PostId="5802" Score="0" Text="I'm old, so time is more important to me than money. Inkscape is good, but there's no way I'd spend time writing tikz or Postscript code to produce pictures." CreationDate="2017-11-01T12:47:00.937" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8822" PostId="5803" Score="0" Text="&quot;*I believe is the job for the fragment shader as I dont have enough processing resources to calculate more geometry.*&quot; Considering that most line systems of this sort use only the CPU to rasterize them, I rather suspect you have plenty of processing resources to calculate more geometry." CreationDate="2017-11-01T14:32:53.923" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8823" PostId="5801" Score="1" Text="&quot;Links to external resources are encouraged, but please add context around the link so your fellow users will have some idea what it is and why it’s there. Always quote the most relevant part of an important link, in case the target site is unreachable or goes permanently offline.&quot; - [Computer Graphics SE Help Center](https://computergraphics.stackexchange.com/help/how-to-answer)" CreationDate="2017-11-01T15:56:32.663" UserId="7431" ContentLicense="CC BY-SA 3.0" />
  <row Id="8824" PostId="5803" Score="0" Text="@NicolBolas If I were just rendering lines I certainly would but the simulation that creates the lines is quite expensive." CreationDate="2017-11-01T16:36:32.640" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="8825" PostId="5804" Score="0" Text="Could you expand a bit more on the 2nd idea? I have used a simple stencil buffer as a mask before but never for something so complicated as telling the GPU not to draw fragments where the same object has already drawn fragments. How exactly would I fill up the stencil buffer?" CreationDate="2017-11-01T16:38:23.830" UserId="2308" ContentLicense="CC BY-SA 3.0" />
  <row Id="8826" PostId="5804" Score="0" Text="It's really just as simple as that. If you used the stencil buffer for masking before, that's exactly what you do. You clear it to all 0s, then configure the test to succeed only where there's 0 and write a 1 (or whatever) for each fragment that passes. This way every pixel gets written only once." CreationDate="2017-11-01T16:41:50.493" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8827" PostId="5804" Score="0" Text="When using alpha-blending for antialiasing together with the seperated version described last, you should not perform testing for the line middle sections but only fill the stencil buffer, then test against that when drawing the connection points." CreationDate="2017-11-01T16:43:52.150" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8828" PostId="5810" Score="2" Text="Can you show your shader code—how are you declaring and using the SSBOs in the shader?" CreationDate="2017-11-02T05:47:51.173" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8829" PostId="5757" Score="0" Text="Dan, here is the project that I was working on: https://anvaka.github.io/fieldplay/ I can finally share it. The source code is here https://github.com/anvaka/fieldplay - this is vector field explorer/simulation. Maybe playing with it would give you better ideas of what I'm trying to do? If you don't have time for this - no worries!" CreationDate="2017-11-02T05:50:51.910" UserId="7512" ContentLicense="CC BY-SA 3.0" />
  <row Id="8830" PostId="5805" Score="1" Text="From the question title I thought you were asking about bilinear interpolation.  For other folks who came here looking for information on that, here's a link (but feel free to explicitly ask a question too) https://blog.demofox.org/2015/04/30/bilinear-filtering-bilinear-interpolation/" CreationDate="2017-11-02T16:28:33.570" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8831" PostId="5811" Score="1" Text="The arrows are there to point to the line which is the problem you are trying to solve right?" CreationDate="2017-11-02T16:30:42.870" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8832" PostId="5811" Score="0" Text="So... what exactly are those lines? Are they drawn via `GL_LINES`? Are they textures?" CreationDate="2017-11-02T22:52:28.457" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8833" PostId="5806" Score="0" Text="Good answer. But it's mostly about using barycentric coordinates, which work best with triangles. So, do GPUs just break a quad into two triangles? If they do, aren't there some issues related to continuity across the junction?" CreationDate="2017-11-03T00:41:18.097" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="8834" PostId="5811" Score="0" Text="Sorry about thedelay, the black lines are not the issue, the issue  are the red line and red dots." CreationDate="2017-11-03T01:10:55.770" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="8836" PostId="5815" Score="0" Text="I *imagine* it's been borrowed from audio where, I guess, it implies you can have accurate reproduction at both high and low volumes (within the same piece of music)" CreationDate="2017-11-03T08:39:24.033" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8837" PostId="2519" Score="0" Text="@ivokabel How is the term $-1+ \sqrt{1+\alpha^2tan^2\theta}\over 2$ derived?" CreationDate="2017-11-03T09:37:25.893" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="8838" PostId="5811" Score="0" Text="Is it a HW renderer or software? One possible reason for this could be a lack of Z precision in that a red surface (though further away) is resulting in the same Z-buffer depth (due to limited precision) at a problematic pixel as the neighbouring cube's blue surface.  Things you could try: 1) Move the front plane closer to the object to try to get more precision or 2) turn off &quot;equals&quot; in the Z test and draw all blue, then red &amp; yellow surfaces." CreationDate="2017-11-03T12:33:06.073" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8839" PostId="5816" Score="0" Text="*&quot;plot these seamns in a 3D matrix&quot;*? Can you give a short explanation what you understand as a &quot;3D matrix&quot;?" CreationDate="2017-11-03T12:33:58.207" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8840" PostId="5806" Score="0" Text="Yes, they do. And yes, there are (or can be), but those are relatively minor depending on your geometry. In fact modern APIs for hardware accelerated rasterization have dropped the concept of a quad altogether, since it has always been just a convenience feature without any special treatment compared to just two triangles. So nowadays even the breaking into two triangles isn't done by the GPU but has to be done by you." CreationDate="2017-11-03T13:32:09.157" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8841" PostId="5811" Score="0" Text="Just to be clearer by &quot;Move the front plane closer&quot; I meant the front clipping plane." CreationDate="2017-11-03T16:42:17.637" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8842" PostId="5816" Score="0" Text="Yup! Essentially as voxels. So I'm imagining a 100x100x100 3D space. I would plot the meshes in that space. Then I would plot the seams in that same space, on top of the meshes. Is that clearer?" CreationDate="2017-11-03T17:47:29.923" UserId="7622" ContentLicense="CC BY-SA 3.0" />
  <row Id="8844" PostId="5817" Score="0" Text="I unwrapped a cube in Blender into six islands to test out what you're using. I got 8 vertices but each was linked to 3 different textures, like so: f 1/1/1 2/2/1 3/3/1 4/4/1 f 5/5/2 8/6/2 7/7/2 6/8/2 f 1/9/3 5/5/3 6/10/3 2/11/3 f 2/12/4 6/13/4 7/14/4 3/15/4 f 3/16/5 7/17/5 8/18/5 4/19/5 f 5/5/6 1/9/6 4/20/6 8/6/6 Is this an example of the vertices having been deduplicated? (Because there is only one instance of each vertex but each is linked to several textures)" CreationDate="2017-11-03T20:29:52.190" UserId="7622" ContentLicense="CC BY-SA 3.0" />
  <row Id="8845" PostId="5817" Score="0" Text="@ganesha123 yeah, I dislike the obj format but this is the one thing it's decent for." CreationDate="2017-11-03T20:40:54.587" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="8846" PostId="5811" Score="0" Text="Sounds like a Z-fighting issue? I don’t think OpenGL guarantees precise polygon rasterization." CreationDate="2017-11-03T23:47:18.773" UserId="7624" ContentLicense="CC BY-SA 3.0" />
  <row Id="8849" PostId="5827" Score="0" Text="thanks for the answer. i use a quad tree, so the removal depends on the quad tree level that is displayed.&#xA;my idea would be to set a threshold based on the LOD level and remove trees up to that threshold (although first fading them is probably a visually good idea). my problem however is the randomization of tree removal, since i don't want to accidentally remove a cluster of trees and produce big holes in my forest that way" CreationDate="2017-11-06T15:04:13.337" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8850" PostId="5827" Score="0" Text="This depends on your data set. If it's small, nothing beats manual tagging for removal. Otherwise, randomly assign an LOD level to trees in each &quot;region&quot; based on desired coverage. How you define those regions largely depends on how your data is stored. You can also probably get fancy and refine the tagging process somehow. I would however expect the simple random tagging to suffice on average. It's also much easier to implement than any other alternative!" CreationDate="2017-11-06T15:13:29.487" UserId="7644" ContentLicense="CC BY-SA 3.0" />
  <row Id="8852" PostId="5827" Score="0" Text="the quadtree is being introduced because if very large data sets. also, the trees are randomly placed based upon settings the customer will use for the forest, so manual tagging is not an option.&#xA;&#xA;okay, thank you. for now i will try the random removal then, otherwise i will experiment with marking nearest neighbors of removed trees to prevent removal of close-by trees." CreationDate="2017-11-06T15:20:33.593" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8853" PostId="5825" Score="0" Text="Are these guaranteed to be the exact color you've shown above, or might they vary a bit from region to region?" CreationDate="2017-11-07T03:21:39.920" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8855" PostId="5825" Score="0" Text="@user1118321 Let’s assume they’re exact for now." CreationDate="2017-11-07T08:53:22.393" UserId="7641" ContentLicense="CC BY-SA 3.0" />
  <row Id="8856" PostId="5824" Score="3" Text="That's a nice explanation, but seems to miss answering the actual question, which didn't ask for what HDR is but just what the D in there derives from etymologically." CreationDate="2017-11-07T09:07:56.210" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8857" PostId="4654" Score="0" Text="&quot;Vulkan also exists but I don't know whether this API is mature enough yet.&quot; It's mature but Apple doesn't support it, so you won't meet your goal of running on any OS." CreationDate="2017-11-07T14:28:38.273" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8858" PostId="5835" Score="1" Text="Although this is a clear answer i would really like to see the process animated. But hey we are graphics programmers that shouldn't be too hard. Ill try tomorrow." CreationDate="2017-11-07T20:49:34.520" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8860" PostId="5835" Score="1" Text="Also, what software did you use to draw/generate this?" CreationDate="2017-11-08T01:22:29.563" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="8863" PostId="5835" Score="0" Text="@Makogan you may want to look at [this post](https://computergraphics.stackexchange.com/questions/405/how-to-produce-simple-2d-illustrations-to-accompany-geometry-answers/) for ideas on how to draw. Though the choice is a highly personal one." CreationDate="2017-11-08T07:32:45.383" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8865" PostId="5835" Score="0" Text="Most of the software recommended there is payed software, I have a thing for free software :p" CreationDate="2017-11-08T08:32:39.480" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="8871" PostId="5787" Score="1" Text="There was a derivation done by Earl Hammon Jr in his talk, I hope that helps. He starts a derivation via Ray Tracing on slide 71. There is more information about it in the Appendix (slide 148ff.) I think.&#xA;&#xA;https://twvideo01.ubm-us.net/o1/vault/gdc2017/Presentations/Hammon_Earl_PBR_Diffuse_Lighting.pdf" CreationDate="2017-11-08T15:53:58.183" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8872" PostId="5787" Score="0" Text="@Tare Thanks so much, this is exactly what I needed! I'm gonna post an answer with the derivation soon." CreationDate="2017-11-08T18:26:06.650" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="8874" PostId="5386" Score="0" Text="So, the only reason not to include all extensions, is that you have a smaller header file generated from glad?" CreationDate="2017-11-09T09:04:13.073" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="8875" PostId="5835" Score="1" Text="@Makogan I made the diagrams in [asymptote](http://asymptote.sourceforge.net/), which is open-source." CreationDate="2017-11-09T09:09:45.870" UserId="7647" ContentLicense="CC BY-SA 3.0" />
  <row Id="8876" PostId="5835" Score="1" Text="@joojaa I've added animations of quadratic and cubic B-spline generation." CreationDate="2017-11-09T12:37:33.663" UserId="7647" ContentLicense="CC BY-SA 3.0" />
  <row Id="8877" PostId="5386" Score="2" Text="@Startec: It's not *just* a smaller header. It means that you can't *accidentally* use extensions that you don't want to. It also means that you get better IntelliSense, since you're only going to see functions and enums that you want to be able to use." CreationDate="2017-11-09T14:25:06.013" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8878" PostId="5835" Score="0" Text="@gilgamec yeah that meskes it easier to follow" CreationDate="2017-11-09T14:56:45.187" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8879" PostId="5835" Score="0" Text="@Makogan atleast half of all the suggested tools are free, its just that if you want nice sofdtware that do all you need quickly you need to pay. But obviously a researcher has access to even nonfree software for free." CreationDate="2017-11-09T14:58:41.900" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8881" PostId="5845" Score="0" Text="+1 - just not need break the font into the parts, because the font itself is stored in the `ttf` as a pairs of x-y coordinates. (the connections between the points could be straight or some are quadratic beziers). Also see my answer above - it is _too complicated_ for my use-case. **Thanks** anyway for the answer it kicked me to the right direction - e.g. start studying the ttf font structure. :)" CreationDate="2017-11-10T09:16:48.450" UserId="7661" ContentLicense="CC BY-SA 3.0" />
  <row Id="8884" PostId="5847" Score="1" Text="See also Giliam de Carpentier's series of articles on terrain on [his blog](http://www.decarpentier.nl/)—look for the &quot;Scape&quot; articles." CreationDate="2017-11-10T23:23:13.840" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8885" PostId="5855" Score="0" Text="So to sum up *never* (even when talking to OpenGL) use `GLvoid` and only use GLtypes when talking to OpenGL.  If you need fixed bitdepth integers in your code that does not talk to OpenGL use `cstdint`?" CreationDate="2017-11-12T02:32:27.357" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="8886" PostId="5855" Score="0" Text="And why never use `GLvoid`?" CreationDate="2017-11-12T02:37:37.610" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="8887" PostId="5855" Score="0" Text="`GLuint` has a meaning that is distinct from the fundamental type `unsigned int`. `GLvoid` is no different from `void`." CreationDate="2017-11-12T02:47:59.233" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8889" PostId="5859" Score="0" Text="Ah, that makes sense. So, I'm guessing from your statement, that the remapped roughness is just to make the artists happy, right? It's not required." CreationDate="2017-11-13T00:12:01.873" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8890" PostId="5850" Score="0" Text="Can you explain more clearly what you're trying to do, which information is given and which you are trying to solve for?" CreationDate="2017-11-13T03:53:08.377" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8892" PostId="5859" Score="0" Text="Yes, it's unnecessary. If you look into Scientific Papers (like the original GGX Paper by Walter et. al), there is no remapping done.&#xA;&#xA;Also from a logical viewpoint, if you use $0.25$ as your roughness or $0.5^2$ as your roughness shouldn't make a difference." CreationDate="2017-11-13T07:18:39.050" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8895" PostId="5864" Score="0" Text="Thank you for the answer. but what is the algorithm if I one wants to implement it and not using 3rd parties? I use c# language." CreationDate="2017-11-13T10:18:02.697" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8896" PostId="5864" Score="0" Text="Just to note that my question referred to how the black and white stripped image is created given the sine or whatever signal." CreationDate="2017-11-13T10:28:14.170" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8897" PostId="5864" Score="0" Text="In that case, could you edit your question and make it clearer? Regardless of language, the short answer is &quot;make an array of values from your function and then convert it into the image type you're using&quot;." CreationDate="2017-11-13T10:40:48.127" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8898" PostId="5864" Score="0" Text="Sorry about the question not being clear enough. I have updated it. I still don't understand how to do it could you give an example please." CreationDate="2017-11-13T12:04:35.277" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8899" PostId="5862" Score="0" Text="You need to explain where you're stuck. Your first example even includes all the Matlab code needed to make the image!" CreationDate="2017-11-13T14:15:18.120" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8900" PostId="5862" Score="0" Text="Thanks. I was not clear as to how get pixel colour based on the function. It works now" CreationDate="2017-11-13T15:11:27.000" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8901" PostId="4968" Score="1" Text="@metalim it seems that, for many implementations as of this writing, the maximum time that the client is allowed to wait is 0. You may check the value with `gl.getParameter(gl.MAX_CLIENT_WAIT_TIMEOUT_WEBGL)`. That MAX_CLIENT_WAIT_TIMEOUT_WEBGL is an enum and not a value is not well presented in the documentation which was (for me at least) a source of confusion." CreationDate="2017-11-13T16:59:00.170" UserId="7679" ContentLicense="CC BY-SA 3.0" />
  <row Id="8902" PostId="5867" Score="0" Text="A simple lerp (linear interpolation) between a light and dark blue would work." CreationDate="2017-11-13T19:39:19.787" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8903" PostId="5855" Score="0" Text="On my system I see `typedef unsigned int    GLuint;` and `typedef void        GLvoid;` so how is `GLvoid` no different but `GLuint` is?" CreationDate="2017-11-13T20:16:25.147" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="8904" PostId="5855" Score="1" Text="@Startec: Headers will redefine `GLuint` based on the platform(s) they work on. `GLvoid` will *always* be defined as `void`." CreationDate="2017-11-13T20:18:25.890" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8905" PostId="5867" Score="5" Text="Even for a given time of day (and a fixed atmospheric turbidity), the color of the sky is different in different directions. [Mitsuba](https://www.mitsuba-renderer.org/) uses the model of [Hosek and Wilkie](http://cgg.mff.cuni.cz/projects/SkylightModelling/), which I assume is the state of the art." CreationDate="2017-11-13T21:49:10.187" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="8907" PostId="5859" Score="0" Text="@Matthias: That looks like an interesting read. I'll make sure to read through it, thanks for the link." CreationDate="2017-11-14T00:43:57.673" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8908" PostId="5863" Score="0" Text="I noticed while learning basic Direct3D recently, you generally specify a single framebuffer when creating your swapchain. I'm guessing this simply gets blitted to the compositor when 'swapping' buffers? How about when switching to fullscreen mode, does a separate front buffer get created for you in that case?" CreationDate="2017-11-14T04:05:09.740" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8909" PostId="5867" Score="1" Text="When you say &quot;sky&quot; do you include clouds and the sun? Just earlier tonight part of the sky was bright pink and peach for a few minutes just before the sun went down." CreationDate="2017-11-14T05:30:16.500" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8910" PostId="5868" Score="0" Text="What about debugging tools? Do you have any experience with Nsight vs CodeXL? Which has the quickest/easiest edit -&gt; run -&gt; debug cycle?" CreationDate="2017-11-14T06:49:34.853" UserId="7674" ContentLicense="CC BY-SA 3.0" />
  <row Id="8911" PostId="5859" Score="0" Text="Edit: I removed my comments to merge them in an answer." CreationDate="2017-11-14T09:01:30.220" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8912" PostId="5863" Score="0" Text="I don't know the specifics about Direct3D, but both those guesses are plausible. Typically the application framebuffer looks like a texture to the compositor, which it'll put on a quad of the appropriate size and position. I have no idea what fullscreen on MS Windows does: it may be something more complicated." CreationDate="2017-11-14T10:47:03.277" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8913" PostId="5863" Score="0" Text="If you ask a new question specifically about D3D fullscreen, it's more likely to be seen by someone who knows." CreationDate="2017-11-14T10:48:05.200" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8914" PostId="5863" Score="0" Text="OK, was just curious about how this worked in general, didn't want to open a duplicate-ish question. I don't think it's API-specific anyway, since OpenGL under Windows uses the same DXGI backend as D3D does." CreationDate="2017-11-14T12:09:42.933" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8915" PostId="5868" Score="0" Text="Although CodeXL is currently being developed by AMD, you can debug NVidia cards with it as well. I think just OpenCL won't work on Nvidia cards.&#xA;I can't say which is better though, and there are more tools (like RenderDoc) out there." CreationDate="2017-11-14T13:52:27.007" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8916" PostId="5868" Score="0" Text="@Tare [OpenCL support is included in the latest NVIDIA GPU drivers](https://developer.nvidia.com/opencl)." CreationDate="2017-11-14T14:33:22.750" UserId="5349" ContentLicense="CC BY-SA 3.0" />
  <row Id="8917" PostId="5868" Score="0" Text="@Pikalek Okay, but is the debugging of OpenCL automatically supported in CodeXL? I haven't used it for a while, but I always got a warning about that." CreationDate="2017-11-14T14:47:34.943" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8918" PostId="5868" Score="0" Text="@Tare Offhand, I don't know either. My general point is to focus on the big picture, not the small details. To me, casual hobbyist dev means low stakes, casual research." CreationDate="2017-11-14T15:17:25.537" UserId="5349" ContentLicense="CC BY-SA 3.0" />
  <row Id="8919" PostId="5871" Score="0" Text="Thank you for your help, I can understand your answer, but I don’t know which function should I choice to calculate the density?" CreationDate="2017-11-15T04:17:39.013" UserId="7675" ContentLicense="CC BY-SA 3.0" />
  <row Id="8920" PostId="5732" Score="1" Text="note that $angle$ here is called the polar angle and $phi$ here is called the azimuth angle.&#xA;how you translate between spherical coordinates and euler angles depends on the way your euler angles are defined, since there are multiple ways to use them.&#xA;if you have access to these books, they have information about coordinate conversions and euler angles:&#xA;Real-Time Rendering 3rd Edition (in the fourth chapter there is information about Euler Angles)&#xA;Mathematics for 3D Game Programming and Computer Graphics 3rd Edition (in the appendix there is a lot about cartesian and spherical coordinates)" CreationDate="2017-11-15T07:52:02.800" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="8924" PostId="5855" Score="3" Text="@Startec You can as well look at it with the logic you use in your question. `GLuint` makes sense, because you communicate that value to the GL, you either put it into a GL function or it's returned from a GL function. But that `GLvoid` function is **your** function, a C++ function that doesn't return anything anyway, let alone a GL value, you don't go `return glWhatever(...);`, since you don't return anything. `void` is just not a meaningful *data* type, whose representation would have any relevance, because it has none." CreationDate="2017-11-15T10:58:36.497" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="8925" PostId="5867" Score="2" Text="You probably need the altitude, time of year, latitude, longitude, etc. to achieve a perfect result. For instance, there are days on the Arctic, when it's always black" CreationDate="2017-11-15T16:56:37.910" UserId="7699" ContentLicense="CC BY-SA 3.0" />
  <row Id="8926" PostId="5867" Score="0" Text="https://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/simulating-sky/simulating-colors-of-the-sky" CreationDate="2017-11-15T17:56:06.767" UserId="7699" ContentLicense="CC BY-SA 3.0" />
  <row Id="8929" PostId="5881" Score="0" Text="I only have the smart alecky comment to make, that nothing in nature lines up perfectly, so I'm sure it isn't exactly 100hz, or the same every frame. But, I have no idea how close it is.  There is probably a bit of inaccuracy in whatever is measuring the hz as well." CreationDate="2017-11-15T23:32:05.740" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="8930" PostId="5881" Score="2" Text="100 Hz is _far_ too low for any PC graphics card clock speed from the last couple of decades. Even if the chip is completely idle and clocked down as far as it goes, it should still be running at some tens or hundreds of MHz. I guess 100 Hz could conceivably be the video refresh rate, not the clock speed. But that would still be strange, as refresh rates are almost always multiples of 60 Hz these days." CreationDate="2017-11-16T00:03:26.527" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8931" PostId="5881" Score="1" Text="100Hz is far too low for any PC graphics card clock, ever. Formerly PAL countries continue to broadcast at 50Hz though, for the record. So multiples of 50 aren't too surprising." CreationDate="2017-11-16T03:27:56.423" UserId="7704" ContentLicense="CC BY-SA 3.0" />
  <row Id="8932" PostId="5877" Score="0" Text="So it’s more the physical hardware make up (rgb filter, pixel size, backlight) that determines the gamut than say software like a color profile?" CreationDate="2017-11-16T11:49:20.447" UserId="7693" ContentLicense="CC BY-SA 3.0" />
  <row Id="8933" PostId="5884" Score="0" Text="Need some help with the tags." CreationDate="2017-11-16T13:52:59.227" UserId="5846" ContentLicense="CC BY-SA 3.0" />
  <row Id="8934" PostId="5884" Score="1" Text="The two images you have show completely unrelated techniques. The first is placing 3D objects on a surface. The second just downsamples a 2D image to 1-bit using [dithering](https://en.wikipedia.org/wiki/Dither). Which are you interested in?" CreationDate="2017-11-16T14:25:16.117" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="8935" PostId="5884" Score="0" Text="@DanHulme ya OK I see what you mean - there is no effort to show 3D in the 2nd one, it's just coming from shading (original lighting) of the model. I've removed it." CreationDate="2017-11-16T14:27:14.963" UserId="5846" ContentLicense="CC BY-SA 3.0" />
  <row Id="8936" PostId="5871" Score="0" Text="That's already in the answer: use the count." CreationDate="2017-11-16T14:31:44.680" UserId="5349" ContentLicense="CC BY-SA 3.0" />
  <row Id="8937" PostId="5883" Score="0" Text="I'm not seeing the part where you use a member barrier to ensure visibility." CreationDate="2017-11-16T16:15:47.317" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="8938" PostId="5885" Score="0" Text="The distance impostor concept is somehow related to the classical &quot;impostor&quot; definition, but I do not get how. The concept should be more clear in this paper http://sirkan.iit.bme.hu/~szirmay/ibl3.pdf" CreationDate="2017-11-16T19:07:07.547" UserId="7686" ContentLicense="CC BY-SA 3.0" />
  <row Id="8939" PostId="5884" Score="1" Text="The kinect can produce depth images for you. The depth image essentially gives you a set of points in 3d space. A point cloud much like what we see above. You'll have to downsample, the resolution will be much too high. Then to get the shade of them (steeper areas should have darker colors) you can take the gradient of the image for your normals. Not sure how his lights are positioned or what kind of lights he's using exactly though. There does seem to be some sort of blur going on or maybe that's the fog." CreationDate="2017-11-17T00:24:22.590" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8940" PostId="5877" Score="0" Text="@ohmmy  a color profile is a measurement of  a physical devices capabilities. You can emulate another device, and much of the standardisation has gone to define what to do when you can not reach the colors. But yes your gamut is fixed by the hardware. Obviously though if you have a wide gamut device it can emulate a device that is not so wide. So you can go down and you can use a intent to describe how to emulate when you can nlt." CreationDate="2017-11-17T00:46:54.953" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="8941" PostId="5877" Score="0" Text="Thanks. Makes sense now that it’s hardware and can only emulate a gamut below what’s been designed originally" CreationDate="2017-11-17T01:07:29.023" UserId="7693" ContentLicense="CC BY-SA 3.0" />
  <row Id="8942" PostId="5883" Score="1" Text="@Nicol Bolas - Do we still need to use a memory barrier when &#xA;1) - We aren't reading/writing from/to a same image.&#xA;2) - We aren't overwriting any pixel of a image by subsequent `ImageStore`&#xA;&#xA;So there wouldn't be any need for that since every invocation is writing to a separate pixel?" CreationDate="2017-11-17T07:24:21.763" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="8944" PostId="5883" Score="1" Text="I think you only need the barrier on the C/C++ side, after the compute shader has run but before passing the texture to the rendering command. Something like glMemoryBarrier(GL_SHADER_IMAGE_ACCESS_BARRIER_BIT);" CreationDate="2017-11-17T08:13:36.553" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="8945" PostId="5883" Score="2" Text="If you found a solution, post it as an answer below." CreationDate="2017-11-17T12:42:24.917" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="8947" PostId="5888" Score="0" Text="With regard to the idleness of threads: for MSAA $T$x, $T \times w \times h$ threads are used for retrieving data of which $w \times h$ threads perform the averaging and store the final results; for SSAA $T$x, $T \times T \times w \times h$ threads are used for retrieving data of which $w \times h$ threads perform the averaging and store the final results. Here, $w$ and $h$ represent the number of ouput texels for the width and height." CreationDate="2017-11-17T17:14:15.997" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8949" PostId="5888" Score="0" Text="Agree with this answer. Using a thread per texture read isn't likely to get your data out of memory any faster than issuing $T$ texture reads per thread—in either case you're almost certainly going to be memory-bandwidth-bound. So by using all those extra threads you're just reducing your occupancy and taking longer to get through all the threads in the dispatch." CreationDate="2017-11-18T00:05:06.693" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8950" PostId="5887" Score="0" Text="FYI it's generally considered more useful to report timings in milliseconds rather than in FPS, especially when comparing deltas between different techniques." CreationDate="2017-11-18T00:06:31.663" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8952" PostId="5869" Score="1" Text="Great job at summarizing everything! I'll also make sure to check out the BRDF Reference and test out some of them." CreationDate="2017-11-18T14:54:55.693" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8953" PostId="5869" Score="0" Text="@DanielKareh you can also take a look at my [brdf.hlsl](https://github.com/matt77hias/MAGE/blob/master/MAGE/MAGE/shaders/brdf.hlsli) which includes even more BRDF components :)" CreationDate="2017-11-18T15:02:09.103" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="8954" PostId="5805" Score="1" Text="I'm on the &quot;split the quad into two triangles&quot; camp. I've written about how interpolation works in a triangle in some detail here: http://www.gabrielgambetta.com/computer-graphics-from-scratch/shaded-triangles.html It doesn't use barycentric coordinates at all, so it complements the other answer nicely :)" CreationDate="2017-11-14T18:33:51.950" UserId="7692" ContentLicense="CC BY-SA 3.0" />
  <row Id="8955" PostId="5792" Score="0" Text="I've written a detailed, step by step derivation of the math here: http://www.gabrielgambetta.com/computer-graphics-from-scratch/basic-ray-tracing.html#ray-meets-sphere Hope this helps :)" CreationDate="2017-11-14T18:31:31.283" UserId="7692" ContentLicense="CC BY-SA 3.0" />
  <row Id="8958" PostId="5895" Score="0" Text="_you MUST bind the VBO's under a VAO to get rendering output, if you just bind the VAO you won't get outpt._ That's not true. You can bind dummy VAO without attached VBOs and still dispatch drawcall." CreationDate="2017-11-20T10:06:30.390" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="8959" PostId="5896" Score="0" Text="Thanks, I'm aware of the Sobel operator, but it slows my algorithm down too much. The MO answer assumes I have the isoline as a set of points. I don't, I have a 2D array of grayscale values." CreationDate="2017-11-20T12:29:04.790" UserId="3085" ContentLicense="CC BY-SA 3.0" />
  <row Id="8960" PostId="5896" Score="0" Text="In that case, you might consider posting your code to the [CodeReview StackExchange](https://codereview.stackexchange.com/) and ask for performance help." CreationDate="2017-11-20T15:50:55.973" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="8961" PostId="54" Score="1" Text="I think the answer is &quot;always&quot; *if* the compute shader is done properly. This is not trivial to achieve. A compute shader is also a better match than a pixel shader conceptually for image processing algorithms. A pixel shader however provides less leeway with which to write poorly performing filters." CreationDate="2017-11-20T18:32:48.447" UserId="7644" ContentLicense="CC BY-SA 3.0" />
  <row Id="8962" PostId="54" Score="0" Text="@bernie Can you clarify what's needed for the compute shader to be &quot;done properly&quot;? Maybe write an answer? Always good to get more perspectives on the subject. :)" CreationDate="2017-11-20T18:36:11.990" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="8963" PostId="5896" Score="0" Text="I could, but the folks over there are less likely to have the specialized expertise for doing this *graphics* thing efficiently. Thanks again." CreationDate="2017-11-20T18:58:40.073" UserId="3085" ContentLicense="CC BY-SA 3.0" />
  <row Id="8964" PostId="54" Score="3" Text="Now look at what you made me do! :)" CreationDate="2017-11-20T21:01:12.340" UserId="7644" ContentLicense="CC BY-SA 3.0" />
  <row Id="8965" PostId="5907" Score="0" Text="That's cool. I'll need to give Vulkan a try one of these days, I had a look at it when it came out but got scared off by the 1000-line hello triangle program!" CreationDate="2017-11-21T12:21:58.317" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="8966" PostId="5907" Score="0" Text="TBH most of those 1000 lines busywork is filling in structs that 80% of the time will not need to change, but are there for the remaining 20% to have a good path. Once you understand the general architecture of the api and can place why each struct is important it's not that horrible." CreationDate="2017-11-21T12:25:33.937" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="8971" PostId="5913" Score="0" Text="yes the only data I have is matrix itself. Only matrices (world, view, proj, projView) are cached inside camera." CreationDate="2017-11-22T13:00:10.290" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="8972" PostId="5913" Score="0" Text="See my edited answer" CreationDate="2017-11-22T13:36:21.773" UserId="7725" ContentLicense="CC BY-SA 3.0" />
  <row Id="8973" PostId="5913" Score="0" Text="but this  [Decompose the OpenGL projection matrix](http://lektiondestages.blogspot.com.tr/2013/11/decompose-opengl-projection-matrix.html) works for glFrustum, maybe some similar formula can work for glPerspective, no way? I will try your steps (thanks), it also seems interesting and matching to my goal (get and convert view frustum coordinates to another view space (light's point of view)" CreationDate="2017-11-22T13:52:55.327" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="8974" PostId="5913" Score="0" Text="That formula also works for getting near and far from glPerspective, only left, right, top and bottom need to be calculated. Maybe m00 (f / aspect) and m11 (f) relation could help to get aspectRatio. Then we will have aspectRatio, near and far, then maybe we could get the remain values, I will give a try" CreationDate="2017-11-22T14:10:10.110" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="8975" PostId="5913" Score="0" Text="Well, I think it does not matter whether the matrix was created with glFrustum or gluPerspective. If the formula from the link you provided works &quot;for glFrustum&quot; it should equally well work &quot;for gluPerspective&quot;. In fact, gluPerspective is built on top of glFrustum. gluPerspective simply &quot;remaps&quot; fov and aspect on left/right/top/bottom and calls glFrustum." CreationDate="2017-11-22T14:19:40.970" UserId="7725" ContentLicense="CC BY-SA 3.0" />
  <row Id="8976" PostId="5913" Score="0" Text="My confusion was that some items used to decompose always is zero in glPerspective, maybe this doesn't change the result. I will compare same view frustum for both" CreationDate="2017-11-22T14:31:58.370" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="8977" PostId="5869" Score="0" Text="Heh, thanks :)The main problem I find is that lots of the lighting models I find on Google or in papers are, I guess, scientific and don't include roughness, so your collection will be pretty useful." CreationDate="2017-11-22T15:56:24.877" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="8980" PostId="5915" Score="4" Text="Nvidia Flex uses positional based fluids (PBF) for simulations and a variation of screen space fluids (SSF) for rendering fluids. Rendering and simulating fluids are two very separate tasks. Just start looking at some fluid simulation code on github and connect the ideas from books/papers/ppts to the code. In particular, the video shows PBF and SSF. But there are many other fluid simulation techniques like FLIP and SPH. http://mmacklin.com/pbf_sig_preprint.pdf" CreationDate="2017-11-23T02:14:48.397" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="8981" PostId="5915" Score="0" Text="Thank you for the information, Mr. Wilson. I will take a look at all these techniques and try them all." CreationDate="2017-11-23T02:50:39.207" UserId="4839" ContentLicense="CC BY-SA 3.0" />
  <row Id="8986" PostId="5867" Score="1" Text="Are you asking about the Earth's sky?" CreationDate="2017-11-23T14:24:44.097" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="8987" PostId="5904" Score="0" Text="For some reason the shadertoy did not run on my browser. Could you post a/some simple image(s) to demonstrate what you mean?" CreationDate="2017-11-23T15:20:25.960" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="8989" PostId="5904" Score="1" Text="Shouldn't it be more like n=]-1, 1[?" CreationDate="2017-11-24T06:35:25.330" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="8996" PostId="5913" Score="0" Text="I tried to implement what you said to get frustum vertices in world space, is it correct? Here: https://gist.github.com/recp/8d9a3f65d3573bd2775b658ffab46212" CreationDate="2017-11-25T15:42:01.100" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="8997" PostId="5627" Score="0" Text="`fwrite(buffer, sizeof(int)*width*height, 1, ffmpeg);` is throwing a Seg fault" CreationDate="2017-11-25T16:06:28.200" UserId="6025" ContentLicense="CC BY-SA 3.0" />
  <row Id="8998" PostId="54" Score="0" Text="In addition to sharing work across threads, ability to use async compute is one big reason to use compute shaders." CreationDate="2017-11-25T17:57:45.973" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="8999" PostId="5924" Score="3" Text="Also, W &lt; 0 for points behind the camera. So if you divide by W first, you then cannot tell whether a point/primitive was behind the camera or in front of it." CreationDate="2017-11-26T03:05:57.547" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9001" PostId="5925" Score="0" Text="Yes, no, possibly. Not as a normal physically based 3D but still traditional animation has done this for ages. But its a trick that relys on a nonphysically correct image an stylized look and feel. Hard to say anything useful as the question is a bit open ended" CreationDate="2017-11-26T15:14:03.567" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9005" PostId="5925" Score="0" Text="Sure, you can pre-render the entire sequence of images along some dense sampling of the path, what's the problem?" CreationDate="2017-11-26T19:36:34.470" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9006" PostId="5925" Score="0" Text="@Rahul, is it any faster than Render Time x number of samples?" CreationDate="2017-11-26T19:43:45.050" UserId="7743" ContentLicense="CC BY-SA 3.0" />
  <row Id="9007" PostId="5925" Score="1" Text="What &quot;efficiency&quot; are you worried about? Pre-rendering image sequences has a big up-front cost per scene but a tiny cost for the client. Are you trying to reduce the amount of data you need to give the client, or the amount of up-front work (e.g. because the scene is generated afresh each time)?" CreationDate="2017-11-27T10:17:01.393" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9008" PostId="5924" Score="1" Text="Last fact seems most important to me" CreationDate="2017-11-28T12:46:25.150" UserId="7737" ContentLicense="CC BY-SA 3.0" />
  <row Id="9009" PostId="5625" Score="0" Text="Nicest introduction by far: [Disney's Practical Guide to Path Tracing](https://www.youtube.com/watch?v=frLwRLS_ZR0)" CreationDate="2017-11-28T13:29:45.853" UserId="18" ContentLicense="CC BY-SA 3.0" />
  <row Id="9010" PostId="5013" Score="2" Text="As a hint for the people wanting to learn this, the delaunay triangulation can be build with voronoi diagrams. A vornoi diagram takes an unordered set of points and calculates vornoi regions such that each region contains exactly one point and the border of a region is set such that the two points of the bordering fields have exactly the same distance to the border.&#xA;&#xA;Once the voronoi diagram is build, the delaunay triangulation is a simple matter of connecting each point with the points of the neighbouring voronoi regions." CreationDate="2017-11-28T14:03:21.553" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9011" PostId="5924" Score="0" Text="But I doubt it's true. Could you prove it or provide an example?" CreationDate="2017-11-28T14:44:29.060" UserId="7737" ContentLicense="CC BY-SA 3.0" />
  <row Id="9012" PostId="5924" Score="0" Text="@Nolan: The point (0, 0, 0, -1). Divide by -1, and it's in the [-1, 1] range. But it can't be in the range of [1, -1], since that is an empty set." CreationDate="2017-11-28T15:10:57.973" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9013" PostId="5924" Score="0" Text="You can't get point (0, 0, 0, -1) after multiplying by the projection matrix. It requires initial point have w coordinate equal 0 or have weird projection planes like negative near or far behind near." CreationDate="2017-11-28T16:50:23.430" UserId="7737" ContentLicense="CC BY-SA 3.0" />
  <row Id="9014" PostId="5924" Score="0" Text="@Nolan: Nowhere in OpenGL is there an assumption that `gl_Position` can only arise by &quot;multiplying by a projection matrix&quot;. The specification must deal with *all* possible values, not just the ones that come about through the usual means." CreationDate="2017-11-28T16:53:26.857" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9015" PostId="5924" Score="0" Text="Also I would argue this point must be clipped. You can multiply (0,0,0,-1) by (-1) and get (0,0,0,1) which denotes same point in 3D but shouldn't be clipped according to its w coordinate." CreationDate="2017-11-28T17:04:57.883" UserId="7737" ContentLicense="CC BY-SA 3.0" />
  <row Id="9016" PostId="5924" Score="0" Text="@Nolan: But it cannot be clipped in NDC space, because in NDC space, the point is just (0, 0, 0). Remember: NDC is 3D space, not 4D. So it cannot tell the difference between a clip-space (0, 0, 0, 1) and (0, 0, 0, -1). The former should not be clipped; the latter should. NDC space can't tell the difference. This is why clipping is done in clip-space; because it has sufficient information to do it correctly." CreationDate="2017-11-28T17:06:22.437" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9017" PostId="5924" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/69391/discussion-between-nolan-and-nicol-bolas)." CreationDate="2017-11-28T17:08:10.540" UserId="7737" ContentLicense="CC BY-SA 3.0" />
  <row Id="9019" PostId="5932" Score="1" Text="It should also be noted that, while OpenGL only requires support for combined depth/stencil, the API still technically allows it for hardware that could handle it. Vulkan, by contrast, makes it *impossible* to separate them. In a Vulkan renderpass, there is exactly one depth/stencil attachment, in a render pass, and that image can provide depth, stencil, or both. But there is only one image allowed there." CreationDate="2017-11-29T03:34:59.240" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9020" PostId="5919" Score="0" Text="There are many ways of texture mapping a sphere with a rectangle. It's pretty well researched under the term [map projections](https://en.wikipedia.org/wiki/Map_projection)" CreationDate="2017-11-29T09:56:29.853" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9021" PostId="5886" Score="0" Text="OK I'll give your suggestions a try, but the BTLNews article [I've linked to in the question](http://www.btlnews.com/crafts/visual-fx/level-256-vfx-produces-the-final-moments-of-karl-brant-for-nerdist-com/) explains that they've started with a 3D representation or possibly a point cloud from a hacked Kinect box. So they start with an actual heigh of the surface, rather than generate one from a color or brightness. Possibly they re-interpolate it to a regular x-y grid of points with continuous z (z being &quot;out of the page&quot;)." CreationDate="2017-11-30T05:53:25.160" UserId="5846" ContentLicense="CC BY-SA 3.0" />
  <row Id="9022" PostId="3806" Score="0" Text="I've just asked [Algorithms to “anti-alias” (or somehow improve) binary 1-bit drawings and fonts](https://computergraphics.stackexchange.com/q/5934/5846), any thoughts?" CreationDate="2017-11-30T06:21:48.943" UserId="5846" ContentLicense="CC BY-SA 3.0" />
  <row Id="9024" PostId="4814" Score="0" Text="I can't imagine how this works in 3d, can you enlighten on that? Should you simply connect the vertex for each cube to the adjacent cubes that shared a crossing?" CreationDate="2017-11-30T06:52:32.053" UserId="7778" ContentLicense="CC BY-SA 3.0" />
  <row Id="9025" PostId="5935" Score="1" Text="Ah! for ellipses (and presumably arbitrary angle lines) PIL does draw fairly &quot;OK-looking&quot; 1-bit curved lines without standard antialiasing, https://i.stack.imgur.com/HGDox.png and in fact, [this answer](https://stackoverflow.com/a/2292690/3904031) shows how to do more arbitrary curves. So combining those with a search for historical bitmapped fonts, I should be good to go! If you'd like to include this info into your answer I'll delete this comment. Thanks!" CreationDate="2017-11-30T07:08:20.040" UserId="5846" ContentLicense="CC BY-SA 3.0" />
  <row Id="9026" PostId="4814" Score="0" Text="Also, how do you form a quad/triangle?" CreationDate="2017-11-30T07:42:27.557" UserId="7778" ContentLicense="CC BY-SA 3.0" />
  <row Id="9027" PostId="5935" Score="2" Text="The problem is, all those techniques trade off spatial resolution, and it's not like these displays have pixels to spare." CreationDate="2017-11-30T10:34:04.010" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9028" PostId="5936" Score="0" Text="windows has a comptr class for managing the handles" CreationDate="2017-11-30T16:23:54.247" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9029" PostId="5936" Score="0" Text="using comptr gives same problem.No idea why." CreationDate="2017-11-30T17:11:52.357" UserId="6064" ContentLicense="CC BY-SA 3.0" />
  <row Id="9030" PostId="5934" Score="1" Text="I don't know all the details of your specific usage case, but if you have a decent refresh rate, you could get anti aliasing by faking greyscale through rapidly turning on and off individual pixels.&#xA;&#xA;There's more to it than just that though, so you might find these two write ups interesting:&#xA;&#xA;https://blog.demofox.org/2017/10/31/animating-noise-for-integration-over-time/&#xA;&#xA;https://blog.demofox.org/2017/11/03/animating-noise-for-integration-over-time-2-uniform-over-time/" CreationDate="2017-11-30T17:11:54.367" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9031" PostId="5940" Score="0" Text="altough to be honest if you touchup a few pixels you can get a good compromize between the two." CreationDate="2017-11-30T21:17:42.623" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9032" PostId="5940" Score="0" Text="Excellent! This is exactly what I needed, thank you." CreationDate="2017-12-01T00:33:43.687" UserId="5846" ContentLicense="CC BY-SA 3.0" />
  <row Id="9033" PostId="5938" Score="0" Text="Thanks for the link. Since I'm new to this having something like this to read is very helpful." CreationDate="2017-12-01T00:37:00.790" UserId="5846" ContentLicense="CC BY-SA 3.0" />
  <row Id="9034" PostId="5934" Score="1" Text="@AlanWolfe that's a great idea! For this particular class of units the data rate is slow and there are no extra data buffers to toggle between, but I will keep this in mind, and if I have a unit where this would be possible I'll give it a go. Thanks!" CreationDate="2017-12-01T00:45:54.533" UserId="5846" ContentLicense="CC BY-SA 3.0" />
  <row Id="9035" PostId="5904" Score="0" Text="@JarkkoL Well the formula for converting floating point to integer is output=floor(input * intmax + n), where n=0.5 without noise, because you want (for example) &gt;=0.5 to round up, but &lt;0.5 down. That's why the noise is &quot;centered&quot; at 0.5." CreationDate="2017-12-01T17:12:47.560" UserId="2306" ContentLicense="CC BY-SA 3.0" />
  <row Id="9036" PostId="5919" Score="0" Text="Asking about how to write an algorithm to do this would be on topic, but software recommendation questions are not on topic on any Stack Exchange site, apart from [softwarerecs.se]." CreationDate="2017-12-01T19:57:42.160" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="9037" PostId="5925" Score="0" Text="I'd look into making BSP trees that were fourth dimensional - the fourth dimension being time." CreationDate="2017-12-01T20:40:59.173" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9038" PostId="5904" Score="0" Text="@SimonF added image of the shadertoy" CreationDate="2017-12-01T20:41:26.943" UserId="2306" ContentLicense="CC BY-SA 3.0" />
  <row Id="9039" PostId="5904" Score="1" Text="It seems you were truncating the output rather than rounding it (like GPUs do) - rounding instead, at least you get proper whites: https://www.shadertoy.com/view/MlsfD7&#xA;(image: https://i.stack.imgur.com/kxQWl.png )" CreationDate="2017-12-01T20:49:07.197" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="9041" PostId="5943" Score="0" Text="(sorry I hit enter accidentally, and edit time limit expired) The usecase in the example is one those situations where it would matter: rendering a floating point grayscale image on a 3-bit display device. Here the intensity changes greatly by just changing the LSB. I'm trying to understand if there is the any &quot;correct way&quot; to have end values map to solid colors, like compressing the value range and have the end values saturate. And what is the mathematical explanation of that? In the example formula, input value of 1.0 doesn't produce output that averages to 7, and that's what bothers me." CreationDate="2017-12-01T23:03:02.153" UserId="2306" ContentLicense="CC BY-SA 3.0" />
  <row Id="9042" PostId="5904" Score="0" Text="You could try to apply triangular weight function to the noise, e.g. replace     float n = n1+n2 - 0.5;&#xA;with:&#xA;float w = 1.5-2.25*abs(ci-0.5);&#xA;float n = (n1+n2-1.0)*w+0.5;" CreationDate="2017-12-02T03:45:32.257" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="9043" PostId="5904" Score="0" Text="Sorry, you were actually right to begin with :) If we round on output, the noise of course needs to be symmetric -1;1, which then results in your original image." CreationDate="2017-12-02T07:18:38.340" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="9044" PostId="5939" Score="0" Text="I'm not actually sure what the usual way to do this is, but if you could somehow know the size of a pixel at this intersection, you could add epsilons to the hit position to get numeric derivatives and use that pixel size to know what mip level to use." CreationDate="2017-12-02T15:59:41.820" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9045" PostId="5939" Score="2" Text="This question is related and has some good info in the answers: https://stackoverflow.com/q/1813303/2817105" CreationDate="2017-12-02T16:01:10.993" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9046" PostId="5936" Score="0" Text="Does the amount of &quot;leaked memory&quot; grow each time you do this or only the first? if only the first it might be something like, even though you released an object (ref count decriment) it's deciding to keep it around until your app closes or something." CreationDate="2017-12-02T16:03:42.947" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9047" PostId="5936" Score="0" Text="Each time that too the same amount by which it was started" CreationDate="2017-12-02T16:10:17.293" UserId="6064" ContentLicense="CC BY-SA 3.0" />
  <row Id="9048" PostId="5936" Score="0" Text="Around 18 mb to be exact" CreationDate="2017-12-02T16:10:48.837" UserId="6064" ContentLicense="CC BY-SA 3.0" />
  <row Id="9049" PostId="5942" Score="0" Text="You mean instead of directly sampling whatever combined noise again at the new scale value per mip?  And the goal is to speed-up mip generation?" CreationDate="2017-12-02T20:21:29.843" UserId="2831" ContentLicense="CC BY-SA 3.0" />
  <row Id="9050" PostId="5926" Score="0" Text="Did you know that you can path trace without doing direct light sampling? You rely on emissive terms to add the lighting energy to the scene.  You would use fresnel to calculate how much reflects (specular) vs is absorbed (diffuse / refraction / subsurface scattering) and roll a random number to choose which of those to do, based on that reflectance amount." CreationDate="2017-12-04T00:15:55.733" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9051" PostId="5952" Score="0" Text="It's amazing that the lerp at the edges gives a perfect looking result. I would expect at least a little deviation :P" CreationDate="2017-12-04T03:15:18.843" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9053" PostId="5949" Score="0" Text="I used the debug macro with the other other answer. Thanks." CreationDate="2017-12-04T16:23:33.083" UserId="7814" ContentLicense="CC BY-SA 3.0" />
  <row Id="9055" PostId="5953" Score="0" Text="I dont think a analytic solution is even possible." CreationDate="2017-12-04T16:39:08.807" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9057" PostId="5957" Score="1" Text="As a first debugging step, you should replace the `Parallel.for` with a normal loop, to make sure a concurrency problem isn't the source of your error." CreationDate="2017-12-05T10:02:41.213" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9058" PostId="5957" Score="0" Text="It was normal loop first. It is changed just for speed. The results are identical." CreationDate="2017-12-05T10:36:34.413" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9059" PostId="4979" Score="0" Text="This is link a good read that explains what a PDF is. TL;DR a PDF is a function that describes the probability of (continuous aka floating point) random numbers.  Generating random numbers from a specific PDF can be challenging and there are a few techniques for doing so.  This talks about one of them.  The article after this one talks about another way.&#xA;https://blog.demofox.org/2017/08/05/generating-random-numbers-from-a-specific-distribution-by-inverting-the-cdf/" CreationDate="2017-12-05T21:25:07.703" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9060" PostId="5955" Score="0" Text="BTW, the Blinn–Phong NDF should have $n + 2$, not $n + 1$, in the numerator (see equation 30 in the paper)." CreationDate="2017-12-05T21:49:17.520" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9061" PostId="5957" Score="2" Text="I wonder if your colors are clipping? They may be going above 1.0 or below 0.0.  Might be worth checking out as part of debugging.  It could be that things are smooth in at least part of the image, but that they are out of range." CreationDate="2017-12-05T23:40:20.683" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9062" PostId="5945" Score="0" Text="I partially answered to a similar question recently. See https://computergraphics.stackexchange.com/a/5773/182" CreationDate="2017-12-06T02:45:01.223" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="9063" PostId="5949" Score="1" Text="You can go even further with the macro and do something like this: https://github.com/bkaradzic/bgfx/blob/8d471959eb3cbc16c0e7fdac25efcf842abd2ad1/src/renderer_gl.h#L994 so your code just becomes: `GL_CHECK(glGenBuffers(1, &amp;buffer));` and reports everything you need." CreationDate="2017-12-06T02:47:15.983" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="9064" PostId="5958" Score="0" Text="Ahh i see. that makes sense. thank you very much. EDIT: Awesome blog post as well!" CreationDate="2017-12-06T06:14:12.077" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9065" PostId="5955" Score="0" Text="Yup, thank you. now I also understand that the exponent in the PDF should be n+1" CreationDate="2017-12-06T06:37:48.110" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9066" PostId="5957" Score="2" Text="&quot; It is changed just for speed.&quot; &#xA;If you want speed, why don't you do it as a separable filter, e.g. do Y operations first into an intermediate buffer and then do the X?&#xA;&#xA;Also, since you aren't doing a true integral, are you normalising your sinc filter taps? i.e. does Sum(Helper.Sinc(Ws * (x - n))) over all n == 1?" CreationDate="2017-12-06T10:03:25.923" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9067" PostId="5925" Score="0" Text="Bleeding edge question :) There is work currently going on with Google Seurat, but unfortunately not too much info out yet. See e.g. https://www.roadtovr.com/preview-google-seurat-ilm-xlab-mobile-vr-rendering/" CreationDate="2017-12-06T13:32:42.047" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="9068" PostId="5957" Score="0" Text="The sinc method wasn't normalised. I modified the code and the image looks more like it but it is very blurry. Is this expected? I have updated the question with the new code and image." CreationDate="2017-12-07T10:03:19.497" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9069" PostId="5957" Score="0" Text="The colour wasn't clamped between 0-255 too. Thanks." CreationDate="2017-12-07T10:03:45.343" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9070" PostId="5948" Score="0" Text="*&quot;In general, non-linear would mean that an increase in a value does not produce a similar increase in perception.&quot;*&#xA;&#xA;That's a very confusing opening line. Perception, IMO, implies human vision, which, as you state, is non-linear." CreationDate="2017-12-07T11:07:39.667" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9071" PostId="5957" Score="1" Text="Yes the normalisation looks about right..though you probably don't need to repeatedly compute the sum of the weights as, in your test, they shouldn't be changing from pixel to pixel.&#xA;&#xA;Having said all this, I'm not sure sinc is the right choice for graphics. Can I suggest you try looking at Mitchell &amp; Netravali's https://www.cs.utexas.edu/~fussell/courses/cs384g-fall2013/lectures/mitchell/Mitchell.pdf ?" CreationDate="2017-12-07T16:55:26.927" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9072" PostId="5962" Score="1" Text="Please expand a bit on how to achieve the desired result. Link only answers are not encouraged here. If you link goes down or the page gets modified your answer becomes empty and irrelevant for any future visitors( here is a (admittedly fairly old) meta discussion on the topic : https://computergraphics.meta.stackexchange.com/questions/98 )" CreationDate="2017-12-07T20:00:47.513" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9073" PostId="5962" Score="1" Text="@SebastiánMestre: This is not a link-only answer, as it gives the key term: *Boolean union operation*. If the link goes down, future visitors can look up Boolean unions in any computational geometry textbook. It would be a link-only answer if what it said was, &quot;Check this link, the answer to your question is in there&quot;." CreationDate="2017-12-08T03:04:22.257" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9074" PostId="5948" Score="1" Text="I've edited to clarify. Thanks for the feedback." CreationDate="2017-12-08T06:05:36.660" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9076" PostId="5966" Score="0" Text="Note that DoF corresponds to combining pinhole cameras each having their own view frustum. So it still corresponds to a view frustum, but not &quot;the&quot; view frustum." CreationDate="2017-12-08T12:39:22.667" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="9077" PostId="5966" Score="0" Text="Yes, that's a nice way of looking at it." CreationDate="2017-12-08T14:49:23.107" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9078" PostId="5966" Score="0" Text="Thank you! So the &quot;random&quot; part of the algorithm only comes into effect when rays interact with diffuse surfaces?" CreationDate="2017-12-08T21:06:54.220" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9080" PostId="5968" Score="1" Text="the cdf's are (as far as i know) always computed from the pdf's of of each of the sperical coordinates, not from a probability function of solid angle. besides, a &quot;bigger&quot; solid angle is not well defined since you could choose any two shapes over the hemisphere with equal solid angle but vastly diferent probability" CreationDate="2017-12-09T03:04:25.760" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9082" PostId="5966" Score="0" Text="It's up to you really. If you purely sample on a space-filling curve, you don't need any randomness, but you can still jitter if you're worried about structured artefacts. If you jitter a regular grid, you'd definitely want to use random numbers for that. I forgot to mention that using Poisson discs is another possibility for structured random sampling; I'll add that to my answer." CreationDate="2017-12-09T10:15:47.263" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9083" PostId="5966" Score="0" Text="To be explicit: if you shoot all your primary rays from the same location in the same direction each sample, you'll have aliasing. Randomly jittering the start position by amounts smaller than the size of a pixel will give you antialiasing through &quot;super sampling&quot;" CreationDate="2017-12-09T14:21:12.747" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9084" PostId="5968" Score="0" Text="Because the differential variable in your integral is dw and so the pdf would be a function of w. Hence the pdf = 1/2pi. If you instead write the differential variable dw as Sin(tetha)d(tetha)d(phi) as in fact dw equals this, then your pdf I think will be sin(tetha)/2pi. But it doesn't make difference as in this case the sin is cancelled out with the sin in the numinator and you will be left with 1/2pi." CreationDate="2017-12-09T20:39:46.007" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9085" PostId="5967" Score="1" Text="I ran some test, and found that ka is 1 for all blending functions with a floating point texture, not 2^(mc)-1 as in reference page. Floating point values are not clamped and negative values are taken as it is. So you were right :)" CreationDate="2017-12-09T21:31:14.340" UserId="3024" ContentLicense="CC BY-SA 3.0" />
  <row Id="9086" PostId="5968" Score="0" Text="which sin(theta) in the numerator cancels out with the sin in the pdf?" CreationDate="2017-12-10T12:42:09.443" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9091" PostId="5889" Score="0" Text="I don't entirely follow. The edges of a triangle are not parallel, so how would they have a vanishing point?" CreationDate="2017-12-11T10:13:54.157" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9092" PostId="5972" Score="0" Text="as far as i know a half edge is a directed edge. That is, an edge with a defined begining and end rather than two points where order is irrelevant. I don't know how that would fit into a triangulation algorithm but i hope this helps." CreationDate="2017-12-12T00:54:59.300" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9093" PostId="5867" Score="0" Text="As the other comments show, there is a very wide range of possible answers here, depending on your requirements. Could you [edit] to explain what you will be using these for, and how accurate they need to be, and whether you just want shades of blue or other sky objects like clouds, stars, sun, moon?" CreationDate="2017-12-12T20:03:23.183" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="9094" PostId="5974" Score="1" Text="Yes that's a good point but I still think it might make a cool poster or something if you had the &quot;standard observer&quot; calibrated diagram." CreationDate="2017-12-12T20:45:20.737" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9095" PostId="5978" Score="0" Text="Ok, how about something more specific: A surface with reflectance properties such that if you shine D65 on it a CIE standard observer would get XYZ tristimulus values which could be projected onto a maxwell triangle to build a diagram in x and y. It would be insanely difficult but should it not be possible in theory?" CreationDate="2017-12-12T20:52:00.900" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9096" PostId="5968" Score="0" Text="Sin(tetha) in the numerator comes from the solid angle definition which equals dw = sin(tetha) x d(phi) x d(tetha). And dw is what you have as differential variable in the main integral." CreationDate="2017-12-12T21:30:24.217" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9097" PostId="5968" Score="0" Text="This is exactly the reason why the final PDF should be sin(theta) / 2pi the sin term doesn't cancel out" CreationDate="2017-12-12T22:51:30.467" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9098" PostId="5973" Score="0" Text="Thank you for your answer, I think I was partly thrown by the dissimilarity between the &quot;half-edges&quot; that the library provided (simply an array of indices) and the more fully fledged structures which came up when searching for half-edges." CreationDate="2017-12-12T23:46:58.187" UserId="6406" ContentLicense="CC BY-SA 3.0" />
  <row Id="9099" PostId="5982" Score="1" Text="An isolated implementation, outside of a full renderer, was posted here: https://groups.google.com/forum/?hl=en#!topic/comp.graphics.algorithms/e11sjI-nzpM" CreationDate="2017-12-13T09:35:29.633" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="9100" PostId="5979" Score="0" Text="There are several open-source libraries for rendering OSM data. Couldn't you use one of those?" CreationDate="2017-12-13T10:12:03.283" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9102" PostId="5979" Score="0" Text="Well I found a few of those but they all dont seem customizable enough. Like what if I have my own data and e.g. would want to render a &quot;hut&quot; on the road side. The way I see it I have to either use the existing renderers and then insert the hut in the front end engine I am using, or render everything completely myself which is what I was going for." CreationDate="2017-12-13T15:26:50.580" UserId="7888" ContentLicense="CC BY-SA 3.0" />
  <row Id="9103" PostId="5972" Score="0" Text="Here is a good practical introduction on the topic: https://fgiesen.wordpress.com/2012/02/21/half-edge-based-mesh-representations-theory/" CreationDate="2017-12-13T16:08:26.687" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="9104" PostId="5968" Score="0" Text="The illumination integral is L = integral E cos(tetha)sin(tetha) d(phi) d(tetha). Which has sin(tetha) as part of dw." CreationDate="2017-12-13T23:44:04.880" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9108" PostId="5986" Score="0" Text="Although I just realized that you were asking about the midpoint algorithm so I haven't really answered the question at all but I hope my post is helpful." CreationDate="2017-12-14T03:34:27.413" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9109" PostId="5968" Score="0" Text="Sorry the above illumination integral should be L(out) = integral L brdf cos(tetha) sin(tetha) d(phi) d(tetha). And so its MC estimator will be &lt;I&gt; = L brdf cos(tetha) sin(tetha) / pdf. Here pdf will be in sin form. If you had your integral in terms of dw then your estimator would be &lt;I&gt; = L brdf cos(tetha) / pdf. The pdf here is 1/2pi." CreationDate="2017-12-14T06:56:29.370" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9111" PostId="5978" Score="0" Text="@Chuck No, it would require each element in the poster to have different wavelength charactersitics, white of the paper would aslo have to be perfect. Its not pssible to do this atleast untill we can print stuff that behaves like butterfly wings. Printing simply does not work this way. Its not just insanely expensive it requires tech we do not have." CreationDate="2017-12-14T10:01:45.817" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9112" PostId="5986" Score="0" Text="Your post is really helpful. I see where I went wrong. So I guess I either need to split up my curve, or do what you said and consider the derivatives. Thanks for the link, by the way, I'll make sure to read it when I have some time." CreationDate="2017-12-14T13:57:47.453" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="9113" PostId="5986" Score="0" Text="Its not really a derivative in the formal sense but it is very similar to the idea. Essentially you want to keep finding higher order differences until you get one that does not depend on t and so you would just be adding a constant every time. When you apply forward differencing you get a series of unconnected points. What you should really be doing is drawing straight lines between them. Here are some possibly helpful links on parametric equations: [1](http://tutorial.math.lamar.edu/Classes/CalcII/ParametricEqn.aspx) [2](http://jccc-mpg.wikidot.com/parametric-equations)" CreationDate="2017-12-14T14:41:14.690" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9114" PostId="5977" Score="0" Text="Thanks for the enlightenment! Never thought about it this way." CreationDate="2017-12-14T15:34:11.940" UserId="7884" ContentLicense="CC BY-SA 3.0" />
  <row Id="9115" PostId="5986" Score="0" Text="Okay, so that code example definitely helped me understand it. My question now is what if you can never differentiate the function into a constant, such as the sine function?" CreationDate="2017-12-14T23:30:41.177" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="9116" PostId="5986" Score="0" Text="Yes that's a good catch, I was thinking about that as well. It seems that this might not work so well when functions involve things like sin, log, etc. One thing to keep in mind though is that this not a derivative. Its `f(x + d) - f(x)` So  you might possibly be able to simplify with some [identities](http://www.sosmath.com/trig/Trig5/trig5/trig5.html) while remembering that d is a constant (however I was not able to). I got as far as `sin(x + d) - sin(x) = sin(x)cos(d) + cos(x)sin(d) - sin(x)` and gave up." CreationDate="2017-12-15T05:00:56.863" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9117" PostId="5986" Score="0" Text="In certain cases you may be able to come up with some sort of approximation or clever trick, see [here](https://math.stackexchange.com/questions/2063396/cubic-spline-between-circles) and [here](https://mathnow.wordpress.com/2009/11/06/a-rational-parameterization-of-the-unit-circle/)." CreationDate="2017-12-15T05:01:03.713" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9118" PostId="5989" Score="0" Text="I tried it, but the result was same. &#xA;You mentioned fs_in.TexCoords is == vec2(0.0,0.0)&#xA;but as you can see, the glass color is brown and there are some shading, not perfect black.&#xA;Maybe it meant that the entire color is darkened by some reason." CreationDate="2017-12-15T11:46:20.380" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9119" PostId="5989" Score="0" Text="To me it looks that there are multiple sub meshes (each one with a set of uvs) Is that right? If that is the case, then you are getting 0,0 but for other reason then." CreationDate="2017-12-15T11:56:11.983" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9120" PostId="5989" Score="0" Text="You're right. but I don't know the reason. The two pair code are same? no?" CreationDate="2017-12-15T12:09:46.700" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9121" PostId="5989" Score="0" Text="Yeah both should work with gl version 330. You could launch your project with RenderDoc or Nvidia Nsight maybe that gives you a clew." CreationDate="2017-12-15T12:16:05.247" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9122" PostId="5988" Score="1" Text="if you use (2), do you comment out the first line of your vertex shader (in the main function, the TexCoords assignement)?&#xA;because if you do, i'm not sure this is legal - you'd have an out variable that never had a value assignment." CreationDate="2017-12-15T12:23:05.730" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9123" PostId="5988" Score="0" Text="I tried both of them. And all result were same." CreationDate="2017-12-15T12:30:37.447" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9125" PostId="5989" Score="0" Text="Oh my god, I found my mistake. I duplicate the same name file other folder, and executed the code. the pairs are same. Sorry to bother you.." CreationDate="2017-12-15T20:57:17.513" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9126" PostId="5988" Score="0" Text="I added comment the below answer. I'm really sorry about that" CreationDate="2017-12-15T20:58:28.127" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9127" PostId="5989" Score="0" Text="Good you found it!" CreationDate="2017-12-16T20:21:07.167" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9128" PostId="5995" Score="0" Text="it is probably due to LoD and mipmap levels not being changed when zooming in, therefore having the same texels stretched over a polygon that takes up more of the screen" CreationDate="2017-12-16T21:46:12.863" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9129" PostId="5966" Score="0" Text="So the jitter is an AA technique, not something strictly related to lighting?" CreationDate="2017-12-17T00:37:27.640" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9130" PostId="5999" Score="0" Text="Thank you for the clarification! I shall now to avoid extra work in my shaders :)" CreationDate="2017-12-17T10:09:24.723" UserId="6938" ContentLicense="CC BY-SA 3.0" />
  <row Id="9131" PostId="5966" Score="0" Text="@PaulFerris That's right, but *especially* in a full MC path tracer, you need to consider AA in your sampling strategy. If you're just getting started, you can ignore it for now but you'll have to come back to it later." CreationDate="2017-12-17T10:10:42.463" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9132" PostId="5952" Score="0" Text="Yea, I was positively surprised too :) I believe it works because we are scaling down the dither-magnitude linearly, at the same rate the signal is decreasing. So then at least the scale matches... but I agree that it is interesting that directly blending the distributions appears to have no negative side-effects." CreationDate="2017-12-17T13:16:16.777" UserId="2463" ContentLicense="CC BY-SA 3.0" />
  <row Id="9134" PostId="5966" Score="0" Text="Good to know.&#xA;My whole renderer runs through DirectCompute so supersampling is trivial (render with n-times more thread groups into an n-times larger buffer, average pixel neighbourhoods in post-processing); I was mostly just very confused about the &quot;correct&quot; way to organise my ray directions when they're emitted from the camera. Thanks for all the help :)" CreationDate="2017-12-17T20:08:07.200" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9135" PostId="6000" Score="0" Text="The clipping library returns a list of points that make up the above figure. I cant distinguish between points that make up the arc and points that make up the lines." CreationDate="2017-12-18T00:51:02.303" UserId="7412" ContentLicense="CC BY-SA 3.0" />
  <row Id="9136" PostId="6000" Score="0" Text="So the dotted purple lines are part of the returned object? Well if the points in the arc are closely packed as you say, you can look at the distance between consecutive points. If that that distance is above some threshold then you can say that you've hit an edge of the arc." CreationDate="2017-12-18T04:17:53.030" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9137" PostId="5995" Score="0" Text="Can you post a screen shot of what you mean by &quot;washed out&quot;?" CreationDate="2017-12-18T09:37:23.923" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9138" PostId="6004" Score="0" Text="This works perfectly and is extremely fast, even though the GL driver of my Pi does not advertise ARB_viewport_array. Thank you!" CreationDate="2017-12-18T10:12:09.403" UserId="7915" ContentLicense="CC BY-SA 3.0" />
  <row Id="9139" PostId="6008" Score="0" Text="is your lighting turned on for rendering? if so, i suggest trying to turn it off" CreationDate="2017-12-18T10:51:22.750" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9140" PostId="6008" Score="0" Text="No, not using any lighting." CreationDate="2017-12-18T10:59:21.103" UserId="7915" ContentLicense="CC BY-SA 3.0" />
  <row Id="9141" PostId="6008" Score="0" Text="What kind of alpha blending are you using?" CreationDate="2017-12-18T11:20:55.923" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9142" PostId="6008" Score="0" Text="I use glEnable(GL_ALPHA_TEST) and glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA) at the initialization of my program." CreationDate="2017-12-18T11:21:53.513" UserId="7915" ContentLicense="CC BY-SA 3.0" />
  <row Id="9143" PostId="6008" Score="0" Text="Is the RGB channel of the font texture all the same colour? It should not contain any anti-aliasing in the RGB channel, only the A channel." CreationDate="2017-12-18T11:48:25.007" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9144" PostId="6003" Score="0" Text="What do you mean by &quot;consider the directions&quot;? Are you trying to integrate over some solid angle instead of a single ray? Are you trying to extend the volume to be heterogeneous or anisotropic?" CreationDate="2017-12-18T12:11:37.010" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9145" PostId="6008" Score="0" Text="@PaulHK, that the 100% the right lead! While the PNGs Cairo produce look normal (only alpha changes), the internal storage format that I use to create textures uses pre-multiplied alpha. I noticed that there were slight color differences in antialiased regions of the PNG (presumably because the premultiplied alpha is divided back before writing the PNG). I.e., a pixel that would be white with 0xaa alpha does not appear as 0xffffffaa but as 0xaaaaaaaa in the binary data. The only question is: how do I blend that data in OpenGL efficiently? Thanks already so much for your help!" CreationDate="2017-12-18T12:38:35.103" UserId="7915" ContentLicense="CC BY-SA 3.0" />
  <row Id="9146" PostId="6008" Score="1" Text="Figured it out! GL_ONE, GL_ONE_MINUS_SRC_ALPHA -- and it works perfectly. Since you brought me on the right track, if you want to write an answer I'll accept it! Thanks again so much." CreationDate="2017-12-18T12:43:23.797" UserId="7915" ContentLicense="CC BY-SA 3.0" />
  <row Id="9147" PostId="5998" Score="0" Text="Also, in a streaming-world game, it might not even be possible to show higher detail in a distant area because the models/textures may not even be loaded at the moment. One could try to stream them in when you zoom the camera, but as mentioned that could produce a lot of popping." CreationDate="2017-12-18T17:12:21.397" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9148" PostId="6003" Score="0" Text="I am integrating over some solid angle." CreationDate="2017-12-18T19:59:35.697" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="9149" PostId="6010" Score="0" Text="Thanks. If I understand correctly, this will give a gradually changing specular highlight across the tri (because each pixel is in a slightly different location). The discontinuity between tris will vanish, because adjacent pixels in the different tris are very close in location. Is that what you mean? [BTW I've gotten a similar effect by letting the specular result from the vertex shader interpolate in the fragment shader]. I agree this is more natural - but I want uniform shading across the quad (so the discontinuity is between quads). A bit like the flashing facets of a jewel." CreationDate="2017-12-18T23:16:24.020" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9150" PostId="6011" Score="0" Text="Thanks. Yes, &quot;flat shading&quot; as [here](https://wikipedia.org/wiki/File:Phong-shading-sample.jpg). And yes, the diffuse + specular reflection of [Phong shading](https://wikipedia.org/wiki/Phong_reflection_model), but without the interpolation (TIL the interpolation is also called &quot;Phong shading&quot;). I believe the specular component increases the angle between the angle of reflection and the viewing direction decreases (brightest when aligned).... specular highlights move as you move your head. I needed a diagram too! I'll look for an online diagram tool (do you know of one?)" CreationDate="2017-12-19T03:12:59.217" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9152" PostId="6009" Score="3" Text="As a minor optimisation, you should be able to compute the average of 4 texels using a single texture2D operation. If you sample from a half-pixel offset the sampling area will cover 4 texels equally." CreationDate="2017-12-19T04:24:26.217" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9153" PostId="6014" Score="0" Text="Thanks, that's really clever! I think it almost doubles the number of 16bit indices, but isn't there some caching advantage to indexing too? TIL ES 3.1 has `gl_VertexID` and `primitive restart` (thought degenerate tris was the only way)... pity there's no `gl_PrimitiveID`. I see it's more flexible, but I think same memory overhead as the provoking-trick." CreationDate="2017-12-19T04:25:47.427" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9154" PostId="6011" Score="0" Text="typo, should be: &quot;specular component *increases* **as** the angle between reflected ray and the direction of the viewer *decreases*&quot;" CreationDate="2017-12-19T04:33:37.173" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9155" PostId="6012" Score="1" Text="Didn't you ask this on SO as well? https://stackoverflow.com/questions/47857995/transforming-screen-position-into-image-space&#xA;&#xA;Anyways as the guy mentioned I don't understand your transformation matrix. You said perspective projection isn't involved but you name it projection anyways? what is the space of world position after you apply the matrix? Camera space? Projection space?" CreationDate="2017-12-19T04:54:13.143" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9156" PostId="6009" Score="2" Text="Your results look exactly like what I'd expect, and what I see from the output of popular video editing applications by the major manufacturers. Due to the low resolution of the Cb and Cr channels, I don't think you're likely to get it any sharper. That's just how 420 subsampling is. It takes up less space because it throws out data, and it isn't careful about it *at all*!" CreationDate="2017-12-19T05:06:15.573" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9157" PostId="6014" Score="0" Text="@hyperpallium Yeah, more indices. They're usually a pretty small memory overhead compared to vertex data, but YMMV. As for cache, a well optimized indexed mesh can be as good or better than strips for general meshes, but I'm not sure if that's true for pure grid meshes." CreationDate="2017-12-19T05:41:55.680" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9158" PostId="6014" Score="0" Text="I can see it's much less than vertex data; but better to have no extra at all (*if* possible...). I probably shouldn't worry too much over efficiency at this stage - it seems a bit hard to predict. BTW I was thinking the way to cache-optimize a (plane) mesh is to split the rows, making it short enough so the vertices shared with the above tris are still in cache when needed (I don't think there's benefit to it being any shorter; and a small detriment with losing the horizontally shared vertices where the row is split.)." CreationDate="2017-12-19T06:24:57.647" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9159" PostId="6014" Score="0" Text="@hyperpallium You might be interested in this article: [Optimal grid rendering is not optimal](https://zeuxcg.org/2017/07/31/optimal-grid-rendering-is-not-optimal/)" CreationDate="2017-12-19T06:40:49.413" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9160" PostId="6014" Score="0" Text="Spot on. Definitely best to not worry about optimizing yet! I wonder if different on mobile GPUs, which probably do fewer clever things under the hood(?)" CreationDate="2017-12-19T06:57:36.677" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9161" PostId="5995" Score="0" Text="@SimonF There is not need to bother with uploading. Have you ever played a game and the textures just popped in to full detail (regardless of zoom or anything, you just running around). If you set &quot;Detail distance&quot; to max then you'll see high resolution textures to a certain extent and beyond that are low-poly, low res textures." CreationDate="2017-12-19T08:40:36.120" UserId="6202" ContentLicense="CC BY-SA 3.0" />
  <row Id="9162" PostId="5995" Score="0" Text="_&quot;Have you ever played a game and the textures just popped in to full detail (regardless of zoom or anything, you just running around)&quot;_ &#xA;No, I haven't and that's *why* I asked if you could post a picture. :-/" CreationDate="2017-12-19T09:47:19.520" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9163" PostId="6013" Score="0" Text="the radius is pixel buffer space, screenPosition.x is screenspace, and screenPositionX is in actual pixel buffer. I still gets wrong cropped rectangle. Thanks" CreationDate="2017-12-19T11:52:28.217" UserId="7918" ContentLicense="CC BY-SA 3.0" />
  <row Id="9164" PostId="6013" Score="0" Text="I have edited the post.. still I get wrong cropped image, out of bounds exception" CreationDate="2017-12-19T11:59:41.200" UserId="7918" ContentLicense="CC BY-SA 3.0" />
  <row Id="9166" PostId="6015" Score="2" Text="Regarding the background mixing with your objects during the blur, my recommendation would be to not render on white, but to render on transparent black. When doing the blur, you should blur using pre-multiplied alpha.  That will make the result such that you can apply it to any background color (using standard alpha blending) and it will look correct.  More information here: https://stackoverflow.com/questions/4854839/how-to-use-pre-multiplied-during-image-convolution-to-solve-alpha-bleed-problem" CreationDate="2017-12-19T17:01:07.867" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9167" PostId="5995" Score="0" Text="@SimonF This post would be a good reference.&#xA;https://www.quora.com/Fallout-4-PC-How-do-I-make-settlement-structures-visible-from-really-far-away&#xA;Anyway, the thing I'm asking is not bug related or anything like that. It's more of a technical nature. The answer below got it right." CreationDate="2017-12-19T17:13:20.943" UserId="6202" ContentLicense="CC BY-SA 3.0" />
  <row Id="9168" PostId="6013" Score="0" Text="Which piece of code gives the out of bounds exception?" CreationDate="2017-12-19T18:13:12.990" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9169" PostId="6013" Score="0" Text="Ogre::Image *out = cropImage(img,  min_x, min_y,  max_x,  max_y);" CreationDate="2017-12-19T20:35:16.427" UserId="7918" ContentLicense="CC BY-SA 3.0" />
  <row Id="9171" PostId="6009" Score="1" Text="@PaulHK, do you mean the case where the texture has linear filtering performed by the hardware?" CreationDate="2017-12-19T22:33:12.147" UserId="213" ContentLicense="CC BY-SA 3.0" />
  <row Id="9177" PostId="6010" Score="0" Text="a common way to get that discontinuity is to interpolate position but not interpolate normals. maybe this works better for you" CreationDate="2017-12-20T02:32:01.397" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9178" PostId="6009" Score="0" Text="Yes, it should be the equivalent to the 4 texture reads in the code above." CreationDate="2017-12-20T04:09:01.543" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9179" PostId="6009" Score="1" Text="To answer OP's original question: I've seen implementations of this which use seperate CrCb and Y textures (called semiplanar on some platforms). By doing this you not only simplify texel coordinate computation but you also get independent filtering on both the Chroma and Luma channels." CreationDate="2017-12-20T04:10:32.813" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9180" PostId="6009" Score="1" Text="I have also seen that. Done via fragments shader. This approach is suboptimal in terms of performance, due to higher bandwidth consumption." CreationDate="2017-12-20T08:02:43.660" UserId="213" ContentLicense="CC BY-SA 3.0" />
  <row Id="9181" PostId="6019" Score="1" Text="Is it rendered in linear colour space ?" CreationDate="2017-12-20T12:46:38.250" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9182" PostId="6019" Score="0" Text="Do you actually have two lights 180 degrees apart? It *should* get dark at the light terminator but it wouldn't normally get light again on the other side unless you have two-sided lighting turned on and shadows turned off, or two lights on opposite sides." CreationDate="2017-12-20T13:28:43.523" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9183" PostId="6019" Score="0" Text="PaulHK, tried it with both and the effect persists, although it's a little less pronounced in linear." CreationDate="2017-12-20T13:32:58.507" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9184" PostId="6019" Score="0" Text="Dan, check out the pics. The lights are more like 60 degrees apart, the problem is when one light's contribution reaches zero, it appears to have a negative contribution to the other light - probably just a perceptual artifact but still annoying." CreationDate="2017-12-20T13:35:01.193" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9185" PostId="6020" Score="0" Text="What kind of output are you looking for? Just a wireframe like this but without the shaded solid behind it? A wireframe with the hidden lines also visible? Or a line around the silhouette of the object, without a wireframe?" CreationDate="2017-12-20T14:34:58.850" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9187" PostId="6020" Score="0" Text="@DanHulme I'm sorry, I should've made clear what result I was looking for. This is an example: https://gyazo.com/000673d8afda399013a1ad72e71b7a62" CreationDate="2017-12-20T14:47:11.383" UserId="7925" ContentLicense="CC BY-SA 3.0" />
  <row Id="9188" PostId="6020" Score="0" Text="I assume you rerender your object after your stencil pass. have you reset your primitive type to triangles rather than lines?" CreationDate="2017-12-20T14:55:11.040" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9189" PostId="6020" Score="0" Text="@Tare By re-render you mean in a different frame or still the same frame?&#xA;The primitive type is reset to triangles before rendering the object (line 9: node_to_display.leaves[i].primitive.primitiveType = this.scene.gl.TRIANGLES;)" CreationDate="2017-12-20T14:58:18.963" UserId="7925" ContentLicense="CC BY-SA 3.0" />
  <row Id="9190" PostId="6020" Score="0" Text="I assume your rendering to be somewhat like this:&#xA;1.) render one pass to fill the stencil buffer&#xA;2.) render one pass (or in the same pass) to render red wherever the stencil pass has written to&#xA;3.) rerender the original image to have it display over the red stencil image&#xA;&#xA;so before step three you'd need to reset your primitive type to triangles rather than lines" CreationDate="2017-12-20T15:04:02.413" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9191" PostId="6020" Score="1" Text="It's been some time since I worked with stenciling, but I think you might be wrong in your way of going about this.&#xA;What I think you are doing is rendering the original image, and then rerendering it's wireframe (by setting the primitive type to line), just that you're rendering red wherever the stencil buffer is set. this would of course lead to the red wireframe you get right now.&#xA;try rendering your second time (the &quot;red pass&quot;) with primitive type triangles, I guess that would lead to the whole object being red." CreationDate="2017-12-20T15:13:59.833" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9192" PostId="6022" Score="1" Text="30x30x1 will cover 900 points. It is up to the shader what to do with the global invocation ID, in your case it would be an index into the particles[] array. If you have a case were the ID is greater than the size of the particles array you can just skip doing any work." CreationDate="2017-12-21T02:55:36.183" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9193" PostId="6013" Score="1" Text="This is a longshot but maybe you need to clamp the max value to cameraRes - 1 because accessing cameraRes on a zero indexed array would be out of bounds." CreationDate="2017-12-21T07:09:05.123" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9195" PostId="6014" Score="0" Text="With using the same provoking vertex, the location will be for that vertex, not the center of the square. But because I have the normal, I can work out the gradient and because I have a 2D height map (i.e. heights over a uniform 2D grid), I can calculate the exact 3D location of the center. There'a  bit of calculation there, but probably faster than the bandwidth cost of explicitly storing the center." CreationDate="2017-12-22T01:30:47.483" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9196" PostId="6014" Score="0" Text="I'm not sure of the meaning of `gl_VertexID`. With the &quot;un-sharing&quot;, can you use the same indexing as for the first method? i.e. `gl_VertexID` is not the index of the vertex, but the index in the list of indices? So if the same vertex is used twice, that vertex has two different `gl_VertexID`'s?" CreationDate="2017-12-22T01:39:36.030" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9197" PostId="6019" Score="3" Text="Unfortunately, this is the correct result of the Lambertian lighting model. The physically valid way of getting rid of it is to replace your point light sources with area lights. Alternatively, if you don't care about correctness you could replace the $\max(n\cdot\ell,0)$ step with a different function that smooths over the kink at $n\cdot\ell=0$." CreationDate="2017-12-22T02:11:28.923" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9198" PostId="6023" Score="0" Text="Yet they are choosing samples from a join pdf of theta and azimuth (as you said, double integral over a rectangle) but the resulting radiance is instead multiplied by 1 / 2pi  instead of sin(theta) / 2pi, why is that?" CreationDate="2017-12-22T02:44:55.153" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9199" PostId="6023" Score="0" Text="If you are referring to Monte Carlo integration, there's no weighting required because the samples are evenly distributed over the hemisphere by uniformSampleHemisphere() function, so each sample has the same the same solid angle. In the case of double integral the samples close to sphere poles have smaller solid angle which is compensated with the sin-term" CreationDate="2017-12-22T04:06:47.043" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="9200" PostId="6023" Score="0" Text="Btw, if you would implement the integration with two nested sums over theta &amp; phi, you would need to add the sin-term because the samples near poles would represent smaller solid angle than the samples at the equator." CreationDate="2017-12-22T04:17:31.323" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="9202" PostId="6029" Score="2" Text="Are you sure that the code you've written results in branches? Most simple `if` statements are compiled to parallel evaluation with a multiply-add to select the desired output. You should check with the profiling tool that goes with your target GPU." CreationDate="2017-12-22T10:28:15.673" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9204" PostId="6023" Score="0" Text="So essentially since the integration is a single recursive sum, each  received differential radiance from a pixel represents the radiance along a single steradian and we multiply that by 2pi to estimate the irradiance arriving at point x, to finally multiply that value with the BRDF to see how much of that irradiance arrives to the camera?" CreationDate="2017-12-22T14:30:35.360" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9205" PostId="6023" Score="0" Text="For hemispherical integration with even sample distribution (no biasing towards any region), each sample covers 2pi/n sr (hemisphere=2pi sr). So for the rendering equation you just add up all the samples multiplied by the solid angle for each sample (sample=incident radiance * brdf * cos(theta)) to calculate the total radiance from a pixel towards the eye." CreationDate="2017-12-22T16:55:41.717" UserId="1952" ContentLicense="CC BY-SA 3.0" />
  <row Id="9207" PostId="6031" Score="1" Text="To put it another way, you want to write code to &quot;bake&quot; the texture into vertex colors, so that the vertex colors approximate the original texture?" CreationDate="2017-12-22T19:04:22.700" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9208" PostId="6014" Score="0" Text="@hyperpallium It's the index into the vertex buffer (ie the value from the index buffer). If a vertex is re-used, it has the same `gl_VertexID` every time. By &quot;un-sharing&quot; I mean that, e.g.: vertices 0–3 are the first quad, 4–7 are the second quad, etc. So if a vertex is used in multiple quads, it's duplicated in the vertex buffer for each quad." CreationDate="2017-12-22T19:20:42.617" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9210" PostId="6014" Score="0" Text="Thanks, so that's much more buffer-expensive than the first (&quot;provoking&quot;)  method. (I see what you mean: but at least you can reuse/share two vertices within the one quad.) Another approach to the heightmap might interact: my heights change, so 2D plane in a &quot;static&quot;-hinted buffer, and heights in a &quot;dynamically&quot;-hinted buffer. But, one could also calculate the 2D location, based on knowing which quad it's in (alternative: send in x,y coords, in just 16bit (or 8bit) Uints). But maybe caching won't catch the duplicates?" CreationDate="2017-12-23T02:47:29.907" UserId="5545" ContentLicense="CC BY-SA 3.0" />
  <row Id="9211" PostId="6031" Score="0" Text="I've reopened your question now that you've clarified it a little, but to make sure you're not disappointed later, I'll say now: if you're looking for an existing piece of software that you can run your mesh through, we won't be able to help. If you're looking for an algorithm that you can use to write some software, we might be able to help." CreationDate="2017-12-23T09:39:40.143" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9212" PostId="6032" Score="0" Text="Thank you. I have added a figure to show that the PRP is the 3D point that corresponds to the 2D vanishing point" CreationDate="2017-12-24T19:37:08.103" UserId="7929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9213" PostId="6040" Score="1" Text="Is this just a way of drawing a lego technic or a format for storing the design of one? If is is the latter, are you looking for something compact/compressed which takes up little storage or something that is convenient to work with and easy to edit?" CreationDate="2017-12-25T03:53:29.420" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9214" PostId="6033" Score="2" Text="The answer is very unclear. You should elaborate on each point." CreationDate="2017-12-25T06:14:17.310" UserId="457" ContentLicense="CC BY-SA 3.0" />
  <row Id="9215" PostId="6040" Score="0" Text="@chuck easy to edit/create, and its drawing lego technic sections which create a full model, a full model is impossible to interpret on small phone screens." CreationDate="2017-12-25T06:42:42.190" UserId="7945" ContentLicense="CC BY-SA 3.0" />
  <row Id="9216" PostId="6039" Score="0" Text="Can you clarify? Do you not understand how piecewise Bézier cubic splines work? Or do you understand that, but not how to use them in this particular application?" CreationDate="2017-12-25T06:50:30.843" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9220" PostId="5991" Score="0" Text="here you are asking three different questions. one about generation of quasi-random number generation, one about importance sampling, and one about mapping from a 3d direction to a coordinate on an env map. Also, are your questions exclusively about implementation? do you understand those concepts and algorithms fully? What have you tried already? what worked and what didn't? It would help if you made what you're asking more clear." CreationDate="2017-12-25T16:28:38.913" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9222" PostId="5998" Score="0" Text="I remember a really long time ago in the roblox game engine you were unable to edit the camera FOV (not sure if you can nowadays) and somebody had built a sniper game where you could click somewhere on the map and your camera would move there. You had full 360 pivot controls at the new camera location and you could click to take shots which were raycast from your player position to where you clicked. It was absolutely ridiculous but still lots of fun trying to out camera manoeuver people." CreationDate="2017-12-25T23:08:04.673" UserId="7740" ContentLicense="CC BY-SA 3.0" />
  <row Id="9223" PostId="210" Score="1" Text="I found a real photo that proves you are missing some occlusion https://imgur.com/a/qcxmK" CreationDate="2017-12-26T03:31:54.170" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="9224" PostId="6039" Score="0" Text="@ user1118321 If I understand how Bezier cubic splines works, what I do not understand is the following. I'm trying to make a hill, what I do not know if it should be if it must have some points to make a curve, for example Q (16,137), S (64,198), T (112,137) and later add the parameters theta = Pi/4 and Phi = Pi/4 to build a feature curve of the hill. Could you tell me how steps 3.1 and 3.2 work, please, since I am very confused" CreationDate="2017-12-26T03:44:02.373" UserId="7943" ContentLicense="CC BY-SA 3.0" />
  <row Id="9225" PostId="6045" Score="0" Text="What do you mean by `ComputeShader.use()` ? And if you are passing SSBO to vertex shader you will have to define interface blocks, afaik you can't use them like uniform variables. Check here&#xA;https://www.khronos.org/opengl/wiki/Shader_Storage_Buffer_Object" CreationDate="2017-12-26T12:01:14.767" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9226" PostId="6045" Score="0" Text="@wandering-warrior Oh, sorry to confuse you. ComputeShader.use() is just glUseProgram(compute.glsl), so we can ingnore it. And I wonder your meaning 'you can't use them like uniform variables'. I don't want to use the data like uniform variables, instead, I just want to pass the whole arrays to vertex shader, probably it means that when I call glDrawElements or glDrawArrays(triangle), the vertex shader would get the array stream." CreationDate="2017-12-26T13:21:21.607" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9228" PostId="6046" Score="0" Text="I think the source of confusion is `target` parameter in `glBufferData`. If buffer object is just data, when why we need to specify this to actually upload raw data?" CreationDate="2017-12-26T15:49:55.953" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="9229" PostId="6046" Score="0" Text="@narthex: Because that's how [OpenGL objects work](https://www.khronos.org/opengl/wiki/OpenGL_Object). Unless you're [using DSA](https://www.khronos.org/opengl/wiki/Direct_State_Access), you have to bind them somewhere to the context to be able to edit them. This is one of the most *consistent* parts of the OpenGL API, and it's basically the first thing anyone should learn about OpenGL." CreationDate="2017-12-26T15:56:16.357" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9232" PostId="6031" Score="0" Text="@DanHulme YEs, I was looking for an algorithm" CreationDate="2017-12-26T21:00:48.330" UserId="7936" ContentLicense="CC BY-SA 3.0" />
  <row Id="9233" PostId="6036" Score="0" Text="Thank you for your response. Since I never worked with openGl, I would need some clarifications about your code. The parameter texture that you've passed to the texture2DRect function is the 2D image associated to texture right ? Can you please give a more complet code having the lines for reading the 3D textured mesh, transferring the color texture to 3D vertices, and saving it to a .ply file ?" CreationDate="2017-12-26T21:03:41.933" UserId="7936" ContentLicense="CC BY-SA 3.0" />
  <row Id="9234" PostId="6036" Score="1" Text="That's beyond the scope of an answer here, I'm afraid. I suggest you look up a modern beginning OpenGL tutorial. Once you've learned how to put geometry on the screen and add a texture to it, it's pretty much the same for a 3D texture. For 3D Textures you'll use a `sampler3D` instead of a `sampler2DRect` and call `texture3D()` instead of `texture2DRect()`. Good luck!" CreationDate="2017-12-26T21:53:18.553" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9235" PostId="6016" Score="1" Text="To blur the different sphere separately, I decided to do it the same way is done in screen space subsurface scattering. I am using depth to calculate the difference between the basic depth and the blurred depth. If the difference is to high, I just don't apply blur." CreationDate="2017-12-26T23:15:24.340" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="9237" PostId="6042" Score="0" Text="For raytracing you can just cast multiple rays per pixel to collect multiple samples per pixel. Then average out the results. Now you can end up getting pretty sophisticated in terms of the sampling algorithm itself especially when you get into indirect lighting, but that's the gist of it: just multiple rays/samples per pixel. I've actually seem some raytracers just fudge the ray by a random subpixel value each time it is being cast to produce an antialiased image and the results actually look quite decent. The randomness of the sampling can help pick up razor thin geometry, e.g...." CreationDate="2017-12-27T04:47:09.523" UserId="2247" ContentLicense="CC BY-SA 3.0" />
  <row Id="9238" PostId="6042" Score="0" Text="... which a uniform sampling method might miss. That's for offline raytracers that are intended to run for long amounts of time to converge towards a decent production image. If your aim is more realtime approximation with interactive framerates, then maybe uniform sampling would work better, like one that is fixed to project 5 rays per pixel. There are also dirt cheap post-processing techniques on the rendered image like FXAA, but those don't look so great but maybe acceptable if your goal isn't to render production-quality images." CreationDate="2017-12-27T04:50:02.390" UserId="2247" ContentLicense="CC BY-SA 3.0" />
  <row Id="9239" PostId="6036" Score="0" Text="@javier What you're requesting is not an algorithm, it's an entire application. We have a finite amount of space in our answers, so it's best to keep your question limited to something specific. Loading a 3D mesh and save a .ply file are totally different problems and we can't possibly answer that without knowing more about the original format and what specific problems you're having with your mesh loading code. You should post separate questions once you've tried to load / save a mesh if you still need help." CreationDate="2017-12-27T23:35:56.337" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="9240" PostId="1540" Score="0" Text="@YvesDaoust Could you provide an example of `plain Cartesian formulation` or link to a resource that describes its use in 3D graphics?" CreationDate="2017-12-27T23:41:08.670" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="9241" PostId="1540" Score="0" Text="@Dan: use y = A.x + b where A is a 3x3 matrix and b a 3x1 vector, instead of y' = A.x' where y', x' are augmented vectors and A a 4x4 matrix." CreationDate="2017-12-28T08:45:17.400" UserId="1703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9245" PostId="1540" Score="0" Text="@YvesDaoust So you're passing a 3x3 matrix and a 3x1 vector to your shaders instead of a 4x4 matrix? Where do you calculate and store `w`?" CreationDate="2017-12-28T18:51:04.843" UserId="6145" ContentLicense="CC BY-SA 3.0" />
  <row Id="9252" PostId="5925" Score="0" Text="I'd think rendering images with depth buffers and using real time height field raymarching for reconstruction would be a decent solution. of course, you might miss some information between &quot;samples&quot;" CreationDate="2017-12-29T15:54:32.293" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9253" PostId="6052" Score="0" Text="Thanks, but the blog post doesn't explain perspective clipping." CreationDate="2017-12-29T18:26:48.860" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="9254" PostId="6052" Score="0" Text="I think it does. What is perspective clipping for you?" CreationDate="2017-12-29T18:47:53.493" UserId="7957" ContentLicense="CC BY-SA 3.0" />
  <row Id="9255" PostId="5557" Score="0" Text="You write that you want a _specific_ answer, but I think that is going to be difficult if you don't give any information about the tools you used to convert it, where you find the image, etc." CreationDate="2017-12-31T02:02:28.940" UserId="7969" ContentLicense="CC BY-SA 3.0" />
  <row Id="9256" PostId="6060" Score="0" Text="I received a message that the project can be done in 2d, any suggestions? Physics does not have to be perfect." CreationDate="2017-12-31T07:00:59.657" UserId="7966" ContentLicense="CC BY-SA 3.0" />
  <row Id="9257" PostId="6059" Score="0" Text="Would I be right in thinking your &quot;raw RGB triplets&quot; are floating-point values, and the problem comes when you clamp them to [0,1]?" CreationDate="2017-12-31T10:53:07.487" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9259" PostId="6059" Score="0" Text="@DanHulme That's right! The same problem could also occur if I work with another colorspace internally and then want to convert to something the display or file format can handle. But in this particular case it is because I use floats with light sources brighter than &quot;one&quot;." CreationDate="2017-12-31T23:23:33.040" UserId="7969" ContentLicense="CC BY-SA 3.0" />
  <row Id="9260" PostId="6032" Score="0" Text="Thanks for the figure. It seems as if I have understand you correctly. Use a combination of gluLookAt() and  gluPerspective() to achieve the desired projection and position change of the camera. 2D vanishing points are something else: Parallel lines in 3D do not intersect - but after projection in the camera plane the projected lines might intersect. The intersection point in 2D is called &quot;vanishing point&quot;. It is possible to infer the projection properties and rotation of a camera from vanishing points and when I first have read your question I thought you were maybe interested in this." CreationDate="2018-01-02T14:56:09.033" UserId="4652" ContentLicense="CC BY-SA 3.0" />
  <row Id="9262" PostId="6040" Score="0" Text="I'm not sure I understand what the actual question is here." CreationDate="2018-01-03T09:24:44.237" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9263" PostId="6039" Score="0" Text="Could you [edit] your question to add the title and author of the paper you're linking? That'll make it easier for interested people to find your question, and it will help out anyone who can't access your link, or if the link dies." CreationDate="2018-01-03T09:27:07.353" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9264" PostId="6070" Score="0" Text="For further information, you might like to consult my answer to [*What are the practical differences when working with colors in a linear vs. a non-linear RGB space?* on Stack Overflow](https://stackoverflow.com/q/12524623/967945)" CreationDate="2018-01-04T11:44:42.567" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9265" PostId="6072" Score="0" Text="*&quot;The reasons for not doing it the 'right' way, is probably that it still looks fine&quot;*&#xA;I suspect it's more that when graphics moved from work stations (such as SGI machines) that had, say, 10bit per channel support and perhaps did linear colour directly, to lower end devices with only 8bpc, that either doing the conversion was too expensive or simply forgotten about." CreationDate="2018-01-04T14:50:22.963" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9266" PostId="6073" Score="0" Text="Where did you find the above image?" CreationDate="2018-01-05T07:05:28.647" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9268" PostId="6073" Score="0" Text="@user1118321 [google](http://c8.alamy.com/comp/H9348Y/chaotic-pointillist-half-tone-circle-pattern-random-dots-H9348Y.jpg) search :)" CreationDate="2018-01-05T08:59:08.467" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9269" PostId="6044" Score="0" Text="I have added another figure indicating that the eye (origin of the view coordinate) does not change, but the position of the PRP." CreationDate="2018-01-05T11:20:35.753" UserId="7929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9270" PostId="6032" Score="0" Text="Thank you for answering. I don't want to change the vanishing point of parallel lines in the 2D projection, but the PRP of the perspective projection matrix. I have added another figure indicating that the eye (origin of the view coordinate) does not change, but the position of the PRP. glLookAt() enables to move the eye, but not the PRP (https://www.khronos.org/registry/OpenGL-Refpages/gl2.1/xhtml/gluLookAt.xml), which is the parameter that I want to change. You can find this general model in &quot;Computer Graphics with OpenGL 4th ed&quot;, page 322." CreationDate="2018-01-05T11:21:37.330" UserId="7929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9272" PostId="6073" Score="0" Text="Did the page it came from have any information about it? Just curious because sometimes the artist will discuss how they created it. I have an idea of one way it might work, but am not sure how close it would be." CreationDate="2018-01-05T16:54:51.663" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9273" PostId="6073" Score="0" Text="@user1118321 feel free tell me your solution" CreationDate="2018-01-05T17:00:04.617" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9274" PostId="6075" Score="0" Text="Are you asking &quot;Why is the inverse of a rotation matrix the same as its transpose&quot;?" CreationDate="2018-01-05T17:08:05.990" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9275" PostId="6078" Score="1" Text="Blender questions belong on [blender.se]. If you could edit out the &quot;how to use Blender&quot; parts of your question, we should be able to help you with the programming part." CreationDate="2018-01-05T18:23:27.127" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9276" PostId="6078" Score="1" Text="First thing to check is that the indices in the file you've shown are 1-based, while in GL they should be 0-based. That is, you need to subtract one from all the indices in the file." CreationDate="2018-01-05T18:24:53.857" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9277" PostId="6073" Score="0" Text="I'm looking some stuff up about it and will write something when I've got something concrete." CreationDate="2018-01-05T19:35:01.107" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9278" PostId="6073" Score="0" Text="You are probably keen on shadertoy fragment shader code with some magic numbers, but I think this could be solved doing some instanced draw of `GL_POINTS` on screen quad. These would have variable size of point as function of distance from screen center and would be textured as circle. Their screen position would be precomputed using [Hammersley Point Set](http://holger.dammertz.org/stuff/notes_HammersleyOnHemisphere.html)" CreationDate="2018-01-05T20:22:05.687" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="9279" PostId="6075" Score="1" Text="That is a rotation or reflection matrix. You can check that the four columns are orthogonal to each other (the dot product of the columns is zero). Also they are of unit length, so the determinant is $\pm$1. A well known property of orthonormal square matrices is that transpose is equal to the inverse." CreationDate="2018-01-05T21:26:36.693" UserId="7957" ContentLicense="CC BY-SA 3.0" />
  <row Id="9280" PostId="6032" Score="0" Text="Okay, I had a look at the book and now it is clear what you want. I have edited my answer accordingly. Sorry, I did not get it earlier." CreationDate="2018-01-05T21:57:00.857" UserId="4652" ContentLicense="CC BY-SA 3.0" />
  <row Id="9281" PostId="6079" Score="0" Text="thanks good explanation but I need your help to implementing this. I tried cellular noise [see this shader](https://www.shadertoy.com/view/4tBfWR) if you help me I accept your question :)" CreationDate="2018-01-06T04:47:52.297" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9282" PostId="6075" Score="0" Text="@MauricioCeleLopezBelon your explanation was truly helpful. Thanks." CreationDate="2018-01-06T06:51:13.260" UserId="7993" ContentLicense="CC BY-SA 3.0" />
  <row Id="9283" PostId="6078" Score="1" Text="@DanHulme Done, I edited the question part to ask for help to modify the code in order to interpret the `wavefront` data correctly. I also subtracted 1 from every index in the file. This did change the rendered mesh, it is still not a cube though. Any help would be appreciated." CreationDate="2018-01-06T10:56:27.283" UserId="2485" ContentLicense="CC BY-SA 3.0" />
  <row Id="9285" PostId="6088" Score="0" Text="have you tried jittering by +- half a pixel width? this should prevent the 'sample zone' belonging to a given pixel intersecting with that of its neighbors" CreationDate="2018-01-07T08:16:25.210" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9286" PostId="4804" Score="0" Text="@StinkySkunk I couldn't have said it better myself. I wrote a Unity3D implementation of Naive Surface Nets in C# and I tried to document my code as thoroughly as I could. Maybe something in there will help: https://github.com/TomaszFoster/NaiveSurfaceNets" CreationDate="2018-01-06T17:19:23.753" UserId="7997" ContentLicense="CC BY-SA 3.0" />
  <row Id="9287" PostId="6088" Score="0" Text="It appears to be working as designed. If you want smooth edges in that situation, you have to use supersampling." CreationDate="2018-01-07T10:37:24.693" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9288" PostId="6088" Score="0" Text="@Sebastian Mmmm...the [image](https://imgur.com/a/mL9vh) still ends up tattered, just with marginally smoother edges. I'm hoping for actual visual noise around the sphere's border." CreationDate="2018-01-07T10:51:13.147" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9289" PostId="6088" Score="0" Text="@Dan I'm not looking for smoothness anymore; my supersampler does that after enough samples already. I was more thinking about some sort of edge fuzziness (rather than the raggedness I have at the moment) that'd gradually fade out as I caught more samples. It sounds like that isn't possible, though?" CreationDate="2018-01-07T10:53:41.570" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9290" PostId="6087" Score="0" Text="The OpenGL wiki has a great explanation of the process: https://www.khronos.org/opengl/wiki/Compute_eye_space_from_window_space" CreationDate="2018-01-08T17:07:27.233" UserId="7644" ContentLicense="CC BY-SA 3.0" />
  <row Id="9292" PostId="6094" Score="0" Text="The second question you link to has a good analysis. What happens when you look at just 1 octave of the noise? Also, what interpolation function are you using? Linear interp will look more blocky than using a smooth step, for example." CreationDate="2018-01-09T06:45:43.723" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9293" PostId="6094" Score="0" Text="I don’t really use interpolation I think" CreationDate="2018-01-09T08:21:14.793" UserId="5965" ContentLicense="CC BY-SA 3.0" />
  <row Id="9294" PostId="6094" Score="0" Text="@user1118321 so multiplying with another octave might work?" CreationDate="2018-01-09T08:22:13.710" UserId="5965" ContentLicense="CC BY-SA 3.0" />
  <row Id="9295" PostId="6094" Score="2" Text="A heightmap generated from Perlin noise won't give you a plausible-looking river system anyway. Rivers &quot;cut&quot; their own valleys through terrain." CreationDate="2018-01-09T13:44:31.723" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9296" PostId="6094" Score="0" Text="@DanHulme I’m talking about a flat land mass so it wouldn’t look like normal rivers anyway" CreationDate="2018-01-09T14:59:49.273" UserId="5965" ContentLicense="CC BY-SA 3.0" />
  <row Id="9297" PostId="6095" Score="0" Text="Documentation don't say anything about when data is sent to GPU (or I don't see it).&#xA;If `Map` doesn't allocate memory why `memcpy` works on returned pointer?" CreationDate="2018-01-09T15:25:54.890" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="9298" PostId="6094" Score="2" Text="No, I'm not suggesting a solution. I'm suggesting a way of determining what the problem is. If you show just your lowest octave, what does it look like? Is it blocky? I don't enough about how the `simplexnoise()` function you're using works to tell you. Is that a function you wrote or something supplied by a library?" CreationDate="2018-01-09T16:56:24.843" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9299" PostId="6093" Score="0" Text="Just want to point out that you *sometimes* see projection matrices that are singular, but the OP says this isn't case in his example." CreationDate="2018-01-09T17:19:57.720" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9300" PostId="6095" Score="0" Text="You must call Map on a resource that has been created (for example after calling CreateCommittedResource())" CreationDate="2018-01-09T18:08:41.923" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9301" PostId="6095" Score="0" Text="Check this: https://github.com/Microsoft/DirectX-Graphics-Samples/blob/master/Samples/Desktop/D3D12HelloWorld/src/HelloTriangle/D3D12HelloTriangle.cpp#L214" CreationDate="2018-01-09T18:09:03.730" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9302" PostId="6095" Score="0" Text="Ok, I get what you mean now. If I get it right: `Map` basically return virtual pointer to GPU memory and when you access it OS/driver send data directly to resource?" CreationDate="2018-01-09T18:20:40.570" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="9303" PostId="6093" Score="0" Text="True, @SimonF, such is the case for an orthographic projection matrix." CreationDate="2018-01-09T18:21:45.163" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9304" PostId="2355" Score="0" Text="Actually @ratchetfreak, There will be a line line of maximal length that __passes through__ two points.  In your case it will likely either hug one wall of the corridor, or connects the adjacent corners.  If neither of those are maximal then it will extend into a far corner.  So I think that the O(n^2) approach is valid, it's just that the two choices of vertices for the line to consider do not necessarily define the start and end of the line; they are merely ON the line." CreationDate="2018-01-09T18:45:44.260" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9305" PostId="2355" Score="2" Text="Also, I'm concerned about a C shape that has very narrow thickness, but a large an open interior; and so a large radius of curvature.  Its diameter (as you define it) would be very small because it would only be a short that follows the curvature of the C.  Yet a cancer nodule the size of the interior size would be quite concerning.  So perhaps it is the convex hull that you want to compute the diameter of." CreationDate="2018-01-09T18:50:07.150" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9306" PostId="4151" Score="0" Text="Just want to make sure that it's not the sampler.  Make sure you are using nearest neighbour sampling only.  The sampler will attempt to produce a linear combination of a pixel and its neighbours, so don't do that.  If you are just slightly off centre then you might be weighing in a value from a neighbouring pixel." CreationDate="2018-01-09T18:59:42.373" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9307" PostId="5002" Score="0" Text="The first thing I think of is that you immediately lose the benefits of ClearType, which renders text with sub-pixel spatial resolution (sacrificing color fidelity) thanks to knowing the micro-geometry of the individual RGB elements of a typical LCD display.  Sampling the ClearType-rendered text and re-rendering it in 3D will exaggerate the poor color and furthermore lose the benefits of the sub-pixel micro-geometry.  Provided you know the geometry of the RGB elements within the VR display, maybe you could employ the same technique?" CreationDate="2018-01-09T20:00:07.540" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9310" PostId="4151" Score="0" Text="@Wyck Yes, I was using nearest neighbor sampling: https://github.com/Strilanc/Quirk/blob/master/src/webgl/WglTexture.js#L168" CreationDate="2018-01-09T21:27:27.127" UserId="5328" ContentLicense="CC BY-SA 3.0" />
  <row Id="9316" PostId="6099" Score="0" Text="&quot;Dimitar&quot; refers to the [presentation by Dimitar Lazarov](http://blog.selfshadow.com/publications/s2013-shading-course/lazarov/s2013_pbs_black_ops_2_slides_v2.pdf), earlier in the same SIGGRAPH course. See page 21 there for the &quot;split approximation&quot;." CreationDate="2018-01-11T22:30:30.317" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9317" PostId="6100" Score="0" Text="I'm missing the extra 1/N in your explanation." CreationDate="2018-01-12T00:23:22.747" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9319" PostId="1770" Score="0" Text="@NathanReed Why would you need the &quot;repeat&quot; mode if you have a quasy unlimited amount of texture space?" CreationDate="2018-01-12T03:13:27.813" UserId="5909" ContentLicense="CC BY-SA 3.0" />
  <row Id="9320" PostId="6100" Score="0" Text="@user8469759 the 1/N is implicit in the integrals. it comes from &quot;averaging&quot; samples in numerical integration" CreationDate="2018-01-12T03:14:44.627" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9321" PostId="6100" Score="0" Text="Still, there's one more factor 1/N. Also I agree the BRDF is almost constant in realistic cases, but I wouldn't be sure of the $cos$ factor and the probability $p$ due to the importance sampling process." CreationDate="2018-01-12T10:30:21.793" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9322" PostId="6104" Score="0" Text="I don't thing you understood what I was trying to do, I am basically trying to make the cubes themselves smooth, i.e instead of sharp cubes I should have a smooth cube like &quot;sphere&quot;" CreationDate="2018-01-12T19:01:43.050" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="9323" PostId="6104" Score="0" Text="ahh i see, i misunderstood" CreationDate="2018-01-12T19:46:14.750" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9324" PostId="6104" Score="2" Text="you have finite and relatively small amount of ways that cubes can be arranged around a vertex. you could pregenerate meshes for all different configurations an replace each visible vertex with a mesh with the corresponding smooth shape... keep in mind that you will need C1 continuity so having those meshes align with the grid at their &quot;endpoint&quot; would be an easy way of doing it" CreationDate="2018-01-12T19:50:01.847" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9325" PostId="6104" Score="1" Text="&quot;you have finite and relatively small amount of ways that cubes can be arranged around a vertex&quot; And they have already been compactly enumerated in the marching cubes algorithm :)" CreationDate="2018-01-13T01:58:26.883" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9326" PostId="6106" Score="1" Text="It's possible to have a surface that's both reflective and emissive, eg the glass surface of a dim light bulb. (Dim, so you can actually _see_ some reflection on top of the emission.) So this maybe isn't as much of a mistake as you thought. Although if the material on the light source is _purely_ emissive then certainly stop tracing there." CreationDate="2018-01-13T06:00:00.003" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9327" PostId="6106" Score="0" Text="Ahh...no, sorry. :)&#xA;My light sources are stars (slowly building a space exploration game), and I haven't made any hybrid surfaces yet :P.&#xA;Wouldn't it be more correct to construct a lightbulb as an emissive filament inside a reflective/refractive volume?" CreationDate="2018-01-13T07:23:00.843" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9328" PostId="6112" Score="2" Text="*&quot;...Some authors even call a C0 composite Bézier curve a &quot;Bézier spline&quot;;[5] the latter term is however used by other authors as a synonym for the (non-composite) Bézier curve, and they add &quot;composite&quot; in front of &quot;Bézier spline&quot; to denote the composite case...&quot;* - Seems up to personal taste." CreationDate="2018-01-13T19:40:49.453" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9329" PostId="6112" Score="0" Text="What would a non-composite Bézier curve be? Just a regular Bézier curve? So, is that statement saying that some people call Bézier curves Bézier splines? Btw, you should at least link us to that statement." CreationDate="2018-01-13T20:13:00.720" UserId="4718" ContentLicense="CC BY-SA 3.0" />
  <row Id="9330" PostId="6112" Score="2" Text="It's right from the Wiki page *you* linked to, just a few sentences down." CreationDate="2018-01-13T20:13:36.163" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9331" PostId="6100" Score="0" Text="those are meant to be &quot;distributed&quot; into the sigma notation sums, that is why there are parentheses. they should really be interpreted as integrals." CreationDate="2018-01-14T02:00:32.393" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9332" PostId="6113" Score="0" Text="Matrices can be row or column major which flips the order that you would apply them in. There's no need to inverse any rotations to accomplish a rotation." CreationDate="2018-01-14T04:30:04.793" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="9334" PostId="6113" Score="2" Text="Whoever told you the first case is confused and you are too. The overall transformation matrix, assuming it will be applied to the left of a column vector, should be $M=TR_2R_1S$." CreationDate="2018-01-14T11:48:42.113" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9335" PostId="6111" Score="0" Text="Thanks Nathan, I was thinking along those lines myself. Unfortunately the cubemaps are engine-supplied per renderer, and Unity's graphics engine is a bit of a black box in a lot of ways. I may just have to bake out my own cubemap per-scene somehow and fall back to that on GLES2." CreationDate="2018-01-15T04:46:39.700" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9336" PostId="6118" Score="0" Text="Was hoping to hear from you! Fantastic answer. Regarding your warning: If I am making an application where a user can upload images (.jpgs) -  if I want to store them compressed I really have no choice but to compress the data &quot;online&quot; right? (I mean other than converting each uploaded image &quot;offline&quot; then reading it back)?" CreationDate="2018-01-15T18:19:49.380" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="9337" PostId="6118" Score="0" Text="Furthermore, I read somewhere that OpenGL is going away from the generic `GL_COMPRESSED*` formats. Is there any situation where they are useful / a good idea?" CreationDate="2018-01-15T18:22:03.633" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="9338" PostId="6118" Score="1" Text="@Startec: What do you mean by &quot;going away from&quot;? It's a part of the specification. And since it wasn't removed in 3.1, the only way to &quot;go away from&quot; it would be to remove it in a newer version of the spec. And that's pretty much not gonna happen. As for when to use it? Pretty much never." CreationDate="2018-01-15T18:28:53.160" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9339" PostId="6118" Score="0" Text="By that I meant: Is a more preferred way  to use non-generic formats? It so it sounds like it. I read this somewhere, although I can't find where. So where can a generic compressed type even legally be used?" CreationDate="2018-01-15T18:32:46.723" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="9340" PostId="6118" Score="1" Text="@Startec: They were never considered a good way to get compressed images, so the status quo hasn't changed. As for where they can be legally used, the standard spells that out adequately. They're internal formats, so they can be used in most places that take internal formats." CreationDate="2018-01-15T18:34:21.047" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9341" PostId="6120" Score="0" Text="What's best depends a lot on your application and what operations you want to do on these spectra. Could you [edit] your question to add some more about that?" CreationDate="2018-01-15T22:17:36.030" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9342" PostId="6120" Score="0" Text="Just edited my question to add a bit of clarification." CreationDate="2018-01-15T22:40:25.303" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="9347" PostId="6125" Score="1" Text="Comforting to know that I'm doing this well. I also like your little quote ;) Well said, your answer is accepted." CreationDate="2018-01-16T21:43:06.743" UserId="6838" ContentLicense="CC BY-SA 3.0" />
  <row Id="9348" PostId="6127" Score="2" Text="Textures should be stored on GPU VRAM in their compressed form, AFAIK they are decompressed by texture samplers at rendering time rather than at load time, as that would defeat the purpose of compression if you were to unpack them to VRAM. You should do something like render the texture via a quad into an RGB buffer if you want the decompressed version." CreationDate="2018-01-17T02:10:36.773" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9349" PostId="6126" Score="0" Text="So convergence rests on the BRDF being zero/nonzero. How does that work? I've been assuming that the BRDF is just a generator function for ray directions..." CreationDate="2018-01-17T04:53:52.907" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9350" PostId="6126" Score="0" Text="Also, why shouldn't light sampling accelerate convergence for large, close light sources? If every ray that isn't obstructed is guaranteed to reach the light (which they *should* be for area lights), then shouldn't light sampling always give a better result for diffuse surfaces?" CreationDate="2018-01-17T04:55:27.003" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9351" PostId="6127" Score="0" Text="Why can you not simply have compressed and uncompressed versions of the images on disk?" CreationDate="2018-01-17T06:12:04.833" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9352" PostId="6127" Score="0" Text="@NicolBolas My guess is it's a demo-scene program where the total size of assets and program is limited and the questioner is trying to push the limit." CreationDate="2018-01-17T09:44:07.407" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9353" PostId="6127" Score="0" Text="If you *really* want the decompressed texels, why not render a 1:1 quad with the texture on it and read back the resulting pixels in the framebuffer." CreationDate="2018-01-17T09:55:55.993" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9354" PostId="6126" Score="0" Text="@PaulFerris the BRDF is a function which says how much light is reflected in any given direction. The ray generator function can be any distribution which throws rays everywhere the BRDF is non-zero (eg. a uniform generator over the entire sphere). You get ideal sampling when the generator exactly matches the BRDF but that is not a requirement of MIS. Some BRDFs are very hard to generate rays for perfectly so a different function is often used. I'll edit the answer for your other comment." CreationDate="2018-01-17T16:54:24.780" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="9359" PostId="6138" Score="2" Text="The question might be a little bit broad." CreationDate="2018-01-21T13:52:29.547" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9360" PostId="6138" Score="0" Text="@ChristianRau Can you give slight ideas at least.basic theoretical aspects ffor quick learn stuff,it would be really helpful.Thank for the editing to.. I had spelled some words wrongly my bad :)" CreationDate="2018-01-21T13:55:44.810" UserId="8043" ContentLicense="CC BY-SA 3.0" />
  <row Id="9361" PostId="6138" Score="0" Text="good question, but still flagging as off-topic. I wish you best of luck finding out what you are after" CreationDate="2018-01-21T15:08:09.327" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9362" PostId="6138" Score="0" Text="@Andreas :( off topic? I couldn't find nice resource about this topic other Graphic Programming Black Book" CreationDate="2018-01-21T16:25:41.753" UserId="8043" ContentLicense="CC BY-SA 3.0" />
  <row Id="9363" PostId="6138" Score="0" Text="...because I think you will find better answers in forums about general driver implementation. No offense meant. Really trying to help :) Maybe you should ask yourself: what is a 'low level' graphics API? how is efficiency measured? what is optimized, or optimal?" CreationDate="2018-01-21T17:25:10.370" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9364" PostId="6140" Score="1" Text="Thank you for answer Dan... :)&#xA;So,Learning to implement our own Graphic API is not worthy right ? We cannot optimized our system since we don't have an agreement or understanding with GPU vendors or their GPU Internals." CreationDate="2018-01-21T19:12:45.933" UserId="8043" ContentLicense="CC BY-SA 3.0" />
  <row Id="9366" PostId="6126" Score="0" Text="That makes sense, thank you. So my misunderstanding was treating the BRDF and the ray generator as the same thing when the BRDF really just gives reflectance along the output rays provided by the generator? W.r.t light sampling, would this (https://imgur.com/a/BC8HV) be an example of the sort of situation you're referring to?" CreationDate="2018-01-22T05:39:48.833" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9367" PostId="6138" Score="0" Text="@Andreas I was getting deviated that's why,Lost between HDL and HAL" CreationDate="2018-01-22T06:12:05.280" UserId="8043" ContentLicense="CC BY-SA 3.0" />
  <row Id="9368" PostId="6139" Score="0" Text="seems to me like it is cuadratic, since you scale the amount of pixel samples both horizontally and vertically by some constant k. $O(width \times height \times k^2)$" CreationDate="2018-01-22T10:27:43.473" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9369" PostId="6140" Score="2" Text="Unless you're going to reverse-engineer the register and data structure interface of a GPU, you can't implement a GPU acceleration API without getting that information from a GPU vendor." CreationDate="2018-01-22T11:39:29.043" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9370" PostId="6137" Score="0" Text="What 'paper' are you referring to?  Where is this derivation coming from?" CreationDate="2018-01-22T11:59:40.660" UserId="7647" ContentLicense="CC BY-SA 3.0" />
  <row Id="9371" PostId="6140" Score="0" Text="Got it thank you Dan,Which means whatever tech comes,we have to depend on set of standard/platform specific APIs." CreationDate="2018-01-22T12:52:04.277" UserId="8043" ContentLicense="CC BY-SA 3.0" />
  <row Id="9372" PostId="6126" Score="1" Text="@PaulFerris Yes. Although be aware that some older BRDF papers will present only a generator function and a sample weighting function to compensate for the difference between the generator and the BRDF. And yes, the tip of that &quot;cube&quot; nearest the sphere is the kind of thing I had in mind. It would likely have more noise when sampling only the light (sphere) vs MIS." CreationDate="2018-01-22T14:45:21.340" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="9373" PostId="6145" Score="0" Text="I suspect that this is where &quot;a very small amount&quot; comes in, you'd still want triple buffering for larger data." CreationDate="2018-01-22T18:40:10.633" UserId="174" ContentLicense="CC BY-SA 3.0" />
  <row Id="9374" PostId="6142" Score="0" Text="Can you include the actual equation you're asking about, and maybe some more background in your question for the benefit of people who may not have those references handy?" CreationDate="2018-01-22T18:52:36.170" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9375" PostId="6145" Score="0" Text="Not sure I understand the question. Presumably your render targets are still double or triple buffered, regardless of how you manage uniform data...?" CreationDate="2018-01-22T18:55:12.750" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9380" PostId="6146" Score="0" Text="Assume I have a `data store` of size 2MB then I use `glBufferSubData` to add new data which is 1.5MB. Wouldn't there be an extra 0.5MB of data in the `data store` which I don't potentially want to draw?" CreationDate="2018-01-23T00:25:19.650" UserId="6546" ContentLicense="CC BY-SA 3.0" />
  <row Id="9381" PostId="6146" Score="0" Text="or I suppose that where I'd just draw less `primitives` with `glDraw*`. Is that correct?" CreationDate="2018-01-23T00:43:22.493" UserId="6546" ContentLicense="CC BY-SA 3.0" />
  <row Id="9382" PostId="6146" Score="1" Text="@Archmede: Why would you draw using data that you don't want to draw from?" CreationDate="2018-01-23T01:04:24.803" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9383" PostId="6146" Score="0" Text="My specific scenario is I fill data with terrain (like minecraft) but when I move somewhere else on the map I'll refill the `data store`  with new terrain but this new terrain might contain less data than the old terrain. So wouldn't there be some data from the old terrain left in the `data store` if the new terrain contained less data? Make sense?" CreationDate="2018-01-23T01:37:57.903" UserId="6546" ContentLicense="CC BY-SA 3.0" />
  <row Id="9384" PostId="6146" Score="0" Text="So what if there's old data there? Why would you tell OpenGL to read from the old data? As I said, &quot;You tell OpenGL exactly how many vertices to read from a vertex buffer.&quot; So why would you tell OpenGL to read from part of a buffer that doesn't contain relevant data?" CreationDate="2018-01-23T01:42:09.520" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9385" PostId="6145" Score="0" Text="This might be a particularity of Metal, I'm not sure. But I think in case of Metal you just manually manage your render targets, with a semaphore. I'm just guessing, but if I remove the semaphore it behaves like double-buffering (GPU displays one frame while CPU is encoding the next)." CreationDate="2018-01-23T02:03:54.623" UserId="8068" ContentLicense="CC BY-SA 3.0" />
  <row Id="9386" PostId="6146" Score="0" Text="I understand that. I misread the difference between `glBufferStorage` and `glBufferData`. I assumed `glBufferSubData` can reallocate the `data store` but I was mistaken. Thanks for your help though, it's greatly appreciated." CreationDate="2018-01-23T02:12:53.143" UserId="6546" ContentLicense="CC BY-SA 3.0" />
  <row Id="9387" PostId="6137" Score="0" Text="https://www.pixar.com/assets/pbm2001/pdf/notesc.pdf (Page C10) $//$&#xA;http://run.usc.edu/cs599-s10/cloth/baraff-witkin98.pdf (is also using the equation)" CreationDate="2018-01-23T03:29:49.097" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9388" PostId="6149" Score="0" Text="Thanks! I'm guessing Blackman-Harris is blurry (no negative lobes), but not as much as the Gaussian?" CreationDate="2018-01-23T08:27:28.580" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9389" PostId="6148" Score="1" Text="What makes you think it is necessarily slower? If you can [edit] your question to add that extra detail, you'll get better answers." CreationDate="2018-01-23T09:30:06.543" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9391" PostId="6150" Score="1" Text="I would also add that SH (or the related [H-basis](https://www.cg.tuwien.ac.at/research/publications/2010/Habel-2010-EIN/)) is useful for normal maps on static objects. A bumpy surface will reflect light from different directions, so you want a directional representation of incoming light at the surface." CreationDate="2018-01-23T18:26:23.997" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9392" PostId="6149" Score="1" Text="@PaulFerris I think that's right, although the two are quite similar in shape, so there's probably not a lot of difference. The filter width can be tuned to trade-off between blurring and aliasing, as well." CreationDate="2018-01-23T19:24:45.653" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9393" PostId="6150" Score="0" Text="Thanks for the link, I hadn't heard of that basis before. Just been reading about spherical Gaussians which can apparently fake rough specular as well, but the math is pretty gnarly!" CreationDate="2018-01-23T20:15:09.983" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9395" PostId="6156" Score="1" Text="Are you requesting a 4D visualisation (which would be projected back to 2D on the screen)? I don't think my brain would cope with that.  Can you not just consider a 2D example lifted into homogeneous 3D space?" CreationDate="2018-01-24T16:34:00.687" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9396" PostId="6157" Score="0" Text="&quot;*However, if i render this with a custom shader that implements blinn lighting, the sphere appears black, because the &quot;viewDir&quot; calculation is negative, since the eye is centered inside the sphere.*&quot; If you're able to see the sphere from the outside, then I submit that your &quot;eye is centered inside the sphere&quot; notion is incorrect. Either that, or you have a different understanding of what the word &quot;eye&quot; means." CreationDate="2018-01-24T17:54:16.477" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9397" PostId="6157" Score="0" Text="I mean that the model view is centered inside the sphere." CreationDate="2018-01-24T18:25:31.790" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="9398" PostId="6157" Score="0" Text="From the glOrtho docs: &quot;Typically, the matrix mode is GL_PROJECTION, and left bottom - nearVal and right top - nearVal specify the points on the near clipping plane that are mapped to the lower left and upper right corners of the window, respectively, assuming that the eye is located at (0, 0, 0). - farVal specifies the location of the far clipping plane. Both nearVal and farVal can be either positive or negative.&quot;  So they even seem to confirm that the eye is at 0,0 but the near clip can be negative so you see stuff &quot;behind&quot; you." CreationDate="2018-01-24T18:28:21.763" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="9399" PostId="6154" Score="0" Text="Thanks for the answer. What happens to the weight function if p is one of the polygon vertices. The Ai triangle becomes a line with zero area? Also what do you mean by signed area?" CreationDate="2018-01-24T20:30:34.333" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9400" PostId="6154" Score="0" Text="I added a paragraph explaining what to do at the boundaries. For the signed triangle area consult [this.](http://demonstrations.wolfram.com/Signed2DTriangleAreaFromTheCrossProductOfEdgeVectors/)" CreationDate="2018-01-24T20:59:56.930" UserId="7724" ContentLicense="CC BY-SA 3.0" />
  <row Id="9401" PostId="6156" Score="1" Text="Yes, the standard way to understand this is to visualize the lower-dimensional analogue of this operation, i.e. a 2D translation represented as a 3D shear." CreationDate="2018-01-25T03:23:53.210" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9402" PostId="6163" Score="0" Text="Would it be reasonable to say that the cosine distribution integrates to $\pi$ because $cos$ is a one-dimensional function? So integrating it over a hemispherical domain gives you the solid angle consumed by the disk passing through the equator?" CreationDate="2018-01-25T07:15:10.197" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9403" PostId="6163" Score="1" Text="Yeah that's an interesting way of looking at it. Intuitively, any infinitesimal patch of the hemisphere's surface will have its projected area onto the equatorial disk scaled by N.L, so the sum of the projected areas is just the area of the disk. Spherical calculus is not really my strong suit though ;)" CreationDate="2018-01-25T10:02:18.347" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9404" PostId="6151" Score="0" Text="That's not exactly accurate, though. Commands that actually send data to the API are *still* synchronous, e.g. you can immediately free the memory that you called `glBufferData` with right after calling it, since it's copied into the API then. What *is* possibly unsynchronized is the transfer of that data to the actual GPU memory. However, the same asynchronous transfer approaches can be applied for data download, too. It's just that in this case you usually have to wait for all previous operations to finish." CreationDate="2018-01-25T12:52:34.560" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9405" PostId="6151" Score="0" Text="@ChristianRau but most of the time the data you send through glBufferData is first copied to another part of memory from which the actual transfer to the GPU will take place asynchronously." CreationDate="2018-01-25T13:02:30.370" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9406" PostId="6151" Score="0" Text="Yeah, but you could as well do that with `glReadPixels`." CreationDate="2018-01-25T13:03:44.503" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9407" PostId="6151" Score="0" Text="@ChristianRau no because the driver cannot know you want that data until you call glReadPixels which is the trigger to make the gpu start sending data to RAM. The glReadPixels call cannot return until the GPU is done and the data is ready to be used." CreationDate="2018-01-25T13:07:39.497" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9408" PostId="6151" Score="0" Text="No, `glReadPixels` *will* return immediately if you tell it to copy that data into a buffer object. Then you can do other things and *then* you can retrieve the data from the buffer whenever you want." CreationDate="2018-01-25T13:16:57.980" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9409" PostId="6151" Score="0" Text="@ChristianRau oh you were using `GL_PIXEL_PACK_BUFFER`, yeah that will let you do transfers async (though you have to assume the driver isn't dumb enough to leave that in vram)" CreationDate="2018-01-25T13:24:04.890" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9410" PostId="6166" Score="0" Text="Awesome, Your comment about GL_LIGHT_MODEL_LOCAL_VIEWER seems right on. I checked our code and we don't set that, and it appears the default is 0 which means that our viewer is always infinitely far away, even in perspective mode (without shaders).  So I assume that even though the specular does move around when i rotate, its because it is rotating the direction of the z-axis.  However, when i translate, the specular doesn't move around, even in perspective.  Thanks!" CreationDate="2018-01-25T15:49:27.037" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="9411" PostId="6139" Score="2" Text="Are you assuming a GPU-accelerated MSAA implementation? Because in that case the GPU absorbs most of the cost.  If your runtime is limited by fragment shader execution time, then the slowdown will be in proportion to the number of pixels where an edge is visible, because this is where you'll execute your shader multiple times." CreationDate="2018-01-25T16:04:06.747" UserId="3386" ContentLicense="CC BY-SA 3.0" />
  <row Id="9418" PostId="6168" Score="0" Text="Thanks, and I guess it's reasonable advice. It just seemed too elaborate and I don't like to have entirely different approaches for 3 and 4 hardware when it's just a seemingly minor hurdle." CreationDate="2018-01-25T18:18:00.657" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9419" PostId="6163" Score="0" Text="I guess that's understandable :P. Thanks for the answers." CreationDate="2018-01-25T19:39:15.643" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9420" PostId="6169" Score="0" Text="on a phong brdf, samples are generated in a space where the reflected direction is the z axis. thats where the dependence comes from" CreationDate="2018-01-26T05:41:23.580" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9421" PostId="6169" Score="0" Text="Still, the phi angle is sampled over a circle uniformly instead of favoring the specular direction! This is the part I didn't get" CreationDate="2018-01-26T11:41:14.783" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9422" PostId="6173" Score="2" Text="&quot;The RGB values are most likely gamma compressed.&quot; That's common for 8-bit images, but you need to actually *know*, not just guess. Monitor calibration is a bit of a red herring nowadays, because applications should be drawing sRGB and the monitor should be interpreting its input as sRGB anyway." CreationDate="2018-01-26T14:50:06.340" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9423" PostId="6173" Score="3" Text="A typical display is calibrated to sRGB, not gamma 2.2.  So you should use the linear to sRGB or sRGB to linear conversions.  And a 50% sRGB value should be 188, not 186.  (see Wikipedia article for sRGB which says that a normalized 50% intensity should get an sRGB value of (1.055*0.5^(1/2.4))-0.055 = 0.735358, which is about 187.516 in 8-bit sRGB, hence the logic of encoding it as 188." CreationDate="2018-01-26T20:32:44.903" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9424" PostId="6174" Score="0" Text="Can't thank you enough, as a side question: after computing a sample according to this model, the resulting radiance should be divided by the pdf specified in the first formula I've posted?" CreationDate="2018-01-26T22:56:02.827" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9425" PostId="6174" Score="0" Text="it should be multiplied by the brdf and divided by the pdf. those usually cancel each other out so it is often not necesary" CreationDate="2018-01-26T23:11:15.717" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9426" PostId="6175" Score="2" Text="Any time you have instancing in your scene, i.e. multiple copies of the same base geometry, you can represent that as a single leaf node with multiple transformation nodes as parents." CreationDate="2018-01-27T08:15:41.513" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9427" PostId="6175" Score="0" Text="I completely missed that common use case, that's a very practical and clear example. Thanks." CreationDate="2018-01-27T09:24:26.123" UserId="8101" ContentLicense="CC BY-SA 3.0" />
  <row Id="9428" PostId="6176" Score="0" Text="Sorry, my question was indeed a little vague. I had in mind scene graphs for 3D rendering. The collision examples are a bit too much into the app-specific side of things from my point view. At the app level, it's easy to imagine almost anything as a graph of relationships between different objects. However, as pointed out by @Rahul, one example is when multiple nodes with different transformations share the same geometry, in that case it's a non-tree graph." CreationDate="2018-01-27T09:30:57.180" UserId="8101" ContentLicense="CC BY-SA 3.0" />
  <row Id="9429" PostId="6175" Score="0" Text="@Rahul You should post that as an answer, even though it's short. It's exactly the example the questioner was looking for." CreationDate="2018-01-27T13:50:07.987" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9431" PostId="6173" Score="0" Text="The monitor calibration image is indeed kind of misleading for my argument. It does show that I should perform gamma expansion, but not which one.&#xA;Am I correct to conclude that most images will likely be encoded with sRGB and I should use the formula provided by @Wyck? ($1.055L^{1/2.4}-0.055$)" CreationDate="2018-01-27T16:01:51.310" UserId="8097" ContentLicense="CC BY-SA 3.0" />
  <row Id="9432" PostId="6177" Score="1" Text="Yes, you can do that. What issue are you having with doing that? What have you tried that hasn't worked? You need to give us more information." CreationDate="2018-01-27T22:48:00.177" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9433" PostId="6177" Score="0" Text="I added more information above, does it help? Thanks." CreationDate="2018-01-27T23:21:01.233" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="9435" PostId="6179" Score="0" Text="Thanks for the solution. It makes sense! Although I'm testing things out, and I'm suspecting that framebuffer[j] isn't a value between 0 and 1. I'm trying to figure out if it's a value between 0 and 255 instead and hoping there isn't some other subtle transformation in the values." CreationDate="2018-01-28T03:01:36.930" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="9436" PostId="6180" Score="0" Text="that is the correct way. maybe darkness comes from not doing calculations in linear space?" CreationDate="2018-01-28T20:42:45.743" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9437" PostId="6180" Score="0" Text="Have you tried increasing the light amounts?" CreationDate="2018-01-28T21:24:31.973" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9438" PostId="6180" Score="0" Text="@SebastiánMestre What do you mean by linear space? I'm sorry, I'm new to this." CreationDate="2018-01-28T23:42:09.827" UserId="7925" ContentLicense="CC BY-SA 3.0" />
  <row Id="9439" PostId="6180" Score="0" Text="@user1118321 No, I've been using only one light, a directional light. In my mind it should be enough and it shouldn't be as dark as it is." CreationDate="2018-01-28T23:43:13.327" UserId="7925" ContentLicense="CC BY-SA 3.0" />
  <row Id="9440" PostId="6180" Score="0" Text="i mean linear colour space. images are usually gamma compressed and can give wrong results when using them in shading. they should first be gamma corrected. you can search what that means online." CreationDate="2018-01-29T00:18:30.707" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9442" PostId="6159" Score="0" Text="As a designer of a GPU lossless image compression system, most of these seem to make sense, with the possible exception of using 32-bit float over 16-bit. Surely the LSBs of floating point data become increasingly less coherent and thus harder to compress." CreationDate="2018-01-29T09:06:14.237" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9443" PostId="6183" Score="2" Text="This is a great example of a homework question where you've &quot;done your homework&quot;: you've shown how far you got and what you have, and where you're stuck. Good work!" CreationDate="2018-01-29T11:14:23.163" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9444" PostId="6183" Score="1" Text="Thank you, now if only someone could help me figure it out,haha :D" CreationDate="2018-01-29T11:22:46.313" UserId="8108" ContentLicense="CC BY-SA 3.0" />
  <row Id="9445" PostId="6184" Score="0" Text="Oh, you were probably typing out your answer when I replied above! Thank you very much for the detailed answer, Dan! Everything is much clearer now." CreationDate="2018-01-29T12:22:44.820" UserId="8108" ContentLicense="CC BY-SA 3.0" />
  <row Id="9446" PostId="6184" Score="0" Text="One question though, about the tea-pot. If the bounding box does not change, am I correct to assume that the enclosed object does not matter? The tea pot fits within the bounding box but the bounding box remains the same (cfr. the question), so it does not affect the surface area, which is still calculated using the 6 surfaces of the cubic bounding box." CreationDate="2018-01-29T12:27:11.987" UserId="8108" ContentLicense="CC BY-SA 3.0" />
  <row Id="9447" PostId="6184" Score="0" Text="Oh of course, this version of the SAH just uses the box volume. Ignore that bit then! I was thinking of a different version that actually uses the triangle areas in the leaf nodes. In that case, ask yourself the same question as for the last part: will changing the objects change any number in the heuristic? Will they change the actual hit-ratio of the box? What does this tell you about how the heuristic relates to the measured performance?" CreationDate="2018-01-29T13:07:05.770" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9448" PostId="6184" Score="0" Text="I would say that changing the objects or the camera position doesn't change the heuristic, seen as though the surface area is based on Crofton's theorem (independent of the camera). So I'd assume that (in line with what you said) the position of the camera does not influence the score. But, it's only a heuristic so the position of the camera WILL influence the hit ratio. E.g: the 1st setup when we view a camera from above (y axis): if a ray hits the bounding box there, we have a much higher probability of actually hitting an object which is not the case if we have a camera along the z-axis." CreationDate="2018-01-29T14:25:22.897" UserId="8108" ContentLicense="CC BY-SA 3.0" />
  <row Id="9449" PostId="6181" Score="0" Text="And these terms aren't adressed anywhere in the paper?" CreationDate="2018-01-29T23:01:46.227" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9450" PostId="6181" Score="0" Text="Not in a meaningful way. The paper deals with a vertex based model (the equations and the logic behind it describe the deformation of the mesh in terms of the displacement of the vertices) but it doesn't discuss triangle-based models. I can only guess what they're about. Moreover I don't know why triangle-based models would be more &quot;expressive&quot; than vertex-based ones." CreationDate="2018-01-30T14:15:30.450" UserId="8107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9451" PostId="6187" Score="0" Text="How much manual effort do you want to put in?  For example, would it be okay if you had to specify the (x,y) coordinates of the start of the color scale and the (x,y) coordinates of the end of the color scale, and manually input the numeric values?  Or are you hoping for an Artificial Intelligence solution that could potentially 1) Find the legend, 2) OCR the labelling and hash marks, 3) determine an appropriate function (potentially non-linear) to map the colors to values, 4) deal with non-data in the image, such as labels." CreationDate="2018-01-30T17:52:34.567" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9452" PostId="6181" Score="0" Text="They are cited from this paper:  http://people.csail.mit.edu/sumner/research/deftransfer/Sumner2004DTF.pdf" CreationDate="2018-01-30T18:05:50.243" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9453" PostId="6190" Score="0" Text="Yeah I should have pointed out I've tried doing edge1.cross(egde2) without any noticeable difference. The dot product of the incident ray and the normal also does not seem to have a noticeable effect when I change the comparison sign or just remove it completely." CreationDate="2018-01-30T21:34:17.907" UserId="6760" ContentLicense="CC BY-SA 3.0" />
  <row Id="9454" PostId="6189" Score="0" Text="The look of the gradients in your first image looks like swapped normals to me. Have you tried rendering your normals to check that they are correct? My suspicion would be directed toward vertex indices." CreationDate="2018-01-31T02:21:01.513" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="9456" PostId="6188" Score="1" Text="you can get into trouble if compression artifacts crop up though" CreationDate="2018-01-31T09:11:01.963" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9457" PostId="6189" Score="0" Text="I have updated my answer to include an image of the normals, and it seems to be what I would expect. Or rather, the normals are consistent on the triangles that make up a plane." CreationDate="2018-01-31T09:39:40.003" UserId="6760" ContentLicense="CC BY-SA 3.0" />
  <row Id="9458" PostId="6188" Score="0" Text="Indeed, this technique would not be very robust in presence of artefacts. Although, the colour look up could be relaxed to find the nearest match instead of an exact match. In any case it remains an approximation." CreationDate="2018-01-31T09:54:04.650" UserId="7724" ContentLicense="CC BY-SA 3.0" />
  <row Id="9459" PostId="6187" Score="0" Text="Wyck, thanks for your answer. I'm willing to put lots of efforts to prepare my classes. And yes it would be totally OK if I had to specify the coordinates of the start and end of the color scale and manually input the numeric values. Keep in mind I'm not searching for a software which takes hundreds of maps and automatically converts to data, I only need to get numbers from a few maps per year (eg 30). So no, I am not looking for the AI approach you mention." CreationDate="2018-01-31T13:41:04.353" UserId="8116" ContentLicense="CC BY-SA 3.0" />
  <row Id="9460" PostId="6187" Score="0" Text="All I want is to be able to do one, or maybe two things: 1) click at any point in the image and get the value corresponding to that particular color and maybe 2) measure the area covered by any particular color (by user input)." CreationDate="2018-01-31T13:43:41.207" UserId="8116" ContentLicense="CC BY-SA 3.0" />
  <row Id="9461" PostId="6188" Score="0" Text="Thanks, I'm aware of the artifacts introduced by lossy formats like JPEG. However for my classes I am not interested in retrieving &quot;pure&quot;, real, accurate data but rather I take my map (jpg) as the &quot;truth&quot; and work from there. For my purposes this is enough. Otherwhise I would not even bother. All I care is that a given RGB value equals some quantity (eg elevation) within a given image. For example, I won't be comparing two JPG. I only work with one image at the time, and only when the scale color bar is part of the image." CreationDate="2018-01-31T13:48:31.220" UserId="8116" ContentLicense="CC BY-SA 3.0" />
  <row Id="9462" PostId="6187" Score="0" Text="I know there are programs (and have used them) to digitize plots (eg a XY plot from a JPG): you click at the beginning and at the end of each axis and input the corresponding values. Then you click on each data point and the program automatically gives you the X,Y coordinate of each data point. I was hoping to find something like this, does it make sense?" CreationDate="2018-01-31T13:53:32.297" UserId="8116" ContentLicense="CC BY-SA 3.0" />
  <row Id="9463" PostId="6187" Score="0" Text="I don't know of a software package that provides this kind of lookup functionality out-of-the-box.  But as a developer of graphical software, I'm inclined to write it from scratch.  Or if you need help implementing a specific operation, then ask for specific help.  Or create an open source project on GitHub and I'll contribute.  :)  Reynold's answer is on the right track for the technique." CreationDate="2018-01-31T15:13:07.300" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9464" PostId="6191" Score="1" Text="You need to search for &quot;Homography&quot;.  Estimate a homography that places the image in the scene based on the normals and transform your image by the homography." CreationDate="2018-01-31T15:35:36.560" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9465" PostId="6189" Score="0" Text="I don't think the problem is in the code you've shown.  It looks more like the light isn't being transformed into the coordinate system of the object correctly.  Where's the code that shades the pixel based on the normal?  Does it perform the N·L calculation in world space or object space? (Transform light to object space or transform object to light space, or transform both to world space?)" CreationDate="2018-01-31T15:47:17.417" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9466" PostId="6179" Score="1" Text="@Stackmm, It *is* a value between 0 and 255.  They are 8-bit normalized, meaning the values in the interval 0.0 through 1.0 are mapped to the 8-bit codes 0x00 through 0xFF.  The 8-bit value you are looking for that equals 0.1 is: `(unsigned char)(0.1 * 255.0 + 0.5)` or about 26." CreationDate="2018-01-31T15:57:17.780" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9467" PostId="6189" Score="0" Text="The lighting is done by sampling a unit sphere or hemisphere. I just use the normal as calculated by the cross product of the edges or the normals from blender. I'll link the code that samples the hemisphere." CreationDate="2018-01-31T16:10:28.110" UserId="6760" ContentLicense="CC BY-SA 3.0" />
  <row Id="9468" PostId="6192" Score="1" Text="The problem is common to all ray based rendering in general for what that's worth!" CreationDate="2018-01-31T18:45:43.530" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9469" PostId="6187" Score="0" Text="example 2 is not easy example 1 is indeed very easy. should be doable in on any sane GUI toolkit quickly." CreationDate="2018-01-31T19:16:52.503" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9470" PostId="6179" Score="0" Text="Thank you very much! That solved everything. I'm glad I learned about the mapping." CreationDate="2018-01-31T22:55:37.343" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="9471" PostId="6179" Score="0" Text="Ah, I failed to notice that you were using `GL_UNSIGNED_BYTE` as the type for the call to `glReadPixels`. You could use `GL_FLOAT` to get floating point values. (Just make sure to allocate space for them. Your current allocation specifies `new unsigned char []`.)" CreationDate="2018-02-01T01:30:05.267" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9472" PostId="6191" Score="0" Text="So it is possible to estimate a homography using normals, right? I have found homography estimation using two images or through finding vanishing points in a single image so far, but not using normals. Is there a specific term for this that I should search for?" CreationDate="2018-02-01T05:07:23.783" UserId="8112" ContentLicense="CC BY-SA 3.0" />
  <row Id="9473" PostId="6196" Score="0" Text="Sorry for all the layout errors... Was typing this on my phone without any preview. I'll correct that knce I'm at my pc." CreationDate="2018-02-01T06:21:05.257" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9475" PostId="6195" Score="1" Text="Really interesting question. I long wondered about what the practical relevance of this function actually is nowadays, especially since there are no ranged calls for any of the more advanced drawcalls that arose later on, like instanced rendering for example. It *seems* like a relic of the past (maybe from the times before VBOs)." CreationDate="2018-02-01T10:53:23.370" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9476" PostId="6181" Score="0" Text="@Wyck so the expressiveness of triangle-based models is just a practical fact, there is no mathematical or computational reason" CreationDate="2018-02-01T14:19:37.097" UserId="8107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9477" PostId="6201" Score="0" Text="I find it hard to believe that there's no explicit way to have double-buffering. Uh, bad news then. I liked when my arrays had a fixed size." CreationDate="2018-02-01T17:01:01.330" UserId="8118" ContentLicense="CC BY-SA 3.0" />
  <row Id="9478" PostId="6201" Score="0" Text="@demanze: Vulkan exists to tell you how the hardware works and for you to interface with it. What you do with that information is up to you. If a hard FIFO implementation has a `maxImageCount` of 1, then you can create your own image to serve as a second buffer for a double-buffering system. Or you can just fail to have your application work, as hard FIFO with only 1 presentable image makes it pretty much impossible to present images on every vblank, no matter how fast you render." CreationDate="2018-02-01T17:09:17.603" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9479" PostId="6201" Score="0" Text="@demanze: If you want 2 presentable images, and the implementation supports 2 presentable images, then you can ask for 2 presentable images. Now it might give you more than that, but it will never give you *less*." CreationDate="2018-02-01T17:10:50.243" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9480" PostId="6201" Score="0" Text="No, I didn't mean that I was afraid there would be only 1. I'm actually scared one implementation could give me 3 or more. In my current prototype, I have an array of size 2 (for pointers to images/imageViews/framebuffers apart of the Swapchain). Now, I could put a maximum of 16 or 64 and keep a fixed sized array but I would rather have a way to have always 2 with the spec backing me up somewhere. :)" CreationDate="2018-02-01T17:16:29.903" UserId="8118" ContentLicense="CC BY-SA 3.0" />
  <row Id="9481" PostId="6199" Score="0" Text="&quot;You copy out the values you know you need, issue a rendering command with that, and then return to the caller&quot; - here is specifying the the indices in an EBO what you mean when you &quot;copy out the values&quot;, and what do you mean by &quot;return to the caller&quot; - the caller being your application here?" CreationDate="2018-02-01T17:40:37.500" UserId="7000" ContentLicense="CC BY-SA 3.0" />
  <row Id="9482" PostId="6199" Score="1" Text="&quot;*what you mean when you &quot;copy out the values&quot;*&quot; Vertex arrays contain the vertex data that you're going to render with. Copying out the values would be copying out the data that will be rendered. &quot;*what do you mean by &quot;return to the caller&quot;*&quot; The caller of `glDrawRangeElements`." CreationDate="2018-02-01T17:43:06.120" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9485" PostId="6191" Score="0" Text="I assume the normals help you estimate depth so that you get 3d points.  You don't need that if the surface is planar though.  Just identify the four corners of the rectangle that you want to map in the target image and use the homography." CreationDate="2018-02-02T02:41:47.837" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9487" PostId="6203" Score="1" Text="&quot;Multisample Anti-Aliasing method does is ..... &quot; we compute 2x, 4x, 8x or 16x times a similar value&quot;&#xA;I think you are describing SSAA, Super Sample AA, not MSAA.  MSAA will only run the pixel shader *up to* the minimum of the sample rate AND the number of objects crossing the pixel." CreationDate="2018-02-02T09:09:53.810" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9489" PostId="6203" Score="0" Text="You are correct, I updated the answer." CreationDate="2018-02-02T09:42:24.417" UserId="110" ContentLicense="CC BY-SA 3.0" />
  <row Id="9491" PostId="6205" Score="0" Text="What is the *valence* of a vertex, edge or face?" CreationDate="2018-02-02T20:44:30.863" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="9492" PostId="6205" Score="0" Text="The valence is the number of neighbors of a vertex (= number of edges joining in that vertex) or face (= number of edges around it)." CreationDate="2018-02-02T22:22:00.857" UserId="8141" ContentLicense="CC BY-SA 3.0" />
  <row Id="9493" PostId="6208" Score="0" Text="Looks good! However, I am wondering if we can use the available value of valence 4 (vertices = 'g' and faces = '274'), to encode equation 3 &amp; 4. Also, for [x3] we have only 1 face." CreationDate="2018-02-03T07:53:48.710" UserId="8141" ContentLicense="CC BY-SA 3.0" />
  <row Id="9494" PostId="6208" Score="0" Text="Ah right I forgot that g is given in the end. Assumin g is known we have two equations in three variables.  We already included that the valence of g is 4 in equation (2) and the single face with valece x3 in equation (1). I still think one more piece of information is missing." CreationDate="2018-02-03T09:30:44.800" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="9495" PostId="6208" Score="0" Text="Ahh, I think No, we have number of edges given in question, let's suppose &quot;q&quot; (see 3rd sentence of question). Still I am unsure how to get final value...&#xA;By the way, If 2*number of edges = Faces, then for valence 4 we have 137 (274/2) edges." CreationDate="2018-02-03T09:59:19.910" UserId="8141" ContentLicense="CC BY-SA 3.0" />
  <row Id="9496" PostId="6208" Score="0" Text="Also, I think given that object is sphere and x3 valence has 1 face only, then x3 should be equal to 2, because valence is nothing but number of neighbors (and for 1 face it can have only 2)." CreationDate="2018-02-03T10:13:27.993" UserId="8141" ContentLicense="CC BY-SA 3.0" />
  <row Id="9497" PostId="6208" Score="0" Text="A face with only two neighbours isn't really a face, a face needs at least 3 neighbours. And the information that we have a sphere tells us nothing but the genus." CreationDate="2018-02-03T11:20:20.060" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="9498" PostId="6212" Score="0" Text="I know what you mean. But my question is how to modify the data. You said &quot; You should avoid making lots of small uploads of data like that.&quot; Of course, we should avoid, but it's just a simple example to describe my question in detail." CreationDate="2018-02-03T20:22:11.120" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9499" PostId="6212" Score="0" Text="@shashack: &quot;*But my question is how to modify the data.*&quot; But you already know how to modify the data. That's how you put it there in the first place." CreationDate="2018-02-03T20:23:46.333" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9500" PostId="6212" Score="0" Text="Ok what if the number of vertex size is 100000000 and I just need to modify just 1 vertex in the buffer data. Do I need to bind the buffer and upload data again? It would take so much time." CreationDate="2018-02-03T20:26:19.953" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9501" PostId="6212" Score="0" Text="@shashack: Then only upload what you need. Just like your two `glBufferSubData` calls each only uploaded part of the total buffer size. There's no limit on how much or ***how little*** data you can upload. I don't know how to make &quot;If you want to upload 12 bytes worth of data, you can.&quot; more clear without literally writing the code for you." CreationDate="2018-02-03T20:27:46.983" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9502" PostId="6212" Score="0" Text="Oh so you mean I can call glBufferSubData super many time? For example, If I have vertex 100000000 size, then  100000000 time?" CreationDate="2018-02-03T20:33:52.820" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9503" PostId="6212" Score="0" Text="@shashack: &quot;*I just need to modify just 1 vertex in the buffer data*&quot; &quot;*I have vertex 100000000 size, then 100000000 time?*&quot; Pick a scenario. Either you need to update one vertex, or you need to update *all of them*." CreationDate="2018-02-03T21:08:18.123" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9504" PostId="6212" Score="0" Text="I got it! Thank you for answering. I couldn't think to use glBuuferSubData so many. Good day!" CreationDate="2018-02-03T21:26:31.587" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="9507" PostId="6215" Score="0" Text="Ok, that makes sense, I guess that if you know the amount of time its going to be idling you could also do some work there :)" CreationDate="2018-02-05T10:26:23.547" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9508" PostId="6215" Score="0" Text="@Nacho you can also put the fenceEvent in a list so you can use WaitForMulitpleObjects and also wait on (for example) Async File IO to complete on the same thread." CreationDate="2018-02-05T10:30:54.590" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9509" PostId="6191" Score="0" Text="This would work only if there is a rectangle and it can be found, right? I need to find areas with no semantically significant content, such as empty walls, and insert a logo there. I couldn't succeed using corner detection-based methods or homography estimation using parallel lines. Therefore I though surface normals can be useful since there is a normal for each pixel." CreationDate="2018-02-05T13:03:01.603" UserId="8112" ContentLicense="CC BY-SA 3.0" />
  <row Id="9510" PostId="6191" Score="0" Text="Normals are as useful as derivatives in Calculus.  Much like how you can integrate derivatives to get the original function (+/- a constant), you can do something similar to your normals to get the original surface (+/- a global translation). But knowing the geometry of the surface doesn't describe WHERE you want to put your image on that surface.  I feel like I still don't understand how you want to describe where the image goes.  But I also feel like you can bypass the problem if you use the homography." CreationDate="2018-02-05T14:18:40.407" UserId="8009" ContentLicense="CC BY-SA 3.0" />
  <row Id="9511" PostId="6215" Score="0" Text="@Nacho The idea is that you shouldn't block on the fence until you have to, because the next thing you're going to do on the CPU depends on prior GPU commands having finished. For example, you might have a thread that composites frames generated from another thread, and the producer thread sends a fence along with other per-frame data, to keep the data and the frames in sync." CreationDate="2018-02-05T19:02:56.293" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9512" PostId="6224" Score="0" Text="I still struggle to understand, the incoming light is $L_i$,. Isn't $f_r$ the one that has low frequency information?" CreationDate="2018-02-06T15:47:15.377" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9513" PostId="6224" Score="0" Text="Moreover... according to what you're saying, low frequency $f_r$ means that $f_r$ is a narrow band signal, let's assume it is very narrow... that means that $f_r$ is almost a constant, therefore it can be factorized. However in the approximation what happens is that you factor $L_i$, and moreover an extra factor $\frac{1}{N}$ comes out, which I still struggle to understand where it comes from." CreationDate="2018-02-06T15:53:01.127" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9514" PostId="6100" Score="0" Text="Hi, what do you mean by &quot;distributed&quot;? could you elaborate that?" CreationDate="2018-02-06T15:57:15.587" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9515" PostId="6226" Score="0" Text="I know I'm being annoying but I still have few unclear points... First $f_r$ is actually $f_r(l,v)$, the integral is computed over the hemisphere parametrized wrt $l$ (here $v$ is fixed). If $f_r(l,v)$ is low frequency (in signal processing terms) that means, I guess, that $$f_r(l,v) = \sum_{k=0}^{N-1} \alpha_k(v) \psi_k(l)$$, I'm assuming that $psi_k$ are spherical harmonics on the unit sphere. Is this interpretation correct? I know you said you don't know the derivation, but I'm trying to work out a bit of rigour for that formula." CreationDate="2018-02-06T17:18:38.590" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9516" PostId="6226" Score="0" Text="Also you mention &quot;the two functions are not correlated&quot;, which functions? And why are you saying they're not correlated?" CreationDate="2018-02-06T17:28:28.660" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9517" PostId="6226" Score="0" Text="The two functions are $f_r$ and $L_i$. I say they are not correlated because the brdf shouldn't depend on the environment" CreationDate="2018-02-06T17:34:10.573" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9518" PostId="6226" Score="0" Text="I'd go as far as to say that this formula cannot be found if not by eye or by some sort of probabilistic analysis. And i don't have a strong enough grasp on probability to produce a proper derivation. Keep in mind that it is often said by CG professionals that &quot;if it looks right, it is right&quot; so rigorous proofs are not always needed. That is, unless you are doing scientific research" CreationDate="2018-02-06T17:38:46.553" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9519" PostId="6221" Score="0" Text="Just a note: &quot;low-frequency&quot; here refers to angular frequency, ie how rapidly the function varies with input direction—nothing to do with the wavelength of light." CreationDate="2018-02-06T19:04:02.467" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9521" PostId="6229" Score="1" Text="The answer to this is a large list of things, but SSAO algorithms fall into this group. PCF shadow sampling as well (making the sampled shadow maps look nicer).  Purely ray based shadows can give nice soft edges if you take enough samples per pixel.  Another technique is to render a shadow map from a random spot on the light source every frame and let something like temporal anti aliasing integrate the correct result over time." CreationDate="2018-02-07T00:45:03.020" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9522" PostId="6219" Score="1" Text="[You already posted this question](https://android.stackexchange.com/q/190621/12442) on [android.se]. Don't post the exact same question on multiple Stack Exchange sites. It leads to information being split up and it wastes the time of people who are trying to help." CreationDate="2018-02-07T09:57:39.677" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9523" PostId="6226" Score="0" Text="Hi Again, just a last question. From the examples you gave me I have the feeling we are actually trying to split the integral and approximate those using montecarlo technique. The importance sampling is applied to the brdf one, while standard montecarlo integration seems to be applied for the $L_i$ term. Is this correct?" CreationDate="2018-02-07T12:12:54.760" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9524" PostId="6231" Score="0" Text="can you specify, what the individual symbols ($\Theta, \Psi, ...$) represent? the symbols in BRDFs differ from book to book and I for one have learned with RealTimeRendering, Physically Based Rendering and Digital Modeling of Material Appearance, not your source. And even now, I can't recall how they did it individually from the top of my head, so I have a hard time grasping what is done in your equations." CreationDate="2018-02-07T13:33:42.107" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9525" PostId="6226" Score="0" Text="Any numerical integration techniques can be applied to either. The point is that they should approximate the integral. You can even do importance sampling with different importance distributions for each integral" CreationDate="2018-02-07T15:11:18.943" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9526" PostId="6226" Score="0" Text="But since L_i will be precomputed from an image, it would be reasonable to use all data without importance sampling" CreationDate="2018-02-07T15:12:54.513" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9527" PostId="6231" Score="0" Text="@Tare I hope is clearer now" CreationDate="2018-02-07T16:06:45.253" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9530" PostId="6234" Score="0" Text="what about `glBlitFramebuffer`? Can't I copy default framebuffer's images or depth to another framebuffer with it?" CreationDate="2018-02-07T18:03:14.550" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9531" PostId="6234" Score="1" Text="@recp: Yes, you could. Or you could just render to your own image and *not* have to do a copy. Why pick the slower option?" CreationDate="2018-02-07T20:15:28.880" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9532" PostId="6234" Score="0" Text="&quot;Why pick the slower option?&quot; maybe this is what I was looking for. I don't know which one is faster so I tried to save texture space. As a solution I will render opaque objects to another framebuff and share its depth buffer with transparency framebuff, this seems better way and fits with your answer I guess" CreationDate="2018-02-07T20:18:20.410" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9533" PostId="6235" Score="0" Text="This is very interesting indeed, but it seems like it would suffer a lot for varying geometry scenes" CreationDate="2018-02-07T21:35:18.593" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="9534" PostId="6235" Score="0" Text="It depends—as long as the transformations you make on individual objects don’t involve deformation (i.e. moving a thing around as opposed to bending it), you can keep using the original volume texture for each object and just apply an appropriate inverse transformation when you’re sampling it. But yes, this approach isn’t as helpful for deformable geometry like animated characters." CreationDate="2018-02-07T21:39:34.217" UserId="506" ContentLicense="CC BY-SA 3.0" />
  <row Id="9535" PostId="6235" Score="0" Text="That's not the only thing I am thinking of, imagine objects appearing and disapearing from your scene (projectiles for example), this seems like it would have a problematic scaling time for something like that" CreationDate="2018-02-07T21:47:34.503" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="9536" PostId="6235" Score="1" Text="You could replace the SDF with a cube map. Nvidia's GPU Gems has a chapter about this. https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch17.html This would result in a lower quality but it is easy to generate and a lot cheaper than SDF to generate, especially when things are animated." CreationDate="2018-02-08T07:23:51.670" UserId="4908" ContentLicense="CC BY-SA 3.0" />
  <row Id="9537" PostId="6229" Score="1" Text="Some minor comments: *Shadow maps*: The main disadvantage is not so much numerical limitations but sampling rate. It's hard to get this above the required Nyquist rate for the final image (without excessively sampling other areas), leading to frequent aliasing artefacts.&#xA;*Shadow Volumes*: I don't understand the &quot;finding the last object&quot; problem.  One problem that sometimes crops up is keeping the volume closed when clipping against the front clip plane, but there are several ways to avoid this problem." CreationDate="2018-02-08T16:01:45.160" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9539" PostId="6241" Score="1" Text="I don't quite understand. What's the problem, or what are you trying to achieve?" CreationDate="2018-02-09T08:29:49.473" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9540" PostId="6243" Score="0" Text="What's the actual question? If your aim is to have a discussion, SE sites don't really work for that. (See [help/dont-ask] for more.)" CreationDate="2018-02-09T13:38:09.740" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9541" PostId="6239" Score="0" Text="You forgot the foreshortening term in the equation." CreationDate="2018-02-09T13:38:35.383" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9542" PostId="6243" Score="0" Text="But http://codereview.stackexchange.com is a part of SE." CreationDate="2018-02-09T13:43:41.223" UserId="5430" ContentLicense="CC BY-SA 3.0" />
  <row Id="9543" PostId="6243" Score="0" Text="It is sad, that there is no ideareview SE." CreationDate="2018-02-09T13:45:56.237" UserId="5430" ContentLicense="CC BY-SA 3.0" />
  <row Id="9544" PostId="6239" Score="0" Text="Yes but it seems like you managed to understand it so it's not a big deal" CreationDate="2018-02-09T14:11:37.297" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9545" PostId="6239" Score="0" Text="I like your reply XD. I see what you mean anyway." CreationDate="2018-02-09T14:26:48.360" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9546" PostId="6242" Score="0" Text="It would be much preferable you'd ask for an actual *algorithm* to do that, as this is more what this site is for, rather than advice for what software to use. The general problem is interesting, but we don't really do software recommendation here." CreationDate="2018-02-09T15:41:19.810" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9547" PostId="6241" Score="0" Text="when I am using phone to watch a video it plays in full screen (1920x1080), but when I tilt it(rotate) the video will be played only in part of the screen, the rest of the  areas of screen is black. I wanted to know whether the resolution of image(video) is changed to achieve this! If so how does this happen. Think in terms of aspect ratio or some thing like that" CreationDate="2018-02-09T16:08:25.027" UserId="8153" ContentLicense="CC BY-SA 3.0" />
  <row Id="9548" PostId="6233" Score="0" Text="@joojaa - I saw that question. But the answers still don't answer how or why exactly the axes coincide when they are supposed to remain perpendicular. Or if is it related to non-ortho axes mentioned in the OP. Why are we using a non-ortho frame." CreationDate="2018-02-09T16:50:39.503" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9550" PostId="6233" Score="0" Text="@joojaa - that's what I want to see with the coordinate frame. Show me a situation where I end up with a rotation repeating another rotation? For this to happen in a physical gimbal the rings must coincide, hence for this to happen with a coordinate frame the axes must coincide. If the axes aren't coinciding as you say, then the gimbal lock never happens as all 3 axes point in different directions maintaining the 3 degrees of freedom." CreationDate="2018-02-09T17:20:07.667" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9553" PostId="6242" Score="0" Text="@ChristianRau thanks for your note! I've edited the question." CreationDate="2018-02-09T18:17:01.890" UserId="2750" ContentLicense="CC BY-SA 3.0" />
  <row Id="9555" PostId="6245" Score="0" Text="Thanks for this. I was actually working with some images converted to numpy matrices which are uint8 and it automatically uses modulo when you exceed the unsigned byte length so that's why I thought maybe modulo was used in graphics somewhere. Can I ask for clarification on the clamping? So is it common to add/multiply images together? If so, in my samples working with grayscale images it would seem like the final image would just instantly oversaturate and &quot;white-out&quot; so to speak. Is this normal and do we still just clamp the values?" CreationDate="2018-02-10T04:36:49.960" UserId="8180" ContentLicense="CC BY-SA 3.0" />
  <row Id="9556" PostId="6233" Score="0" Text="@joojaa - Every time some one asks this question people start giving examples and pictures of &quot;physical gimbal&quot;. I'm asking you to restrict yourself to a coordinate frame even though I know that the concept originated from the physical gimbal. I can understand what's happening in the physical gimbal but not in the coordinate frame. In the picture it says rotation direction is lost but &quot;HOW&quot; is it happening when the 3 coordinate axes don't coincide according to you?" CreationDate="2018-02-10T10:42:34.173" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9557" PostId="6233" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/72953/discussion-between-joojaa-and-wandering-warrior)." CreationDate="2018-02-10T10:52:00.943" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9559" PostId="6247" Score="0" Text="Ok wait. your post was helpful but I think I finally understand. Will post my own answer shortly." CreationDate="2018-02-10T14:01:25.413" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9560" PostId="6246" Score="0" Text="Thanks! Normalization into 0-1 floating point values makes sense to me because then multiplying won't cause the threshold to be exceeded. What would you do in the case of adding/subtracting images together? Would you clamp the values between 0-1 after the arithmetic?" CreationDate="2018-02-10T15:41:30.773" UserId="8180" ContentLicense="CC BY-SA 3.0" />
  <row Id="9561" PostId="6246" Score="0" Text="Yes, once you've completed all the operations you need to on the image, only then do you clamp it. So if you're adding 2 images, then blurring the result, or scaling it, or whatever, you do all of those things at the higher bit depth and then clamp the final output (if needed)." CreationDate="2018-02-10T15:43:40.317" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9562" PostId="6242" Score="0" Text="Also note that there is a [Software Recommendation Stack Exchange](https://softwarerecs.stackexchange.com). Perhaps your original question would be appropriate there? (I'd check the FAQ first to be sure.)" CreationDate="2018-02-10T15:46:04.730" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9563" PostId="6246" Score="0" Text="I guess this doesn't work for grayscale, right? Let's say I have a standard 8-bit RGB image, 8-bits for each channel.&#xA;&#xA;Image 1 is a solid red color image where every pixel is [200, 0, 0].&#xA;Image 2 is a solid dark red color image where every pixel is [100, 0, 0].&#xA;&#xA;When I up-convert to a higher bit depth, do I literally just use 16-bits for each channel, so my final image would be an image where every pixel is now [300, 0, 0]? Now my choices are (A) Leave it at this 16-bit per channel depth or (B) Convert back to 8-bit in which case the image will clamp back to [255, 0, 0]. Is that right?" CreationDate="2018-02-10T15:53:01.867" UserId="8180" ContentLicense="CC BY-SA 3.0" />
  <row Id="9564" PostId="6246" Score="0" Text="This absolutely works for grayscale. It also works for other color spaces like HSV, YCbCr, La*b*, etc. And yes, you have the right idea about what I'm suggesting." CreationDate="2018-02-10T15:59:25.987" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9565" PostId="6219" Score="0" Text="I'm voting to close this question as off-topic because it is about the inner working of a phone unrelated to the topic of computer graphics." CreationDate="2018-02-10T17:11:45.653" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9566" PostId="6245" Score="0" Text="Yes, it's pretty common, though we usually think of images as representing fractional values in [0, 1] rather than integers 0 to 255. Alpha blending, for instance, involves adding and multiplying. Depending on the images and the operations you're doing, saturating to white may indeed happen, but in other cases (like alpha blending layers of images on top of each other) it's not really an issue. Also, if you work with HDR images, you can leave them unclamped while you do a bunch of operations on them, then apply a [tone mapping](https://en.wikipedia.org/wiki/Tone_mapping) function at the end." CreationDate="2018-02-10T18:56:30.607" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9575" PostId="6192" Score="0" Text="Would backface culling be a solution to this?" CreationDate="2018-02-12T06:02:51.487" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9580" PostId="6265" Score="0" Text="I don't think I've misunderstood the meaning of $D$. According to the original paper written by torrance, the microfacets distribution $D$ quantify the amount of microfacets that would reflect the the light coming from a direction into another. Isn't that what it is?" CreationDate="2018-02-13T09:34:26.557" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9581" PostId="6266" Score="0" Text="So literally the roughness map is the distribution parameters per point. Is that correct?" CreationDate="2018-02-13T09:35:15.517" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9582" PostId="6265" Score="0" Text="Not as in total numbers, but essentially, yes. Your wording/question structure made me assume you got it wrong." CreationDate="2018-02-13T09:43:02.467" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9583" PostId="6265" Score="1" Text="Not as in total, but in percentage yes (it's a distribution function, in sense of probability but built upon the number of microfacets facing the halfway vector between incident light and reflection direction." CreationDate="2018-02-13T09:48:21.610" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9585" PostId="6269" Score="0" Text="i understand,but my idea is: 110011 + hash code = 00ii00 = 0110, i is complex number and converted to 1,1 is converted to 0 and 00 is 0." CreationDate="2018-02-13T16:03:53.910" UserId="8204" ContentLicense="CC BY-SA 3.0" />
  <row Id="9586" PostId="6269" Score="0" Text="so 01101110 will become 101001,let's decode: original zero is always invert to 1,1 is always converted to zero,two zero after converted is converted to only 0." CreationDate="2018-02-13T16:16:26.610" UserId="8204" ContentLicense="CC BY-SA 3.0" />
  <row Id="9587" PostId="6266" Score="0" Text="That is correct." CreationDate="2018-02-13T16:20:47.560" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9588" PostId="6269" Score="2" Text="I have no idea what you're talking about. I don't think you're describing a hash code." CreationDate="2018-02-13T18:58:18.497" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9589" PostId="6267" Score="0" Text="I'm not sure how can I derive this projection matrix. Can you explain it more clearly?" CreationDate="2018-02-13T22:17:12.740" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9590" PostId="6273" Score="0" Text="I don't quite understand how the last object information is helpful for standard shadow mapping. Isn't any element behind &quot;first one&quot; along direction from light source occluded? Can you explain what are you trying to achieve? Are you up to some kind of  transparency? If so then &quot;depth peeling&quot; might be the keyword." CreationDate="2018-02-13T23:33:54.620" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="9591" PostId="6273" Score="0" Text="Can you clarify what's meant by &quot;first&quot; and &quot;closest&quot; here? Are you talking about closest distance to the light source, closest distance to the camera, or something else? A diagram might help. I'm having a hard time understanding the question." CreationDate="2018-02-13T23:34:48.683" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9592" PostId="6269" Score="0" Text="then i quantized the code with my own algorithm,i have test this several times with different code length and it can recreate 99% of code. 101001 = 001.after recreated = 01101110,this is normalized and it works for all different binary code." CreationDate="2018-02-14T00:10:12.183" UserId="8204" ContentLicense="CC BY-SA 3.0" />
  <row Id="9593" PostId="6269" Score="0" Text="i even still can use hash code,but i use real time hash with only hash the high logic value in a binary code." CreationDate="2018-02-14T00:13:01.460" UserId="8204" ContentLicense="CC BY-SA 3.0" />
  <row Id="9594" PostId="6273" Score="0" Text="@narthex go outside on a sunny day and look at something casting a shadow, a tree, a car, a sign. Put your hand on the shadow and lift it up a couple centimeters, you will notice that there are 2 shadows now, one is your hand one is the bigger shadow of the object." CreationDate="2018-02-14T00:32:42.787" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="9595" PostId="6273" Score="2" Text="The latter shadow is caused by your hand blocking light from the rest of the sky, not from it extra-blocking light that the first object already blocked. The usual way to approximate that is with ambient occlusion, but I’d be interested to hear your approach." CreationDate="2018-02-14T00:44:09.030" UserId="506" ContentLicense="CC BY-SA 3.0" />
  <row Id="9596" PostId="6273" Score="0" Text="It's not light from the rest of the sky, as you have noticed when you go outside, shadows are not balck, this is because air refracts light so at all points in space there is residual  light rays bouncing on the air molecules, what we commonly know as &quot;ambient ligth&quot;. When your hand is close to the ground it blocks a little more of these residual light rays, thus generating a second, darker, shadow. Taking this into consideration, if you know how far away you are from the object that last occluded the light from you, you can calculated a shadow intensity modifier." CreationDate="2018-02-14T00:54:11.550" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="9597" PostId="6273" Score="0" Text="If what you're after is simply the inversion of order, why not simply invert the depth test?" CreationDate="2018-02-14T08:38:01.440" UserId="2817" ContentLicense="CC BY-SA 3.0" />
  <row Id="9598" PostId="6217" Score="0" Text="Perhaps if you posted an image from a different game where you feel there's a discontinuity it might make things clearer. Also, it's a bit difficult to tell from a single frame." CreationDate="2018-02-14T09:08:25.027" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9599" PostId="6268" Score="0" Text="Are you implying that everyone keeps a database of images on their computer that's indexed by the &quot;universal hash&quot;?" CreationDate="2018-02-14T09:09:21.453" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9600" PostId="6269" Score="1" Text="@Lan... it sounds like you have no idea how compression works. Do some research to understand the pigeon hole issue." CreationDate="2018-02-14T09:41:24.667" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9601" PostId="6273" Score="0" Text="Very interesting. I actually have been observing shadows like you said but to spot a phenomenon know as variable penumbra size - the longer distance from occluder to receiver, the bigger penumbra. You probably mean some Ambient Occlusion or Sky Occlusion." CreationDate="2018-02-14T09:58:42.230" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="9602" PostId="6273" Score="0" Text="At some point, when distance is big, the shadow dissapears into blurriness. But it depends on light angular size." CreationDate="2018-02-14T10:07:24.417" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="9603" PostId="6267" Score="0" Text="I have added the complete derivation of a simpler version, plus the explanation on the differences to the matrix you wanted to achieve. hope that helps." CreationDate="2018-02-14T10:24:33.613" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9605" PostId="6274" Score="0" Text="By crush z, I mean making them 0 to make points become 2d" CreationDate="2018-02-14T13:45:55.453" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9606" PostId="6267" Score="0" Text="Thank you very much, I understand more thoroughly now. 1 question though, so the third row, mapping [-1, 1], aren't there any matrices multiplication that can get to that row? What I mean is a combination of scaling, translating, shearing, etc" CreationDate="2018-02-14T15:32:27.753" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9607" PostId="6226" Score="0" Text="Related question, just asked, https://dsp.stackexchange.com/questions/47180/how-to-model-a-generic-narrow-band-signal" CreationDate="2018-02-14T17:11:43.767" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9608" PostId="6278" Score="0" Text="What does it mean to &quot;remesh&quot; a mesh?" CreationDate="2018-02-15T01:40:17.160" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9609" PostId="6274" Score="0" Text="@ManhNguyenHuu: But the points *never* &quot;become 2d&quot;. That's the point. They're *always* three-dimensional positions, even in window-space." CreationDate="2018-02-15T01:49:01.053" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9610" PostId="6271" Score="0" Text="&quot;*So why do we need homogenous divide?*&quot; Perhaps the questions you need to ask yourself are 1) what are the alternatives? 2) are they more efficient or more amenable to hardware implementation?" CreationDate="2018-02-15T01:54:52.107" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9611" PostId="6278" Score="2" Text="Typically those remeshers voxelize a mesh and then use algorithms like dual contouring to recreate a polygonal mesh. @NicolBolas It's usually a way to clean up the topology in modeling. For example, if you use boolean/CSG operations to create a mesh, the resulting topology might be nasty and not work so well for subdivision surfaces and might not be so suitable for sculpting (the control points might be unevenly distributed). &quot;Remeshing&quot; would keep the overall shape but give you a very uniform topology that can be easier to work with for some cases." CreationDate="2018-02-15T04:43:53.073" UserId="2247" ContentLicense="CC BY-SA 3.0" />
  <row Id="9612" PostId="6239" Score="0" Text="If this answer helped you, it would be great if you upvoted it or selected it as correct." CreationDate="2018-02-15T05:26:21.993" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9613" PostId="6267" Score="0" Text="Although I guess it's possible, there is no matrix multiplication that I know of that does this. I also don't see the reason why you would want to add another multiplication, that achieves nothing else than the single matrix but adds an overhead of calculations?" CreationDate="2018-02-15T08:04:16.830" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9615" PostId="6283" Score="0" Text="I already thought about that solution but I wanted to do stick everything inside the shader. I'll keep this as a backup solution, I guess :)" CreationDate="2018-02-15T17:30:37.923" UserId="2372" ContentLicense="CC BY-SA 3.0" />
  <row Id="9616" PostId="6283" Score="2" Text="@MaT: If you want everything inside the shader, then presumably the frequency is also computed within the shader as some function `float frequency = f(_Time)`. Then you should do some calculus by hand to figure out $g(t) = \int_0^t f(\tau)\,\mathrm d\tau$ and implement that in the shader as well. Then you can do `float s = sin(g(_Time))`." CreationDate="2018-02-16T03:15:16.187" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9617" PostId="6278" Score="0" Text="Thanks @TeamUpvote. In this case I am not looking for uniform topology (necessarily), as I just want the meshes for machine learning purposes." CreationDate="2018-02-16T20:43:52.527" UserId="7622" ContentLicense="CC BY-SA 3.0" />
  <row Id="9618" PostId="6268" Score="0" Text="I also went through a stage of thinking up lots of brilliant ideas to use hashes to get around limits on compression. It took a long time to realise why they definitely cannot work. I recommend reading about [data compression](https://en.wikipedia.org/wiki/Data_compression)." CreationDate="2018-02-16T21:41:34.287" UserId="231" ContentLicense="CC BY-SA 3.0" />
  <row Id="9619" PostId="6278" Score="2" Text="It's a bit tricky -- mainly the fact that the input mesh is quad-based shouldn't matter so much, at least I can't think of remeshing algorithms that would care, so to speak -- only thing important typically is the output requirements. Voxelization followed by an algorithm to turn those voxels back into a polygonal mesh tends to be one of the most straightforward ways to do it with lots of control over the resolution of the results. Is Blender's remesh inadequate in some way? I think it's using the method I proposed: https://docs.blender.org/manual/en/dev/modeling/modifiers/generate/remesh.html" CreationDate="2018-02-17T05:21:19.370" UserId="2247" ContentLicense="CC BY-SA 3.0" />
  <row Id="9620" PostId="6273" Score="2" Text="&quot;Ambient light&quot; *is* light from the rest of the sky, or the rest of the environment in general if you're indoors. Anyway, you should definitely look into depth peeling (from the perspective of the light source), as narthex suggested in the very first comment." CreationDate="2018-02-17T05:32:57.970" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9621" PostId="6278" Score="1" Text="When I used that tool, either it would lose the quality of the mesh (shape was distorted). To avoid that it lets you increase the poly count, but to keep the shape the same you need to blow up the poly count by 10x or more." CreationDate="2018-02-17T09:05:36.267" UserId="7622" ContentLicense="CC BY-SA 3.0" />
  <row Id="9622" PostId="6287" Score="0" Text="Even if not using classic rasterization on top of the raytraced image, there's still some nice convenience in having the resulting imagine in an FBO or a texture, provided you don't *just* want to write it out into an image file you have to display it at *some* point. Besides that, there's also nice convenience things an inherently graphical task might still profit from, like builtin matrix types or builtin access to the hardware's advanced texture filtering functionality." CreationDate="2018-02-17T16:26:14.133" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9623" PostId="6287" Score="0" Text="I don't have an Nvidia card but AMD one. Thanks for the links helps a bit. But I think compute shader would be better, since I'd have to use OpenGL anyways to output the result to a window. Keeping this question open a little more in case anybody has some more interesting details or first hand experience." CreationDate="2018-02-18T11:04:05.743" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9624" PostId="6290" Score="0" Text="I dont think that's the case. From what I read in 3D Math Primer and here I quote it: &quot;If we interpret the rows of a matrix as the basis vectors of a coordinate space, then multiplication by the matrix performs a coordinate space transformation. If aM=b, we say that M transformed a to b.&quot;, then the way I was building the matrix was correct. 3D Math Primer use row matrices." CreationDate="2018-02-18T18:24:39.197" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9625" PostId="6291" Score="0" Text="Can you post an image showing what you mean?" CreationDate="2018-02-18T18:52:39.997" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9626" PostId="6291" Score="0" Text="@user1118321 sure, I updated question and added images" CreationDate="2018-02-18T19:18:35.753" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9627" PostId="6291" Score="0" Text="@recp: Additive blending is not transparency. The math simply doesn't work out that way." CreationDate="2018-02-18T20:42:37.693" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9628" PostId="6291" Score="0" Text="@NicolBolas thanks for your comments; yep additive blending itself is not transparency but weighted, blended OIT algorithm requires additive blending ONE, ONE. I'm using GL_ONE_MINUS_SRC_ALPHA/GL_SRC_ALPHA in compositing pass" CreationDate="2018-02-18T20:47:36.217" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9629" PostId="6291" Score="0" Text="I want to disable additive blending ( or additive blending LOOK ) in shader if alpha is 1. I want to do this for specific fragments/pixels, this is why I want to do it in shader ( or with helper framebuffs / buffs ... )" CreationDate="2018-02-18T20:50:31.607" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9630" PostId="6291" Score="0" Text="@recp: My overall point is that you're doing the wrong thing. It's as wrong as taking 2 seconds and multiplying it by 3 seconds and saying that you have a velocity of 6 m/s. You're not getting the right look because you're using the math incorrectly. Additive blending has a meaning, and if it's not doing what you want, then you're not using it correctly. Simply having an alpha of 1 mean &quot;opaque&quot; won't help you. After all, an alpha of 0.99 won't be &quot;nearly opaque&quot;, so it's going to look wrong if it suddenly jumps to &quot;opaque&quot;." CreationDate="2018-02-18T23:07:44.283" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9631" PostId="6292" Score="0" Text="The OP said they are using [weighted, blended OIT](http://jcgt.org/published/0002/02/09/), which does use `glBlendFunci(0, GL_ONE,  GL_ONE)` in a transparency accumulation pass." CreationDate="2018-02-19T05:29:17.703" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9632" PostId="6290" Score="1" Text="@ManhNguyenHuu - Nathan is correct. Your quote doesn't imply anything about from which space does &quot;a' transforms to. IF you construct a matrix with the rows or columns as the basis vectors and multiply it with the vector of that space it will give you the world space representation of that vector" CreationDate="2018-02-19T08:36:08.823" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9633" PostId="6292" Score="0" Text="That is correct, plus the additive blending blends (RGB * A, A) * Weight. I don't know how dual source blending works I'll investigate it, it seems interesting! But on the other hand I need to two different render targets: accumulation (vec4: RGBA16F) and revelage (float)" CreationDate="2018-02-19T11:34:11.413" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9634" PostId="6292" Score="0" Text="Maybe I can use three render targets (the first two for dual-source blending and last one for revelage), in the first pass dual-source is disabled and in other pass it is enabled (full screen quad), then compositing all passes? This would add extra pass only for dual-source blending but if it will fix the artifact I will implement it" CreationDate="2018-02-19T11:41:48.593" UserId="6929" ContentLicense="CC BY-SA 3.0" />
  <row Id="9636" PostId="6298" Score="0" Text="Thanks. I asked a purer version of this question at [math stackexchange](https://math.stackexchange.com/questions/2657669/radius-of-circle-tangent-to-sphere-which-obscures-that-sphere/2657673#2657673) and got an equivalent answer which gives another hint at how to solve this." CreationDate="2018-02-19T22:40:40.187" UserId="8233" ContentLicense="CC BY-SA 3.0" />
  <row Id="9637" PostId="6300" Score="1" Text="I have it on good authority that for OpenGL on macOS, the limit is 32 virtual screens, fwiw. Not sure if the limit holds for Metal." CreationDate="2018-02-20T03:53:29.420" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9638" PostId="6302" Score="0" Text="lol this is what happens when you stop programming for a couple of years :-) thanks for idiot-checking me" CreationDate="2018-02-20T08:16:54.957" UserId="8233" ContentLicense="CC BY-SA 3.0" />
  <row Id="9640" PostId="6303" Score="2" Text="If you need the reflections off wavy water to be that realistic, you should use ray-tracing. Any rasterization technique is going to involve *some* trade-offs." CreationDate="2018-02-20T16:12:51.840" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9641" PostId="6303" Score="0" Text="Check new [McGuire's paper](http://research.nvidia.com/publication/real-time-global-illumination-using-precomputed-light-field-probes) which introduces real time raytracing using grid of cubemaps. For dynamic objects, voxel cone tracing might be good because voxelization in real time is quite gpu friendly." CreationDate="2018-02-20T16:39:10.993" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="9642" PostId="6303" Score="0" Text="You can also combine both SSR and VCT when SSR fails. The downside of voxels is finite precision measured in voxel size." CreationDate="2018-02-20T16:54:51.763" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="9643" PostId="6300" Score="0" Text="Yeah, I couldn't find anything about documented limits either, I'm more concerned about what the practical limitations are, like if the driver or OS get buggy beyond a certain number of cards. The miners seem to be saying that windows used to hit a wall at 8 GPUs but the creators update fixed that, now it sounds like some drivers might have that same limit so they're saying you can use 8 AMD cards + 8 Nvidia cards. No word on if you can run monitors off all those cards though. The PCIe bandwidth definitely seems like it would become an issue, but I suspect that would be application dependent." CreationDate="2018-02-20T17:01:29.560" UserId="7385" ContentLicense="CC BY-SA 3.0" />
  <row Id="9644" PostId="6300" Score="0" Text="The miners also seem to be saying that Linux has fewer issues working with a large number of GPUs, so that might be an option as well, still I haven't seen proof of any systems outputting from more than 4 GPUs simultaneously." CreationDate="2018-02-20T17:04:27.373" UserId="7385" ContentLicense="CC BY-SA 3.0" />
  <row Id="9645" PostId="6305" Score="0" Text="Just to make sure you don't do work you don't need to do: you are aware that OpenGL and DirectX do clipping on their own, right?&#xA;&#xA;One thing you also may consider is at which axis you are looking. Some rendering APIs look along negative z when in camera space, so if you clip depending on positive z values but look along the negative z axis, then there would be an error in your case.&#xA;&#xA;For the original question: there are algorithms for clipping, that calculate which parts of triangles you need to draw. It has been some time though, so I'd need to look up even the names." CreationDate="2018-02-21T07:07:52.393" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9646" PostId="6305" Score="0" Text="Upon a quick search, the two algorithms we looked at in university where [Cohen-Sutherland][1] and [Cyrus-Beck][2]. They are only for line clipping thought - Polygons need their own algorithms. I suggest looking into the [Wikipedia algorithm list][3] to get a feeling for what you have to consider when trying to do clipping yourself.&#xA;&#xA;  [1]: https://en.wikipedia.org/wiki/Cohen%E2%80%93Sutherland_algorithm&#xA;  [2]: https://en.wikipedia.org/wiki/Cyrus%E2%80%93Beck_algorithm&#xA;  [3]: https://en.wikipedia.org/wiki/Clipping_(computer_graphics)#Algorithms" CreationDate="2018-02-21T07:13:40.700" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9649" PostId="6303" Score="0" Text="Thank you for the comments. UE4's planar Reflections looks very realistic. Is there any article about how Planar Reflection works? https://docs.unrealengine.com/latest/INT/Engine/Rendering/LightingAndShadows/PlanarReflections/index.html" CreationDate="2018-02-21T14:56:02.000" UserId="8235" ContentLicense="CC BY-SA 3.0" />
  <row Id="9650" PostId="6275" Score="0" Text="+1 Great question (most students have these same questions and doubts, but just accept the homogeneous coordinates solution without a deeper reflection about it)... and great answer! Indeed, I think that this answer would be a good roadmap for anyone trying to explain/teach the need for homogeneous coordinates in a CG course." CreationDate="2018-02-21T15:03:02.447" UserId="5681" ContentLicense="CC BY-SA 3.0" />
  <row Id="9651" PostId="6305" Score="0" Text="@Tare Im not using openGL or directX actually. Im writing a simple ray tracer and there is preciously little info to start" CreationDate="2018-02-21T16:38:05.843" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9652" PostId="6312" Score="0" Text="To clarify: When I mentioned “cross-program”, I was referring to for example an executable calling into a third-party library from the same thread, rather than anything like cross-process interop. But it may fit the same ballpark regardless." CreationDate="2018-02-21T17:10:22.483" UserId="7215" ContentLicense="CC BY-SA 3.0" />
  <row Id="9653" PostId="6312" Score="0" Text="@haasn: In that case, the client third-party library and the calling layer should both agree on such things. That is, as part of the client library's documentation, it says &quot;you must have enabled extensions/features X, Y, Z&quot; or it has a way to inform you of what extensions/features are enabled." CreationDate="2018-02-21T17:38:49.297" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9654" PostId="6311" Score="0" Text="Thank you for your lenghty and detailed response. However the sun still looks unrealistic ( I attached an image in the description above). Is this a tone mapping issue? I want my sun to look more like a glow." CreationDate="2018-02-21T17:49:20.640" UserId="8242" ContentLicense="CC BY-SA 3.0" />
  <row Id="9655" PostId="6311" Score="2" Text="It sounds like you want to be implementing a bloom filter?" CreationDate="2018-02-21T17:50:10.393" UserId="7215" ContentLicense="CC BY-SA 3.0" />
  <row Id="9659" PostId="6307" Score="1" Text="You probably also want to look into a bloom effect to simulate the glare of the sun." CreationDate="2018-02-21T18:28:56.290" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9660" PostId="6307" Score="0" Text="@NathanReed She probably mean radial blur light shafts." CreationDate="2018-02-21T18:40:58.213" UserId="4958" ContentLicense="CC BY-SA 3.0" />
  <row Id="9663" PostId="4513" Score="0" Text="Related: https://stackoverflow.com/questions/726379/multiple-viewports-in-opengl/36048277#36048277" CreationDate="2018-02-21T22:50:19.510" UserId="5388" ContentLicense="CC BY-SA 3.0" />
  <row Id="9664" PostId="6305" Score="0" Text="Just define 6 3D-planes for the frustum. Near clipping and far clipping planes should have normal aligned with z-axis in camera space. The other four planes should be defined according to the field-of-view angle that you defined. All plane normals should point to the inside of the clipping volume. Then you have to calculate polygon intersection with each plane and clip accordingly." CreationDate="2018-02-21T23:40:37.523" UserId="7957" ContentLicense="CC BY-SA 3.0" />
  <row Id="9665" PostId="6303" Score="1" Text="Have you considered using the so-called planar reflections using an extra camera from the other side of the reflective plane? Usually warping the uv maps of the projected image acording to normals is enough to fake the bumpiness convincingly" CreationDate="2018-02-22T01:35:42.217" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9670" PostId="6316" Score="3" Text="This doesn't account for multiple layers of transparent geometry. What happens if i'm looking through a window at another transparent object?" CreationDate="2018-02-22T07:57:48.010" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9673" PostId="6313" Score="5" Text="*&quot; in early 3d (2000 - 2004)&quot;*  Hah! You youngsters." CreationDate="2018-02-22T09:01:18.807" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9674" PostId="6275" Score="1" Text="Good answer. I'd add to the last part, even in the case of a simple projection transform with no other matrix multiplies the Z component doesn't carry over untouched - it ends up in the form A*Z + B, used for near and far clipping and perspective-correct interpolation. So even in the simplest case you still need to store the original Z value somewhere." CreationDate="2018-02-22T09:43:23.457" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9676" PostId="6314" Score="0" Text="What steps have you taken so far to try to narrow down the cause of the problem?" CreationDate="2018-02-22T12:20:57.837" UserId="182" ContentLicense="CC BY-SA 3.0" />
  <row Id="9680" PostId="6314" Score="0" Text="I've reviewed over the way that you calculate the t value for a cube, over and over again, that seems to be correct. The way that the shadows are calculated seems to be correct because the circles clearly work. And I'm assigning the ray values and the outrecord values fairly identical to the sphere. I'm beginning to think its not in this class, but I've searched my other classes and they all seem to be correct." CreationDate="2018-02-22T15:55:54.573" UserId="8246" ContentLicense="CC BY-SA 3.0" />
  <row Id="9681" PostId="6100" Score="0" Text="Hi, I'm sorry to bring this up again. I'm noticing that your explanation, for which I've managed to use as inspiration to work out some math, is missing the $\cos$ factor. Would your splitting explanation still be valid?" CreationDate="2018-02-22T18:41:56.520" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9682" PostId="6319" Score="0" Text="You can use multiple GBuffers instead (Deep GBuffers) to capture multiple layers of transparency; which is still &quot;deferred&quot; shading." CreationDate="2018-02-22T18:44:07.040" UserId="2287" ContentLicense="CC BY-SA 3.0" />
  <row Id="9683" PostId="6100" Score="0" Text="@user8469759 oh i missed that in my explanation. I might be wrong but: Similarly to the $1/N$, i think it should be kept on both sides. This is because the $\cos$ is sort of a conversion from one domain of light direction to another. You can think of it like when you do substitution while solving an integral; you should keep the &quot;$du/dx$&quot;" CreationDate="2018-02-22T19:25:35.480" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9684" PostId="6275" Score="0" Text="@russ - true that, but thinking from his perspective, even tho the final Z value is A*Z + B, we still have the original vector IF we aren't overwriting/ storing the modified vector in the original variable. So he could argue we still have the original Z value stored. That's why tried to give an explanation where the division factor isn't just &quot;-z&quot; it's something entirely else." CreationDate="2018-02-22T19:32:53.403" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9685" PostId="6100" Score="0" Text="Not sure I'm following" CreationDate="2018-02-22T19:54:23.800" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9686" PostId="6275" Score="0" Text="&quot;why not just use the space for a single float more and get rid of this stupid check&quot; Why not let the type system keep track of which things are points and which vectors at compile time, and get rid of this stupid extra float that needs to be multiplied and added at runtime " CreationDate="2018-02-22T21:30:48.550" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9687" PostId="6275" Score="0" Text="Also FYI, to the point about SIMD, storing an xyzw vector as a SIMD register actually isn't best practice. Better is to go SOA and have one register of 4 vectors' X values, another of the 4 Y values, etc. This generalizes a lot better to larger SIMD widths. Note that a lot of newer CPUs have 8-wide SIMD, and GPUs have 32- or 64-wide SIMD." CreationDate="2018-02-22T21:37:52.833" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9688" PostId="6100" Score="0" Text="The $\cos\theta$ is a conversion factor from the light incoming from direction $\theta$ to the intensity of light projected on the surface." CreationDate="2018-02-22T23:01:17.830" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9689" PostId="6196" Score="0" Text="Sorry for the wait...ended up getting caught up with bdpt and forgetting about materials for a while :). It sounds like GGX/Smith is a strictly specular NDF and Oren-Nayar is a strictly diffuse NDF (i.e. it's meaningless to expect diffuse illumination from GGX/Smith or specular illumination from Oren-Nayar, even if you swap out the underlying macrosurfaces), is that right?" CreationDate="2018-02-22T23:53:32.453" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9691" PostId="6314" Score="0" Text="Can we see the code which generates the ray towards the light source ? Maybe a quick debug check you can do is to output per-pixel object normals to your image buffer to make sure nothing weird is going on there" CreationDate="2018-02-23T02:50:01.423" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9693" PostId="6196" Score="0" Text="Yes, that is (mostly) correct. One thing about Oren-Nayar is that it incorporates some aspect of specular lights, because it has some view dependency. However, there are no real highlights, there is just a shadowing effect where $n \cdot v \rightarrow 0$ (with $n$ is surface normale and $v$ is view vector), so I'd not go as far and say that that effect is real specular lighting effect." CreationDate="2018-02-23T06:35:48.447" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9694" PostId="6330" Score="0" Text="If the eye is at the origin then the view vector is the negative position (usually also normalized to have length 1)" CreationDate="2018-02-23T06:47:34.667" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9695" PostId="6332" Score="0" Text="It is very common (specially in more advanced brdfs) to use the vector that points from the shaded fragment to the eye as the view vector instead. It would be good if you mentioned this in your answer." CreationDate="2018-02-23T07:01:53.580" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9696" PostId="6332" Score="0" Text="you are right, i'll add it" CreationDate="2018-02-23T07:11:07.477" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9697" PostId="6331" Score="0" Text="It would be helpful to see the code for getBarycentricCoordinate." CreationDate="2018-02-23T08:41:46.640" UserId="7724" ContentLicense="CC BY-SA 3.0" />
  <row Id="9698" PostId="6275" Score="0" Text="@Nathan Reed - don't understand what you meant by &quot;type system&quot; in your first comment? If you meant something like separate classes for points and vector each having a `*` operator defined for itself and a matrix and the actual `1` or `0` hardcoded inside that function, then it isn't applicable in for example shaders." CreationDate="2018-02-23T09:10:12.350" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9700" PostId="6100" Score="0" Text="I can understand the meaning but my point is that Karis formula doesn't take into account that when he splits the formula. Also, if $f_r$ is low frequency, which means essentialy the lobe looks more and more a diffuse one, the cosine factor would change that shape hence the splitting can't be applied anymore. You could though apply the splitting if the light is low frequency, in the sense that the intensity doesn't change much with the direction." CreationDate="2018-02-23T10:32:13.990" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="9701" PostId="6325" Score="0" Text="I feel like this is pretty close, but I'd love a slightly more in-depth answer - specifically how this texture lookup works and *why* specifically this method results in artifacts of the shadow on walls and ceilings." CreationDate="2018-02-23T11:02:52.883" UserId="8245" ContentLicense="CC BY-SA 3.0" />
  <row Id="9703" PostId="6196" Score="0" Text="Great, thank you! My mostly-unfinished materials system defines a &quot;material&quot; as a vector of BRDF probabilities; would I get reasonable results if I had two randomly-selected + separate specular/diffuse microfacet BRDFs (given distinct specular/diffuse roughnesses + PDFs for the individual BRDFs)?" CreationDate="2018-02-23T13:08:25.943" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9704" PostId="6196" Score="0" Text="I am not entirely sure, how you are doing this. If you want to know, if you can reasonable results by combining an arbitrary diffuse BRDF with an arbitrary specular BRDF, then yes, it is possible. In fact, if you look through somewhat modern games and papers, that's what people have been doing for quite some time - they choose one BRDF for diffuse and one for specular lighting and then manually tweak the properties until the result looks good. The combinations are up to the teams: A lot of people used GGX+Lambert, but also Beckmann+Lamber or either specular BRDF with Oren-Nayar has been used." CreationDate="2018-02-23T13:37:22.687" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9705" PostId="6325" Score="0" Text="@SteffanDonal fair point. I will add some details of what I had in mind." CreationDate="2018-02-23T14:19:02.863" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="9706" PostId="6331" Score="0" Text="@Reynolds I just edit and add the code for getBarycentricCoordinate" CreationDate="2018-02-23T15:41:40.333" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9707" PostId="6196" Score="0" Text="Suppose I only want to deal in combinations of pure BXDFs (mostly for code simplicity). Each axis of the material probability vector stores the chance of selecting one of these pure BXDFs (e.g. 40% of the rays incident with $(0.4, 0.6)$ will be processed with a pure difffuse BRDF), and I render the material by accumulating as many samples as possible and using the probabilities of each BRDF to weigh their value appropriately." CreationDate="2018-02-23T19:40:33.560" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9708" PostId="6196" Score="0" Text="So my question is, will this work if the pure diffuse/pure specular contributors are microfacet BRDFs? From the sounds of your comment (+ the note about blending being possible so long as the result is normalized) it sounds like it will, but I just want to make sure anyways" CreationDate="2018-02-23T19:49:04.033" UserId="7868" ContentLicense="CC BY-SA 3.0" />
  <row Id="9714" PostId="6331" Score="0" Text="What language are you using? Having negative barycentric coordinates means you're sampling *outside* of the triangle." CreationDate="2018-02-24T22:57:21.890" UserId="1981" ContentLicense="CC BY-SA 3.0" />
  <row Id="9715" PostId="6339" Score="0" Text="Ah! I was thinking the inverse preserved original multiplication order so it effectively created a mirror transformation, but it doesn't. Does this explain why it works to use it as the view matrix - because any subsequent rotations of the camera will end up being done first, thus keeping an object in view?" CreationDate="2018-02-25T21:14:37.390" UserId="8268" ContentLicense="CC BY-SA 3.0" />
  <row Id="9716" PostId="6339" Score="0" Text="No, the inverse undoes the original transform, so multiplying the two together will give you an identity matrix. Not quite sure what you're asking about the view matrix, but a view matrix in general is the inverse of the camera's transform - it moves the camera to the origin with no rotation. Applying this matrix to the other objects in your scene means that as your camera moves through the world, after applying your view matrix the camera is a fixed point, and the world is moving around it with the inverse motion." CreationDate="2018-02-26T06:24:10.137" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9723" PostId="6328" Score="0" Text="Not necessarily. The main lights that cover the whole scene need to be done for each pixel, right enough. But the less-important lights that only affect a small local area can be rasterized as geometry, and a pixel shader run additively over just their area of effect. Good illustration of this technique used in GTA5 at http://www.adriancourreges.com/blog/2015/11/02/gta-v-graphics-study-part-2/  (last 2 slides)" CreationDate="2018-02-27T05:25:25.383" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9724" PostId="6328" Score="0" Text="@russ that is not really related to my answer or to the nature of deferred shading. What you propose is just a technique for LODing distant light sources" CreationDate="2018-02-27T05:30:40.267" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9725" PostId="6328" Score="0" Text="Did you look at the last couple of slides where they're rendering the streetlamps and car lights at night? I'm not talking about the billboarding of distant light sources, rather the technique of rasterizing a sphere, cone, or whatever that covers a light's area of effect, and running an additive pixel shader only for the pixels it covers." CreationDate="2018-02-27T07:08:59.060" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9726" PostId="6328" Score="0" Text="Seems the term I'm looking for is 'Light Volumes'. Better illustration here - http://www.spellcasterstudios.com/?p=339" CreationDate="2018-02-27T07:25:38.953" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9727" PostId="6328" Score="0" Text="@russ ah i see what you mean. I will edit my answer to touch on that subject later." CreationDate="2018-02-27T07:29:20.873" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9728" PostId="6173" Score="1" Text="Given that you are only doing half-toning, you could probably get away with a *much* cheaper approximation of gamma/sRGB such as using a square root." CreationDate="2018-02-27T08:46:41.673" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9729" PostId="6313" Score="0" Text="There were more earlier games from 90's :) Those were incredible in their times." CreationDate="2018-02-27T11:00:16.990" UserId="8249" ContentLicense="CC BY-SA 3.0" />
  <row Id="9730" PostId="6343" Score="0" Text="Yes this is the technique the person is looking for.  It is perfectly crisp because the shadows are actual rendered geometry.  It would be nice if you could provide a simple description of the technique!&#xA;Wikipedia has some decent info too: https://en.wikipedia.org/wiki/Shadow_volume" CreationDate="2018-02-27T14:49:51.247" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9731" PostId="6313" Score="1" Text="https://en.wikipedia.org/wiki/Shadow_volume &lt;-- that" CreationDate="2018-02-27T14:50:03.383" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9732" PostId="6278" Score="0" Text="A 2d mesh or a surface mesh?" CreationDate="2018-02-27T17:32:05.100" UserId="192" ContentLicense="CC BY-SA 3.0" />
  <row Id="9733" PostId="6343" Score="0" Text="@AlanWolfe Shadow volumes would explain the crispness, however does not fit the OP's statement that the shadows extend above the character (closer to the light than the shadow caster). The classic shadow volume algorithm doesn't have that artifact." CreationDate="2018-02-27T21:41:03.067" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9734" PostId="4563" Score="1" Text="I read this comment so I joined computergraphics and upvoted this answer." CreationDate="2018-02-27T22:48:06.533" UserId="8289" ContentLicense="CC BY-SA 3.0" />
  <row Id="9739" PostId="6173" Score="0" Text="@Wyck a typical display is non calibrated." CreationDate="2018-02-28T11:08:03.270" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9740" PostId="6343" Score="0" Text="Exactly @NathanReed - Shadow volumes were one of my first research points but it just didn't seem right." CreationDate="2018-02-28T12:49:34.270" UserId="8245" ContentLicense="CC BY-SA 3.0" />
  <row Id="9741" PostId="6278" Score="0" Text="Edited to say 3D mesh." CreationDate="2018-02-28T18:57:19.157" UserId="7622" ContentLicense="CC BY-SA 3.0" />
  <row Id="9742" PostId="6351" Score="1" Text="did you profile and see which steps take a long time?" CreationDate="2018-03-01T15:30:38.810" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9743" PostId="6351" Score="2" Text="&quot; (Admittedly using python). Are there alternative solutions ?&quot; C++ is a good alternative. How fast do you need it to go? It may simply be unachievable in Python." CreationDate="2018-03-01T15:32:09.767" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9744" PostId="6351" Score="1" Text="@ratchet freak: Will do and post resuts." CreationDate="2018-03-01T16:31:12.697" UserId="8303" ContentLicense="CC BY-SA 3.0" />
  <row Id="9745" PostId="6351" Score="0" Text="You can 1) try C++   2) try to convert your code to make it work on the gpu  3) using spatial data structures" CreationDate="2018-03-01T17:42:42.857" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9746" PostId="6350" Score="0" Text="Perhaps I'm misunderstanding what you're doing, but why does the light increase from 1 to 4 at a distance of 0.5? Shouldn't it decrease everywhere that's not the center of the light?" CreationDate="2018-03-02T03:13:31.550" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9748" PostId="6351" Score="0" Text="Are you just firing primary rays or are you also  doing secondaries, e.g. reflections?" CreationDate="2018-03-02T09:49:13.113" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9749" PostId="6353" Score="0" Text="At the first boundary, wouldn't D^2 + 1 be 1.25?" CreationDate="2018-03-03T05:21:11.770" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9750" PostId="6357" Score="0" Text="Thanks. I don't yet know how to obtain the time. I am using Windows 10." CreationDate="2018-03-03T05:47:17.710" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="9751" PostId="6357" Score="0" Text="I haven't dealt with Windows in a few years, but [this question](https://stackoverflow.com/questions/3729169/how-can-i-get-the-windows-system-time-with-millisecond-resolution) on Stack Overflow seems like it might have some good ideas. At least there are lots of different answers so one of them will probably do the trick." CreationDate="2018-03-03T05:50:50.153" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9752" PostId="6353" Score="0" Text="Yeah, so you'd have an intensity of 0.8" CreationDate="2018-03-03T09:26:29.957" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9753" PostId="6357" Score="0" Text="If you want high precision time in the win32 API you can use QueryPerformanceCounter and QueryPerformanceFrequency" CreationDate="2018-03-03T11:09:18.030" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9754" PostId="6350" Score="0" Text="It increases because I am using 1/(distance*distance) as falloff, so 1/(0.5*0.5) = 4" CreationDate="2018-03-03T13:43:15.103" UserId="8305" ContentLicense="CC BY-SA 3.0" />
  <row Id="9756" PostId="6353" Score="0" Text="&quot;to get the right attenuation you need to track the total distance back to the light source and square that&quot;  &#xA;&#xA;Why is that? What is the difference between a light source with intensity 1 at distance 2 and a light source with intensity 0.25 at distance 1?  &#xA;&#xA;The +1 constant does get more reasonable results, but it still doesn't seem physically correct.  &#xA;&#xA;I'll look at tracking individual rays/distances, but I was hoping to aggregate light at each point in the grid, so I don't &quot;know&quot; the original distance." CreationDate="2018-03-03T14:06:30.700" UserId="8305" ContentLicense="CC BY-SA 3.0" />
  <row Id="9757" PostId="6353" Score="1" Text="Because the distance to the original light is what matters for intensity. The ratios of the squares change as you move away from the light. Let's say you have a grid size of 2 - when moving from distance 2 to distance 4, the light should get 4x dimmer, but when moving from distance 10 to 12 it will only get 1.44x dimmer." CreationDate="2018-03-03T14:28:53.433" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9759" PostId="6357" Score="0" Text="Is it possible to do this using frame rate instead of time?" CreationDate="2018-03-04T01:23:39.743" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="9760" PostId="6357" Score="0" Text="If you know the frame rate and know how many frames you've already processed, then you can calculate the time by multiplying the frame rate by the number of frames." CreationDate="2018-03-04T01:58:05.843" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9761" PostId="5681" Score="1" Text="You can make your pool allocate groups of objects, say 64 objects at a time when requesting an object, by linking the first object in that block to your 'used list' and the other 63 to your 'free list' to avoid hammering the memory allocator." CreationDate="2018-03-04T06:16:54.823" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9762" PostId="6357" Score="0" Text="1/FPS = time taken per frame, so yes you can use that. However to calculate FPS you would have needed to take a snapshot of some timer anyway ?" CreationDate="2018-03-04T06:19:32.640" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9765" PostId="6350" Score="0" Text="Are you like... propagating your light one grid cell at a time, without knowing where it originally came from? That's a bit what it looks like but I'm uncertain I understand what you're trying to do." CreationDate="2018-03-04T15:13:23.077" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="9767" PostId="6350" Score="0" Text="Yep. Each cell stores its light output in various directions. A shader runs for each grid cell, and is responsible for taking light from neighboring cells, casting it through the current cell (reflecting it if it hits something, in this case it doesn't), and writing the result to the current cell. So I always know the position and approx direction of the light, but I don't know where it came from (because all light in a particular direction is combined)" CreationDate="2018-03-04T16:21:09.353" UserId="8305" ContentLicense="CC BY-SA 3.0" />
  <row Id="9769" PostId="1746" Score="0" Text="xyY is indeed linear, normalized." CreationDate="2018-03-04T18:38:52.920" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="9772" PostId="1746" Score="0" Text="@troy_s Its energetically linear, but its not linear in percieved color distance. Its just really hard to make a space that is uniform in perceptual distance between 2 points." CreationDate="2018-03-04T20:51:57.023" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9773" PostId="5520" Score="0" Text="I would be suspicious of this line&gt; Vector samplePositionLight = samplePosition + sunDirection * (tCurrentLight + segmentLengthLight * rand.NextDouble());     &gt;&gt; This should walk the sample-to-light in even steps, why is it doing a random length walk ?" CreationDate="2018-03-05T08:06:14.887" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9774" PostId="5520" Score="0" Text="I've implemented this algorithm before and also found it unnecessary to generate multiple rays for each 'pixel'. A single ray, which raymarches from the view direction, and for each of those steps marches towards the light was enough to get high quality results. Have you tried just one call to ComputeIncidentLight per pixel? (Also see my previous post, you have modified the sample-&gt;light ray march in a way that may have broken it)" CreationDate="2018-03-05T08:55:51.550" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9775" PostId="5520" Score="0" Text="@PaulHK, well even with one sample ppx you still need 16 x 8 samples for marching algorithm. You may think this isn't costly but if you do, then you may use Monte Carlo method instead of marching the along the ray in steps." CreationDate="2018-03-05T12:16:17.097" UserId="6041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9776" PostId="6348" Score="0" Text="I haven't seen any library which provides this sort of functionality and is GPU-accelerated as it'd be quite cumbersome. The only things I've seen close are CUDA's libraries for math, linear algebra etc.&#xA;&#xA;You can either code your data structure and routines manually which again would be pita OR instead of going for the GPU why not go towards multi-threading on the cpu. Check out OpenMP and for ray-intersection with polyhedra you could construct a spatial data structure if there are many of them." CreationDate="2018-03-05T16:48:42.700" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9777" PostId="6314" Score="0" Text="what Paul said, I don't see the code anywhere for shadows, its just for intersections. How do you compute your shadows?" CreationDate="2018-03-05T16:58:05.927" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9778" PostId="6357" Score="0" Text="Thanks. I think the question I am really asking is the following: What I want to do is make sure the speed of my motion is the same regardless of whether I oscillate between 0 and 1 or between 0 and 10. In the response, user1118321 says &quot;You can make it go from 0 to 10 by multiplying the result by 10:..&quot;, but this seems to increase the speed of my motion. How do I maintain speed?" CreationDate="2018-03-05T20:10:04.313" UserId="5183" ContentLicense="CC BY-SA 3.0" />
  <row Id="9779" PostId="1746" Score="0" Text="Perceptually uniform is a far better term than &quot;linear&quot;. There is already enough stupidity around that term." CreationDate="2018-03-05T21:51:07.897" UserId="5556" ContentLicense="CC BY-SA 3.0" />
  <row Id="9780" PostId="6370" Score="1" Text="you should check wikipedia for converting from Axis-angle to euler angle rotations." CreationDate="2018-03-05T22:26:45.147" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9781" PostId="6360" Score="1" Text="Never heard of anything like that. And it doesn't make sense, `glTex[ture]Storage` allocates, `glTex[ture]SubImageXD` copies. Those are two entirely orthogonal concepts which modern GL makes a big deal about explicitly separating. I'm pretty sure they just mean the latter there. However, with the SuperBible I also don't really know if they might not have added their own custom functions (even if it would be odd to name them like genuine GL calls)." CreationDate="2018-03-06T01:53:42.637" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9782" PostId="5520" Score="0" Text="But stepping by rand.NextDouble() in numLightSamples steps means you don't end up at isectSun.Dist as the original code does, this i think is the cause of your problem." CreationDate="2018-03-06T02:08:50.527" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9783" PostId="6357" Score="1" Text="When you multiply the rotation by 10 you're changing the amplitude of the rotation, but not the frequency. So it will rotate 10 degrees per second instead of 1 degree per second. If you want to also change the frequency to match, you should divide time by 10. Then you'll get 10 degrees per 10 seconds, which is still 1 degree per second." CreationDate="2018-03-06T02:28:53.340" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9784" PostId="5520" Score="0" Text="Yes 16x8 samples/pix is heavy, you can reduce this a little and get away with good results (usually lower samples makes the atmosphere shadow area more coarse). You could also look into rendering the atmosphere at a lower resolution and upscale" CreationDate="2018-03-06T03:56:59.813" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9785" PostId="1746" Score="0" Text="@troy_s Right, good name for it, changed. I was actually sitting here after answering and thinking asking a question on mathematics ehat would be the minimum requirement for linear. So as to check would linear ever eeven qualify for color." CreationDate="2018-03-06T05:38:31.507" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9787" PostId="6373" Score="0" Text="Stereographic projection implies projecting the surface of a sphere onto a plane. So unless your model is also spherical you're gonna get self-intersections on the plane. Is this what you want?" CreationDate="2018-03-06T10:53:43.707" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9789" PostId="6374" Score="1" Text="SNORMs and UNORMs are signed/unsigned *integers* divided by (2^M-1)" CreationDate="2018-03-06T13:48:32.523" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9790" PostId="6374" Score="0" Text="oh, I see. So all bits go directly into encoding the variable, rather than splitting it into exponent and mantissa, thus the (slightly) better precision?" CreationDate="2018-03-06T13:49:53.247" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9791" PostId="6374" Score="2" Text="Well, all the bits are used in floats too :-)   Perhaps a better description is that range is more limited and that the precision throughout the range is uniform, rather than, as with floats, variable.  FWIW colour channel data, e.g,. RGB888, uses 8-bit unorms." CreationDate="2018-03-06T13:54:06.593" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9792" PostId="6135" Score="0" Text="Please, do not add another question to this post. If there is anything else you need help with, then post another question." CreationDate="2018-03-06T15:49:42.347" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9793" PostId="6373" Score="0" Text="@russ It is nearly spherical (a polyhedron)." CreationDate="2018-03-06T16:53:36.207" UserId="8325" ContentLicense="CC BY-SA 3.0" />
  <row Id="9794" PostId="6375" Score="1" Text="Nice answer but the part about 3 vs 5 digits of precision is misleading. It makes it sound as if a normalized integer is always better. Floating point has uniform relative precision while a normalized integer has uniform absolute precision. It just happens that the latter is what you want for model vertices." CreationDate="2018-03-06T18:48:54.933" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="9795" PostId="6375" Score="0" Text="I don't need really large position values, this was probably badly stated on my part. What I meant was models at &quot;really large [i.e. far away from origin] positions in the world&quot;. Imagine a car simulator that spans more than 500km * 500km levels. For some reason I missinterpreted one of the articles sentence to say something about this. In any case, thanks for the answer, I'll edit the &quot;disservice&quot; question to mark it for future readers." CreationDate="2018-03-07T06:47:02.287" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9797" PostId="6348" Score="0" Text="FWIW GPUs excel at computing results in image space. Do you need the actual geometric data of, say, union/intersection/difference or just *images* of the results of these operations?" CreationDate="2018-03-07T11:53:37.530" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9798" PostId="6373" Score="0" Text="Do you need an actual stereographic projection? Or just any conformal one-to-one projection to the plane would also work. There are better approaches to map surfaces (or any topological disk) to a plane. You acn even map a topological sphere (closed surface) to the plane without seams decomposing it in two charts. Closed surface of high genus can be also mapped but would require more charts." CreationDate="2018-03-08T01:17:29.530" UserId="7957" ContentLicense="CC BY-SA 3.0" />
  <row Id="9799" PostId="6373" Score="0" Text="@MauricioCeleLopezBelon I wanted sterographic, but that works too." CreationDate="2018-03-08T01:18:27.273" UserId="8325" ContentLicense="CC BY-SA 3.0" />
  <row Id="9802" PostId="6383" Score="0" Text="Are you sure scene.objects[k].vertices.size() is the same size as the number of indices?" CreationDate="2018-03-08T11:09:24.993" UserId="7724" ContentLicense="CC BY-SA 3.0" />
  <row Id="9803" PostId="6348" Score="1" Text="@SimonF Indeed I would need the results of the operation. Image data (i.e. the 2D-Projection) is usually not needed." CreationDate="2018-03-08T12:02:51.987" UserId="8297" ContentLicense="CC BY-SA 3.0" />
  <row Id="9804" PostId="6383" Score="0" Text="@Reynolds No, it doesn't. But I thought the whole point of having indices is that you dont need to duplicate vertices? Because one vertices could belong to many triangles" CreationDate="2018-03-08T13:51:14.133" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9805" PostId="6383" Score="0" Text="The second arguments is the number of indices you want to read from your array indices. So it should be indices.size()." CreationDate="2018-03-08T14:28:44.317" UserId="7724" ContentLicense="CC BY-SA 3.0" />
  <row Id="9806" PostId="6383" Score="0" Text="@Reynolds Oh my, thank you very much. I find openGL documentation very confusing." CreationDate="2018-03-08T14:33:57.473" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9807" PostId="6386" Score="0" Text="What are you planning to use the tangent plane for? Texturing?" CreationDate="2018-03-08T20:53:07.643" UserId="506" ContentLicense="CC BY-SA 3.0" />
  <row Id="9808" PostId="6386" Score="0" Text="Does [this Mathematics Stack Exchange answer](https://math.stackexchange.com/questions/752252/find-plane-by-normal-and-instance-point-distance-between-origin-and-plane) provide enough information?" CreationDate="2018-03-09T03:12:08.377" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9809" PostId="6387" Score="0" Text="It's speculation but [this reddit thread](https://www.reddit.com/r/pcgaming/comments/3xgmz9/vulkan_api_specification_is_complete_and/) suggests that it's perhaps &quot;graphics rendering&quot;." CreationDate="2018-03-09T03:29:04.543" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9810" PostId="6386" Score="0" Text="@Noah Witherspoon, i will embed an image into this tangent plane. (So it will look like so natural)" CreationDate="2018-03-09T04:39:59.687" UserId="8130" ContentLicense="CC BY-SA 3.0" />
  <row Id="9811" PostId="6386" Score="0" Text="@user1118321, i will look and test that question immediately. I hope it will work. I will feedback about it. Thanks" CreationDate="2018-03-09T04:41:59.217" UserId="8130" ContentLicense="CC BY-SA 3.0" />
  <row Id="9813" PostId="6387" Score="0" Text="This is purely speculative, but another low-level, explicit, IHV-specific used the &quot;gr&quot; prefix for its function names: [GLIDE](http://www.gamers.org/dEngine/xf3D/glide/glidepgm.htm)." CreationDate="2018-03-09T07:55:29.940" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9822" PostId="6348" Score="0" Text="I think you can accelerate your algorithms more easily using heterogeneous computing, meaning using a mix of CPUs and GPUs. For that you have several technologies available: C++ AMP, AMD HCC, C++ SYCL, OpenCL" CreationDate="2018-03-10T11:37:19.470" UserId="7957" ContentLicense="CC BY-SA 3.0" />
  <row Id="9826" PostId="6388" Score="0" Text="I think the OP wants arbitrary regular polygons though, like in the Desmos they linked, not just circles." CreationDate="2018-03-11T02:14:35.797" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9828" PostId="6395" Score="2" Text="&quot;*I don't want to render the left and right scene into one texture*&quot; &quot;*I don't want to use geometry shader and layered rendering*&quot; This is called &quot;defining yourself into a corner&quot;: you've imposed arbitrary limitations on yourself, such that there is no valid solution. Remove one of these restrictions, and you can do what you need." CreationDate="2018-03-11T15:16:18.537" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9831" PostId="6395" Score="0" Text="can i use &quot;glViewportArrayv(ARB_shader_viewport_layer_array)&quot; to reach the goal?" CreationDate="2018-03-12T00:21:09.797" UserId="5048" ContentLicense="CC BY-SA 3.0" />
  <row Id="9832" PostId="6396" Score="0" Text="can i use &quot;glViewportArrayv(ARB_shader_viewport_layer_array)&quot; to reach the goal?" CreationDate="2018-03-12T00:21:16.290" UserId="5048" ContentLicense="CC BY-SA 3.0" />
  <row Id="9833" PostId="6395" Score="0" Text="@NicolBolas in the question [Multiple viewports with modern OpenGL?](https://computergraphics.stackexchange.com/questions/4513/multiple-viewports-with-modern-opengl),&#xA;i have read  answer about similar situation, only the difference is the output. you and Christian_B's answer is give me some hope" CreationDate="2018-03-12T00:26:26.940" UserId="5048" ContentLicense="CC BY-SA 3.0" />
  <row Id="9834" PostId="6395" Score="1" Text="&quot;*can i use &quot;glViewportArrayv(ARB_shader_viewport_layer_array)&quot; to reach the goal?*&quot; You specifically said you didn't want to use layered rendering. And viewport arrays are all about doing layered rendering, since each viewport in the array is hooked to the corresponding array layer in the framebuffer. So either you're willing to use layered rendering or you can't use `glViewportArray` or ARB_shader_viewport_layer_array." CreationDate="2018-03-12T01:36:10.267" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9836" PostId="6395" Score="0" Text="@NicolBolas maybe my oringnal description is not proper, i can use layered rendering but not want to use geometry shader. Afaik, geometry shader and layered rendering(gl_layer) are bounded. but the new feature ARB_shader_viewport_layer_array may change this situation. i will edit my original description. Then can i make my goal achievement? and how in detail?" CreationDate="2018-03-12T04:51:23.853" UserId="5048" ContentLicense="CC BY-SA 3.0" />
  <row Id="9838" PostId="6388" Score="0" Text="Instead of using sqrt I think `max(sin(x),0)` is better" CreationDate="2018-03-12T07:01:08.823" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9839" PostId="6388" Score="0" Text="@NathanReed that's right I need regular polygons not just circles" CreationDate="2018-03-12T07:39:01.307" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9841" PostId="6393" Score="0" Text="I've not used a Kaiser windowed sinc, but I did find with (some) other windowed sincs that you can get some very strange behaviour once you'd normalised all the non-zero sample weights. In particular, if you plot the weight applied to the &quot;centre&quot; sample as the centre of the window moved from being located on the centre weight to being just off it, the magnitude would increase (rather than decrease as you'd expect with a sinc)." CreationDate="2018-03-12T12:04:01.887" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9842" PostId="6399" Score="1" Text="Hell, I sometimes tend to not even check the uniform locations and just let `glUniform` silently abort on -1. ;-)" CreationDate="2018-03-12T15:07:48.133" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9845" PostId="4994" Score="0" Text="&gt;throughput = throughput * material-&gt;Eval(wi, wo, normal) / pdf;&#xA;&#xA;Should this line be `throughput + material-&gt;...` ?" CreationDate="2018-03-13T08:00:17.400" UserId="6407" ContentLicense="CC BY-SA 3.0" />
  <row Id="9848" PostId="6385" Score="0" Text="I'm not familiar with graphtoy, but I think I can explain how to do this in terms of raw math - would that help?" CreationDate="2018-03-13T13:54:40.200" UserId="5349" ContentLicense="CC BY-SA 3.0" />
  <row Id="9849" PostId="6385" Score="0" Text="@Pikalek Anyway, I'd appreciate if you answer my question" CreationDate="2018-03-13T14:33:49.753" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9851" PostId="4994" Score="0" Text="No. Multiplying is correct. The throughput tracks what kinds of surfaces you've already bounced off of. IE, the allowable color of the bounce lighting. For example, if we have pure white light, and bounce off a pure red surface, any remaining bounce lighting will only be red. Because all green and blue light is absorbed by the red surface. Put another way, if we shine pure blue light at a pure red surface, it will be completely black. (This assumes the simplified model of RGB light, where each channel is independent)" CreationDate="2018-03-13T15:31:49.263" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="9852" PostId="1728" Score="0" Text="@BenediktBitterli What? Isn't everything done through the fragment shader?  He's got some of this stuff on shadertoy, you can't make a &quot;low poly version&quot; of this because you don't have access to the fragment shader or any geometry in the first place..." CreationDate="2018-03-13T15:52:41.167" UserId="6530" ContentLicense="CC BY-SA 3.0" />
  <row Id="9853" PostId="6256" Score="0" Text="you need to keep in mind that doubles on GPUs, unlike on CPUs, actually significantly tank performance unless you are using compute cards like quadros or Teslas.   On the Nvidia side, up until recently you had a 1:32 ration between fp throughput and double precision throughput.  compute 7.0 I believe that ratio is down to 1:2, but I wouldn't count on that getting any better any time soon, as Quadros are sold based on their inclusion of double to float precision tradeoff, and teslas have equal performance on both, whilst not having really any other advantages over consumer gaming GPUs." CreationDate="2018-03-13T16:09:36.193" UserId="6530" ContentLicense="CC BY-SA 3.0" />
  <row Id="9854" PostId="6406" Score="0" Text="You're looking for circular holes (a two-dimensional thing) in the boundary of a solid?  Are you looking for where the boundary has a hole?  Or are you looking for topological handles, like a torus?" CreationDate="2018-03-14T08:14:23.213" UserId="7647" ContentLicense="CC BY-SA 3.0" />
  <row Id="9856" PostId="6412" Score="0" Text="I want to use OpenCL mainly for reasons of familiarity and portability." CreationDate="2018-03-15T09:55:17.970" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9857" PostId="6412" Score="2" Text="Familiarity, fair enough. As for portability, you can run OpenGL anywhere you can run OpenCL (although Apple's version is pretty badly out of date). Still worth learning GLSL though, it's pretty easy if you already know any of the C-variant languages." CreationDate="2018-03-15T10:03:54.343" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9858" PostId="6412" Score="0" Text="By portability I meant portable to DirectX.&#xA;&#xA;I certainly don't want to do my own rasterisation! Your answer is very elucidating and I think I'm beginning to understand what OpenCL should and shouldn't be used for in a graphics pipeline.&#xA;&#xA;One thing I would like to do is manipulate textures and potentially generate textures before they are used in the pipeline - am I right in thinking this would be something OpenCL would be suitable for? It seems I can use a shared GL texture/surface in OpenCL and write to it, and don't see any reason this wouldn't be possible." CreationDate="2018-03-15T10:05:26.770" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9859" PostId="6412" Score="0" Text="Yeah, for asset creation and manipulation it's fine. I've even seen a paper where they were doing Marching Cubes in OpenCL to generate a vertex buffer each frame then rendering it with OpenGL - but the cost of syncing between APIs was eating a significant chunk of frame time." CreationDate="2018-03-15T10:59:36.140" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9860" PostId="6414" Score="0" Text="If that's what you want, rasterization is *not* going to get you there. Or at least, not rasterization as most GPUs implement it." CreationDate="2018-03-16T00:25:46.217" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="9861" PostId="6414" Score="0" Text="I suspect you can modify the geometry based on the resolution of the current grid to achieve proper rasterization i.e be waare the saterizer works in 2D and modufy the geometry acoordingly." CreationDate="2018-03-16T01:08:46.490" UserId="7462" ContentLicense="CC BY-SA 3.0" />
  <row Id="9862" PostId="6415" Score="0" Text="Are you sure you're specifiying your angle in degrees? Modern versions of GLM use radians everywhere and the docs are not always clear on this." CreationDate="2018-03-16T04:57:39.493" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9863" PostId="6415" Score="0" Text="@russ so GLM uses radians? I thought it uses degree. I will test with radians and update the result if it indeed fixes the problem" CreationDate="2018-03-16T05:08:38.003" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9864" PostId="6415" Score="0" Text="Depends on version. Older versions were inconsistent, you could define GLM_FORCE_RADIANS before including it to make it use radians everywhere. Since version 0.9.6 this has become the default. From the doc - &quot;Finally, here is a list of all the functions that could use degrees in GLM 0.9.5.4 that requires radians in GLM 0.9.6: rotate (matrices and quaternions), perspective, perspectiveFov, infinitePerspective, tweakedInfinitePerspective, roll, pitch, yaw, angle, angleAxis, polar, euclidean, rotateNormalizedAxis, rotateX, rotateY, rotateZ and orientedAngle.&quot;" CreationDate="2018-03-16T05:30:20.623" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9865" PostId="6415" Score="0" Text="@russ You are indeed correct. Thanks very much. Please create an answer so that I can accept it." CreationDate="2018-03-16T12:10:47.117" UserId="8191" ContentLicense="CC BY-SA 3.0" />
  <row Id="9866" PostId="6347" Score="0" Text="I am afraid this answer is actually incorrect. The rotations are around the origin while the line may not cross the origin. You first need to translate so that the line will pass through the origin and of course translate back at the end." CreationDate="2018-03-16T13:09:59.510" UserId="5079" ContentLicense="CC BY-SA 3.0" />
  <row Id="9867" PostId="6347" Score="0" Text="True but I think we don't need it anyways. Because we are only interested in the slope or the angle the line makes with the axis. So translation doesn't affect the angle. So for example we have a triangle and we want to flip around line `y=x+1` Well that's the same line as `y=x` the only difference being the second one passes through the origin. In both the cases we rotate around by the same angle i.e 45 degrees" CreationDate="2018-03-16T15:21:00.593" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9871" PostId="6422" Score="0" Text="What's your question? It's unlikely that anyone else can debug your problem for you without a MCVE." CreationDate="2018-03-17T14:13:45.550" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9872" PostId="6422" Score="0" Text="@DanHulme I would like to know if the way im applying the transformation to the ray, as well as the hitpoint/normal is correct." CreationDate="2018-03-17T14:21:16.210" UserId="8394" ContentLicense="CC BY-SA 3.0" />
  <row Id="9873" PostId="6365" Score="0" Text="It's a little unclear what `topLeft` and the computation for it actually *is* and in what context it is used. That matrix you show is certainly correct. But we don't know what `topLeft` is *supposed* to be. Are you sure it isn't intentionally doing the *reverse* of the transformation?" CreationDate="2018-03-17T14:46:54.870" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9874" PostId="6422" Score="1" Text="Probably not the only problem here but your normal transform is wrong: https://computergraphics.stackexchange.com/questions/1502" CreationDate="2018-03-17T15:46:52.573" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="9875" PostId="6422" Score="0" Text="I tried using the transpose of the inverse for the normal, but it created black areas where it shouldn't, and the refraction was inverted. Since the normal is computed from sphere center to hitpoint, I also tried to compute it after transforming the hitpoint and  sphere center to world space, so I could avoid the normal transformation, but that didn't work either" CreationDate="2018-03-17T16:12:41.600" UserId="8394" ContentLicense="CC BY-SA 3.0" />
  <row Id="9876" PostId="6422" Score="0" Text="Nevermind, I just figure it out. It was a problem related to the camera updating when moving, which wasnt computing the Up vector properly, and was causing the ray to be perturbed" CreationDate="2018-03-17T16:18:54.963" UserId="8394" ContentLicense="CC BY-SA 3.0" />
  <row Id="9877" PostId="6347" Score="1" Text="You do need it. Let's take an extreme example: line equation x = 2 and point P at {3, 0}. obviously the mirrored point is at {1, 0}. If we try it with your method, the series of operations will yield {0, 3}, then {0, -3}, finally {-3, 0} which is way different than the correct result." CreationDate="2018-03-17T17:56:36.500" UserId="5079" ContentLicense="CC BY-SA 3.0" />
  <row Id="9878" PostId="6347" Score="0" Text="True. thanks for pointing out I'll add it to the answer." CreationDate="2018-03-18T06:50:22.820" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9879" PostId="6422" Score="0" Text="Non-uniformly scaled spheres are ellipsoids. It's pretty easy to write down the equation of an ellipsoid and intersect it with a ray (just solving a quadratic). With that approach, you avoid all the confusing transformation stuff." CreationDate="2018-03-18T10:25:16.050" UserId="6968" ContentLicense="CC BY-SA 3.0" />
  <row Id="9880" PostId="6419" Score="0" Text="It turns out a median filter is a really simple way to get pretty decent denoising." CreationDate="2018-03-18T18:56:58.417" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9881" PostId="2167" Score="0" Text="Perhaps I'm misunderstanding some part of this. Wouldn't you want to find the maximum absolute distance in your texture and use that as the constant?" CreationDate="2018-03-19T02:18:29.380" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9882" PostId="6115" Score="4" Text="Couldn't you simply try it out and see which is fastest for your case?" CreationDate="2018-03-19T02:20:17.587" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9883" PostId="6365" Score="0" Text="is it an inverse mapping?" CreationDate="2018-03-19T13:08:22.667" UserId="8317" ContentLicense="CC BY-SA 3.0" />
  <row Id="9884" PostId="7427" Score="0" Text="I implemented it but I'm not sure that is correct. However thanks!" CreationDate="2018-03-20T16:21:36.823" UserId="4981" ContentLicense="CC BY-SA 3.0" />
  <row Id="9885" PostId="7429" Score="2" Text="This is a common pattern. You can see a more elaborate version of it in this presentation: https://www.gdcvault.com/play/1022186/Parallelizing-the-Naughty-Dog-Engine" CreationDate="2018-03-20T20:51:47.607" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="9886" PostId="7427" Score="0" Text="Ok, the code now seems work. Practically I implemented a sort of skinning algorithm following the suggest." CreationDate="2018-03-20T20:52:01.010" UserId="4981" ContentLicense="CC BY-SA 3.0" />
  <row Id="9887" PostId="7438" Score="0" Text="Could you specify if your line is font (text) rendering or something you draw yourself some other way?" CreationDate="2018-03-23T19:47:06.040" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="9888" PostId="7436" Score="0" Text="&gt; It does not relate to low level libraries., &gt; So in fact processing is often lower level stuff than what many people would do day to day. --- So does it relate to low-level libraries or not?" CreationDate="2018-03-24T00:21:27.467" UserId="8434" ContentLicense="CC BY-SA 3.0" />
  <row Id="9889" PostId="7428" Score="0" Text="Is it possible that the drawing has not completed yet? I've had this happen when I've forgotten to do a `glFlush()` in OpenGL or the equivalent in Metal (for example a completion handler on the command buffer)." CreationDate="2018-03-24T03:49:03.597" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9891" PostId="7436" Score="0" Text="@lakesare No it does not. Lower, does not mean low. In fact low level graphics libraries are rare, mainly because modern systems really dont want you poking in the fabric of the OS. So even something as fundamental as OpenGL is a high level library youd need to go into something like Vulkan to go low enough to be considered low level. But without this kind if high level abstraction it would be impossible to be compatible across devices." CreationDate="2018-03-24T08:50:19.200" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="9892" PostId="7438" Score="0" Text="@Olivier Both line and text actually rendered using OpenGL, though the effect is most appearant for lines aligned to pixel grid in the direction of the striped pixel geometry." CreationDate="2018-03-24T09:32:47.093" UserId="3041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9893" PostId="1983" Score="0" Text="@Pikalek Sorry for it took me almost a year to reply. Remember what is at issue: I want to put this formula into the Wikipedia article about the shape of the Earth's horizon. But there are editors who won't let me do this until I find a *reputable source* saying that this is indeed the correct formula for the horizon. It is not about whether the formula is *correct*. What you post on Wikipedia should of course be correct, but that is not enough. It should also be 'attributable to a reliable, published source'. Otherwise it is 'original research', which cannot be posted on Wikipedia." CreationDate="2018-03-25T18:18:21.953" UserId="2574" ContentLicense="CC BY-SA 3.0" />
  <row Id="9894" PostId="5351" Score="0" Text="Thank you for the answer! I will try to locate these sources.&#xA;&#xA;As far as your statements that this is not really original research or even about the Earth: to see why *as far as Wikipedia goes* both of these are disputable, see [here](https://en.wikipedia.org/wiki/Wikipedia:No_original_research#Synthesis_of_published_material) and also e.g. [here](https://en.wikipedia.org/wiki/Wikipedia:No_original_research#Routine_calculations). Many Wikipedia editors will ultimately agree with you, but some will create problems. The easiest way to deal with the latter ones is to show an appropriate source." CreationDate="2018-03-25T18:47:14.927" UserId="2574" ContentLicense="CC BY-SA 3.0" />
  <row Id="9895" PostId="7445" Score="0" Text="I would remove the phrase &quot;must be&quot; and replace it with &quot;is usually&quot;. There are other systems such as Metal Compute and I would assume some equivalent Direct X mechanism. I believe that Vulkan also has a compute engine." CreationDate="2018-03-26T01:58:35.073" UserId="3003" ContentLicense="CC BY-SA 3.0" />
  <row Id="9896" PostId="7442" Score="0" Text="It won't be as hard as the Poincare conjecture, since that's a question about homeomorphism type assuming already a certain homotopy type. I would love to see an implementation of the Ricci flow that I can watch by the way. Do you know of one?" CreationDate="2018-03-26T02:20:52.950" UserId="8451" ContentLicense="CC BY-SA 3.0" />
  <row Id="9897" PostId="7442" Score="0" Text="I may have overstepped my areas of knowledge. I was simply wagering a known minimum point to cover his solution, there may be better points also considering it is polygon mesh. Also we don't know the performance requirements. There may be some approximations which work for real time processing, or more than likely the result is not time dependent and the holes can be stored before run time or included with the models. I'll try to look further into this. Most likely there is probably a best solution available in academics for 3d graphics." CreationDate="2018-03-26T06:32:56.067" UserId="8444" ContentLicense="CC BY-SA 3.0" />
  <row Id="9898" PostId="7445" Score="0" Text="That's true but computer shaders use the GPU in graphics mode, the question was about General Purpose, where the GPU is used in computing mode. They are two different pipelines and depending on the purpose of the application, one or the other will normally be used, switching between them is &quot;expensive&quot; computationally. Good point anyway ;)" CreationDate="2018-03-26T20:15:29.240" UserId="8450" ContentLicense="CC BY-SA 3.0" />
  <row Id="9900" PostId="6399" Score="1" Text="As with most things CG (and most things programming, even), it is probably a tradeoff, and should be evaluated on a case-by-case basis. Run some benchmarks. If the memory used by your cache is limiting something else in your program then it might become an issue, you will have to think very hard to decide if the extra speed is worth the memory overhead" CreationDate="2018-03-27T14:03:58.747" UserId="5703" ContentLicense="CC BY-SA 3.0" />
  <row Id="9901" PostId="53" Score="0" Text="I've come up with an accurate way to predict rasters without a scan line query, see answer below." CreationDate="2018-03-27T14:24:27.237" UserId="8460" ContentLicense="CC BY-SA 3.0" />
  <row Id="9902" PostId="7449" Score="2" Text="what do you need reflected? uniform and attrib locations?" CreationDate="2018-03-27T15:36:34.000" UserId="137" ContentLicense="CC BY-SA 3.0" />
  <row Id="9903" PostId="7449" Score="0" Text="Yes, I think I'm only looking for names + bindings (also from inside UBOs) also uniform types (UBOs,SSBOs,images etc)" CreationDate="2018-03-27T17:02:35.217" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9904" PostId="7450" Score="0" Text="Your steps 3-5 are already a kind of tone mapping operator (i.e. a map from HDR to LDR values). Interchange 3-5 with 2 and it should fix the problem. Later you can replace your naive &quot;clamping&quot; tone map with something else, like Reinhard, if you want." CreationDate="2018-03-28T03:44:51.540" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9905" PostId="7452" Score="0" Text="For most of our shaders, we use fixed slots but in some cases we need to retrieve uniform bindings (eg. I'm generating UBOs for bindless textures)." CreationDate="2018-03-28T09:31:28.850" UserId="7107" ContentLicense="CC BY-SA 3.0" />
  <row Id="9906" PostId="7453" Score="0" Text="What sort of differences?" CreationDate="2018-03-28T13:50:11.653" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="9907" PostId="7455" Score="0" Text="When you say &quot;sub-texel shading&quot; are you really asking &quot;can I do texture super-sampling&quot;? i.e. sampling the texture at a rate higher than once per screen pixel?&#xA;&#xA;FWIW, anisotropic filtering is generally implemented in this sort of fashion whereby, depending on the ratio of anisotropy, additional texture samples are taken and then blended together.&#xA;&#xA;Alternatively, are you asking if super-sample antialiasing is available in HW?" CreationDate="2018-03-28T15:04:03.203" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9908" PostId="7455" Score="0" Text="It sounds to me like you're actually trying to make your own texture sampler, so you can use other sampling techniques. Is that right?" CreationDate="2018-03-28T15:29:33.650" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9909" PostId="7450" Score="1" Text="I've heard folks say that at 2 you should tone map the samples, combine them, reverse tone map the result, then continue the process at step 3.  This link is relevant:&#xA;https://gpuopen.com/optimized-reversible-tonemapper-for-resolve/" CreationDate="2018-03-28T15:53:52.723" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9910" PostId="7453" Score="0" Text="Usually the differences were like little outlines around objects, like if anti aliasing were on one but off on the other. But we turned anti aliasing off always when running the tests. And it didn't always happen, just very rarely with certain objects." CreationDate="2018-03-28T16:08:40.193" UserId="3332" ContentLicense="CC BY-SA 3.0" />
  <row Id="9911" PostId="7450" Score="1" Text="Combining tonemapped samples and then reversing the tonemap is also pretty commonly used in temporal AA, I think. It _is_ a hack, but it may be necessary for small sample counts like 4 spp with box filtering. I suspect with larger sample count and a nicer filter (e.g. bicubic with 1.5–2px radius), the problem would solve itself the “right” way." CreationDate="2018-03-28T16:14:01.930" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9913" PostId="7455" Score="0" Text="Dan, essentially yes. Though the interpolation would potentially use more data than just the texture being sampled (pre-computed edge map, maybe another high res texture overlayed)." CreationDate="2018-03-28T17:08:57.597" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9914" PostId="7455" Score="0" Text="Simon, no, I guess I'm asking if super-sampling is available - overridable - in gpu SW (i.e. fragment shader - or compute shader if the compute shader could be fitted seamlessly into the pipeline)." CreationDate="2018-03-28T17:19:18.077" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9915" PostId="7455" Score="0" Text="FWIW ... in Half Life 2, I noticed what appeared to be something like this, used everywhere. When you get close to a texture, the base texture looks very blurry, but a much higher resolution texture is overlaid. At least, this is certainly what it looks like is happening to me. Guess it's one reason why the 14 year old game still looks kind of crisp today!" CreationDate="2018-03-28T17:26:04.560" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9916" PostId="7450" Score="0" Text="I agree with @NathanReed. If your filter radius is only 1 pixel and a small sample count, the high values are going to blow away any samples with lower values. Especially in HDR." CreationDate="2018-03-28T17:37:07.263" UserId="310" ContentLicense="CC BY-SA 3.0" />
  <row Id="9917" PostId="7458" Score="0" Text="Thanks, a very useful answer. As for the performance of doing the interpolation in SW rather than hardware, I'm not *too* concerned, as it's only the very close up textures which would be affected, and because I would do any complex calculations offline and bake them into a map." CreationDate="2018-03-28T17:37:07.523" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9918" PostId="7450" Score="0" Text="You're pretty much stuck with hacks. In situations where a single sample is bright enough to push the pixel filter past the display range, you're only option is to clamp or tone map the samples before filtering.&#xA;&#xA;Tone mapping before filtering will remove energy, which may not be a problem in this particular scene, but will become important with 'fuzzy' features like depth of field, glossy reflections, motion blur, etc." CreationDate="2018-03-29T11:22:52.240" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="9923" PostId="7455" Score="0" Text="Not sure if its exactly what you're looking for, but Quilez has an interesting technique of separating the tex coord into int and fractional parts, and manipulating the fractional part yourself before you sample the texture. http://www.iquilezles.org/www/articles/texture/texture.htm" CreationDate="2018-03-30T01:47:09.960" UserId="1937" ContentLicense="CC BY-SA 3.0" />
  <row Id="9924" PostId="3969" Score="0" Text="[Here](https://aty.sdsu.edu/explain/optics/rendering.html) I found some info about this question" CreationDate="2018-03-29T12:09:13.693" UserId="8469" ContentLicense="CC BY-SA 3.0" />
  <row Id="9925" PostId="7464" Score="0" Text="two things that bother me with your matrix calculations: 1. you rotate your `projectionMatrix` around the camera angle - but you also use `LookAt` for your view matrix, which would include the `_cameraAngleDeg`, wouldn't it? in fact, I don't see why you want to rotate your `projectionMatrix` at all, in theory your viewing position and direction of looking is set up with the `viewMatrix`. The second thing is the order of transformations in your `worldMatrix`. You should probably scale before you rotate an object." CreationDate="2018-04-03T06:20:31.843" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9927" PostId="7460" Score="1" Text="In the distribution term, you should take care that whenever cos(theta) is less or equal to 0, the distribution term should return 0 (Xi operator) http://www.graphics.cornell.edu/~bjw/microfacetbsdf.pdf" CreationDate="2018-04-04T08:44:08.877" UserId="8394" ContentLicense="CC BY-SA 3.0" />
  <row Id="9928" PostId="7455" Score="0" Text="Russ, it's not exactly what I was looking for, but it's really interesting! I'm thinking that the underlying technique might possibly be of use in what I'm trying to do, with some modification, so thanks." CreationDate="2018-04-04T08:49:46.880" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9929" PostId="7460" Score="0" Text="somehow I missed it, thanks for helping, though this doesn't really change the unfortunate output of my path tracer" CreationDate="2018-04-04T22:57:44.277" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9930" PostId="7469" Score="0" Text="The reflectance is modelled by physical principles of Radiometry, which means basically Fresnelreflection if the light reflects off the surface of an object or some approximation like Lambert reflection if the light enters the objects interior and leaves the object again at some point. Is this what you're asking?" CreationDate="2018-04-05T07:42:36.170" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9931" PostId="7469" Score="0" Text="I've updated the question to specify a little more about what I'm asking. Hopefully, it isn't as vague :)" CreationDate="2018-04-05T16:28:47.577" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="9932" PostId="6406" Score="2" Text="To count number of holes you want to compute Betti numbers. To get actual holes, you want to get generators of the corresponding homology groups. Although, they are not unique and you might have to add some condition. Anyway what do you mean by &quot;exact coordinate of a hole&quot;? How do you define a hole? (Btw. I do not have the time to give a full answer, so Im posting only a comment.)" CreationDate="2018-04-05T20:13:54.757" UserId="1613" ContentLicense="CC BY-SA 3.0" />
  <row Id="9933" PostId="7471" Score="1" Text="Hmm this seems promising; I'll try it out and see how it goes" CreationDate="2018-04-05T20:30:18.053" UserId="6380" ContentLicense="CC BY-SA 3.0" />
  <row Id="9934" PostId="7472" Score="0" Text="It might be helpful if you think about how the tangent and bitangent are literally the vectors that the u and v axis travel in across the triangle." CreationDate="2018-04-06T02:25:36.347" UserId="56" ContentLicense="CC BY-SA 3.0" />
  <row Id="9935" PostId="7469" Score="1" Text="I have written my comment after you have updated it ;) However your update was what made me comment. In terms of the general specular BRDF model, there is the F (Fresnel reflection), D (Normal distribution function) and G (Shadowing / Masking Term). Do you want to know about any of these in particular? You said `microgeometry for the surface` is known, therefore D and G are already there. You also said `scattering of light on a surface is understood`, which would be F. Apart from the normalization, you really have your BRDF now, there is nothing more to look for... Therefore I'm asking ;)" CreationDate="2018-04-06T05:41:42.600" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9936" PostId="7469" Score="1" Text="Isn't the F (Fresnel reflection), D (Normal distribution function) and G (Shadowing / Masking Term) strictly for the Microfacet model though? What about materials that are significantly different like hair or cloth? How are BRDFs for these types of materials made? What happens in circumstances where microfacet theory doesn't work best?" CreationDate="2018-04-06T05:46:08.783" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="9937" PostId="7469" Score="1" Text="Now we're getting somewhere ;) I have never tried to shade this, however cloth is not easy to render correctly and is often just approximated with &quot;standard&quot; BRDFs or BSSDFs (to include sub surface scattering). I have also seen some work done on BTF (Bidirectional Texture Function), but they use a whole lot of textures which is not always practical - also they need &quot;measurements&quot;, afaik no analytic way exists.&#xA;In general, I would recommend you look into Digital Modeling of Material Appearance (Dorsey et al.) and Physically Based Rendering (Pharr et al.), there is a lot of information there." CreationDate="2018-04-06T06:00:58.440" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="9938" PostId="7469" Score="0" Text="[Digital Modelling of Material Appearance](https://books.google.co.in/books?hl=en&amp;lr=&amp;id=a9I4HHu79xcC&amp;oi=fnd&amp;pg=PP1&amp;dq=Digital+Modeling+of+Material+Appearance+(Dorsey+et+al.)&amp;ots=2ZFQI9pX6v&amp;sig=DOTbMJBl_P7Y0xzRd3X3fP8Z0_c#v=onepage&amp;q=Digital%20Modeling%20of%20Material%20Appearance%20(Dorsey%20et%20al.)&amp;f=false) seems to be exactly what I needed! There isn't really much in PBRT on how to model digital appearance." CreationDate="2018-04-06T06:06:42.407" UserId="5256" ContentLicense="CC BY-SA 3.0" />
  <row Id="9940" PostId="7471" Score="0" Text="Alternatively, if you are after something that is quick to code (but not likely to be quite as good) another possible approach would be (a) find the average, (b) find the colour furthest from the average -call it X, (c) find the colour furthest from X, call it Y, then (d) run several iterations of k-means (https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm) starting with X and Y as the initial reps.  It might do the trick." CreationDate="2018-04-06T08:46:26.440" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9942" PostId="7460" Score="1" Text="I have gone a little bit deeper into your code, and have notice you are not taking into account energy conservation. When sampling microfacet normals, you need 2 terms, although the second one is usualy 2*PI*random, you need the microfacet normal to compute the fresnel term for the microfacet model. Then, you would use the half vector to compute another fresnel term and use it as 1 - newFresnelTerm to ensure energy conservation. The final formulae for the brdf should look like diffuse * (1- fresnelDiffuse) + specular * brdf, and then use it in the rendering equation" CreationDate="2018-04-07T11:17:38.433" UserId="8394" ContentLicense="CC BY-SA 3.0" />
  <row Id="9943" PostId="7460" Score="0" Text="To be clear, the material I'm simulating has the diffuse component set to 0, I'm currently only simulating the specular part of it&#xA;I thought the above sampling code would sample the incoming direction, and that the halfvector between wo and wi is the microfacet normal, is this correct? If not I may have to re-evaluate what I wrote" CreationDate="2018-04-07T14:32:22.207" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9944" PostId="7460" Score="1" Text="Yes, that was meant for the reflected ray. I have review my microfacet implementation to check for comparsion. I have noticed that when computing the final microfacet brdf, you should check whether the denominator is 0, in which case the brdf should return 0, otherwise you could run into a indetermination of the type 0 / 0." CreationDate="2018-04-07T15:15:33.153" UserId="8394" ContentLicense="CC BY-SA 3.0" />
  <row Id="9945" PostId="7478" Score="0" Text="Thanks. This really helped me, barycentric coordinates, of course! ISimply use the UVs derived from the min/max of the triangles UV, and you get a regular grid over the triangle. Then simply intersect and done!" CreationDate="2018-04-09T13:51:09.363" UserId="8508" ContentLicense="CC BY-SA 3.0" />
  <row Id="9946" PostId="7460" Score="0" Text="I'll be sure to fix it, though I'm currently not getting any inf values as radiance, by the way many thanks for your help" CreationDate="2018-04-09T22:30:08.113" UserId="7871" ContentLicense="CC BY-SA 3.0" />
  <row Id="9948" PostId="7485" Score="0" Text="What about them do you have trouble understanding? It's hard to explain without knowing what the specific problem is." CreationDate="2018-04-10T07:54:45.897" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9949" PostId="7485" Score="0" Text="@DanHulme my specific problem Is that I don't know how functions used to draw 3d shapes like dot product in cone shape and like using two length in tours shape" CreationDate="2018-04-10T07:55:58.683" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9950" PostId="7485" Score="0" Text="@DanHulme I edit my question and add some detail that what is my problem I hope you help me to understanding this shapes." CreationDate="2018-04-10T08:58:00.390" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9951" PostId="5608" Score="0" Text="Sorry for this late acknowledgement, but I had to shelve this project owing to other more pressing priorities, but now that I've some time to return to it, I'll investigate your suggestion. Thanks a ton!" CreationDate="2018-04-10T14:45:28.950" UserId="6785" ContentLicense="CC BY-SA 3.0" />
  <row Id="9952" PostId="7487" Score="1" Text="Hi, can you clarify what you want to do—merge the 5–7 geometries down to 1, keeping all the verts and faces from all of them? Or do you mean the tank has modeled interior spaces like the cabin, seats, controls etc and you want to remove these, keeping only the exterior surface?" CreationDate="2018-04-10T18:28:02.083" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9954" PostId="7487" Score="1" Text="I want to remove interior spaces. There are some interior item overlapping/intersecting with the exterior surface; for those items, I want to do boolean union to merge them." CreationDate="2018-04-10T21:39:26.803" UserId="8547" ContentLicense="CC BY-SA 3.0" />
  <row Id="9955" PostId="7487" Score="1" Text="Look into csg (constructive solid geometry) for the boolean operations you are looking for. Most 3d programs have them (Mesh lab, Maya, etc)" CreationDate="2018-04-10T22:10:43.243" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="9957" PostId="7485" Score="0" Text="Please do not edit your question after an answer has been posted. If you have follow up questions, post them separately." CreationDate="2018-04-11T08:59:51.210" UserId="7709" ContentLicense="CC BY-SA 3.0" />
  <row Id="9958" PostId="7493" Score="1" Text="The other answers indicate that shapes other than an ellipse are possible. Can you demonstrate why they may be incorrect?" CreationDate="2018-04-11T10:57:14.477" UserId="209" ContentLicense="CC BY-SA 3.0" />
  <row Id="9959" PostId="7494" Score="0" Text="If you could instantly see the results of shaders by executing them on the CPU, we wouldn't need GPUs at all!" CreationDate="2018-04-11T12:13:58.367" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9960" PostId="7485" Score="0" Text="@Federico I'm sorry I removed new part , I wanted ask as a new question but I thought Instead of asking several questions It's better to ask one question." CreationDate="2018-04-11T13:01:24.953" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9961" PostId="7485" Score="0" Text="No, the opposite is quite true instead, SE sites prefer several well defined questions to one overly broad question" CreationDate="2018-04-11T13:27:54.353" UserId="7709" ContentLicense="CC BY-SA 3.0" />
  <row Id="9962" PostId="7485" Score="0" Text="I think your edit made it clear that you have a wider problem understanding linear algebra or signed distance fields, that explaining a bunch of shapes to you one at a time won't fix. Perhaps if you could isolate the common factor that's hard to understand about all the shapes, that would make a better question." CreationDate="2018-04-11T13:34:00.067" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9963" PostId="7485" Score="0" Text="@DanHulme that's right I should learn more algebra and ask my specific problems thanks." CreationDate="2018-04-11T13:42:15.833" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9964" PostId="7494" Score="0" Text="why do you want debug it on CPU? What kind of shader do you want to prototype?" CreationDate="2018-04-11T13:46:01.773" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9965" PostId="7494" Score="0" Text="I want to step through the code and see how my variables change. The framework does not have to be exactly hlsl/glsl/metal interpreter. Bare C++ and some facilities like texture sampling, shader intrinsic functions. It will help to develop and debug a shader and then I will port C++ implementation into the real shading language." CreationDate="2018-04-11T14:01:34.223" UserId="8553" ContentLicense="CC BY-SA 3.0" />
  <row Id="9966" PostId="7496" Score="0" Text="You can't necessarily decompose a mesh into cuboids, spheres, and cylinders. You can decompose it into triangles, though." CreationDate="2018-04-11T14:24:56.737" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9967" PostId="7496" Score="0" Text="@DanHulme, why I cannot decompose mesh into cuboids, spheres, cylinder?" CreationDate="2018-04-11T14:42:14.520" UserId="8547" ContentLicense="CC BY-SA 3.0" />
  <row Id="9968" PostId="7496" Score="0" Text="Because not all solids can be expressed as the union of finitely many cuboids, spheres, and cylinders." CreationDate="2018-04-11T15:11:38.007" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="9969" PostId="7495" Score="0" Text="That would pretty much result in the exact same outcome and isn't really related to his issue." CreationDate="2018-04-11T15:12:26.943" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="9970" PostId="7477" Score="0" Text="May I ask why the downvotes?" CreationDate="2018-04-11T16:45:54.757" UserId="2485" ContentLicense="CC BY-SA 3.0" />
  <row Id="9971" PostId="7497" Score="0" Text="`theta` is the angle between the view vector and the light vector. In volume rendering Henyey-Greenstein its used to describe the probability of forward scattering, isometric scattering, or backward scattering" CreationDate="2018-04-11T23:26:12.933" UserId="8394" ContentLicense="CC BY-SA 3.0" />
  <row Id="9972" PostId="7496" Score="0" Text="Something like https://perso.telecom-paristech.fr/boubek/papers/SphereMeshes/ ?" CreationDate="2018-04-12T06:01:55.513" UserDisplayName="user106" ContentLicense="CC BY-SA 3.0" />
  <row Id="9973" PostId="7497" Score="0" Text="@Nadir, but then what version of the equation is used? Additionally they use that wierd conal sampling and accumulate to a single value, and they use it as if its one angle then?" CreationDate="2018-04-12T18:14:13.307" UserId="6530" ContentLicense="CC BY-SA 3.0" />
  <row Id="9974" PostId="7501" Score="0" Text="You can also turn lots of things on and off in UE4 itself, using the ['Show' menu in the editor viewports](https://docs.unrealengine.com/en-us/Engine/UI/LevelEditor/Viewports/ShowFlags)." CreationDate="2018-04-12T21:04:51.613" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="9975" PostId="7482" Score="1" Text="I don't think std::vector&lt;&gt; is thread safe, you should be using something like a mutex when calling push_back()" CreationDate="2018-04-13T08:35:45.200" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9976" PostId="7484" Score="0" Text="Thanks. Didn't know about discard - though after thinking about the problem, what I had in mind was returning a flag indicating that alpha should be set to 0 for &quot;out of bounds&quot;, which I guess is a clumsy version of discard. I think this only works if the &quot;height map&quot; is interpreted as a &quot;depth map&quot;, i.e. we always trace from the eye to points below the real x-y plane of the texture ... which I guess is fine! Can't try it yet, laptop kaput and awaiting new one." CreationDate="2018-04-13T22:25:33.187" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9977" PostId="7484" Score="0" Text="To clarify, you don't trace from the eye to below the x-y plane of the surface, you still trace to the x-y plane. But when determining whether a point on surface intercepts the ray, all points are below the surface. Thus some rays won't intercept the surface at all. In these cases, we discard/&quot;transparify&quot; them, thus creating a proper silhouette at the edges." CreationDate="2018-04-14T12:27:53.637" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9978" PostId="7484" Score="0" Text="Problems will still arise at the texture boundary with an adjacent texture whose height map doesn't tile with the current height map - you would see straight through the model at certain angles.  Though thinking about it, this would even be the case with &quot;true&quot; geometry from hardware tessellation of a height map - the problem is inherent to inferring geometry from height maps - I'm sure it is surmountable with some thought (and some work in the vertex shader)." CreationDate="2018-04-14T12:28:03.817" UserId="8375" ContentLicense="CC BY-SA 3.0" />
  <row Id="9980" PostId="7482" Score="0" Text="Hi @PaulHK, Could this be part of the reason for the half-picture behavior? I got some more information about the memory, The objects are stored in memory so when I call the render function, the &quot;hits&quot; itself might not be registering because of the asynchronous memory accesses. But I was thinking that this would cause more random behavior, like seeing a random distribution of pixels on the picture every time." CreationDate="2018-04-14T18:00:54.953" UserId="8532" ContentLicense="CC BY-SA 3.0" />
  <row Id="9983" PostId="7482" Score="0" Text="You could remove your completion queue (future_vector) and just have your workers directly write to an RGB array, you don't need thread synchronisation for that. You will need a mechanism to detect when all workers are completed though, maybe something like an atomic counter and semaphore." CreationDate="2018-04-15T04:04:54.733" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9984" PostId="7482" Score="0" Text="The half written image is probably caused by the main thread not waiting for workers to complete. In addition to that it is hazardous for multiple threads to access the vector&lt;&gt; and that could cause issues if 2 threads update the internal state of the vector&lt;&gt; at the same time." CreationDate="2018-04-15T04:09:08.320" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="9985" PostId="7482" Score="0" Text="I thought if I declare the vector in the async function, it makes the structure in thread memory, does it not? You are probably right, I will try and fix it." CreationDate="2018-04-15T04:34:14.767" UserId="8532" ContentLicense="CC BY-SA 3.0" />
  <row Id="9988" PostId="7513" Score="2" Text="An easier to implement algorithm would be Marching Cubes. Though it would produce worse results than dual contouring for a torus. It would likely be easier to understand dual contouring after implementing marching cubes. Or use one of the thousand implementations of it in libraries like VTK or CGAL. Also marching cubes is easily understood in 2d (called marching squares)." CreationDate="2018-04-15T22:53:55.777" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="9989" PostId="7513" Score="0" Text="@AndrewWilson yeah that's right [marching cubes](https://www.youtube.com/watch?v=B_xk71YopsA) Is solution but I haven't Idea to Implement this algorithm In Raymarching" CreationDate="2018-04-16T04:38:10.713" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9990" PostId="7513" Score="0" Text="Oh, I think I get what you mean now. But ray marching is completely unrelated to the generation and usage of the polygon. Marching cubes produces a polygon from a 3d image (voxel grid/scalar field). You should create a 3d image of a resolution you want and evaluate the signed distance function at every point in the image to create a signed distance field. Then use 0 as your threshold (ie surface is at 0)." CreationDate="2018-04-16T05:09:23.300" UserId="113" ContentLicense="CC BY-SA 3.0" />
  <row Id="9991" PostId="7513" Score="0" Text="@AndrewWilson thanks can you tell me how Aiekick Implemented [Sdf to mesh](https://sketchfab.com/Aiekick)?" CreationDate="2018-04-16T05:13:07.823" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9992" PostId="7513" Score="0" Text="@AndrewWilson please see this [video](https://twitter.com/twitter/statuses/984221543705251840) sdf generated by 2D plane?!" CreationDate="2018-04-16T05:19:56.167" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9993" PostId="7515" Score="0" Text="I don't need this.what I want Is a scanner that scan my Raymarching objects then create the mesh from It." CreationDate="2018-04-16T05:28:02.460" UserId="6541" ContentLicense="CC BY-SA 3.0" />
  <row Id="9994" PostId="7501" Score="0" Text="@mbl 'bloom' is an example, not my real confusing effect.&#xA;&#xA;Something like 'Sub-Surface Shading' or 'Ambient Occlusion', I can hardly tell what effect exactly it shows when with or without it." CreationDate="2018-04-16T06:03:48.100" UserId="5222" ContentLicense="CC BY-SA 3.0" />
  <row Id="9996" PostId="7520" Score="0" Text="Thank you for your answer ! But now I would like to ask if the min and max values are just bounding boxes, where the actual coords of the vertices ?" CreationDate="2018-04-17T20:43:54.493" UserId="8587" ContentLicense="CC BY-SA 3.0" />
  <row Id="9997" PostId="7520" Score="0" Text="glTF stores the actual mesh data in a binary chunk.  This can be delivered a number of ways: As a separate .bin file, as a base64-encoded blob inside the JSON, or as a binary section of a .glb file.  Each `accessor` points to a `bufferView`, and typically each `bufferView` references a `buffer` with a single binary data blob.  So, the accessors and bufferViews act to slice portions of data out of the binary blob." CreationDate="2018-04-17T21:24:14.403" UserId="1908" ContentLicense="CC BY-SA 3.0" />
  <row Id="9998" PostId="7520" Score="0" Text="Thanks again, so I need to look for the coordinates in the binary file somehow." CreationDate="2018-04-18T05:17:40.867" UserId="8587" ContentLicense="CC BY-SA 3.0" />
  <row Id="9999" PostId="7520" Score="0" Text="How did you get the values from the acessors ?" CreationDate="2018-04-18T09:46:57.143" UserId="8587" ContentLicense="CC BY-SA 3.0" />
  <row Id="10000" PostId="7520" Score="0" Text="I'm using the [glTF Extension for VSCode](https://marketplace.visualstudio.com/items?itemName=cesium.gltf-vscode) (Disclaimer, I'm a contributor).  Place the cursor inside an accessor block and press ALT+D to decode the binary." CreationDate="2018-04-18T15:02:42.903" UserId="1908" ContentLicense="CC BY-SA 3.0" />
  <row Id="10003" PostId="7526" Score="2" Text="Is this a ray-tracer you control, or are you trying to make a scene that will work with a particular (off the shelf) renderer?" CreationDate="2018-04-19T10:49:35.760" UserId="2041" ContentLicense="CC BY-SA 3.0" />
  <row Id="10004" PostId="7526" Score="0" Text="@DanHulme I'll be in control." CreationDate="2018-04-19T14:42:06.067" UserId="8595" ContentLicense="CC BY-SA 3.0" />
  <row Id="10005" PostId="7531" Score="0" Text="But `frac(x / y) * y` is exactly what normal modulus operator compiles to (on GCN at least) and it gives incorrect results on both Nvidia and AMD. I'm not sure about `fmod`, though. It seems to give slightly different assembly code. I will check it, thanks." CreationDate="2018-04-19T19:45:37.593" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="10006" PostId="5727" Score="0" Text="The object with 6 vertices and 8 triangles is called an octahedron. https://en.wikipedia.org/wiki/Platonic_solid" CreationDate="2018-04-19T23:55:18.010" UserId="8598" ContentLicense="CC BY-SA 3.0" />
  <row Id="10008" PostId="7528" Score="0" Text="&quot;color depends on the direction&quot; — by that do you mean that I can represent the projector white light going through the slide and taking on the color of that pixel?" CreationDate="2018-04-20T02:48:25.963" UserId="8595" ContentLicense="CC BY-SA 3.0" />
  <row Id="10010" PostId="7531" Score="0" Text="So I have chacked it and `frac(x / y) * y`, operator `%` and `fmod` give incorrect results on Nvidia and AMD. `x - floor(x / y) * y` works correct on Nvidia but AMD not so much." CreationDate="2018-04-20T07:57:58.610" UserId="3123" ContentLicense="CC BY-SA 3.0" />
  <row Id="10011" PostId="7525" Score="0" Text="Hm.. sounds interesting, I'll have a look over VESA. I'm fine with linux, yet most of the users for this program are using Windows. I'll start here, see how far I can get. Thanks for your time." CreationDate="2018-04-20T10:24:19.043" UserId="5780" ContentLicense="CC BY-SA 3.0" />
  <row Id="10012" PostId="7536" Score="2" Text="This article &quot;A pixel is not a square&quot;(http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf) would be helpful to your understanding of a pixel." CreationDate="2018-04-20T18:42:39.267" UserId="120" ContentLicense="CC BY-SA 3.0" />
  <row Id="10013" PostId="7536" Score="1" Text="@TheBusyTypist fun reading but quite misleading. I do love how it starts with &quot;This is not a religious issue&quot; and then pretty much makes it into one." CreationDate="2018-04-20T20:49:20.013" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="10014" PostId="7535" Score="1" Text="You can raytrace a silhouette without any kind of shading. No need to involve physics. Also, scanline rendering is not always faster. The algorithmic complexity is different so there are cases where a raytracer will win." CreationDate="2018-04-20T21:09:27.623" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="10015" PostId="7536" Score="1" Text="afaik, pixels are considered as areas, that is why we have the concept of multiple samples per pixel and it's a way of implementing anti aliasing." CreationDate="2018-04-21T02:38:52.707" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="10016" PostId="7536" Score="0" Text="@TheBusyTypist Interesting read, but when you calculate an image you want to make it perfect. He's mixing the display of the result with the result and he's complaining about the positioning (_pixel center_), which does make some algorithms easier to handle. As the wandering warrior says, it's useful to consider a pixel as being a small square to mix partial pixels together and get _perfect_anti-aliasing. Here, though, I'm wondering about the potential effect in Ray Tracing. Maybe what I need to do is generate a bigger image and then scale it down to make sure I get the right colors..." CreationDate="2018-04-21T05:40:54.240" UserId="8595" ContentLicense="CC BY-SA 3.0" />
  <row Id="10017" PostId="7534" Score="0" Text="Yeah, the dithering is going to happen when you convert from RGB to paletted, unless you make sure to only use a predefined 256 colors to start with..." CreationDate="2018-04-21T05:46:07.370" UserId="8595" ContentLicense="CC BY-SA 3.0" />
  <row Id="10018" PostId="7533" Score="1" Text="Is there a reason you wouldn't want to try with MPEG4? It's much better quality than GIF and can be streamed (a.k.a. sent to the users at a specific/sufficient rate instead of all at once like a GIF, in other words, throttle the sending of that data file so it can still be played back full speed)." CreationDate="2018-04-21T05:47:40.880" UserId="8595" ContentLicense="CC BY-SA 3.0" />
  <row Id="10019" PostId="7536" Score="0" Text="@wandering-warrior No a pixel is a sample its no more a square than a sound samples are lines. Though, considering it a square is not a bad model, just not the best one. Evaluating the sample over better sampling is better. In fact if pixel is a area it should consider stuff outside of a square area. Because the perfect representation of a pixel is not square." CreationDate="2018-04-21T12:49:10.790" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="10020" PostId="2296" Score="1" Text="Excellent answer, this deserves to be framed!" CreationDate="2018-04-21T23:13:12.343" UserId="8608" ContentLicense="CC BY-SA 3.0" />
  <row Id="10021" PostId="7541" Score="0" Text="See also: http://reedbeta.com/blog/quadrilateral-interpolation-part-2/#inversion  The last term can be thought of as the delta of P11 from where it would be if the equation were planar." CreationDate="2018-04-21T23:27:06.573" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="10023" PostId="7512" Score="0" Text="I think, I currently have understood this concept better. The way I think about this function is to add dw term so BRDF = ((dL / dw) / (dE / dw)). dE / dw is input radiance from this w direction. dL / dw is partial outgoing radiance that is contributed from input direction w. . Is there any error in the way I see this now?" CreationDate="2018-04-22T11:06:59.993" UserId="8565" ContentLicense="CC BY-SA 3.0" />
  <row Id="10024" PostId="7517" Score="0" Text="I think, I currently have understood this concept better. The way I think about this function is to add dw term so BRDF = ((dL / dw) / (dE / dw)). dE / dw is input radiance from this w direction. dL / dw is infinitesmall partial outgoing radiance that is contributed from input direction w. . Is there any error in the way I see this now?" CreationDate="2018-04-22T11:14:39.463" UserId="8565" ContentLicense="CC BY-SA 3.0" />
  <row Id="10025" PostId="7541" Score="0" Text="I believe I've seen it named &quot;twist&quot; or &quot;twist vector&quot; in some articles, if that's any help.  It is indeed what allows curvature in what would otherwise be a parallelogram." CreationDate="2018-04-22T13:07:09.067" UserId="5717" ContentLicense="CC BY-SA 3.0" />
  <row Id="10027" PostId="7542" Score="0" Text="Do you want to know triangle intersection with a line?" CreationDate="2018-04-22T23:06:36.783" UserId="6806" ContentLicense="CC BY-SA 3.0" />
  <row Id="10028" PostId="5409" Score="1" Text="Try a large absorption coefficient ( say vec3(8,8,1) ), for thin objects like the ones in your screenshots it may not be thick enough to have much of an effect." CreationDate="2018-04-23T02:05:04.360" UserId="3073" ContentLicense="CC BY-SA 3.0" />
  <row Id="10029" PostId="7517" Score="0" Text="I don't quite understand your concept. First of all $dL$ already has a $d\omega$ term in it. So $dL / d\omega$ isn't radiance. Dunno what to call it. Don't try to make it a ratio of radiances." CreationDate="2018-04-23T10:37:07.503" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="10030" PostId="7542" Score="0" Text="@shashack Yes, specifically with a ray." CreationDate="2018-04-24T02:44:42.180" UserId="7334" ContentLicense="CC BY-SA 3.0" />
  <row Id="10032" PostId="7482" Score="1" Text="Hey PaulHK, thanks for helping me out, I fixed the bug, it was dumb, I posted an answer to anyone who might look at this." CreationDate="2018-04-24T06:38:39.570" UserId="8532" ContentLicense="CC BY-SA 3.0" />
  <row Id="10033" PostId="7548" Score="0" Text="It seems like **fixed-pipeline** and **programmable-pipeline** would be useful tags in this community, is there any reason why they aren't already available?" CreationDate="2018-04-24T09:46:47.167" UserId="8621" ContentLicense="CC BY-SA 3.0" />
  <row Id="10036" PostId="7517" Score="0" Text="Well dL/dw is not radiance. But it is **infinitesimal** outgoing radiance that is contributed from radiance from w direction hitting the surface.The outgoing radiance is the integral of this infinitesimal outgoing radiance from every input direction." CreationDate="2018-04-24T12:23:28.743" UserId="8565" ContentLicense="CC BY-SA 3.0" />
  <row Id="10037" PostId="7550" Score="0" Text="This answer is a good start but needs to be expanded a bit. For example, which format is the most appropriate for extracting the vertices information of a model?" CreationDate="2018-04-24T13:57:43.630" UserId="8621" ContentLicense="CC BY-SA 3.0" />
  <row Id="10038" PostId="7550" Score="0" Text="You don't even need to do that. The good old fixed-function GL 1.1 already had vertex arrays. So just issue a `glDrawArrays`. In the same way you can directly transfer the vertex layout into a bunch of `glVertex/Normal/WhateverPointer` calls. Fixed-function doesn't mean no arrays. In fact in this way the question's premise is quite a bit flawed and he can basically use the very same data and layout that he would use in a &quot;modern&quot; programmable pipeline, just with the old pointer functions instead of `glVertexAttribPointer` and with client arrays instead of buffers." CreationDate="2018-04-24T14:34:16.087" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="10039" PostId="7548" Score="1" Text="The question here is, how much do you *already* know about OpenGL? Do you already know how to render such a model with the modern programmable pipeline? Do you know how to render anything with the old fixed-function pipeline? Or don't you know *anything* about OpenGL?" CreationDate="2018-04-24T14:40:46.263" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="10040" PostId="7548" Score="0" Text="In the latter case, you might really want to start somewhere lower down the line. And in the former case the question is a little oddly constructed, since the data your model is represented in is quite orthogonal to the whole fixed-funxtion-vs-programmable question and you can pretty much use any universal vertex/index-array represenation. But if you're actually asking how to even draw *anything* in legacy OpenGL, I'm afraid a third-party tutorial would be more appropriate for such a broad question." CreationDate="2018-04-24T14:41:30.303" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="10041" PostId="7517" Score="0" Text="Nope I still don't understand. First of all, $L_o$ is the outgoing radiance, then that makes $dL_o$ the infinitesimal outgoing radiance not $dL_o/d\omega$. Your second part is somewhat right, the total outgoing radiance is the integral of the infinitesimal outgoing radiances. However that seems like a recursive definition so although you might or might not have this in mind, do keep note that you are actually integrating the differential irradiances from every direction and multiplying by the BRDF to get $L_o$" CreationDate="2018-04-24T17:06:13.573" UserId="6046" ContentLicense="CC BY-SA 3.0" />
  <row Id="10042" PostId="3965" Score="0" Text="you can try to perform actual statistics error distribution instead of just add noise uniformly, for an even better result. http://www.tannerhelland.com/4660/dithering-eleven-algorithms-source-code/" CreationDate="2018-04-25T07:38:58.433" UserId="1614" ContentLicense="CC BY-SA 3.0" />
  <row Id="10043" PostId="7550" Score="0" Text="@ChristianRau Perhaps you could add a MWE as part of an alternative answer?" CreationDate="2018-04-25T11:35:41.377" UserId="8621" ContentLicense="CC BY-SA 3.0" />
  <row Id="10044" PostId="7550" Score="0" Text="@allsey87 At the moment the question and its context is a little unclear, though." CreationDate="2018-04-25T13:41:31.907" UserId="6" ContentLicense="CC BY-SA 3.0" />
  <row Id="10045" PostId="3965" Score="0" Text="@v.oddou Maybe. Error diffusion would require an extra fullscreen pass, I think, versus adding noise which can be done inline in a shader." CreationDate="2018-04-25T15:55:28.370" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="10046" PostId="7548" Score="0" Text="why are you doing this? that pre-compiled source file will be just as big as the data file you would otherwise be loading. also, if you want the data linked into your program just to avoid an external file, you can just make static arrays of raw data, no need to pre-compile the GL commands and the data together. it doesn't sound like good program design to me..." CreationDate="2018-04-26T12:17:24.490" UserId="2049" ContentLicense="CC BY-SA 3.0" />
  <row Id="10047" PostId="7552" Score="0" Text="a bounding box is just a 3D &quot;range&quot;. ranges have very long history in set theory, predating all computer languages." CreationDate="2018-04-26T12:21:12.637" UserId="2049" ContentLicense="CC BY-SA 3.0" />
  <row Id="10048" PostId="7552" Score="0" Text="@atb What do you mean by range? my machine translator doesn't always translate very well. https://www.deepl.com/translator" CreationDate="2018-04-26T12:50:52.643" UserId="8629" ContentLicense="CC BY-SA 3.0" />
  <row Id="10049" PostId="7548" Score="0" Text="What do you aim to gain from no disk?" CreationDate="2018-04-26T16:04:16.680" UserId="38" ContentLicense="CC BY-SA 3.0" />
  <row Id="10050" PostId="7552" Score="0" Text="for example: http://mathworld.wolfram.com/Range.html" CreationDate="2018-04-26T16:21:59.463" UserId="2049" ContentLicense="CC BY-SA 3.0" />
  <row Id="10051" PostId="7554" Score="0" Text="&quot;*otherwise it's impossible to determine if the primitives are inside or outside the view volume defined with glFrustum()*&quot; That page defines the core profile of OpenGL; `glFrustum` *does not exist* in modern OpenGL anymore. Projection is the responsibility of the writer of the shader, not of the rendering pipeline." CreationDate="2018-04-26T16:34:56.687" UserId="2654" ContentLicense="CC BY-SA 3.0" />
  <row Id="10052" PostId="7557" Score="1" Text="it'd be really helpful, if you added a screenshot and/or explanation of what is happening and of what you want to achieve. `move x, y location of the texture` as in move the object that the texture is applied on? offsetting the texture on the object?" CreationDate="2018-04-27T08:52:57.290" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="10053" PostId="7557" Score="0" Text="its like moving the object... I am trying a code to set the x,y,w,h of the texture (rectangle) I am expecting that I can used this . pos.x += 0.2 because I already used this. pos.x += 0.2  it works if the vertex shader dont used this gl_Position = gl_ModelViewProjectionMatrix * gl_Vertex; and texture2DRect .. if I am not mistaken it works on texture2D but I cannot modify my fragment and vertex shader because it is necessary for openframeworks." CreationDate="2018-04-27T09:07:58.020" UserId="8517" ContentLicense="CC BY-SA 3.0" />
  <row Id="10054" PostId="7559" Score="0" Text="Is my interpretation of the other parameters correct?" CreationDate="2018-04-27T09:56:23.053" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="10055" PostId="2045" Score="0" Text="Your link to Mesa implementation is now dead." CreationDate="2018-04-27T10:01:18.557" UserId="4647" ContentLicense="CC BY-SA 3.0" />
  <row Id="10056" PostId="7557" Score="0" Text="`I found the solution pos.x += 0.2 already working but it is not visible. if I tried bigger number like pos.x += 100 it is clear that the textures moves.` You should read up on how texture coordinates work. They are within the range of [0, 1]. They can repeat outside that - but in that case, the coordinates (0.2, 0.2) and (100.2, 50.2) are still the same." CreationDate="2018-04-27T10:09:55.633" UserId="7008" ContentLicense="CC BY-SA 3.0" />
  <row Id="10057" PostId="7559" Score="2" Text="Not quite. Albedo changes its meaning depending on whether the model is set to be a metal or not. When it's a metal, albedo determines the color of the specular component. When it's not a metal (it's dielectric then, in most implementations with a fixed IOR of 1.4), albedo drives the color of the diffuse component.&#xA;&#xA;Roughness is a parameter that determines how wide or narrow the specular lobe is.&#xA;&#xA;Neither normal map or roughness have any influence on the amount of energy that the specular component reflects. Only albedo controls energy loss." CreationDate="2018-04-27T14:47:13.953" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="10058" PostId="7559" Score="0" Text="Is the document you provided a reference for all of these?" CreationDate="2018-04-27T14:53:51.027" UserId="228" ContentLicense="CC BY-SA 3.0" />
  <row Id="10059" PostId="7559" Score="0" Text="To my knowledge, pretty much every shader that calls itself &quot;PBR&quot; these days is derived from that paper, one way or the other." CreationDate="2018-04-27T19:03:30.893" UserId="4546" ContentLicense="CC BY-SA 3.0" />
  <row Id="10061" PostId="7542" Score="0" Text="Generate a bunch of random rays that you know should hit, and a bunch that shouldn't, and make sure you get the right answers. To generate rays that should hit, pick points in the triangle using the barycentric coordinates, and pick a random ray origin. Include points on edges and at vertices. For generating rays that should not hit, use out of bounds barycentric coordinates." CreationDate="2018-04-29T18:13:20.967" UserId="8448" ContentLicense="CC BY-SA 3.0" />
  <row Id="10066" PostId="3875" Score="0" Text="I can't help saying thanks! Acturally, I first understand this as stratified sampling with 4 strata. And I make the number of &quot;strata&quot; customizable, like 9, 16... Now, I know it is a &quot;trapezoid&quot; filter(not so common in other renderers). And as the number of my &quot;strata&quot; tends to be infinity, the filter becomes a box filter, where I'm painting the lily... Interesting!" CreationDate="2018-05-01T15:48:04.977" UserId="5944" ContentLicense="CC BY-SA 3.0" />
  <row Id="10067" PostId="7566" Score="3" Text="A thin glass plate should not magnify like this (if it did, you would see this every time you look through a window in real life!). Could you share how the glass plate is set up in your Mitsuba scene file? Perhaps there's something off about it." CreationDate="2018-05-01T21:24:09.160" UserId="48" ContentLicense="CC BY-SA 3.0" />
  <row Id="10068" PostId="7566" Score="1" Text="[Here](https://drive.google.com/file/d/1BHOzY16NYf9h2aYsGTb3ZpRiSlVTwf88/) it is" CreationDate="2018-05-01T21:33:47.477" UserId="6698" ContentLicense="CC BY-SA 3.0" />
  <row Id="10069" PostId="7568" Score="0" Text="Ok, what about the data strucure? Does it have the functionality to generate k-d trees or some other structure? If not is there any other library that could be used in conjunction with assimp?" CreationDate="2018-05-02T02:46:52.530" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10071" PostId="7568" Score="2" Text="You will get an array of vertices / triangle lists / materials / etc. KD-Tree splitting a mesh is something you need to do yourself, this will be tied to your compute implementation anyway so having a third party library handle your KD-Tree could result in a less than ideal tree. I don't think there is a standard for transforming a mesh to KD-Tree as different implementations have different split heuristics depending on the programmer and application (e.g. Min triangles per leaf / KD-Rope / maximum depth / triangle area based measurement / sparse leaf nodes / etc)." CreationDate="2018-05-02T02:53:20.540" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10072" PostId="7568" Score="0" Text="Dang, that's a lot of work then. I was hoping to find a library for that because I'm basing my path tracer on the gpu. So I'll have to send the structured data in the form of heap on the GPU and use the traversing scheme on the heap. So wanted to stay away from coding on the cpu side. Anyways thanks for the help, keeping the question open for a little longer in case someone wants to add something." CreationDate="2018-05-02T06:36:46.107" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10073" PostId="7566" Score="1" Text="Just a side note: IoR of 1.1 is a heavily unrealistic value for glass, which is typically in interval 1.5-1.6 causing much stronger refraction effect." CreationDate="2018-05-02T08:28:14.967" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="10074" PostId="7568" Score="1" Text="Assimp is primarily a loader (and a good one at that). I feel what you're looking for is a mesh library. If you do not care for supporting all sorts of formats, that's what you should probably look up. libMesh, openMesh, CGAL are some C++ based libraries I know of. Most of these libraries will support some sort of mesh import." CreationDate="2018-05-02T19:04:31.160" UserId="77" ContentLicense="CC BY-SA 4.0" />
  <row Id="10075" PostId="7565" Score="0" Text="Do you want there to be a particular object that has gravity in the scene, or do you want all the particles to be gravitationally attracted to each other? Also, is your particle system 2D or 3D?" CreationDate="2018-05-03T02:02:38.400" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10076" PostId="7571" Score="0" Text="&quot;since you didn't really ask about that specifically, I assume you understand this&quot; I do, but the added context is nice :) Thanks for this very thorough answer! Just to make sure I understood, you're saying that the only case where the inverse transpose isn't redundant is when, say, a mesh is being rescaled on one axis but not the others? Follow up question: if I do want to calculate the normals in camera space (for instance, to avoid floating point rounding errors), how do I do it? Would using `inverse(transpose(modelViewMatrix))` be enough?" CreationDate="2018-05-03T07:13:29.853" UserId="8660" ContentLicense="CC BY-SA 4.0" />
  <row Id="10077" PostId="7571" Score="0" Text="`he inverse transpose isn't redundant is when, say, a mesh is being rescaled on one axis but not the others?` there is also the shearing transformation and i am not sure about that, but i doubt it leads to normal matrix = model matrix. but in general, that's the most likely case to brake equality. `if I do want to calculate the normals in camera space` you can do lighting in world space - you will need to transform your lighting and viewing vectors to world space and then to tangent space, but otherwise it doesn't matter.still, the view matrix part is wrong,it's just the `modelMatrix` you want" CreationDate="2018-05-03T07:17:57.270" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10078" PostId="7570" Score="0" Text="FYI The w=0 part is for all directions. Stuff that you don't want to have translation affect." CreationDate="2018-05-03T08:36:39.503" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="10079" PostId="7567" Score="0" Text="Here is a good source of the models you are looking for: http://casual-effects.com/data/ for loading them I recommend this header only obj loading library: https://github.com/syoyo/tinyobjloader" CreationDate="2018-05-03T13:55:11.987" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10080" PostId="7565" Score="0" Text="Hi, all of the particles should be affected by gravity, for example my particles just emits up right now, but I want them to fall off like if you fire water out of a hose. The system is currently in 2D." CreationDate="2018-05-03T19:32:44.380" UserId="8651" ContentLicense="CC BY-SA 4.0" />
  <row Id="10081" PostId="2045" Score="0" Text="Why do we need directx when there is an opengl?" CreationDate="2018-05-03T21:22:00.113" UserId="537" ContentLicense="CC BY-SA 4.0" />
  <row Id="10082" PostId="7572" Score="1" Text="Based on opeGLES (mobile) i have benchmarked compressed textures (ETC1) to be faster than their RGB888 equivalents, I guess this is, as you say, due to lower memory bandwidth." CreationDate="2018-05-04T02:15:06.347" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10083" PostId="7572" Score="0" Text="It's going to be a trade off depending on whether you are memory bound or not." CreationDate="2018-05-04T11:09:09.173" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="10084" PostId="7578" Score="1" Text="The integral over the hemisphere is $\pi$, not $2\pi$, because $\int_\Omega \cos\theta d\omega = \int_0^{2\pi}\int_0^{\frac{\pi}{2}} \cos\theta \cdot \sin\theta d\theta d\phi = \pi$. Have a look at https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/diffuse-lambertian-shading" CreationDate="2018-05-04T11:32:04.223" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10086" PostId="7577" Score="0" Text="You're almost there, just add cylinders along each edge of the bounding box as well. If you then want to render it transparent and not see all the intersections between the spheres, cylinders, and boxes, there is a simple way to do that too, but it is too long for this comment box." CreationDate="2018-05-04T15:40:19.247" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10087" PostId="317" Score="0" Text="What do you mean by &quot;nothing feasible&quot;? Wine runs hundreds of D3D-based programs without problems, and has done so for years. Of course there are bugs and unimplemented parts, but that doesn't make the whole API unusable." CreationDate="2018-05-04T16:45:21.410" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="10088" PostId="7577" Score="0" Text="@Rahul, how to render the intersections between spheres, cylinders, boxes to be transparent? You can write it as an answer if you wish. Thanks." CreationDate="2018-05-04T19:56:29.720" UserId="8547" ContentLicense="CC BY-SA 4.0" />
  <row Id="10089" PostId="7584" Score="0" Text="How is 'skybox' defined ? Have you tried to output with alpha=1 also, e.g.   FragColor=vec4(texColor,1); - You will need to set the output type to vec4 for FragColor." CreationDate="2018-05-05T09:20:56.070" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10090" PostId="7574" Score="0" Text="Hey, thank you for this answer, I will accept this answer soon,&#xA;&#xA;by any idea, do you know how do I make my particles make a curve-fly like?&#xA;&#xA;For example I want it to be a fountain particle system. It now just emits upwards and fades out. I want them to fly a curve and lose energy over time as it almost hits the ground for example or half way to the ground." CreationDate="2018-05-05T11:24:57.830" UserId="8651" ContentLicense="CC BY-SA 4.0" />
  <row Id="10091" PostId="7574" Score="0" Text="I think the first step would be to emit them with an initial angle that's just slightly off from directly up. So instead of an initial velocity of &lt;0, 1&gt; (or whatever), you could move the x part of the velocity vector by a small amount. Maybe a random amount between -0.1 and 0.1, for example, so the initial velocity vector is maybe &lt;0.05, 1&gt;, so there's some movement to the right for one particle, and &lt;-0.02, 1&gt; so there's leftward movement for another particle. Once you have gravity in there, they'll naturally form a parabolic arc." CreationDate="2018-05-05T15:00:21.200" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10092" PostId="7574" Score="0" Text="I can't get the gravity to work, maybe you could take a closer look to my code... but I am working on it right now...&#xA;&#xA;Is there any way I could contact and chat with you about this? I think it would simplify things more for both of us..." CreationDate="2018-05-05T15:59:08.337" UserId="8651" ContentLicense="CC BY-SA 4.0" />
  <row Id="10093" PostId="7574" Score="0" Text="If you want people to look at your code, please edit your question and add it in. That way when other people have the same question, they can find a complete answer in one place." CreationDate="2018-05-05T18:17:29.627" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10094" PostId="7574" Score="0" Text="How can I see if my gravity thing is working?And how can I do what you suggested? Move the x-part of the velocity vector by a small amount...Does it work via Noise Texture?" CreationDate="2018-05-05T18:42:15.237" UserId="8651" ContentLicense="CC BY-SA 4.0" />
  <row Id="10098" PostId="7574" Score="0" Text="I would start by making your particle birth rate really low, like only a few particles per second. I'd set the velocity vector for each particle to a constant like &lt;0.2, 1.0&gt;, and run the simulation. If you see the particles arcing off to the right in a parabolic arc, then you know it's working. And yes, to get random velocities, you'll either need to seed their initial velocities with random values or use some sort of noise technique to generate the randomness, such as a noise texture." CreationDate="2018-05-06T15:35:53.050" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10099" PostId="7584" Score="0" Text="Do you have [OpenGL debug output](https://www.khronos.org/opengl/wiki/Debug_Output) turned on? If not, that would be my first step, and see if it gives you any errors for this shader. My guess is there's some problem binding uniforms/textures when both diffuse and reflection are turned on." CreationDate="2018-05-06T22:34:07.783" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10101" PostId="7589" Score="0" Text="OpenGL (and presumably DX) does clipping in &quot;clip-space&quot;, that is vertices after perspective projection so the view frustum becomes a unit cube were clipping edges is simpler." CreationDate="2018-05-07T12:56:40.107" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10102" PostId="7591" Score="0" Text="Commenting on my own question: MikuMikuDance lets you create mirrors. If you have two mirrors that should reflect each other, one of them won't show the other mirror, while the other one will show the mirror as a gray screen. In this case, draw order may be important, but the rules for transparent objects won't solve it. Order-independent transparency may reduce the need to manually set draw order without eliminating it." CreationDate="2018-05-08T01:49:23.387" UserId="8682" ContentLicense="CC BY-SA 4.0" />
  <row Id="10103" PostId="7584" Score="0" Text="I didn't know about that, i'll try it and get more info, thanks" CreationDate="2018-05-08T03:54:32.397" UserId="8437" ContentLicense="CC BY-SA 4.0" />
  <row Id="10104" PostId="7574" Score="0" Text="Nice, I just had a mistake, they now go off in a shape, now I need somehow to randomize all particles such that it creates a nice effect like fire/smoke or something... Is there any way to manipulate the color to a fire color/smoke color?" CreationDate="2018-05-08T11:35:32.403" UserId="8651" ContentLicense="CC BY-SA 4.0" />
  <row Id="10105" PostId="7574" Score="0" Text="That would be a separate question. Why don't you write that up and explain what you're looking for in more detail?" CreationDate="2018-05-08T15:51:41.887" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10106" PostId="7594" Score="1" Text="&quot;*For this, I wanted to look into the assembly version of my shader*&quot; Unless you're getting the platform-specific machine code, anything you see will be essentially useless for telling if your optimization was successful. The most effective way to tell if you gained something from an optimization is to benchmark it. Oh, and OSG is not exactly known for being performance-friendly." CreationDate="2018-05-08T16:49:52.223" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10107" PostId="7596" Score="0" Text="&quot;You could write code that could work on OpenGL 3.0 or extensions&quot; so why are suffixes ever added to extensions?" CreationDate="2018-05-08T18:14:47.810" UserId="7000" ContentLicense="CC BY-SA 4.0" />
  <row Id="10108" PostId="7594" Score="2" Text="`OSG is not exactly known for being perfomance-friendly` that decision is not in my hands, I have to work with it. My optimization is branching elimination - while I wasn't aware how hardware dependend optimizations are, I was under the assumption that branch elimination is generally preferable. However I now have three `sign` calls, which is why I wanted to do this in the first place. Nevertheless, thank you for the input." CreationDate="2018-05-08T18:19:11.433" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10109" PostId="7596" Score="1" Text="1) Not all extensions represent core OpenGL functionality of any OpenGL version. However nice ARB_bindless_texture may be, it's not part of any core OpenGL version. 2) Not all extensions were/are released contemporaneously with the core equivalent." CreationDate="2018-05-08T18:27:31.067" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10110" PostId="7597" Score="2" Text="&quot;*Say the declaration is:*&quot; ... I don't know what you mean by that declaration. It's just a `GLuint`; the declaration doesn't mean anything." CreationDate="2018-05-08T18:46:02.107" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10111" PostId="7597" Score="0" Text="that declaration was simply to say that we can assume that there is already a successfully compiled program under that ID." CreationDate="2018-05-08T19:28:13.217" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10112" PostId="7574" Score="0" Text="Okay, I will do so. one more question concerning the gravity:&#xA;It now drags my particles to one side, how can I randomize the direction in which it draws in, for example, I want few to drag to the right as well not only to the left." CreationDate="2018-05-08T22:07:00.487" UserId="8651" ContentLicense="CC BY-SA 4.0" />
  <row Id="10113" PostId="7598" Score="0" Text="Are you writing a software rasterizer? When rendering on GPU this gets taken care of for you." CreationDate="2018-05-09T03:27:15.527" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10114" PostId="7598" Score="0" Text="@russ Yes......" CreationDate="2018-05-09T05:15:09.817" UserId="8677" ContentLicense="CC BY-SA 4.0" />
  <row Id="10115" PostId="7595" Score="0" Text="As I already mentioned in the code above, I wanted to take out branching and came up with a solution that uses `sign` three times. I wasn't aware, just how hardware dependent this is, I assumed for example that NVidia graphics cards would turn up more or less the same results. Also, one often reads to get rid of branching to optimize code, which I took to be a general advice, rather than it being `firmly in platform-dependent territory`. I guess from your answer though, that the SPIR-V route is probably the best for me at this point, because of OSG. In any case, thanks for the thorough answer." CreationDate="2018-05-09T06:12:53.440" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10117" PostId="7595" Score="1" Text="@Tare Well, yes, that is indeed pretty general advice. There are definitely some clear optimization advices that pertain to how GPUs and shaders generally work and the principles of how they're programmed. However, that is also rather high-level stuff that doesn't usually require looking into specific assembly too much. I was more thinking of real micro-optimization stuff. I didn't mean to say looking at the assembly can't give you a general idea how your code maps to GPUs, just that you shouldn't rely too much on the specific instructions being all too accurate guidance for every machine." CreationDate="2018-05-09T08:26:44.673" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10118" PostId="7601" Score="0" Text="I assume you mean pixel by “the new point”? Anyway, say, I have 10000 probes and therefore 10000 cubemaps and I determine 8 probe/cubemap indices for the current pixel in the fragment shader. How would I access these 8 cubemaps, or rather, how would I pass 10000 cubemaps to the shader to be able to pick the necessary 8 out of them?" CreationDate="2018-05-09T12:02:54.450" UserId="8541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10119" PostId="7602" Score="0" Text="Thank you for a detailed answer. Unfortunately, bindles textures are not a thing on Mac, so I have to discard that option, unfortunately. A for the cubemap arrays, they won’t store thousands of layers, will they? Will have to ask OpenGL for particular numbers, but there is a limit of few hundred layers IIRC. What do you think about packing “cubemap faces” (no actual cubemaps are involved) as series of chunks of a large texture? Basically pack all shadow maps to a texture, making it an atlas, then, in fragment shader, compute offsets and transform direction vector into UV coords." CreationDate="2018-05-09T13:40:23.407" UserId="8541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10120" PostId="7602" Score="0" Text="Also, I’m considering 10x10x3bytes faces, which should fit" CreationDate="2018-05-09T13:41:37.773" UserId="8541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10121" PostId="7602" Score="0" Text="@ПавелМуратов: &quot;*A for the cubemap arrays, they won’t store thousands of layers, will they?*&quot; That depends on the implementation. Also, there's no such thing as &quot;3bytes&quot; per pixel. You *will* get 4-bytes per pixel." CreationDate="2018-05-09T13:42:36.907" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10122" PostId="7602" Score="0" Text="Hmm, how are 24bit depth values are stored then?" CreationDate="2018-05-09T13:52:01.477" UserId="8541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10123" PostId="7602" Score="0" Text="@ПавелМуратов: With either 8-bits of stencil or 8-bits of wasted space." CreationDate="2018-05-09T13:54:23.073" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10124" PostId="7604" Score="0" Text="Are you sure that the inverse bind pose (or mOffsetMatrix) transforms a point from root space to bone space? Because I have read conflicting opinions on that matter https://github.com/assimp/assimp/pull/1803" CreationDate="2018-05-09T19:38:15.903" UserId="8699" ContentLicense="CC BY-SA 4.0" />
  <row Id="10125" PostId="7604" Score="0" Text="That's what it's *supposed* to do. But I don't know how Assimp handles it specifically. Maybe there's some confusion over the matrix vs its inverse?" CreationDate="2018-05-09T19:58:42.767" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10126" PostId="7604" Score="0" Text="Thanks for the prompt reply. So basically, I need to transform a vertex from mesh space to world space, then use inverseBindTransform to get it from world space to bone space, where I would multiply it with the desired animation pose, then I would again transform that into world space again right?" CreationDate="2018-05-09T20:04:36.890" UserId="8699" ContentLicense="CC BY-SA 4.0" />
  <row Id="10127" PostId="7604" Score="0" Text="Not exactly. Your vertex is in mesh space. You transform it from mesh space (let's say that's also root space) into local joint/bone space with the inverse bind matrix, then from local joint space back into root/mesh space, but this time with the actual animation matrix. Usually that whole thing isn't really applied in order like this but instead builds a single matrix, the one transforming a vertex for *one* joint from root space into animated root space. And those are the joint matrices you combine with usual blend skinning (or other method) to get the matrix you transform your vertex with." CreationDate="2018-05-09T20:16:28.137" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10128" PostId="7604" Score="0" Text="And this animated vertex, now still in mesh/root space, but properly animated, you then transform with the normal model-to-world and world-to-camera or whatever global transformation matrices." CreationDate="2018-05-09T20:18:05.203" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10129" PostId="7600" Score="0" Text="The article &quot;Hyperbolic Interpolation&quot;, by Jim Blinn (https://books.google.com.br/books?id=KBDS6GAwSwAC&amp;pg=PA174&amp;lpg=PA174&amp;dq=hyperbolic+interpolation+jim+blinn&amp;source=bl&amp;ots=PRr9NrLZBm&amp;sig=wxKauSqlcby9ZLRoA-8W1of7I6k&amp;hl=pt-BR&amp;sa=X&amp;ved=0ahUKEwjAw6W7gfraAhUMi5AKHTNfCXwQ6AEIMzAC#v=onepage&amp;q=hyperbolic%20interpolation%20jim%20blinn&amp;f=false) may be of some help also (actually, that was the place were I've learned how to properly interpolate triangle attributes subjected to a perspective projection)." CreationDate="2018-05-10T01:43:29.817" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="10131" PostId="7604" Score="0" Text="I dont think I can directly transform it from mesh space to bone space. The way I am calculating inverse bind, I concatenate every transformation matrix starting from root node, so its actually in world space. So I need to move vertex from mesh space to world space before I can multiply it with inverse bind transform to bring it to bone space, right? The way I am doing is (CBA)^-1 but it is wrong somehow" CreationDate="2018-05-10T03:10:47.870" UserId="8699" ContentLicense="CC BY-SA 4.0" />
  <row Id="10132" PostId="7608" Score="0" Text="Thank you for the explanation again emackey ! :D Tonight I will check my bin file for the values." CreationDate="2018-05-10T05:40:38.740" UserId="8587" ContentLicense="CC BY-SA 4.0" />
  <row Id="10133" PostId="7604" Score="0" Text="Well, then from world space. The point is that you use the same matrices for the inverse bind matrix that you use for the actual animation, just inverse and for the animation frame that the mesh is defined in. But it's also really hard to argue and figure out what actual multiplication order you should use in a comment thread. I gave the general idea how it works above, the rest are technicalities and maybe a little more reading of introductory material how skeletal animation works in general. You just always need to be aware in which space your data is defined and in which space you want it." CreationDate="2018-05-10T13:08:41.187" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10135" PostId="7433" Score="0" Text="The problem here is the definition of &quot;real-world computer graphics tools&quot;. As long as there is no more or less exact definition of this term, answers will be heavily biased." CreationDate="2018-04-26T08:15:39.913" UserId="8219" ContentLicense="CC BY-SA 3.0" />
  <row Id="10136" PostId="7608" Score="0" Text="I realised that it has count 70, not 5. I have 70 frames. Why it's storing my frame datas instead of the 5 keyframes ?? I really need the keyframes, I am so angry now because if it doesn't store the actual keyframes then my one month work will lost..." CreationDate="2018-05-10T16:05:17.390" UserId="8587" ContentLicense="CC BY-SA 4.0" />
  <row Id="10137" PostId="7608" Score="0" Text="https://imgur.com/a/esok5xO as you can see there are 5 keyframes in 70 frames" CreationDate="2018-05-10T16:16:22.133" UserId="8587" ContentLicense="CC BY-SA 4.0" />
  <row Id="10138" PostId="7613" Score="0" Text="I thought the local transform of A should take you to B local space, then the local transform of B take you to C local space? for example, for a vertex in world space to get into C space, it needs to be multiplied by CBA*v. Then to get it from C space to world space you would get the inverse of it, which is (CBA)^-1 or A^-1*B^-1*C^-1?" CreationDate="2018-05-11T04:40:23.903" UserId="8699" ContentLicense="CC BY-SA 4.0" />
  <row Id="10139" PostId="7613" Score="1" Text="@ManhNguyen Local transform takes you from the local space to the parent space, not the other way around. C space to world space would be ABC*v." CreationDate="2018-05-11T04:46:37.067" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10140" PostId="7613" Score="0" Text="Usually the matrices are set up to describe object in parent space. But it is offcourse possible to store the inverses as the matrix like @ManhNguyen suggests. Ive never seen it done, somebody asked this i think and the answer was that for some reason it feels unnatural. Just as possible to set it up this way just like its possible to have a transposed matrix order in which case all calculations are reverse. So the dev needs to know this." CreationDate="2018-05-11T04:54:20.820" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="10141" PostId="7613" Score="0" Text="@ManhNguyen that wouldnt work if you wanted to have multiple separate objects. But if you shift your reasoning one node up then it would work. Modeller has some leeway in deciding how the computation happens, but it seems that describing the world as a function of objects is a bit out there, but possible." CreationDate="2018-05-11T05:07:15.840" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="10142" PostId="7613" Score="0" Text="@joojaa I followed the math you guys give me and the animation is only correct for 1 out of the 2 models I am working on. Both are loaded by Assimp. The only differences I can think of are that 1 is a fbx file and the other is a dae file and that 1 has multiple mesh not situated at root scene and the other does. Is there something I am missing? The Assimp loader is able to load both models correctly." CreationDate="2018-05-11T05:27:21.023" UserId="8699" ContentLicense="CC BY-SA 4.0" />
  <row Id="10143" PostId="7616" Score="0" Text="Hinting is only for vertical or horizontal edges in the font glyphs, so I don't think it'll work for your rotations at all. You might need to use SDF font rendering, or rasterise the vector font each frame." CreationDate="2018-05-11T09:37:39.930" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10144" PostId="7616" Score="1" Text="@DanHulme &quot;Hinting is only for vertical or horizontal edges&quot; - That would be news to me. Reading the docs I understand hinting as a manual shift of the spline in any direction, sacrificing relative distance preservation between (edge-) features in favor of stronger &quot;main&quot; features, such as the three vertical bars in &quot;m&quot;. I don't see why this would be a horizontal/vertical thing only... Guess I gotta re-read some books and articles." CreationDate="2018-05-11T10:07:46.173" UserId="3041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10145" PostId="7616" Score="0" Text="@DanHulme I didn't think using SDF for 16pix font was applicable since many features are optimized on sub-pixel basis. Gonna look at it closer though. Thanks!" CreationDate="2018-05-11T10:13:17.030" UserId="3041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10146" PostId="7616" Score="1" Text="OK, I was thinking specifically of stem hints, and control point hints are also a thing in TrueType. [The old FontForge docs have a brief explanation](http://fontforge.github.io/overview.html#Hints). But the point is, hinting is about fitting the glyph onto a square grid, so that beams cover a whole pixel, so you don't get jaggies or blur. Reorienting the square grid obviously invalidates the hints." CreationDate="2018-05-11T11:25:25.807" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10147" PostId="7616" Score="0" Text="@DanHulme Ok, that makes sense. I would need a separate set of hints for each orientation since which parts of a glyph become beams is in part derived from orientation. Guess I'll have to ask the design department to make a font for each orientation they want to show ;-) Really though, I'll have to look closer at alternative solutions such as SDF. Thanks again." CreationDate="2018-05-11T11:49:17.167" UserId="3041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10148" PostId="7618" Score="0" Text="Hmm, yeah... the helper function essentially amounts to the following in my case:&#xA;&#xA;```commandList-&gt;CopyBufferRegion(meshPart.m_mainBuffer.Get(), 0, meshPart.m_uploadBuffer.Get(), 0, totalBufferSize);```&#xA;&#xA;&#xA;I'll keep my Map+memcpy and just call CopyBufferRegion myself without going through the helper. Thanks for the reality check." CreationDate="2018-05-11T19:14:43.657" UserId="8705" ContentLicense="CC BY-SA 4.0" />
  <row Id="10151" PostId="7627" Score="3" Text="Don't post the same question again to add new details. Instead, use the **edit** button when necessary." CreationDate="2018-05-13T10:02:37.603" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10152" PostId="7628" Score="0" Text="Well, `textureGather` is already a slight bit hacky in that it forces you to just a single channel for the apparently sole reason that it can't return 4 `vec4`s API-wise. So I'd guess the lack of a `textureGather` for 3D textures might be simply due to the interface problems of returning 8 values instead of 4 from a function." CreationDate="2018-05-13T21:33:04.617" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10153" PostId="7630" Score="0" Text="Thank you for your very helpful and perfectly explained answer :) Greatly appreciate it - just required an explicit explanation like the one above." CreationDate="2018-05-14T09:32:23.437" UserId="8725" ContentLicense="CC BY-SA 4.0" />
  <row Id="10155" PostId="7433" Score="0" Text="@haggikrey, ones I can earn money with." CreationDate="2018-05-14T13:31:47.083" UserId="8434" ContentLicense="CC BY-SA 4.0" />
  <row Id="10156" PostId="7509" Score="0" Text="It does work. ive used this in a job nack in 2000 for removing extra geometry of cad models." CreationDate="2018-05-14T20:10:18.150" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="10157" PostId="7631" Score="0" Text="Could you edit your answer to clearly distinguish between initilization code and use code? (the difference between initializing the texture vs using it on the render loop)" CreationDate="2018-05-15T00:10:38.107" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10158" PostId="7632" Score="1" Text="This question reminds me of a paper about verifiable visualization that tries to accomplish exactly that (https://ieeexplore.ieee.org/abstract/document/5290733/). Although it talks about a different topic (verification of isosurface extraction), the approach is smiliar to the one you suggest: finding analytic solutions for some simple cases and verifying if the implementation converges to that very same solution. I believe that the main point here would be to define which parameters should be accounted for in order to be able to state that the implementation is correct." CreationDate="2018-05-15T02:23:35.187" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="10159" PostId="7632" Score="1" Text="When you talk about &quot;constant background radiance&quot;, are you referring to something similar to the &quot;furnace test&quot;? I'm not so sure if it would be a good setup in this case because we would not be able to check viewpoint dependent phenomena. Maybe a different setup would be more effective in this sense (but I have not clue about that right now)." CreationDate="2018-05-15T02:23:42.597" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="10161" PostId="7632" Score="0" Text="You are right. Glossy surfaces could be a problem. Even if the procedure is done for a particular outgoing direction, it does not prove that the model is correctly implemented for the other outgoing directions.&#xA;&#xA;Also, I am surprised that I couldn't find a useful resource or a question when I google it." CreationDate="2018-05-15T11:09:11.483" UserId="6698" ContentLicense="CC BY-SA 4.0" />
  <row Id="10162" PostId="7608" Score="0" Text="The 5 vs 70 keyframe question was [resolved on GitHub here](https://github.com/KhronosGroup/glTF-Blender-Exporter/issues/216)." CreationDate="2018-05-15T12:47:12.143" UserId="1908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10166" PostId="7633" Score="0" Text="In your method, is morphNormalN a delta or the actual normal to morph towards?" CreationDate="2018-05-15T14:57:04.627" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10168" PostId="7633" Score="0" Text="@bram0101 the spec says &quot;deviations in the Morph Target&quot; and looking at the data it looks like morphNormal0 a delta, not the &quot;new normal&quot; of the morph target position" CreationDate="2018-05-15T19:58:27.907" UserId="4820" ContentLicense="CC BY-SA 4.0" />
  <row Id="10169" PostId="7633" Score="0" Text="Actually I realized I never checked if they altered the morphNormals prior to this" CreationDate="2018-05-15T20:05:25.040" UserId="4820" ContentLicense="CC BY-SA 4.0" />
  <row Id="10170" PostId="7636" Score="0" Text="I have never plotted a function in a shader before, but wouldn't it suffice mathetmatically to just add a linear function $f(x)=mx+b$ wherever $f(x_0)\neq f(x_1)\forall x_0, x_1\in {\rm I\!R}$ and change the gradient $m$ (with the special case of the &quot;linear function&quot; becoming vertical of course)?" CreationDate="2018-05-16T07:14:59.887" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10171" PostId="7636" Score="0" Text="If you mean that if my function f(x) = sin(x), I do ft(x) = sin(x) + mx + b.&#xA;&#xA;That will not make the sine curve look rotated. (Only the y coordinates are modified not the x).&#xA;&#xA;But maybe I misunderstood what you meant." CreationDate="2018-05-16T07:22:40.667" UserId="3434" ContentLicense="CC BY-SA 4.0" />
  <row Id="10172" PostId="7636" Score="0" Text="Yes, that's what I meant, I think you are right." CreationDate="2018-05-16T07:27:18.380" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10173" PostId="7637" Score="0" Text="Do you have the spec for the format?" CreationDate="2018-05-16T09:05:20.993" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="10177" PostId="7637" Score="0" Text="@ratchetfreak this is the closest I know to a spec: http://paulbourke.net/dataformats/directx/ | Also, msdn documentation on the same: https://msdn.microsoft.com/en-us/library/windows/desktop/bb173014(v=vs.85).aspx" CreationDate="2018-05-16T17:57:25.257" UserId="77" ContentLicense="CC BY-SA 4.0" />
  <row Id="10178" PostId="7638" Score="0" Text="I tried this method out. One of the problems I faced was that the textures went missing. Furthermore, the dimensions all got messed up. Is there something I am missing? (I just imported a .X, deleted the default light source/camera and exported to .glb)" CreationDate="2018-05-16T17:59:51.970" UserId="77" ContentLicense="CC BY-SA 4.0" />
  <row Id="10179" PostId="7640" Score="0" Text="They are absolutely inderect, yes. Idea to move calculation to vertex shader is interesting. There is a problem though. I don’t simply add or substract 1 from tex coords, I use floor and ceil to find the bounding box of a coordinate in texel space, and because of that, I don’t see how that can be solved in a vertex shader." CreationDate="2018-05-17T07:03:29.503" UserId="8541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10180" PostId="7638" Score="0" Text="The glTF exporter does have some tricky rules for materials given that it is trying to do PBR.  It ships with documentation on that.  The dimensions should be simple to get right though.  If you can produce a non-proprietary blend file that shows the dimensions problem, maybe you could post an issue on that." CreationDate="2018-05-17T16:47:30.527" UserId="1908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10181" PostId="7645" Score="0" Text="Second point of this [answer](https://stackoverflow.com/a/17896385/3125070): &quot;The pixel shader deals with fragments, not vertices, but when you do make use of coordinates (via SV_Position for example), those coordinates are in screen space (offset by 0.5)&quot;" CreationDate="2018-05-17T17:49:55.947" UserId="8748" ContentLicense="CC BY-SA 4.0" />
  <row Id="10182" PostId="7645" Score="0" Text="_&quot;I can think of why you would wanna do a shift of -0.5&quot;_ I can't either. But I can find that in many slides online. So there's this 0.5 offset happening somewhere..." CreationDate="2018-05-17T17:51:58.020" UserId="8748" ContentLicense="CC BY-SA 4.0" />
  <row Id="10183" PostId="7645" Score="0" Text="I think he is talking about the 0.5 offset to get to the pixel &quot;center&quot;. `0.5` makes sense but `-0.5` doesn't for me. See my edited answer." CreationDate="2018-05-17T18:24:56.783" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10186" PostId="7638" Score="0" Text="Thanks, I will do that. Keeping the question open right now to see if any other method comes up. :)" CreationDate="2018-05-18T06:47:04.323" UserId="77" ContentLicense="CC BY-SA 4.0" />
  <row Id="10187" PostId="7638" Score="0" Text="Update: Blender doesn't seem to import .X files anymore :/ Only export is supported." CreationDate="2018-05-18T10:51:53.180" UserId="77" ContentLicense="CC BY-SA 4.0" />
  <row Id="10188" PostId="7638" Score="0" Text="That's lousy.  It still has import in the name, but I don't see it in my import menu either.  I wonder when that changed." CreationDate="2018-05-18T21:08:36.883" UserId="1908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10192" PostId="7656" Score="0" Text="`based on spherical coordinates (which I don't really understand - 3D Vectors are about the limit of my math)` you should really look into this - once you get the hang of it, it seems pretty obvious. with cartesian coordinates, you simply describe the extend on each axis for a point. with spherical coordinates, you rather specify the direction, in which the point lies. imagine you look straight up. the polar angle determines, how far you rotate your head down (y axis, y is up). the azimuthal angle determines, how far you rotate your body around (around the y-axis). that's the gist of it" CreationDate="2018-05-22T08:24:14.637" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10193" PostId="7656" Score="0" Text="I understand the gist of it, but I don't understand it well enough to translate the referenced paper into an implementation. For example, I don't expect to understand this anytime soon: &quot;We sample the projected area by generating samples on the half disks proportionally&#xA;to their respective projected areas. For this, we use a polar parameterization of the disk (r, φ) in which we scale the angle φ in order to account for the difference of projected areas of its two halves.&quot;" CreationDate="2018-05-22T08:44:52.740" UserId="5180" ContentLicense="CC BY-SA 4.0" />
  <row Id="10194" PostId="7656" Score="0" Text="While I can't explain this to you atm, they are probably working with solid angles. This, too, is a concept you should familiarize yourself with when working with BxDFs. It is somewhat confusing, especially in the beginning, but it is really worth it. Basically you cut out a piece of a surface of a sphere and see how big the area of the cut out piece is in relation to the squared radius of the sphere. I doubt you can get to the bottom of your question without understanding solid angles first." CreationDate="2018-05-22T12:43:14.610" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10195" PostId="7649" Score="0" Text="[Relevant meta discussion about paywalled links](https://computergraphics.meta.stackexchange.com/questions/276/whats-the-policy-on-paywalled-links)" CreationDate="2018-05-22T20:17:30.257" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10196" PostId="7649" Score="1" Text="I added a link and some more info. Thanks!" CreationDate="2018-05-22T21:48:31.417" UserId="8761" ContentLicense="CC BY-SA 4.0" />
  <row Id="10199" PostId="7497" Score="0" Text="sorry for the delay, I recently implemented this clouds. The equation used is the standard equation (1) which you can find here https://www.astro.umd.edu/~jph/HG_note.pdf . About the cone sampling, the only angle used is that between the raymarch direction and the light vector at the point you start cone-sampling" CreationDate="2018-05-25T17:08:38.560" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="10202" PostId="7668" Score="0" Text="I just stumbled on [glTF](https://github.com/KhronosGroup/glTF) which is pretty wide and instructive; I might add it as the answer if nobody has a better one." CreationDate="2018-05-29T07:09:03.957" UserId="8660" ContentLicense="CC BY-SA 4.0" />
  <row Id="10203" PostId="7666" Score="0" Text="Thanks for your thoughts, Calvin. I've studied the paper you provided and it seems that they use the same tetrahedron/occlusion approach as Guerilla Games does. And Guerilla Games mention using same probe system for dynamic and static geometry which implies that lit surfaces are occluders themselves. I guess they don't have thin geometry or something, because in my case, the whole thing looks awful! And so, I switched to simply multiplying interpolation weights by clamped cosines between fragment-to-probe and fragment normal vectors, which is what Quantum Break's engine does, apparently." CreationDate="2018-05-29T09:19:56.017" UserId="8541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10204" PostId="7666" Score="0" Text="Some leakage is present, of course, but still, it looks much better than all other things I've tried. I'll have to wait untill multiple light bounces are done to see whether leakage is masked by them. If I'll find final result to be pleasing, I'll post an answer." CreationDate="2018-05-29T09:28:00.547" UserId="8541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10205" PostId="7668" Score="1" Text="That's...not remotely an answer, though. It's just a file format. You wanted to ask a general question, so you'll get general answers about how to approach the problem. Saying &quot;use this data format&quot; doesn't really answer your question at all." CreationDate="2018-05-29T16:14:31.717" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10206" PostId="7668" Score="0" Text="I was thinking more about the associated cheatsheet that does have a general-ish description of how skinning works and of the math involved, but yeah, either way it's not complete by itself." CreationDate="2018-05-29T16:55:23.760" UserId="8660" ContentLicense="CC BY-SA 4.0" />
  <row Id="10209" PostId="7650" Score="0" Text="It is okay to use Fresnel term as a pdf to choose between reflection and refraction. Because this estimator will contain a Fresnel term on both nominator and denominator, they are cancelled. So, you can directly use brdf or btdf." CreationDate="2018-05-30T04:40:51.163" UserId="6698" ContentLicense="CC BY-SA 4.0" />
  <row Id="10210" PostId="7675" Score="0" Text="BINGO we got a winner!  This was exactly the problem,  I'll edit my question to show how I fixed this, I sort of had an intuition that it was going to be something like this, but there's no way I would have been able to figure this out any time soon with out your help!" CreationDate="2018-05-30T19:10:52.350" UserId="6530" ContentLicense="CC BY-SA 4.0" />
  <row Id="10211" PostId="7529" Score="0" Text="Are you using DX or are you using vulkan?  Both support HLSL..." CreationDate="2018-05-30T19:58:01.160" UserId="6530" ContentLicense="CC BY-SA 4.0" />
  <row Id="10212" PostId="7674" Score="0" Text="Glad it worked :) By the way, you don't need to divide by length(rayDir) to get cosA - since you already normalized it in calculateFragmentRay." CreationDate="2018-05-31T04:02:55.990" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10213" PostId="7674" Score="0" Text="@russ good catch, I'll edit it out" CreationDate="2018-05-31T04:20:26.743" UserId="6530" ContentLicense="CC BY-SA 4.0" />
  <row Id="10214" PostId="7671" Score="0" Text="Thank you for the answer. Well, I was trying to compute hue and saturation from Cb and Cr values. I read hue = atan2(Cr/Cb) and saturation is sqrt(Cr^2+ Cb^2). Saturation I see is nearly 0.5. Same for hue, for Red color it turns out to be 108 degrees when measured from X axis or 18 degrees when measured along Y-axis anticlockwise. Not 0 in any case." CreationDate="2018-05-31T08:08:23.793" UserId="8758" ContentLicense="CC BY-SA 4.0" />
  <row Id="10215" PostId="7678" Score="2" Text="Usually, the last matrix is the first action to be applied, so you read them in reverse order. But it depends on your modeling factors mathematicians like to look from inside out, but it is not impossible to read the ther way around it only transposes your computation." CreationDate="2018-05-31T12:09:52.417" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="10217" PostId="7682" Score="0" Text="Just wow, even reflections are supported.. It would also be nice if you incoporate this into your another answer https://computergraphics.stackexchange.com/a/7666/8541, since these are related." CreationDate="2018-06-01T08:36:36.413" UserId="8541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10221" PostId="5096" Score="0" Text="I looking for this type of algorithm. Have you found any implementation or replacent??" CreationDate="2018-06-03T09:44:38.217" UserId="8824" ContentLicense="CC BY-SA 4.0" />
  <row Id="10222" PostId="7685" Score="0" Text="What you are doing is what a GPU as a graphics processor is designed to do. Is it a requirement to do it in Scala or can you use a graphics API such as openGL or DirectX?" CreationDate="2018-06-03T12:41:37.873" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="10223" PostId="7685" Score="0" Text="Yes, I wanted to know about the geometric mechanisms involved in perspective projection and in filling a face with interpolation. The best solution would consist in executing parts of my code (e.g. : interpolation between the 4 points of my face) by the GPU and not the CPU. Perhaps GPGPU could be useful for me ?" CreationDate="2018-06-03T19:19:28.770" UserId="8709" ContentLicense="CC BY-SA 4.0" />
  <row Id="10224" PostId="5096" Score="0" Text="@user43968, well, unfortunately none of them are simple. Currently I'm trying to implement this one with continious dijkstra approach. It's not nlog(n), but it's subquadratic, which is not bad too https://dl.acm.org/citation.cfm?id=161156." CreationDate="2018-06-04T11:49:37.733" UserId="2644" ContentLicense="CC BY-SA 4.0" />
  <row Id="10225" PostId="7691" Score="0" Text="Many thanks to @NicolBolas who helped editting the format. I am new to StackOverflow. After doing another editting, I noticed that your name was gone... Didn't mean to do that" CreationDate="2018-06-05T15:06:48.967" UserId="8826" ContentLicense="CC BY-SA 4.0" />
  <row Id="10226" PostId="7694" Score="0" Text="There are special debuggers for graphics like Nvidia Nsight or even the built in one to visual studios that have many features. Most have the ability to take screenshots, pick pixels, and view detailed history of how it got there." CreationDate="2018-06-06T05:24:21.263" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="10227" PostId="7692" Score="0" Text="can you post the line number in the pbrt code too" CreationDate="2018-06-06T13:32:35.547" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10228" PostId="7692" Score="0" Text="one thing is that you need to reevaluate both back and forth pdfs of the end s and end t vertices of the subpath when computing MIS.&#xA;http://rendering-memo.blogspot.com/2016/03/bidirectional-path-tracing-8-combine.html" CreationDate="2018-06-06T13:53:19.013" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10229" PostId="7698" Score="0" Text="I'm not sure this is the answer you want - but if you did the resize with a nearest-neighbor interpolation that should ensure no new colors. It will, however, result in a blocky displacement map, which I assume you don't want." CreationDate="2018-06-07T01:25:19.047" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10230" PostId="7698" Score="0" Text="@user1118321 My understanding is that the way displacement maps work in Blender will smooth things naturally - and if not, that can be made to happen. Anyways, since information has been created that wasn't in the original, i see no reason not to find work-arounds instead of working with bloated files. So, i need to look into nearest-neighbor interpolation? Will Image Magick do that?" CreationDate="2018-06-07T01:55:48.873" UserId="8841" ContentLicense="CC BY-SA 4.0" />
  <row Id="10231" PostId="7695" Score="0" Text="I'll look into RenderDoc then. Re: shaders, is there any tool out there that can give you more powerful debugging (eg the values of intermediaries variables), or is color-based debugging the standard?" CreationDate="2018-06-07T02:07:33.320" UserId="8660" ContentLicense="CC BY-SA 4.0" />
  <row Id="10232" PostId="7699" Score="0" Text="Re: scene-testing, I'm currently using glTF sample scenes to test my engine; I might build more complex scenes at some point to test what's going wrong more finely." CreationDate="2018-06-07T02:11:51.380" UserId="8660" ContentLicense="CC BY-SA 4.0" />
  <row Id="10234" PostId="7698" Score="0" Text="I've not used Image Magick, but from what I've read about it, it almost certainly does." CreationDate="2018-06-07T03:22:55.547" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10235" PostId="7695" Score="2" Text="I just spent 1h on RenderDoc and I can already confirm its usefulness. It made what was an amalgam of mysterious untractable bugs ridiculously easy to understand. Thanks a lot!" CreationDate="2018-06-07T05:50:40.157" UserId="8660" ContentLicense="CC BY-SA 4.0" />
  <row Id="10236" PostId="5697" Score="0" Text="You also can use Transfinite interpolation where you take the refracted triangle as a four sides polygon by having vertices at sides. https://en.wikipedia.org/wiki/Transfinite_interpolation" CreationDate="2018-06-07T06:35:22.193" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10237" PostId="5697" Score="0" Text="or this link:http://ebrary.free.fr/Mesh%20Generation/Handbook_of_Grid_%20Generation,1999/chap03.pdf" CreationDate="2018-06-07T08:11:44.587" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10238" PostId="7698" Score="0" Text="What image format are you using? JPEG can introduce quite a lot of unwanted colours." CreationDate="2018-06-07T08:42:13.127" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10239" PostId="7698" Score="0" Text="Tgey will smooth but if you use NN it is still blokier since the reconstruction filter would produce a different result. But since the result interpolates why would it matter to you? And why would a smaller file matter to blender? It would still need to expand the file to full memory footprint" CreationDate="2018-06-07T13:42:45.063" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="10240" PostId="7698" Score="0" Text="@joojaa this is for essentially a game. It will be downloaded many times so the bandwidth matters." CreationDate="2018-06-07T13:55:32.400" UserId="8841" ContentLicense="CC BY-SA 4.0" />
  <row Id="10241" PostId="7698" Score="0" Text="@PaulHK 16-bit PNG" CreationDate="2018-06-07T13:56:42.563" UserId="8841" ContentLicense="CC BY-SA 4.0" />
  <row Id="10242" PostId="7700" Score="0" Text="The size difference between the two tile sets isn't that huge - the tiles from the original image are 11,520 on a side, while the ones from the resized image are 8,192 on a side. I'm hopeful, but we'll see." CreationDate="2018-06-07T14:05:49.433" UserId="8841" ContentLicense="CC BY-SA 4.0" />
  <row Id="10243" PostId="7700" Score="0" Text="@kimholder that is a quite big difference once you square the numbers" CreationDate="2018-06-07T14:08:55.753" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="10244" PostId="7700" Score="0" Text="@joojaa it's about double. If high frequency content cancelled out the smaller size, i'd be content. Right now the smaller files are 5x bigger." CreationDate="2018-06-07T14:13:01.190" UserId="8841" ContentLicense="CC BY-SA 4.0" />
  <row Id="10245" PostId="7699" Score="0" Text="I forgot about a link I wanted to include, about Unity3D's approach - I've edited my answer to include it." CreationDate="2018-06-07T21:39:02.910" UserId="8796" ContentLicense="CC BY-SA 4.0" />
  <row Id="10247" PostId="7704" Score="0" Text="Seems to me he is is simulating global Illumination, which is basically indirect lightning due to light coming at surface after being reflected from other surfaces. So yeah, we don't just shoot rays from intersection point (IP) to light directly, instead we shoot random rays from IP in order to simulate that indirect lighting coming from different surfaces." CreationDate="2018-06-10T13:45:17.733" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10249" PostId="7709" Score="0" Text="oh right. that is even better and a pretty simple soltuion, thank you." CreationDate="2018-06-11T09:20:13.830" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10251" PostId="7691" Score="0" Text="Don't forget to also accept your answer!" CreationDate="2018-06-11T14:56:26.187" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="10252" PostId="3577" Score="0" Text="Vulkan now works on macOS (previously OS X): https://moltengl.com/" CreationDate="2018-06-11T15:54:53.287" UserId="8443" ContentLicense="CC BY-SA 4.0" />
  <row Id="10253" PostId="5963" Score="0" Text="I am trying to do exactly &quot;Using an iPhone X, I can get a mesh of the face (using ARFaceGeometry), which I can output as an .obj file using Model I/O.&quot;. Can you tell me how you accomplished this? Thanks!" CreationDate="2018-06-11T18:38:39.183" UserId="6599" ContentLicense="CC BY-SA 4.0" />
  <row Id="10254" PostId="7704" Score="0" Text="These are two different illumination model: Local vs Global illumination. Global illumination considers all the light paths and its interaction with surfaces. A diffuse surface for example does not only receive light from a light source but scatters light in multiple direction around its local hemisphere which needs to be recursively traced. The later approach is very costly and needs statistical approach(monte carlo integration) to solve." CreationDate="2018-06-13T08:31:33.887" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10255" PostId="7711" Score="0" Text="Hmm how did you remove the summation, I would appreciate though if you will tell what's wrong in my calculation rather than proving from a different way." CreationDate="2018-06-13T11:22:55.973" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10256" PostId="7711" Score="0" Text="the summation is written as N times the f(x)/p(x) which cancels out one of the N factor in variance. I think you missed this on your last proof. Have look at the edit." CreationDate="2018-06-13T12:20:32.980" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10257" PostId="7711" Score="0" Text="Yes I know that, but it can only be done if we are taking the  $\sum  E[f(x_i)] $. That is when we are taking the expected value inside the summation, that reduces $f(x_i)$ to $f(x)$ and then we can write it as $N*f(x)$. However in my proof I am having problem taking the Expected value of the whole square of the summation. You can't take the E operator inside of the whole square term. example,  $E [ (  \sum f(x_i) )^2]$" CreationDate="2018-06-13T19:57:52.220" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10258" PostId="7529" Score="0" Text="I'm using DirectX11" CreationDate="2018-06-13T21:16:40.067" UserId="3123" ContentLicense="CC BY-SA 4.0" />
  <row Id="10264" PostId="7720" Score="0" Text="There are early  2 spheres centered at the origin. Then  the MVmatrix is different for the 2 objects namely I draw a sphere with a translate value and another sphere with another translation value thus in the same rendering cycle I modify the MVmatrix. Isn't this correct? Actually I would realize that a sphere, the Earth, goes around another sphere the Sun but I have some difficult for this rason I want know if my math was mistakes less." CreationDate="2018-06-17T07:58:13.407" UserId="4981" ContentLicense="CC BY-SA 4.0" />
  <row Id="10265" PostId="7720" Score="0" Text="Normally you have a 'transform' - rotation scale and translation - for each object in the world. You can calculate a separate 'model' matrix for each object from these that places it correctly in the world. Then you calculate a single 'view' matrix for the camera, which is the inverse of its transform like you describe above. To then get a single object's position relative to the camera, you multiply view*model to get the modelView for that object." CreationDate="2018-06-17T13:16:46.943" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10266" PostId="7720" Score="0" Text="Yes, you are right in general. But in my case is a little more complicated because in the meantime I have to rotate the Earth around the sun. However, assured that my way of generating an orbiting camera is correct I will open a more specific topic. Thanks!" CreationDate="2018-06-17T15:01:29.637" UserId="4981" ContentLicense="CC BY-SA 4.0" />
  <row Id="10267" PostId="7722" Score="1" Text="My first guess would be overflows in one of the shaders. OpenGL numbers don't have the same range on different implementations and it's easy to get caught out that way. Check your shaders don't deliberately rely on overflow being a modulo, and try increasing everything to `highp` and see if the behaviour changes." CreationDate="2018-06-17T16:26:16.797" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10271" PostId="7726" Score="0" Text="Thank you for your response! So if I understand correctly, basically any BSSRDF function can closely approximate the corresponding RTE solution given the proper coefficients and modeling assumptions. Also, what happens when you have a scattering medium that is not as dense,  like for water or glass? Is the BSSRDF actually easier to use because there's less back-scattering? I don't have enough reputation to upvote, but I will wait a few hours to see if other users have different viewpoints, and if not I will accept your answer. Thanks!" CreationDate="2018-06-18T23:35:10.637" UserId="8892" ContentLicense="CC BY-SA 4.0" />
  <row Id="10272" PostId="7726" Score="1" Text="As I understand it BSSRDFs work especially well when the scattering is small scale, ie high scattering coefficients. For air/water/glass it's often sufficient to do single scattering in a path tracer and leave multiple scattering out of it (which is biased but, again, often good enough). For cases where you can't ignore the multiple scattering, I don't know of a better approach than full volumetric path tracing or volumetric photon mapping." CreationDate="2018-06-19T03:16:57.690" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10282" PostId="163" Score="0" Text="@user18490 's link to Scratchapixel has moved to https://www.scratchapixel.com/lessons/advanced-rendering/rendering-distance-fields . The rest of the site (https://www.scratchapixel.com/index.php) looks useful as well." CreationDate="2018-06-21T08:22:27.337" UserId="8905" ContentLicense="CC BY-SA 4.0" />
  <row Id="10283" PostId="7729" Score="0" Text="You compute the normal &quot;for each vertex of a cube&quot; not &quot;for a cube&quot;, my example case was to compute the normal for a vertex shared by 8 cubes, where such vertex is for example &quot;on&quot; and all the others &quot;off&quot;. If you read the paper and use their formula you would get $(0,0,0)^T$ which doesn't make sense as normal." CreationDate="2018-06-21T09:29:46.780" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="10284" PostId="7729" Score="0" Text="The paper specifically says that normals are only calculated for vertices that are &quot;off&quot;, but which have a least 1 vertice in the cube that is &quot;on&quot;. Calculating a normal for a vertice that is inside the volume doesn't make any sense. Put another way, if you follow the edges of every cube to all the connecting vertices and compute the normal that normal will be non-zero." CreationDate="2018-06-21T14:37:19.810" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="10285" PostId="7729" Score="0" Text="excuse me -- vertex (not vertice)" CreationDate="2018-06-21T14:47:26.553" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="10286" PostId="7729" Score="0" Text="Say you have a point cloud consisting of only 1 point, and you want to estimate the normals that are on the mesh that surrounds that point, what would you do? (this is only applying case 1 given by the paper)." CreationDate="2018-06-21T14:49:09.880" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="10287" PostId="7729" Score="0" Text="Using case 1 from the paper:Compute the normal at v2 results in (1,0,0) compute the normal at v4 results in (0,1,0) ...interpolate the normal between v2,v4 the normal would start at (1,0,0) pass through (0.5,0.5,0.0) and end up at (0,1,0). The result would be a octahedron but would look like a sphere when rendered. Do the same for all the vertices ignoring results that are inside the volume or not connected to the volume" CreationDate="2018-06-21T15:30:33.103" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="10288" PostId="7729" Score="0" Text="Just as a last  comment I would recommend reading this: https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch01.html" CreationDate="2018-06-21T15:32:40.737" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="10289" PostId="7731" Score="0" Text="You might try posting ads on game development/programming oriented sites, if owners accept it. I *think* that more people who check out such websites are likely to be interested in graphics programming in general, compared to &quot;regular&quot; job sites (that's true for me, at least ;))" CreationDate="2018-06-21T20:40:08.017" UserId="8796" ContentLicense="CC BY-SA 4.0" />
  <row Id="10290" PostId="7731" Score="0" Text="Interesting, in the institute Im doing my PhD right now the neuroscientists want to build VR enviroment, too. After some discussion and confusion I realised that they do not want to build a VR headset for mice, which I deemed basically impossible, but they want to build a cave https://en.m.wikipedia.org/wiki/Cave_automatic_virtual_environment and that should be doable. I would advise to be carefull about the usage of VR which has nowdays quite specific meaning." CreationDate="2018-06-21T21:39:11.870" UserId="1613" ContentLicense="CC BY-SA 4.0" />
  <row Id="10291" PostId="7731" Score="2" Text="Questions asking for recommendations are off-topic on this site. Please see the [help section](https://computergraphics.stackexchange.com/help/on-topic) on what to ask here." CreationDate="2018-06-22T02:30:22.433" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10293" PostId="7735" Score="1" Text="From you video it looks like the ray direction, not ray-origin is being skewed by changes to the position. Does view_m have a translation component ? You should be multiplying normalised r with W=0 also when multiplying direction vectors by a matrix." CreationDate="2018-06-22T05:43:45.230" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10294" PostId="7735" Score="0" Text="I construct the rotation matrix using glm:rotate(mat4, float, vec3). I don't think that creates a translation component. But I agree with your diagnosis" CreationDate="2018-06-22T05:47:49.040" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10295" PostId="7735" Score="0" Text="I've just editted my comment, try view_m*vec4(normalize(r), 0));" CreationDate="2018-06-22T05:48:46.900" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10296" PostId="7735" Score="0" Text="@PaulHK I did the suggested change however the outcome is the same" CreationDate="2018-06-22T06:55:41.523" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10297" PostId="7735" Score="0" Text="Presumably you start from 'r' in the direction of 'dir' ? What is the purpose of r+= camera_pos; ? Should you be starting from 'camera_pos' ?" CreationDate="2018-06-22T08:23:17.757" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10298" PostId="7737" Score="0" Text="Since there is no code or anything it's kind of hard to give advice. My best guess is that in your gaussian blur for the pixels above/behind the pink cube you still sample from all neighbouring pixels - including the pink cube. Thus, it blurs with a pink color. So in your kernel you need to disregard neighbouring pixels, if they come from a non-blurred object." CreationDate="2018-06-22T10:17:11.157" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10299" PostId="7737" Score="0" Text="Sorry for not posting any code: didn't add it here because I was just doing plain old convolution but with different kernels for each pixel. _&quot;So in your kernel you need to disregard neighbouring pixels, if they come from a non-blurred object&quot;_: How do I do this?" CreationDate="2018-06-22T12:29:46.850" UserId="8748" ContentLicense="CC BY-SA 4.0" />
  <row Id="10300" PostId="7737" Score="0" Text="That depends on what you're working with. If this is a glsl shader, then you probably have what you called *sigma* as a texture available and read from a rendered image to write into the output. So rather than reading from neighbouring pixels of your sigma texture and adding them to your output, you could multiply the the pixels color with the sigma value before adding it to the output. Since *sigma=0* for the perfect focus, this would stop the pink color from leaking to the background. Note that you need to adjust the kernel normalization though, otherwise the picture darkens around the cube" CreationDate="2018-06-22T12:34:29.067" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10302" PostId="7735" Score="0" Text="You are correct in that this solved the issue. If you can explain why this caused the weird effects we were seeing I'll accept your answer." CreationDate="2018-06-22T20:21:48.287" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10304" PostId="7739" Score="1" Text="All convex n-gons can be represented using a triangle fan, using the first vertex as the centre of the fan and every other vertex (1+n), this is assuming the vertices are CCW order." CreationDate="2018-06-23T07:48:32.803" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10305" PostId="7739" Score="1" Text="If you have nonconvex polygons, you would need to perform [triangulation](https://en.wikipedia.org/wiki/Polygon_triangulation), e.g. using an ear clipping algorithm. I'm not sure how nonplanar nonconvex polygons are usually handled." CreationDate="2018-06-23T10:52:52.740" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10306" PostId="7742" Score="0" Text="I didn't get that completely, is it luminance (intensity?) alone or depth-aware as well? Do you have any pseudo code or some code available." CreationDate="2018-06-23T13:17:31.137" UserId="8748" ContentLicense="CC BY-SA 4.0" />
  <row Id="10308" PostId="7737" Score="1" Text="Have a look at this blog post: [Bokeh depth of field in a single pass](http://tuxedolabs.blogspot.com/2018/05/bokeh-depth-of-field-in-single-pass.html) by Dennis Gustafsson. It goes through the development of a bokeh-from-depth filter step by step, including various artifacts encountered with simpler implementations, and how to fix them." CreationDate="2018-06-23T21:30:58.367" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10309" PostId="7744" Score="1" Text="Are you familiar with path tracing? It lends itself well to a tail-recursive design, as you can accumulate the total radiance and &quot;throughput&quot; (product of reflectances) along the path. You would not have a for-loop here, since the path does not branch into multiple child rays—only a top-level for-loop to fire many paths per pixel." CreationDate="2018-06-24T03:48:37.443" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10311" PostId="7744" Score="0" Text="Yes, I have a path tracer which uses tail recusion. But Its not the same as montecarlo sampling imho. Any indication that it is (mathematically) equivalent is  most welcome" CreationDate="2018-06-24T10:06:23.910" UserId="8920" ContentLicense="CC BY-SA 4.0" />
  <row Id="10314" PostId="7735" Score="0" Text="Vectors representing directions have their `w` or fourth component set to `0`. This is because directions can't be translated. If you multiply a matrix with a direction having `w = 1` it's `x,y,z` will change depending on the translation component which you obviously don't want." CreationDate="2018-06-25T03:32:13.230" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10315" PostId="7735" Score="0" Text="That was not the issue however" CreationDate="2018-06-25T03:38:04.640" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10321" PostId="7751" Score="0" Text="There is definitly a problem with the matrix, because it is not set to an identity matrix, even when i call `glm::mat4 model = glm::mat4(); `, so the translation isn't happening, and the triangle is not drawn." CreationDate="2018-06-26T09:48:50.120" UserId="8935" ContentLicense="CC BY-SA 4.0" />
  <row Id="10323" PostId="7751" Score="0" Text="Yes i added the `glUseProgram(shader);` before ;`uniformModel = glGetUniformLocation(shader, &quot;model&quot;);` but i think now the problem is on the cpu side because when i breakpoint the render loop the matrix is always [0,0,0,0] instead of identity matrix" CreationDate="2018-06-26T09:57:48.123" UserId="8935" ContentLicense="CC BY-SA 4.0" />
  <row Id="10325" PostId="7747" Score="0" Text="Sorry for the late reply and thanks for the answer, cleared up a lot of things. When you are talking about the coherence problem in path tracing, I assume you are talking about the rays generated for global illumination? Because the primary rays sent from the pixel to the first object hit will be closer thus no coherence problems there." CreationDate="2018-06-26T10:42:28.500" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10326" PostId="7754" Score="1" Text="I don't know anything about Veldrid, but usually this is accomplished by having a single full-screen quad with a trivial geometry shader." CreationDate="2018-06-26T13:43:47.550" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10327" PostId="7754" Score="0" Text="@DanHulme Thanks, but how would I set the uniforms on the fragment shader?" CreationDate="2018-06-26T13:45:54.653" UserId="8125" ContentLicense="CC BY-SA 4.0" />
  <row Id="10328" PostId="7747" Score="0" Text="Yeah, the sampling rays over the hemisphere, which are randomly generated each time. Since you have to run shaders/kernels in batches of 32 or 64, the whole batch will take as long as the longest path unless you do some kind of sorting and deferred traversal. If you add in the memory bandwidth cost of all the rays heading in different directions it's pretty easy to get bogged down even on GPU" CreationDate="2018-06-26T14:40:12.057" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10329" PostId="7747" Score="0" Text="Hmm, this actually seems like a nice idea to handle direct lighting in first pass and indirect in the second pass after sorting all the rays generated and sorting them in the first one. Keeping the question open for a while longer in case any comes up with anything else." CreationDate="2018-06-26T16:00:12.917" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10330" PostId="7754" Score="1" Text="From a quick look at the docs you linked I'd guess you use GraphicsDevice.UpdateBuffer on a DeviceBuffer whose type is [BufferUsage.UniformBuffer](https://mellinoe.github.io/veldrid-docs/articles/shaders.html#uniform-buffer)." CreationDate="2018-06-26T16:11:44.287" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10331" PostId="7759" Score="0" Text="So, yes I know the &quot;true normal&quot; at any point on the surface, but microfacet BRDFs work by treating every point as having many very small facets whose normals are different from the true normal, and you pick one from a probability distribution. Connecting edges in bidirectional path tracing requires being able to determine the probability of generating point x + 1 given point x, which would depend on what normal you get from the distribution. I want to be able to say &quot;ok I need this particular normal to make this bounce happen&quot; so I can ask the distribution how likely that is." CreationDate="2018-06-27T21:57:37.860" UserId="8946" ContentLicense="CC BY-SA 4.0" />
  <row Id="10332" PostId="7759" Score="0" Text="For reflection, normalize(R + I) = N unless dot(R, I) == 0, in which case there's no collision anyway. Refraction is the tricky part." CreationDate="2018-06-27T22:03:43.867" UserId="8946" ContentLicense="CC BY-SA 4.0" />
  <row Id="10333" PostId="7759" Score="0" Text="haha. of course R+I. Man I wrote some very useless complicated stuff. I can recommend this blog post which is a very good treatment of fersnel. https://seblagarde.wordpress.com/2013/04/29/memo-on-fresnel-equations/" CreationDate="2018-06-27T22:07:44.943" UserId="8920" ContentLicense="CC BY-SA 4.0" />
  <row Id="10334" PostId="7760" Score="0" Text="Hmm according to a link here, you can rotate images through openVG api using the 2D GPU to achieve effects like cover flow animation.&#xA;&#xA;https://www.embedded.com/design/real-world-applications/4410726/2/Introduction-to-OpenVG-for-embedded-2D-graphics-applications" CreationDate="2018-06-28T15:19:48.083" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10335" PostId="7760" Score="0" Text="OpenVG is pretty dead though. I don't think anyone is supporting it any more." CreationDate="2018-06-28T15:27:24.903" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10336" PostId="7760" Score="0" Text="@wandering-warrior: Thank you for the link! I haven't found something like this before. It's surely a good starting point for diving deeper into all this strange stuff :-)" CreationDate="2018-06-28T15:31:08.580" UserId="6346" ContentLicense="CC BY-SA 4.0" />
  <row Id="10337" PostId="7760" Score="0" Text="@Dan Hulme: You may be right, but if it can be comfortably done using 2D, and that micro is some Cents cheaper, this will be an argument nobody would dare to disagree. Not in this business :-)" CreationDate="2018-06-28T15:41:12.860" UserId="6346" ContentLicense="CC BY-SA 4.0" />
  <row Id="10338" PostId="7761" Score="0" Text="I think he is more interested in the list of tasks a 3D GPU can perform that a 2D one can't. Based on that he wants to select whether to choose the 2D or 3D Gpu." CreationDate="2018-06-29T02:14:37.573" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10339" PostId="7736" Score="1" Text="Thanks. We actually tried stackexchange, but they told us that they changed their business model and would rather be suited for recurring job offers than for a one time offer." CreationDate="2018-06-29T02:59:25.777" UserId="8907" ContentLicense="CC BY-SA 4.0" />
  <row Id="10343" PostId="7765" Score="0" Text="Thank you so much! I've read that paper several times and somehow just glossed over that part." CreationDate="2018-07-02T17:39:13.573" UserId="8946" ContentLicense="CC BY-SA 4.0" />
  <row Id="10344" PostId="7750" Score="0" Text="Yeah, vec3 and std140 don't play nice at all! Anything I've done with compute shaders I just used vec4 and try to pack something else into the w if possible." CreationDate="2018-07-03T05:32:56.410" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10345" PostId="7761" Score="0" Text="@wandering-warrior: Actually I'd like to understand what a GPU does when I say, for instance, &quot;rotate object x by y degrees around the z axis&quot;. I can write an algorithm based on trigonometric functions for a CPU, but how does this compare to that highly sophisticated stuff they implement in a GPU? I, as just a user of APIs (be it OpenVG or OpenGL or whatever) don't know whether the rotation mentioned above needs 1MFlops, or 10, or 0.1 or whatever." CreationDate="2018-07-03T09:55:57.047" UserId="6346" ContentLicense="CC BY-SA 4.0" />
  <row Id="10346" PostId="7741" Score="0" Text="According to the above formula, pixels with lower depth value will not get colors from nearby pixels at higher depth value (i.e, no bleeding of bg into objects). However, after some careful thinking and experimenting, I found that colors don't bleed **only** near the focus plane, and hence for a plane even a little away from focus plane, color of the object bleeds into the background and vice versa. Pictorial explanation [here](https://imgur.com/a/cwB6fmW). How do I account for that?" CreationDate="2018-07-03T11:53:53.063" UserId="8748" ContentLicense="CC BY-SA 4.0" />
  <row Id="10347" PostId="7741" Score="0" Text="@SaravanabalagiRamachandran The algorithm I gave already has some of this effect, namely the part where the foreground bleeds on to the background. However, the part where the background bleeds in to the foreground (because the foreground only becomes part of the light that would hit that particular pixel) is not in the algorithm. This would make it a lot more complicated for only a small detail. Something that should also be considered, is that you do not know enough information (you only know the closest object's color) to correctly do DoF, so you have to make some compromises." CreationDate="2018-07-03T17:07:44.517" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10348" PostId="7761" Score="1" Text="@mic- that's somewhat difficult to answer. I thought you were doing this yourself though. I don't know much about embedded GPUs but you might wanna search for graphics pipeline on the wiki. On modern GPUs we have shaders and this rotation part is usually handled by the vertex shader stage of the graphics pipeline where you multiply the vertices (in the form of a vector) with a Rotation matrix. So it's essentially a vector-matrix multiplication for every vertex you have." CreationDate="2018-07-04T03:14:36.907" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10352" PostId="7771" Score="1" Text="The first place to start would be to batch geometry by materials/API state changes in order to minimise the number of render state changes required to render the whole scene. You may want to consider spatial subdivision on top of that so you can cull geometry thats not visible." CreationDate="2018-07-05T08:14:28.520" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10354" PostId="7761" Score="0" Text="@wandering-warrior: I wish I had some more experience in this. I just got the task to find a &quot;suitable&quot; Microcontroller to do all this stuff. I'm a little bit surprised that no benchmarks seem to exist which cover examples like mine. Bullet points like &quot;one trillion triangles per second&quot; may be fine for the expert, but surely not for the beginner, respectively the user looking at this from a higher application level." CreationDate="2018-07-06T08:21:24.750" UserId="6346" ContentLicense="CC BY-SA 4.0" />
  <row Id="10355" PostId="7761" Score="1" Text="@mic Integrating a GPU is not really a beginner task. You really need help from whoever will be writing the software to know the performance needs." CreationDate="2018-07-06T08:45:43.387" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10357" PostId="7778" Score="0" Text="I see, how should I specify the parameters to get the equivalent of `Texture_array[0] = texture` then? In other words, how can I assign an entire texture to a single layer of a texture array?" CreationDate="2018-07-07T08:49:23.763" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10358" PostId="7778" Score="0" Text="An array texture *is* &quot;an entire texture&quot;. What you're doing is uploading a 2D slice to an array texture." CreationDate="2018-07-07T13:12:32.760" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10359" PostId="7778" Score="0" Text="Ok, how do I select which slice of the array texture to load the current byte array into?" CreationDate="2018-07-07T17:01:59.933" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10360" PostId="7778" Score="0" Text="@Makogan: If you only wanted to upload a single row of pixel data in a 2D texture, how would you tell OpenGL which row to upload to? That's your answer." CreationDate="2018-07-07T17:16:06.077" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10361" PostId="7778" Score="0" Text="I have never done it so I have no experience with that operation either. And it's quite clear that I have some miss understanding about how glTexSubImage3D works, since I was completely convinced that passing 0 as a parameter would instruct OpenGL to load the pixel data into the 0th layer of the specified texture. &#xA;&#xA;So I would like to request a more straightforward answer if possible." CreationDate="2018-07-07T19:48:34.100" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10362" PostId="7722" Score="0" Text="I hadn't had a chance to actually look into this. I just did, I'm not really sure what to look for in the shaders, but I ran a project-wide search and found only a couple uses of mod, fmod, % (mostly used as a literal in strings) and for some reason C remainder used mostly in unrelated things. There was a line using modulo to normalize coordinates but I tried reverting that commit (from 2010 or so) and behavior was largely the same. Using highp also didn't really change anything." CreationDate="2018-07-08T18:25:13.870" UserId="8889" ContentLicense="CC BY-SA 4.0" />
  <row Id="10363" PostId="7722" Score="0" Text="Were is the vertex format specified and were are vertices prepared on the CPU side, that would be the first place I would look." CreationDate="2018-07-09T06:19:53.347" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10364" PostId="7780" Score="0" Text="The function OrenNayer() takes a light vector a normal vector, a roughness value and a view vector as a parameter." CreationDate="2018-07-09T06:21:42.787" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10365" PostId="7780" Score="0" Text="Look at how the variable 'rd' is setup, that's the view vector." CreationDate="2018-07-09T06:24:01.720" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10366" PostId="7780" Score="0" Text="but that vector points towards the scene, not the lens? It's the same direction used for the ray/sphere intersection tests." CreationDate="2018-07-09T07:07:55.600" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10367" PostId="7780" Score="0" Text="That should be the view vector." CreationDate="2018-07-09T07:24:31.887" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10368" PostId="7783" Score="0" Text="thanks for the answer I'll had a closer look to your answer. I need to have rectangles for a reason of weight. I want (like for the example I've shown above) to have 9 rectangles (so 9 {x, y , width, height} coordinates) instead of having a large amount of pixels coordinates or lines (which are also rectangles when you think about it)" CreationDate="2018-07-09T07:25:44.937" UserId="6844" ContentLicense="CC BY-SA 4.0" />
  <row Id="10369" PostId="7780" Score="0" Text="yes? So this (https://i0.wp.com/upload.wikimedia.org/wikipedia/commons/thumb/e/ed/BRDF_Diagram.svg/800px-BRDF_Diagram.svg.png)&#xA;isn't an accurate diagram?" CreationDate="2018-07-09T07:28:50.263" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10371" PostId="7784" Score="0" Text="duplicate of https://computergraphics.stackexchange.com/q/259/137 and https://computergraphics.stackexchange.com/q/280/137" CreationDate="2018-07-09T08:18:19.633" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="10372" PostId="7784" Score="4" Text="Possible duplicate of [Is a constant condition more costly than switching shaders?](https://computergraphics.stackexchange.com/questions/280/is-a-constant-condition-more-costly-than-switching-shaders)" CreationDate="2018-07-09T08:18:41.110" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="10373" PostId="7779" Score="0" Text="IMHO, the alpha channel *should* always be linearand treated as such, though I've no idea what Unity is doing.&#xA;&#xA;Oh!  It just occurred to me: are *you* expecting a *perceptually linear* fade-out? I.e. Over, say, 16 frames, frames 4, 8, &amp; 12 should *look*, respectively, 3/4s, 1/2, and 1/4 as bright as the original?" CreationDate="2018-07-09T08:24:58.057" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10374" PostId="7779" Score="0" Text="Yeah that's what I mean. When using the gamma rendering mode that's what happens, but in linear mode it gets bunched to one end - rendering a black quad over a white one at .95 alpha gives a final pixel value of 63,63,63." CreationDate="2018-07-09T08:34:17.763" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10375" PostId="7779" Score="0" Text="So blending a black over white with alpha=0.95 implies you want something that's only 5% of &quot;fully bright&quot;? &#xA;&#xA;Assuming an sRGB mapping, then 61 (i.e. hex 3D) __is__ 5% brightness. If your display is reasonably calibrated, you can verify this by creating a, say 5x4 black image, set one pixel to white, and then tile that to a suitable size, e.g 200x200 pixels.  Then draw a, say, 50x50 square in the middle and fill it with 0x3D3D3D.   Take a few steps back, squint, and they should be almost indistinguishable. (Again it relies on your monitor behaving correctly).&#xA;Maybe I should post the image." CreationDate="2018-07-09T13:37:01.920" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10376" PostId="7722" Score="0" Text="game/glutil.hh and glutil.cc. As I said on the bounty explanation, I know I'm currently doing some outdated/inefficient things but I'd rather fix it first and then correct those." CreationDate="2018-07-09T15:03:49.263" UserId="8889" ContentLicense="CC BY-SA 4.0" />
  <row Id="10377" PostId="7722" Score="0" Text="Yesterday I was playing around with using colors as debugging (as explained on several answers scattered around the web) and noticed that I'm getting widly different values (and representations for most things when comparing MacOS and Windows) Also, while using colors I see them making out the shapes of the distorted graphics as they were in the first screenshot. Definitely the vertex position is being affected as is the normal. In mac, normals are always 0, in Windows, they're... not." CreationDate="2018-07-09T15:25:20.420" UserId="8889" ContentLicense="CC BY-SA 4.0" />
  <row Id="10378" PostId="7722" Score="0" Text="If this is the draw call issued every frame then providing buffer with data and then pointing this buffer to vao and disabling it every frame doesn't make much sense, it should be done once. Also, I'd suggest splitting your buffer into multiple buffers containing positions, normals etc to simplify stride and pointers to your data." CreationDate="2018-07-09T19:05:04.243" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="10379" PostId="187" Score="0" Text="BMP, being a binary format, is not a good counterpart to OBJ. A better match would be [XPM](https://en.wikipedia.org/wiki/X_PixMap), which is textual." CreationDate="2018-07-09T19:50:53.160" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="10380" PostId="7782" Score="1" Text="I don't see the point of adding this answer when it is pretty much exactly what NicolBolas already said. His answer is the solution to the problem you asked for and ought to be the accepted answer, not one that simply repeats what he said and adds information that is irrelevant to the question asked." CreationDate="2018-07-09T21:54:04.690" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10381" PostId="7722" Score="0" Text="@narthex I know it doesn't make much sense; and I intend to fix it eventually; I just haven't gotten around to it because on a previous attempt to port the code to Core OpenGL 3.3 (now living on a stale branch here, https://github.com/performous/performous/tree/issue/migrate_to_opengl_33/game) I fixed those issues but it did nothing for the issue posed in the original question." CreationDate="2018-07-09T23:58:26.373" UserId="8889" ContentLicense="CC BY-SA 4.0" />
  <row Id="10382" PostId="7786" Score="0" Text="Keep in mind that light intensity falls off with the square of distance, so neither of them would generate white, unless the intensities were set appropriately." CreationDate="2018-07-10T02:17:40.093" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10383" PostId="7786" Score="0" Text="I wouldn't call the cylinder rays, more like infinitely thin rays that either fully hit or fully miss geometry." CreationDate="2018-07-10T02:25:43.300" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10385" PostId="7782" Score="0" Text="The added information is not irrelevant, the question as the title is posted asks, how to assign a texture to a single entry of a texture array. Nicol Bolas' answer correctly identifies the mistake I made. However he does not say how to solve it or how to achieve the final, desired result of `texture_arrray[0]=texture`&#xA;&#xA;As a matter of fact I asked Nicol Bolas in the comments how to actually do what I was attemtping. If you read the comments you noticed there's no direct answer. So after figuring out how to solve the issue I made this answer which ACTUALLY answers the question in the title." CreationDate="2018-07-10T04:08:51.377" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10386" PostId="7782" Score="0" Text="Take into account that I actually get mroe points by awarding Nicol bolas' answer than I do by accepting my own. If I haven't accepted that answer as the actual answer is because I think his answer is incomplete as it does not fully and directly answer the question. It just identifies the problem without explaining how to achieve the desired result." CreationDate="2018-07-10T04:13:29.770" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10389" PostId="7789" Score="0" Text="FWIW the self-intersection problem is sometimes referred to as (surface) acne. That might help with searching for solutions, e.g. : https://graphics.stanford.edu/pub/Graphics/RTNews/html/rtnews2c.html" CreationDate="2018-07-10T09:12:31.327" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10390" PostId="7790" Score="0" Text="*&quot; but with the performance advantage of doing the same simple algorithm many times.&quot;* ... and the performance *disadvantage* of doing more rays ;-)" CreationDate="2018-07-10T09:14:22.217" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10391" PostId="7790" Score="1" Text="@SimonF I'd rather do four simple rays with no edge cases than one complex cone with special handling for partial occlusion; more if I have a GPU to do them on." CreationDate="2018-07-10T09:16:24.013" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10392" PostId="7788" Score="1" Text="Yes, this *is* cumbersome. But it has already been explained in the answers to [this question](https://computergraphics.stackexchange.com/q/7773/6) that there is no way for OpenGL to directly handle those varying sizes in a texture array. This of course assumes that you *need* a texture array to begin with and aren't just using it for storage convenience. Otherwise texture views *might* offer some flexibility in reinterpreting an existing array layer. But in that case you of course can't have *array texturing* and would just be stuck with individual textures again." CreationDate="2018-07-10T09:54:04.210" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10393" PostId="7790" Score="0" Text="No worries :-) It was a tongue-in-cheek remark. FWIW 30 years(!) ago I was developing an Amantides-style cone ray tracer (https://dl.acm.org/citation.cfm?id=808589).  There are certainly pros and cons to it compared to simple rays." CreationDate="2018-07-10T10:39:28.787" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10394" PostId="7779" Score="0" Text="RE: *&quot;EDIT: this is the result I get after blending a .95 alpha black quad over a pure white button. It could be bad gamma on my display as Simon F suggested in the comments, but it looks a lot brighter than 5% to me.&quot;*&#xA;Ahh but our visual system is non-linear - much as our hearing is too. For example, the analogue volume control on a hifi amplifier will use a logarithmic potentiometer rather than a linear one (https://electronics.stackexchange.com/questions/101191/why-should-i-use-a-logarithmic-pot-for-audio-applications).  You simply need to change your per-frame alpha non-linearly." CreationDate="2018-07-10T10:47:25.557" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10395" PostId="7779" Score="1" Text="Ok that makes sense. So a pixel value of 63 would be physically around .05 brightness on a properly calibrated monitor, but perceptually it will appear brighter. Thanks for your help." CreationDate="2018-07-10T11:15:22.883" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10396" PostId="7790" Score="0" Text="@SimonF Oh yeah, that trade-off was a lot different before SIMD and then GPUs made it cheap to trace a zillion coherent supersampling rays." CreationDate="2018-07-10T11:55:14.243" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10398" PostId="7788" Score="1" Text="Possible duplicate of [How do you load multiple textures into an array texture with OpenGL?](https://computergraphics.stackexchange.com/questions/7773/how-do-you-load-multiple-textures-into-an-array-texture-with-opengl)" CreationDate="2018-07-10T22:46:54.273" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10400" PostId="7793" Score="0" Text="Looks like Gaussian blur" CreationDate="2018-07-11T02:12:46.437" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10401" PostId="7797" Score="2" Text="Ambient is a (rough) approximation of indirect lighting. For example, an outdoor object would be lit by the sun but the surfaces facing away from sun would be black if using only lambert shading, we can use dark gray for the ambient colour to mimic the effect of indirect lighting from the ground and sky." CreationDate="2018-07-12T02:41:55.637" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10402" PostId="7780" Score="0" Text="@PaulFerris The diagram is accurate. You say the shader 'totally ignores the rule for `wo` and `wi`'. Where do you expect them to see in the shader code? How would the code need to change if it took into account the rule for `wo` and `wi` ?" CreationDate="2018-07-12T03:47:28.037" UserId="5353" ContentLicense="CC BY-SA 4.0" />
  <row Id="10403" PostId="7780" Score="0" Text="Sorry, I miswrote before (and definitely didn't mean to come across that strongly). iq's BRDF uses his camera direction without flipping, so it still points into the scene. That goes against the diagram, which has $\omega_o$ pointing towards the camera. Yet it works fine, and that's where I'm unsure." CreationDate="2018-07-12T05:23:49.907" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10404" PostId="7798" Score="0" Text="Mhmm, the rule holds for the light vector, and I knew about the dot-product already. I should have said iq's view-vector points into the scene and opposes the camera ray in the diagram, that's what's confusing me." CreationDate="2018-07-12T05:35:13.553" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10405" PostId="7785" Score="1" Text="That would depend on the hardware that'll be running the threads, and how exactly your algorithm is implemented (its hardware usage). If you experiment with the different thread numbers and run your hardware vendor's performance profiler tools, you can get an idea of how the thread groups are utilizing your hardware in general. To my (little) experience, there's no general solution to this and it always depends on the hardware and the shader code specifics." CreationDate="2018-07-12T06:32:48.323" UserId="5353" ContentLicense="CC BY-SA 4.0" />
  <row Id="10406" PostId="7785" Score="0" Text="Thank you. I recently heard that it was recommended to use a multiple of the GPU's &quot;Thread Warp&quot;/&quot;Wavefront&quot; size, but that this should not prevent from doing tests with the specific shader and HW" CreationDate="2018-07-12T07:08:55.367" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="10407" PostId="7785" Score="1" Text="Exactly. At the end of the day, the code will run on the hardware and the hardware will determine the performance. Understanding the resource usage of a shader program through the right tools and figuring out how the algorithm scales over different architectures / configurations through little experimentation is the way to go in my humble opinion. It would be interesting to hear from the more seasoned programmers though." CreationDate="2018-07-12T07:51:04.527" UserId="5353" ContentLicense="CC BY-SA 4.0" />
  <row Id="10408" PostId="7793" Score="0" Text="@PaulHK I think the question is about the way it's used, to pad out the image, rather than about the type of blur itself." CreationDate="2018-07-12T08:41:42.037" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10409" PostId="7722" Score="0" Text="Try using apitrace to see any errors you missed. You can also inspect the GL state at each API call with this tool. Maybe also GLSL Debugger could help." CreationDate="2018-07-12T11:21:23.207" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="10410" PostId="3589" Score="0" Text="Your second link in your last comment seems to be broken (redirects to main page of BitBucket)." CreationDate="2018-07-12T11:55:09.677" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="10412" PostId="3589" Score="0" Text="@Ruslan: Comments can't be edited, but the correct link is at &quot;bitbucket.io&quot; rather than &quot;.org&quot;. They changed their URLs since then." CreationDate="2018-07-12T13:27:26.503" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10413" PostId="7722" Score="0" Text="@Ruslan I was asking around #OpenGL on freenode the other day, and they helped me realize the bug is in the geometry shader used for Stereo3D (one of the devs REALLY likes stereo 3d)." CreationDate="2018-07-12T16:10:34.847" UserId="8889" ContentLicense="CC BY-SA 4.0" />
  <row Id="10414" PostId="7804" Score="2" Text="I don't have experience with Vulkan, but in OpenGL there's nothing to stop you binding the same buffer as a a vertex array buffer and a shader storage buffer at the same time, you just need to set memory barriers appropriately. I'd be surprised if Vulkan didn't let you do this. Presumably though you'll want to make a fresh copy of the data in the compute shader anyway? Like reading your original buffer and writing to a new one? Otherwise you'll be cross-producting vertices that may have already been transformed." CreationDate="2018-07-13T10:38:36.663" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10416" PostId="7815" Score="0" Text="I haven't actually written the model loading part at all so I'm still open to ideas. My main approach was to load any model/scene through assimp and set a post processing option which &quot;triangulates&quot; anything having more than 3 indices. This assures I only need to check for ray-triangle intersection. However according to assimp it won't modify lines and points. &#xA;&#xA;Thus I wanted to know if ignoring these points and lines would make any real difference to the appearance of the model?" CreationDate="2018-07-15T09:34:36.360" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10417" PostId="7815" Score="0" Text="@wandering-warrior That depends on the particular model and the format you're loading. Try it on some models you're interested in and see what happens. Chances are they won't contain any points or lines anyway." CreationDate="2018-07-15T09:41:04.080" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10418" PostId="7783" Score="0" Text="I've implemente your solution but it doesn't work yet. for the rectangle I was searching an implementation of [this](https://stackoverflow.com/questions/5919298/algorithm-for-finding-the-fewest-rectangles-to-cover-a-set-of-rectangles-without)" CreationDate="2018-07-15T09:57:12.323" UserId="6844" ContentLicense="CC BY-SA 4.0" />
  <row Id="10420" PostId="7809" Score="0" Text="[This blog post](https://anteru.net/blog/2018/more-compute-shaders/) offers a peek at what AMD GCN assembly looks like (which, unlike some other GPU vendors, is actually documented in public)." CreationDate="2018-07-15T20:32:14.387" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10421" PostId="7640" Score="5" Text="This answer is somewhat outdated/wrong. First, doing math on the texture coordinates before sampling is not a big source of performance issues. You might be thinking of _dependent_ texture fetches, where the texture coordinate depends on the output _of an earlier texture sample_ — that will serialize those samples, and because they have a long latency, it can greatly extend the latency of the whole shader. But that's not what we have here; all the sample coords just depend on a single shader input, not on other samples." CreationDate="2018-07-16T01:35:39.087" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10422" PostId="7640" Score="2" Text="Moreover, vertex interpolation isn't free either; GPUs have limited space for interpolated values (using too many may hurt occupancy), and depending on hardware some pixel shader instructions may be needed to perform the actual interpolation anyway. So it's not necessarily a win to move a bunch of values into interpolators vs using a small amount of math to calculate them directly in the pixel shader." CreationDate="2018-07-16T01:38:08.693" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10423" PostId="7640" Score="0" Text="It may depend on exact hardware and OS. My experience has been that removing any sort of indirect texture fetch, not just dependent, has generally helped performance, particularly on mobile hardware. But I haven't used every piece of hardware out there, so my suggestion was mainly meant to address what I've experienced in case it was helpful." CreationDate="2018-07-16T01:39:36.893" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10424" PostId="7811" Score="2" Text="What distance do you use for your near &amp; far clip planes?" CreationDate="2018-07-16T03:09:50.933" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10425" PostId="7811" Score="1" Text="There are a few ways of solving this, as PaulHK mentioned, tweaking the near plane of your camera projection can help solving depth precision issues, other solutions: reversed z and logarithmic depth buffer. Here is a nice article that shows the precision of the different algorithms: https://developer.nvidia.com/content/depth-precision-visualized" CreationDate="2018-07-16T11:19:29.007" UserId="7107" ContentLicense="CC BY-SA 4.0" />
  <row Id="10426" PostId="7811" Score="0" Text="Planar distance is at 2000 for far and 0.01 for near" CreationDate="2018-07-16T18:49:40.343" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10427" PostId="7814" Score="0" Text="Thanks for your answer! It was rally helpful to me :). About the part with the shader example, I'm not sure if I understood well, but I think you meant that the shader, instead of being compiled and assembled, is only compiled and not assembled because instruction sets are different for each GPU, even from the same &quot;family&quot;." CreationDate="2018-07-16T21:25:05.737" UserId="9028" ContentLicense="CC BY-SA 4.0" />
  <row Id="10429" PostId="7813" Score="0" Text="I'll take a look. Thanks for the infos and the link :)." CreationDate="2018-07-16T21:28:55.600" UserId="9028" ContentLicense="CC BY-SA 4.0" />
  <row Id="10430" PostId="7814" Score="1" Text="@sebastienfinor What I'm trying to say about that is that there isn't necessarily an intermediate &quot;assembly language&quot; representation in the compilation process." CreationDate="2018-07-17T08:17:21.873" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10431" PostId="7821" Score="0" Text="When you say &quot;specify those losses&quot;, do you mean as an input to the algorithm, or are you looking for a way to measure how good the final result is?" CreationDate="2018-07-17T09:39:23.530" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10432" PostId="7821" Score="0" Text="@Dan Hulme: I mean the final results. My idea is that it shall be independent from the algorithm." CreationDate="2018-07-17T09:43:32.197" UserId="6346" ContentLicense="CC BY-SA 4.0" />
  <row Id="10433" PostId="7821" Score="0" Text="You can use your own quantisation tables in JPEG. The 0-100 quality level is software-defined and depends on which software you're using." CreationDate="2018-07-17T09:55:22.337" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10434" PostId="7821" Score="0" Text="@PaulHK: Of course. I just wanted to show that I've somehow understood how the actual compression works ;-)" CreationDate="2018-07-17T10:39:53.353" UserId="6346" ContentLicense="CC BY-SA 4.0" />
  <row Id="10435" PostId="7824" Score="0" Text="This is not an answer as I do not know of any sample implementations, however Disney did release something very similar to this. It is called Ptex. http://ptex.us/  Definitely worth looking at." CreationDate="2018-07-17T17:00:46.827" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10437" PostId="7798" Score="2" Text="@PaulFerris Well yeah, I agree with you. I think the shader is buggy and the correction would be `    float nv = dot(n, -v);` as the parameter passed in is a ViewRay, pointing  away from the camera. In this sense, the nv should be between `n` and `-v`. Doing that also fixes the ugly horizontal line on one of the spheres." CreationDate="2018-07-18T03:38:58.433" UserId="5353" ContentLicense="CC BY-SA 4.0" />
  <row Id="10438" PostId="7825" Score="0" Text="Could you take a still image from your video and add it to the question? You're always more likely to get answers if people can see the problem without having to click through to external links." CreationDate="2018-07-18T08:22:39.087" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10439" PostId="7825" Score="0" Text="This looks a bit like the typical self-intersection issues you sometimes get with ray tracers, were the intersection point of a ray is sometimes slightly inside the object you are testing (rounding error), so resuming from that point can result in a instant  hit on the same object. I'm guessing those rings are shadows ?" CreationDate="2018-07-18T09:29:17.860" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10440" PostId="7825" Score="0" Text="@PaulHK I am not sure that is the case. I am adding an offset to the rays at the start, to ensure that they begin outside of the object." CreationDate="2018-07-18T20:08:20.220" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10441" PostId="7825" Score="0" Text="How are you offsetting the rays? Along the ray direction or the hit surface normal ?" CreationDate="2018-07-19T02:49:55.410" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10442" PostId="7825" Score="0" Text="@PaulHK along the direction" CreationDate="2018-07-19T04:24:10.683" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10443" PostId="7827" Score="1" Text="As a first start, the keyword you might be looking for is **contour finding/detection**. The point is finding connected curves along the black-white-discontuities in a binary image. There's a multitude of algorithms for that, but looking for that term might give you some first introductions to the problem." CreationDate="2018-07-19T09:05:04.073" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10444" PostId="7829" Score="1" Text="You're the only one with the source code, so we can't debug this problem for you. The obvious suggestion would be to check that they're both using the same handedness for their co-ordinate systems. OpenGL is right-handed until you get to screen-space, which is left-handed." CreationDate="2018-07-19T09:05:46.400" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10445" PostId="7825" Score="1" Text="show your code, and take a still of the pattern" CreationDate="2018-07-19T14:18:51.683" UserId="6530" ContentLicense="CC BY-SA 4.0" />
  <row Id="10447" PostId="7798" Score="0" Text="Mmmm, ok. I missed the line and didn't even think to test with negative rd. Thanks for the update :)" CreationDate="2018-07-20T00:23:46.637" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10449" PostId="7825" Score="1" Text="If you change your shadow colour to say red, does the ringed artefacts also turn red ? Just watched your video again and it seems the artefact gets stronger the further away from the centre of the mesh the camera goes. What is the dimensions of your scene ?" CreationDate="2018-07-20T02:38:32.893" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10455" PostId="7836" Score="0" Text="I see. So, one way to make a scene not appear distorted on a hemisphere is to render the scene with a fish-eye camera at 180 degrees?" CreationDate="2018-07-22T05:16:05.400" UserId="9050" ContentLicense="CC BY-SA 4.0" />
  <row Id="10457" PostId="7836" Score="0" Text="Indeed, that would be a way to eliminate distortion." CreationDate="2018-07-22T23:03:05.080" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="10464" PostId="7851" Score="0" Text="I don't think your question is quite a duplicate, but my answer to [Ray Tracing Shadows: The Shadow Line Artifact](https://computergraphics.stackexchange.com/questions/4986/ray-tracing-shadows-the-shadow-line-artifact) should help you." CreationDate="2018-07-24T08:02:59.817" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10465" PostId="7829" Score="1" Text="The top left image matches the bottom right and the top right matches the bottom left. Are you sure you are actually getting mirroring and not just a 180 rotation?" CreationDate="2018-07-23T18:18:47.710" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="10466" PostId="7829" Score="0" Text="A 180 rotation is a reflection" CreationDate="2018-07-23T19:44:46.363" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10467" PostId="7829" Score="1" Text="A rotation is actually not a reflection. The determinant of a matrix that does rotation will always be 1, and the det of a matrix that does reflection will alwayd be -1." CreationDate="2018-07-23T22:43:08.727" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="10468" PostId="7829" Score="0" Text="You are correct I was too fast to answer. However in the images yes, it is a mirroring effect and nota 180 rotation. The reason why I know (and should have put it on the images) is that, on teh second floor, there is a shield in one side and a door in the other. In the images aboive you see the shield in both shading versions, which would not be possible with a 180 rotation." CreationDate="2018-07-23T23:14:43.173" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10470" PostId="7855" Score="0" Text="I'm not sure this is totally robust. What happen if we have 2 adjacent cubes forming a rectangle and our ray slightly penetrates one cube near the shared boundary of both these cubes, while we ignore the cube we penetrate we can still hit the neighbour cubes side on the way out to the light. For this case (cubes) it may be easier to determine which cube face the initial ray enters and only consider the voxels on the opposite side of that plane altogether." CreationDate="2018-07-24T08:37:03.713" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10471" PostId="7855" Score="0" Text="Yeah, I still think getting the offset right is best, especially if the non-voxel raytracer is spawning rays from the real geometry to be traced against the voxelisation. That's never going to be perfect." CreationDate="2018-07-24T08:41:17.670" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10472" PostId="7846" Score="1" Text="This means that when I set parameters for `GL_TEXTURE_2D` with `glTexParameter` while e.g. `GL_TEXTURE0` unit is active, I'm actually setting the parameter for the texture object currently bound in `GL_TEXTURE0`'s `GL_TEXTURE_2D` ''slot&quot;. And these parameters will remain in effect if I bind the same texture object to another texture unit. Correct?" CreationDate="2018-07-24T16:29:51.230" UserId="7282" ContentLicense="CC BY-SA 4.0" />
  <row Id="10473" PostId="7846" Score="0" Text="Yes. That's how *all* functions that set state into OpenGL objects work." CreationDate="2018-07-24T16:58:54.067" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10475" PostId="7857" Score="0" Text="Is it possible some intersections / shadow-ray-origins are landing inside your shape ? If your surfaces are 1 sided then from the inside you would almost certainly get a path to your large light, which would explain the bright pixels. I can see you have ring-artifacts on the sphere with explicit light sampling, which is usually a symptom of numerical imprecision. How is your shape defined? Is ray marching involved?" CreationDate="2018-07-25T08:50:16.977" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10476" PostId="7857" Score="0" Text="Yes, everything's ray-marched directly inside the shader; the fractal is a quaternionic julia DE and the sphere is just the standard spherical SDF. I thought the same thing after I posted this, so I changed to an $8\epsilon$ offset instead. It fixed the ringing on the sphere, but didn't do anything for the noise on the julia's surface." CreationDate="2018-07-25T10:40:22.317" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10479" PostId="7861" Score="5" Text="The code you're quoting there is really old OpenGL! If you're new to the API and just learning, do yourself a favor and learn modern-style OpenGL (at least version 3.3, core profile). The only time you should need to use display lists or glBegin is if you're maintaining or modding apps from 10+ years ago." CreationDate="2018-07-26T17:04:34.217" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10480" PostId="7863" Score="0" Text="Check these links.&#xA;https://stackoverflow.com/questions/8704801/glvertexattribpointer-clarification&#xA;&#xA;https://stackoverflow.com/questions/23314787/use-of-vertex-array-objects-and-vertex-buffer-objects" CreationDate="2018-07-26T18:33:55.170" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10481" PostId="7857" Score="0" Text="Very thin sections of geometry can be a problem for ray marchers as you can step through them and if using a SDF you can get increased distance when the ray steps through a thin wall making this error hard to detect. Another issue is acute angle V shaped geometry were a ray could land in the part of the V where both faces are so close together that adding an epsilon from the surface of one side of the V means you end up behind the neighbouring surface." CreationDate="2018-07-27T02:02:55.547" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10482" PostId="7857" Score="0" Text="Mmm, that's why I wanted to adapt the offset to the local roughness. Tighter v-shapes should have smaller epsilons." CreationDate="2018-07-27T02:32:38.367" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10483" PostId="7857" Score="0" Text="I guess that doesn't stop shadow rays in rough areas being occluded after they leave, though. What would you suggest?" CreationDate="2018-07-27T02:35:09.517" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10484" PostId="7866" Score="1" Text="By definition, every API in C and C++ is Imperative, since those languages are Imperative. Even OOP is merely a subset of Imperative programming." CreationDate="2018-07-27T02:35:31.770" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10487" PostId="7857" Score="0" Text="That's a good question, I would try maybe a few small fixed ray march steps instead of adding an epsilon. If your shape is defined in such a way that you can test if a point is inside or outside you can consider the ray to be occluded if its epsilon adjusted start position is inside." CreationDate="2018-07-27T05:39:53.150" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10488" PostId="7857" Score="0" Text="So...you're suggesting I add an epsilon, intersection-test, add a smaller epsilon in the other direction if occluded, test again, and so on until the starting point is unoccluded? That's an interesting idea, thanks. I'll set things up and tell you how it goes :)" CreationDate="2018-07-27T08:25:06.000" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10489" PostId="7857" Score="0" Text="RE: inside/outside checks - yes, I'm exclusively using signed distance fields. Every shape is positive on the outside and negative on the inside." CreationDate="2018-07-27T08:26:56.820" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10490" PostId="7868" Score="0" Text="Does that 1d banding only affect the edges of your voxel map? it looks suspiciously like texture clamping." CreationDate="2018-07-27T09:30:36.830" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10491" PostId="7857" Score="0" Text="Finally got around to drawing some more tests, that solution is amazing! Way less noise on the Julia (https://imgur.com/a/FtSzCNr) and only faint ringing on the sphere (https://imgur.com/a/HQtkk77). Would love it if you could write up that last suggestion as an answer so I can upvote/accept :D." CreationDate="2018-07-27T12:50:06.247" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10492" PostId="7868" Score="0" Text="@PaulHK that's the only place where I have seen it, but it shouldn't cause texture clamping because the light is inside the voxel map, so the light only needs to travel inside the voxel map." CreationDate="2018-07-27T17:56:39.087" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10493" PostId="7868" Score="1" Text="In voxel raymarching there can be a number of issues. Your step size is too big and some rays miss voxels. Your offset is small so the self shadowing occurs when ray at starting point immediately hits voxel. Whole ray length from start to end is too short. Quality can depend on number of samples, quality of samples  and voxel size." CreationDate="2018-07-27T20:04:46.460" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="10494" PostId="7869" Score="0" Text="&quot;how would I handle the aspect ratio?&quot; How do you want to handle it? You can fit it all in. You can do a uniform scale until the output is filled. You can non-uniformly stretch in an infinite number of ways. What do you want the output to look like?" CreationDate="2018-07-28T00:40:39.677" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10495" PostId="7869" Score="0" Text="Aspect ratio should be independent of render target resolution. You would render to your texture using information from your view frustum (view-projection matrix) which contains the aspect ratio, this should be set to the same aspect ratio as your window. That will ensure once the texture is stretched to your window it will have a matching aspect ratio." CreationDate="2018-07-28T06:24:22.327" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10496" PostId="7867" Score="0" Text="Some helpful diagrams: https://github.com/jose-villegas/VCTRenderer" CreationDate="2018-07-28T08:50:16.150" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="10497" PostId="7869" Score="0" Text="@user1118321 - Check my edited post.&#xA;and, PaulHK - Yes the kernel will use the view frustum's aspect ratio. I want to know if OpenGL automatically stretches or is there a function for this. Refer to the update post" CreationDate="2018-07-28T10:22:03.883" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10499" PostId="7869" Score="0" Text="OpenGL doesn't come with any functions to match aspect ratios. In your case you may just want to draw a fullscreen quad, which is the easiest primitive to setup (vertices are NDC coords between -1 to 1, no transformation in vertex shader required)" CreationDate="2018-07-30T02:10:58.683" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10500" PostId="7869" Score="0" Text="Hmm, isn't there any other method (I mean an algorithm not openGL function) to fit the image maintaining the aspect ratio? As in a way of determining the amount of space covered by a particular resolution image in a particular low size window when made to fit to the window." CreationDate="2018-07-30T02:14:46.750" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10503" PostId="7869" Score="0" Text="Thanks - I'll look into it. Also what in your opinion would be faster. blitting once or texturing on quad. While the first lets me bypass the shader pipeline, I've heard that blitting is slow. Some people say it's faster tho." CreationDate="2018-07-30T02:39:56.820" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10506" PostId="7875" Score="0" Text="Thanks for finding that out. Quite annoying when processing generic DDS to hit a variant which is very hard to detect in the data itself :/" CreationDate="2018-07-30T13:08:31.503" UserId="9096" ContentLicense="CC BY-SA 4.0" />
  <row Id="10507" PostId="7876" Score="0" Text="There's quite a few &quot;magic&quot; numbers in your code, can you explain what the constants are used for ?" CreationDate="2018-07-31T02:30:27.470" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10508" PostId="7857" Score="0" Text="Another interesting thing you could try for shadow rays is reverse ray-marching, marching from light to surface instead of the traditional path from surface-&gt;light. One headache with sphere raymarching is that its easier to walk into the field than it is to walk out of it, starting a ray near a surface results in small distances so you need several iterations to 'climb' out of it, you're only interested in the first occluder anyway so light-&gt;surface should result in less iterations over all." CreationDate="2018-07-31T03:52:49.400" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10509" PostId="7857" Score="0" Text="Wouldn't that present the same problem for points below the surface?" CreationDate="2018-07-31T05:47:13.187" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10510" PostId="7857" Score="0" Text="It avoids making many small steps at the start. If we didn't have to worry about rounding then a ray from the surface would have a SDF distance of close to 0 so we would need to introduce epsilons/fixed steps to get things started, this step in itself can introduce artefacts. However walking it from the other direction doesn't have this problem as approaching from the outside means we start with large steps. Also since it will stop at the first occluder it should take less ray march steps globally." CreationDate="2018-07-31T06:05:48.860" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10511" PostId="7857" Score="0" Text="Also with WRT numerical imprecision, what happens if you start a ray slightly inside the surface, it will have a negative distance and a naive sphere marching would walk a ray in the wrong direction. At the very least you need abs(distance) on to ensure its always moving in the correct direction / away from surface." CreationDate="2018-07-31T06:08:49.463" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10512" PostId="7857" Score="0" Text="&gt;.&lt;. I can't believe I missed that, thanks.&#xA;What I meant was that subsurface points will be unreachable from the light source because light rays will hit the surface before they ever reach the point, and if you check within epsilon units of the point you re-introduce the precision errors from before." CreationDate="2018-07-31T06:15:04.020" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10513" PostId="7857" Score="0" Text="Yeah, I don't think you could ever fully fix SDF artefacts. Glancing rays are another issue (say a ray which is close and parallel to a surface would need a huge number of steps to escape and most ray marchers would hit some pre-defined max-steps limit and assume its either solid or empty, yet change the angle slightly so it 'climbs' out then you suddenly get different results. In a path tracer this would manifest as a source of random noise." CreationDate="2018-07-31T06:19:17.723" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10514" PostId="7857" Score="0" Text="eh, your help from before pushed things down a lot, and I wrote a small denoiser to minimize leftover speckling/fireflies :); here's a current sphere @ 4096spp (https://imgur.com/a/AynFlY2) and here's a current Julia (https://imgur.com/a/7Cu66gr) (same sample count)." CreationDate="2018-07-31T08:07:33.167" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10515" PostId="7857" Score="0" Text="RE: glancing rays - yeah, that's definitely another noise source. I don't really mind having some grain though, and I feel like ringing looks ok so long as it's subtle enough. Avoiding harsh black-on-white or white-on-black pixels is good enough for me :)." CreationDate="2018-07-31T08:10:46.593" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="10516" PostId="7876" Score="0" Text="The coefficient of 6 when calculating the ray origin r. Is simply to prevent voxel self intersection.&#xA;&#xA;The coefficient of 0.2 when moving the ray inside the inner loop is the step size of teh ray. Simply a value that gave good results in a decent amount of time.&#xA;&#xA;the coefficient called &quot;coeff&quot; is the aperture of teh voxel cone/pyrammid&#xA;&#xA;FInally multiplying by 0.8 and then adding 0.2 restricts the possible values of shadows to the interval (0.2, 1) which essentially prevents black shadows" CreationDate="2018-07-31T21:56:38.453" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10518" PostId="7876" Score="0" Text="The word &quot;pyramid&quot; has only one &quot;m&quot;." CreationDate="2018-08-01T19:34:55.987" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10520" PostId="7830" Score="1" Text="`cudaSetDevice(rand() % 4)`? (I'm kidding...mostly...)" CreationDate="2018-08-02T05:13:03.360" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10523" PostId="7830" Score="0" Text="I almost just settled for this myself but I got a headache thinking which seeding method would be best for load balancing" CreationDate="2018-08-02T15:14:53.833" UserId="9047" ContentLicense="CC BY-SA 4.0" />
  <row Id="10524" PostId="7887" Score="0" Text="Thanks for the great answer. Sorry I can't up-vote due to lack of reputation." CreationDate="2018-08-03T04:43:22.597" UserId="9113" ContentLicense="CC BY-SA 4.0" />
  <row Id="10525" PostId="7884" Score="0" Text="That's probably something only the people who have the source code can tell you." CreationDate="2018-08-03T04:55:56.710" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10526" PostId="7871" Score="0" Text="Did you run [a graphics debugger](https://www.khronos.org/opengl/wiki/Debugging_Tools) and inspect your variables?" CreationDate="2018-08-03T06:42:12.450" UserId="5353" ContentLicense="CC BY-SA 4.0" />
  <row Id="10527" PostId="6067" Score="0" Text="If you want to see a multi-platform renderer, you can [check this one out](https://github.com/ConfettiFX/The-Forge)" CreationDate="2018-08-03T06:54:59.147" UserId="5353" ContentLicense="CC BY-SA 4.0" />
  <row Id="10528" PostId="7871" Score="0" Text="I ran renderdoc" CreationDate="2018-08-03T19:32:04.777" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10539" PostId="7894" Score="1" Text="Check if your `Mat` is continuous in memory and not strided by using the `isContinuous` flag. Maybe OpenCV allocates extra bytes for padding, that's why it's running short?. Also check this link.&#xA;https://stackoverflow.com/questions/9097756/converting-data-from-glreadpixels-to-opencvmat" CreationDate="2018-08-06T09:05:44.927" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10540" PostId="7894" Score="0" Text="@gallickgunner thanks a ton it was indeed a problem with cv::Mat and glPixelStorei, your link was really helpful, how can I mark this as answered or make it point to the link?" CreationDate="2018-08-06T11:35:20.293" UserId="116" ContentLicense="CC BY-SA 4.0" />
  <row Id="10541" PostId="7894" Score="0" Text="Just quote part or whole of the answer and post the link and mark it yourself :)" CreationDate="2018-08-06T11:47:48.540" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10542" PostId="7897" Score="0" Text="You add a link but you don't say how it relates to your question. Please [edit] your question to explain fully what you understand and what you don't." CreationDate="2018-08-06T19:20:07.537" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10543" PostId="7898" Score="0" Text="I'm given camera center point, so should I use it as $e$?" CreationDate="2018-08-06T20:03:07.597" UserId="6029" ContentLicense="CC BY-SA 4.0" />
  <row Id="10544" PostId="7898" Score="0" Text="What exactly do you mean by the camera center? Cameras aren't usually defined using a center point. Maybe it's the eye position, check the diagram (it's the left most point from where all lines are emerging)" CreationDate="2018-08-06T20:12:52.190" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10545" PostId="7898" Score="0" Text="It's the center where the camera originates. The it also gives me $up$ vector, which points upwards from the camera, also an angle and a direction (where the camera points)." CreationDate="2018-08-06T20:14:11.807" UserId="6029" ContentLicense="CC BY-SA 4.0" />
  <row Id="10546" PostId="7898" Score="0" Text="Yes then that's the camera's eye position, I haven't ever heard of it called by the name camera's center. Rays will originate from that point. Camera's are usually defined by 3 basis vectors, Side, Up and Look_at (X, Y, Z) respectively. The 4th vector is the Eye. Together these 4 vectors can make up the camera's view matrix." CreationDate="2018-08-06T20:18:03.977" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10550" PostId="104" Score="4" Text="Note that as of 2018-08-07, it doesn't support anything higher than GLSL 1.2, and it's not actively maintained." CreationDate="2018-08-07T07:00:59.927" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="10551" PostId="7896" Score="0" Text="Thanks! I'll accept your answer, I just have one more question. The terminology is weird here, `With zero alpha meaning that the color does not contribute and partial alpha means that some percentage of the color contributes` - it contributes? to what? How could this be written more naturally? Could we say &quot;0 alpha meaning that the color does not contribute to the final pixel&quot;?" CreationDate="2018-08-07T07:37:55.900" UserId="9133" ContentLicense="CC BY-SA 4.0" />
  <row Id="10552" PostId="7890" Score="0" Text="I almost get the same result, but even with Wolfram Alpha I get 37.77 pixels, and 0.7082 mm with your example. Also, when using focal length in millimetres, a huge scaling occurs as the focal plane in metres reaches it.&#xA;When graphed as such, https://www.desmos.com/calculator/mlblfdvknw, it's noticeable. Is there a way to convert the units properly?" CreationDate="2018-08-07T09:32:18.720" UserId="9041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10553" PostId="7903" Score="0" Text="What do you mean by 5. Translate tx, ty. And explain what do you mean by correct image values for vertices? Do you want the image to fit to the crop area even after transformation, is that what you mean by correct?" CreationDate="2018-08-07T12:45:08.770" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10554" PostId="7903" Score="0" Text="By 5 I mean simply drag along x or y axis.&#xA;For the second question - yes and no. For now i want to compute image vertices and then try to fit it or to undo 5th (translate) operation." CreationDate="2018-08-07T12:55:36.233" UserId="9141" ContentLicense="CC BY-SA 4.0" />
  <row Id="10555" PostId="7903" Score="0" Text="Your image looks how I would expect it to look after it has been rotated around its own centre and then translated by some amount in both X &amp; Y ? What is your expected result ?" CreationDate="2018-08-07T13:03:09.403" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10556" PostId="7903" Score="0" Text="I created POC in my app to validate algorithm eg. I'm rotating image, scaling it and translating along both axis to fit my image's top left corner to crop's top left corner.&#xA;&#xA;My example response: [-4.5, -6.0][924.2, -1.5][-9.6, 1056.1][919.0, 1060.6]&#xA;&#xA;And as you can see first vertex is not even close to [0,0]&#xA;&#xA;I'm wondering that maybe translations are not applied in correct order.." CreationDate="2018-08-07T13:13:08.330" UserId="9141" ContentLicense="CC BY-SA 4.0" />
  <row Id="10557" PostId="7906" Score="0" Text="Thanks.. I think it would be great if you can use a picture to better describe the interpolation problem.." CreationDate="2018-08-07T14:41:00.567" UserId="2687" ContentLicense="CC BY-SA 4.0" />
  <row Id="10558" PostId="7905" Score="0" Text="The underlying principle of Bresenham (the error value getting accumulated and moved back to 0 as events happen) is nice but no-one seems to teach it" CreationDate="2018-08-07T14:58:54.173" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="10559" PostId="7906" Score="0" Text="@Bla... I updated my answer. Does that explain the interpolation problem, or at least the preference of using triangles that are not so long?" CreationDate="2018-08-07T15:27:52.403" UserId="5989" ContentLicense="CC BY-SA 4.0" />
  <row Id="10560" PostId="7896" Score="0" Text="Yes, &quot;does not contribute to the final color of the pixel&quot; would be a good way to say it, or maybe &quot;zero alpha meaning that the color has no influence on the final pixel color&quot;." CreationDate="2018-08-07T15:48:28.013" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10561" PostId="7903" Score="0" Text="I don't understand what do you mean, but are you sure your image's top left corner should coincide with the crop's? How are you validating this?" CreationDate="2018-08-07T18:47:58.017" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10562" PostId="7890" Score="0" Text="Sorry, I should've been more explicit. One way to do it is to convert all the millimeters to meters by dividing by 1000. Be sure you do that _before_ squaring or any other operation. You're right about the 37.77, I must have typo'd it when I did the calculation initially." CreationDate="2018-08-07T18:50:21.510" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10563" PostId="7890" Score="0" Text="And here's an updated version of your graph with the conversions: https://www.desmos.com/calculator/j2gcokbykz" CreationDate="2018-08-07T18:59:55.037" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10564" PostId="7903" Score="0" Text="I'm creating crop tool on mobile phone. You can drag image along X, Y axis, scale it and rotate it. I need to compute image vertices after all these translations. That is my problem and that is my question :)" CreationDate="2018-08-08T05:48:40.740" UserId="9141" ContentLicense="CC BY-SA 4.0" />
  <row Id="10568" PostId="7905" Score="0" Text="Yep, that fixes the floating point exception, now I just need to find the math logic. What algorithm you'd recommend?" CreationDate="2018-08-08T16:38:10.517" UserId="9143" ContentLicense="CC BY-SA 4.0" />
  <row Id="10569" PostId="7905" Score="2" Text="Well, if you want to draw lines one pixel at a time, Bresenham's algorithm is the right one. But why do you want to draw lines one pixel at a time? Any OpenGL implementation will draw them all at once for you." CreationDate="2018-08-08T17:04:40.040" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10570" PostId="7890" Score="0" Text="Many thanks! I was going from M to MM but I hadn't adjusted the scales properly" CreationDate="2018-08-08T21:00:31.167" UserId="9041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10574" PostId="5524" Score="0" Text="You could try doing a few passes of setting each pixel to the majority of its vicinity." CreationDate="2018-08-09T18:40:37.697" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="10575" PostId="7890" Score="0" Text="So...will you accept the answer again, since it seems to work now? :)" CreationDate="2018-08-10T03:03:35.257" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10582" PostId="7916" Score="0" Text="In what way is your laptop GPU worse? It's a higher-performance part and should work much better." CreationDate="2018-08-13T08:51:22.467" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10583" PostId="7916" Score="0" Text="Any 3D games look much better on my phone than on my PC." CreationDate="2018-08-13T08:57:43.767" UserId="9176" ContentLicense="CC BY-SA 4.0" />
  <row Id="10584" PostId="7916" Score="0" Text="Could you be more specific? Do they have better framerate, higher resolution, or something else? Are you using the open-source drivers on your laptop or NVidia's drivers?" CreationDate="2018-08-13T09:00:07.397" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10585" PostId="7884" Score="0" Text="@user1118321 We are all those people. https://github.com/google/skia" CreationDate="2018-08-13T09:04:47.887" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10587" PostId="7915" Score="0" Text="If your path tracer has a limited path length then what happens when you reach that limit, do you discard that path or accumulate it as black ? I would imagine a lot of paths in an unbiased tracer would miss that square light source." CreationDate="2018-08-13T12:21:27.137" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10589" PostId="7884" Score="0" Text="Then why is this even a question? OP can simply go look. Perhaps you could post that as an answer?" CreationDate="2018-08-13T15:49:11.270" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10590" PostId="7916" Score="0" Text="It depends on the settings. For example if I set the same resolution the frame rate on my phone seems better. I tried both open-source and Nvidia drivers and it didn't seem to make a big difference.&#xA;Anyway, for the sake of argument we can assume my computer doesn't have any dedicated graphics card. Is it possible to use my phone's gpu as an external one for my pc?" CreationDate="2018-08-13T16:10:58.063" UserId="9176" ContentLicense="CC BY-SA 4.0" />
  <row Id="10591" PostId="7915" Score="0" Text="@PaulHK The paths that miss the square light are accumulated as black. I think this causes the lack of bright colours: they are averaged out. Is that approach wrong?" CreationDate="2018-08-13T16:54:37.043" UserId="9170" ContentLicense="CC BY-SA 4.0" />
  <row Id="10592" PostId="7915" Score="0" Text="The images should be of similar brightness yes. If the path tracer isn't using direct light sampling (aka next-event estimation) then it will have a lot more variance but it should not affect the average. At a guess, maybe something is wrong/missing in a probability normalization somewhere." CreationDate="2018-08-13T18:15:12.280" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10597" PostId="7915" Score="0" Text="@NathanReed I have added some code to my question in order to illustrate how I shade intersection points. I have also provided some more details (in words) on how the path tracer works." CreationDate="2018-08-13T22:09:51.957" UserId="9170" ContentLicense="CC BY-SA 4.0" />
  <row Id="10598" PostId="7916" Score="0" Text="Almost certainly no. The amount of latency and bandwidth bottleneck would render it too slow to be usable. I don't think there exists software to proxy a GPU over USB or WIFI or whatever communication channel your phone has available." CreationDate="2018-08-14T03:50:57.623" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10599" PostId="7915" Score="0" Text="@Jeroen what do you mean by changing the sensitivity?" CreationDate="2018-08-14T04:40:12.773" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="10600" PostId="7919" Score="0" Text="You could take a sample at the RED/GREEN/BLUE wavelengths from the table to make a rough RGB for the White/Green/Red materials." CreationDate="2018-08-14T07:20:53.020" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10601" PostId="7919" Score="0" Text="@PaulHK - if i understand correct, you mean i should pick 3 wavelengths values from table 2 corresponding to each of the R/G/B. Then i should pick the reflectance values corresponding to them to get my RGB color for each of the `white` `red` and `green` columns?  I could do that but I'm actually writing a general interface using Assimp which loads obj/ply files and reads info from them.  Isn't this approach valid for only this particular case? I couldn't find a .obj format for cornell box. Is there any other way?" CreationDate="2018-08-14T07:35:09.023" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10602" PostId="7915" Score="0" Text="Does you direct illumination version use an ambient factor ? I don't see how your ceiling is a flat shade of gray; I guess your light is flush to the ceiling so no part of the surface on the ceiling has line-of-sight to the light source ?" CreationDate="2018-08-14T07:59:47.433" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10603" PostId="7919" Score="0" Text="It looks like this version of the Cornell box is used to validate spectral tracers and was derived from real world observations" CreationDate="2018-08-14T08:22:52.393" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10604" PostId="7919" Score="0" Text="would the one from POVRay be any use ? https://github.com/POV-Ray/povray/blob/master/distribution/scenes/radiosity/cornell.pov" CreationDate="2018-08-14T08:25:59.230" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10605" PostId="7919" Score="0" Text="Hmm, ok are there any other similar models or some other ways I could check first to see If my path tracer is converging correct or not? I was actually interested in comparing my render of cornell box to the iamges provided in that link. But since they are for spectral tracers, is there something similar for non spectral ones?" CreationDate="2018-08-14T08:29:00.800" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10606" PostId="7919" Score="0" Text="You could use smallpt as a reference renderer, you can extract the cornell scene used there into your own renderer, their scene is made using spheres only.. &gt; http://www.kevinbeason.com/smallpt/" CreationDate="2018-08-14T08:38:45.077" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10607" PostId="7919" Score="0" Text="Ok I'll look into it. Thanks for the help." CreationDate="2018-08-14T08:48:04.607" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10608" PostId="7915" Score="0" Text="@Hubble See edit." CreationDate="2018-08-14T12:51:35.987" UserId="9170" ContentLicense="CC BY-SA 4.0" />
  <row Id="10609" PostId="7915" Score="0" Text="@PaulHK I have removed the ambient factor for the direct illumination. The area light is placed right underneath the ceiling (distance 0.0001), but any ray that might end up in that tiny gap is returning black as well (see the code for shading emissive materials)." CreationDate="2018-08-14T12:53:39.263" UserId="9170" ContentLicense="CC BY-SA 4.0" />
  <row Id="10610" PostId="7917" Score="0" Text="Passing `NULL` to `TexSubImage` functions is not allowed. You need to fix that, regardless of the rest of your parameters." CreationDate="2018-08-14T13:15:20.547" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10611" PostId="7917" Score="0" Text="Yup, that fixed everything. I guess in the case of NVIDIA, 2 wrongs made a right? Add your comment as an answer, and I will accept it as the answer." CreationDate="2018-08-14T17:13:04.150" UserId="9180" ContentLicense="CC BY-SA 4.0" />
  <row Id="10612" PostId="7915" Score="0" Text="For the direct illumination image, did you multiply the light's contribution by the light falloff, which is an inverse square falloff. Basically, multiply by 1/(d^2) where d is the distance from the light source. For a path tracer this is not needed, but for direct illumination and next-event estimation it is needed, because maths." CreationDate="2018-08-14T17:49:29.847" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10615" PostId="7922" Score="0" Text="Thank you for your answer, Bram. You are right about the lower bright spots on the walls for the direct illumination. Odd, I hadn't noticed that before. However, I think I have already taken the distance attenuation into account. See the added shading function in my question. Did I implement something wrong?" CreationDate="2018-08-15T17:16:44.867" UserId="9170" ContentLicense="CC BY-SA 4.0" />
  <row Id="10616" PostId="7922" Score="1" Text="__l += light.getPower() * cos * cosp / (wi.lengthSquared() * rectangle.getArea());__           The larger the area of the light, the brighter it becomes since it takes up more area on the hemisphere. So you multiply it with the area instead of dividing it. Also (probably the main thing) __L += l / light.getSamplesNumber();__ There is a typo, you added instead of multiplied. Also a small optimization, when doing this on a single double, just use /= because else you are doing an unnecessary extra multiply. For vectors (three doubles) doing it that way would be faster (probably)." CreationDate="2018-08-15T18:12:26.710" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10617" PostId="7922" Score="0" Text="You are right about the area. Thank you for pointing that out! By chance, the light source in my scene has area 1, so your correction does not influence the result. I don't really understand your comment on the **L += ...** part. Surely, we need to add up the radiances from all light sources in the scene? Otherwise, we always get L = 0, since that is the default." CreationDate="2018-08-15T18:33:34.053" UserId="9170" ContentLicense="CC BY-SA 4.0" />
  <row Id="10618" PostId="7922" Score="0" Text="The __L += ....__ part was indeed a misread from me. That was correct, I just read the lower case L as a 1. I just edited my answer to include what should actually be the error." CreationDate="2018-08-15T19:14:32.333" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10619" PostId="7922" Score="0" Text="It is amazing how a forgotten normalization can cause so much trouble. I am really grateful for your correction, Bram! I was getting desparate after all those days of code staring and re-checking... It just didn't cross my mind that the path tracer was okay. Thanks again for your effort, I'm gladly giving you the bounty etc. :)" CreationDate="2018-08-15T23:38:56.247" UserId="9170" ContentLicense="CC BY-SA 4.0" />
  <row Id="10629" PostId="7932" Score="0" Text="angle along Z axis doesn't clarify plus you pictures don't even have labels for Axes. What is clarified is that you don't want to rotate around Z but along Z. Then there are only 2 options left either you want to rotate around X or Y. What do you want to do?" CreationDate="2018-08-17T14:04:15.927" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10630" PostId="7934" Score="0" Text="I don't understand why I'd never get an image in that first case? Can you elaborate a little. Also about my 2nd question, I think if i explicitly cast direction towards light that would be light sampling and if i am randomly casting according to the surface that would be BRDF sampling. Then MIS aims to combine both of these. Is this correct?" CreationDate="2018-08-18T05:22:53.970" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10632" PostId="7934" Score="0" Text="The number of rays needed would grow exponentially depending on path depth. For 3 bounces it would be 64*64*64 rays required,adding another bounce would be *64 again, and this is all caused by a single primary ray. So the final image would need an astronomical number of rays. Also this technique would introduce statistical bias as rays at a lower bounce depth will have heavy influence based on the millions of samples their children receive." CreationDate="2018-08-18T13:42:09.660" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10634" PostId="7934" Score="0" Text="@PaulHK - Aha! True. Never thought about that statistical bias. Can you also tell me about that 2nd question?" CreationDate="2018-08-19T06:10:39.230" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10636" PostId="7938" Score="0" Text="There will, however, be a final clamping on the unbound pixel radiances in the `toInt` method. Though the average of `0.1, 0.4, 8.0, 0.8, 0.6`; 1.98 finally clamps to 1.0, whereas the average of `0.1, 0.4, 1.0 (clamped separately), 0.8, 0.6`; 0.58 finally clamps to itself and is thus more aggressive in this case to avoid fireflies at much lower sample rates. So this indeed works as a back at the cost of bias to remove fireflies due to the high variance at low sample counts." CreationDate="2018-08-19T06:27:51.427" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10637" PostId="7938" Score="0" Text="It reminds me of a related [Twitter discussion](https://twitter.com/CasualEffects/status/983009137331126272?s=19) originated from the &quot;ray tracing tip of the day&quot; by Morgan McGuire." CreationDate="2018-08-19T06:41:07.963" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10639" PostId="7932" Score="0" Text="Sorry for not adding the coordinates. I wish to transform the point P(x,y,z) to origin O(0,0,0) without varying the vector V(i,j,k) orientation. I then want to rotate along the vertical axis (Z) by an angle and move back to point P which would form Vector V'(i',j',k'). (This is based on many examples in internet). Kindly let me know if you need any further information for your suggestion a solution. If possible please provide a code snippet. Thank you" CreationDate="2018-08-19T13:38:41.090" UserId="9207" ContentLicense="CC BY-SA 4.0" />
  <row Id="10640" PostId="7932" Score="0" Text="let me phrase differently. Suppose the random vector is `(1,0,0)` which  is your X-axis (assume front one). In this case, rotating along Z-axis (vertical axis) means you rotate the vector around Y-axis (right one). In another case let's say the vector is (0,1,0). Now rotating along Z means you rotate around X-axis. What exactly do you want in both cases? What happens when the vector is random?" CreationDate="2018-08-19T14:10:31.923" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10641" PostId="7932" Score="0" Text="As far as I am aware &quot;rotate along&quot; has no meaning. Could you clarify what you want (and perhaps why), preferably with specific examples? &quot;Rotate around&quot; the z axis would mean that the z coordinate stays fixed. &quot;Translate in the z direction&quot; would mean that only the z coordinate changes. Are either of those what you meant by &quot;rotate along&quot;?" CreationDate="2018-08-19T21:39:55.240" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10642" PostId="7934" Score="0" Text="The 64 primary rays will not hit the same object in the same spot so you will have to calculate everything for each sample." CreationDate="2018-08-20T06:17:51.080" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="10645" PostId="7944" Score="0" Text="Ok and what about discarding this PDF factor in actual calculations? Since as I said neither Shirley's book nor CGPP show this factor in their pseudocode listings." CreationDate="2018-08-21T21:39:40.963" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10647" PostId="7944" Score="0" Text="You cannot discard it if you are doing Monte Carlo integration." CreationDate="2018-08-22T03:29:33.413" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="10648" PostId="7947" Score="1" Text="Also, you probably want to weight by alpha during the averaging, so that the (invisible) color in transparent areas doesn't contaminate the average." CreationDate="2018-08-22T06:06:09.933" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10650" PostId="7940" Score="0" Text="In order to be able to help you, you should post a describtion of what you want to do, which boolean operation(s) that you have carried out, their order, the operands, your expectations and the difference between your expections and the actual output. Including pictures. Also: Which library have you used, and do you have examples where it performs as you expect?" CreationDate="2018-08-22T07:53:05.870" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="10651" PostId="7948" Score="0" Text="Hi Beyond, FWIW, Jim Blinn's article &quot;Compositing, Part I: Theory&quot; (https://graphics.stanford.edu/courses/cs248-02/blinn_theory.pdf) has a discussion on how to do filtering/averaging for colours with alpha. This states the correct way is to use premultiplied alpha  (AKA associative alpha)." CreationDate="2018-08-22T08:49:33.790" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10652" PostId="7947" Score="0" Text="@NathanReed Not just &quot;*probably*&quot;, Jim Blinn says to be correct you should use [premultiplied alpha in filtering calcs](https://graphics.stanford.edu/courses/cs248-02/blinn_theory.pdf)  .  :-)" CreationDate="2018-08-22T08:53:09.563" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10653" PostId="7947" Score="0" Text="Does this mean that ideally, I convert back to sRGB after *all* processing steps have been done (not only the downscaling)? Is alpha gamma corrected too? My images are not premultiplied: pixels with zero alpha may have any RGB color. How do I take in the alpha when averaging?" CreationDate="2018-08-22T09:39:44.553" UserId="6092" ContentLicense="CC BY-SA 4.0" />
  <row Id="10654" PostId="7948" Score="0" Text="Sure, the trick is that not all files are saved that way. So in order to do it correctly you need to know which type of file you're dealing with :)" CreationDate="2018-08-22T10:52:11.710" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="10655" PostId="7951" Score="0" Text="so when data is tightly packed and there is no interleaving , like a situation where pos data is first and then the whole normals are , the offset bcomes one ." CreationDate="2018-08-22T11:15:05.737" UserId="2430" ContentLicense="CC BY-SA 4.0" />
  <row Id="10656" PostId="7951" Score="1" Text="@mahdimahzuni then you'd use separate binding points for each attribute, the offset in `glVertexAttribFormat` become 0 and the offset in `glBindVertexBuffer` is where each attribute begins." CreationDate="2018-08-22T11:20:42.040" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="10657" PostId="7951" Score="0" Text="tnx superb explanation, performance wise is it bad to use two binding points or are they similar ?" CreationDate="2018-08-22T11:25:21.210" UserId="2430" ContentLicense="CC BY-SA 4.0" />
  <row Id="10658" PostId="7948" Score="0" Text="Sorry, looks like I misread your post! Wasn't entirely awake at the time." CreationDate="2018-08-22T12:59:57.850" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10659" PostId="7953" Score="0" Text="So, &gt;&gt;very&lt;&lt; geneally speaking, today, we are treating multiple GPUs the same way as we are treating miltiple CPU cores?" CreationDate="2018-08-22T15:01:08.370" UserId="5215" ContentLicense="CC BY-SA 4.0" />
  <row Id="10660" PostId="7953" Score="0" Text="@Karlovsky120 Yes, that's pretty much exactly it in the vulkan API, you can't address individual cores on your GPU (not even in the actual assembly your GPU uses) , you essentially treat them as separate &quot;cores&quot; in and of their own right, and can do explicit memory synchronization, dispatch different workloads etc.., **or** you can treat them as the same object (with device groups).  In fact in Vulkan, even one GPU is essentially treated as a separate core." CreationDate="2018-08-22T15:07:44.930" UserId="6530" ContentLicense="CC BY-SA 4.0" />
  <row Id="10661" PostId="7947" Score="0" Text="@NathanReed Interesting. I would have thought there would be cases where you want to and cases where you don't. But I will certainly defer to Mr. Blinn." CreationDate="2018-08-22T16:00:51.183" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10662" PostId="7947" Score="0" Text="@piegames - Yes, you probably want to convent back to sRGB after you've done all the averaging. Otherwise the color you're left with is in a different color space than the ones you started with. Alpha does not get gamma corrected. As  mentioned in the comment above, you probably want to make sure that your values are premultiplied, so you can skip the un-pre-multiply step I mentioned.  I'll update the  post to remove that." CreationDate="2018-08-22T16:01:15.483" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10664" PostId="7932" Score="0" Text="I will rephrase my question:&#xA;1)I need to rotate the vector V with the base point P by an angle. &#xA;2) The rotation axis is say for now is about a local Y axis at point P&#xA;3) The local Y axis orientation is similar to the global Y axis. &#xA;Hope there could be some solution to this. I'm programming in VB.net for a 3D point calculation project. Kindly provide me some insight to approach this problem. Thank you." CreationDate="2018-08-22T17:10:10.080" UserId="9207" ContentLicense="CC BY-SA 4.0" />
  <row Id="10666" PostId="7954" Score="0" Text="Are your triangles one sided? Your artefacts looks a lot like rays starting inside of a 1 sided triangle mesh, which usually occur were were you get AO, because of the V channels and your epsilon method it is easier to penetrate the surface and start a ray inside the mesh. If you are using 1 sided triangles then you can make them 2 sided so any rays starting from the inside get trapped in there and contribute something useful to AO :)" CreationDate="2018-08-23T07:09:13.840" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10667" PostId="7954" Score="0" Text="**I'm properly accounting for this by slightly translating the outgoing rays** How do you offset your ray origins? Do you use an absolute or relative offset along the ray?" CreationDate="2018-08-23T14:48:24.897" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10669" PostId="7956" Score="0" Text="**down-sampling the clamped subpixel image with a box filter** is a clever observation explaining the intent behind both the subpixel raster and clamping before averaging." CreationDate="2018-08-23T18:31:31.067" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10670" PostId="7956" Score="0" Text="**I don't know if it's biasing**, it still remains a bias. You will converge to different radiance images with and without (i.e. pure path tracing) this clamping, since clamping is a non-linear operation." CreationDate="2018-08-23T18:33:58.627" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10672" PostId="7955" Score="0" Text="I'll post an answer after gathering all the info, in case nobody else does, but here's a breadcrumb trail to more info in the meantime: https://twitter.com/Atrix256/status/1032666876264894464" CreationDate="2018-08-23T19:04:27.827" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10673" PostId="7954" Score="0" Text="As there is more than one possible cause for these artefacts, it would help answerers to be able to see your code. Are you happy to include it here?" CreationDate="2018-08-23T19:49:33.150" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10675" PostId="7955" Score="4" Text="&quot;*I often hear that tessellation shaders aren't great for performance reasons.*&quot; From whom? For what reasons? Where does this particular accusation come from? Basically, what is the foundation for this question?" CreationDate="2018-08-23T22:14:31.933" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10676" PostId="7955" Score="0" Text="SIGGRAPH presentations mostly" CreationDate="2018-08-23T22:22:25.580" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10677" PostId="7954" Score="0" Text="@Matthias added code, i'm current translating like this `Vector3D anAdjustedIntersect = theIntersectPoint.add(aReflectionVector.scale(.00001));`  &#xA;---&#xA;@trichoplax I've added my code to the original post, sorry if it's a little hard to follow, it's kind of a first pass.  &#xA;---&#xA;@PaulHK I thought that I was, I will double check my intersection code to make sure" CreationDate="2018-08-23T22:57:48.153" UserId="9231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10679" PostId="7955" Score="0" Text="@AlanWolfe which one in particular? If you see it a lot in SIGGRAPH you should be able to point it out quite promptly and give us a specific reference." CreationDate="2018-08-24T14:04:49.887" UserId="6530" ContentLicense="CC BY-SA 4.0" />
  <row Id="10680" PostId="7955" Score="1" Text="I don't have a reference nope. If you think my question is invalid, go check the responses on twitter from seasoned well known graphics folks explaining the answer. It's a real thing." CreationDate="2018-08-24T16:44:07.533" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10681" PostId="7961" Score="0" Text="Nevermind I fixed this issue by sampling the heightmap to get the edge normals. That way the data isn't divergent and it won't cause cracking. However if anyone has anything useful to add, for any users in the future, then feel free!" CreationDate="2018-08-24T17:20:35.553" UserId="8893" ContentLicense="CC BY-SA 4.0" />
  <row Id="10682" PostId="7956" Score="0" Text="I was referring to down-sampling. Down-sampling is optional, clamping is not." CreationDate="2018-08-24T20:05:30.883" UserId="8045" ContentLicense="CC BY-SA 4.0" />
  <row Id="10683" PostId="7956" Score="0" Text="There is still a final clamping in the `toInt` function." CreationDate="2018-08-24T20:39:25.607" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10685" PostId="7961" Score="2" Text="Glad to hear you fixed your own problem. If you'd like to, [self answers are encouraged](https://computergraphics.stackexchange.com/help/self-answer) and that would help future readers (and doesn't stop others adding alternative answers later too)." CreationDate="2018-08-25T22:02:22.493" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10686" PostId="7932" Score="0" Text="@Raghav - you are giving new information bit by bit in every comment. Is there a separate global and local frame/space? Are you working with your vector in the local or the global reference frame? That alone changes a lot of things. Try editing your question and all the details and by all I mean give us the whole scenario what exactly is going on." CreationDate="2018-08-26T08:19:59.523" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10687" PostId="7945" Score="0" Text="What do you mean by component?" CreationDate="2018-08-26T14:49:50.143" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="10688" PostId="7741" Score="0" Text="Doesn't the CoC radius represent the region in which a pixel distributes its energy instead of the region from which that pixel gathers energy? So shouldn't the `isInBokehShape` use the CoC radius of pixel `(i,j)` instead of `(x,y)`?" CreationDate="2018-08-26T16:57:13.633" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10689" PostId="7741" Score="1" Text="@Matthias Yes, that is actually correct. I based the algorithm on the Camera Lens Blur in After Effects (but added the depth check), which does it using $(x,y)$ instead of $(i,j)$. Although, I do think that in most cases it would not really matter, unless you are going for the most physically accurate in which case you would be better off ray marching the DoF against the depth image." CreationDate="2018-08-26T17:20:23.103" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10690" PostId="7968" Score="0" Text="Omg, I don't know about the 2nd problem but after reading your first point it actually does come to my mind, that my RNG uses global invocation IDs as seed. This do imply that at every launch, each pixel would get the same sequence of Random numbers. Will get back to this shortly after updating the results. Thanks." CreationDate="2018-08-26T17:40:20.573" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10691" PostId="7968" Score="0" Text="@gallickgunner Here you find an example [ShaderToy](https://www.shadertoy.com/view/MlcczX) robustly accumulating (e.g., Welford) radiance estimates with 1spp/frame (and correct gamma correction) using different seeds per frame." CreationDate="2018-08-26T18:45:06.263" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10692" PostId="7968" Score="0" Text="@bram0101 - Thanks a lot bro. Been on this for a couple of days and that's what happens if you don't rest :) Corrected both mistakes. The path tracer seems to converge now but I still have to take care of a better technique for RNG.  I am trying to tackle it with a similar approach mentioned in the link given by matthias. Use a time variable, passed by the cpu, to randomize the seed for each pixel, each pass." CreationDate="2018-08-26T19:55:43.283" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10693" PostId="7970" Score="0" Text="Is the thesis &quot;Genetic Operators in Metropolis Light Transport&quot; anywhere available?" CreationDate="2018-08-26T19:55:49.767" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10694" PostId="7970" Score="0" Text="@0xbadf00d As the mentor of that thesis student, I know that this thesis is available for the complete KU Leuven association. The availability for an external audience depends on the optional self-archiving of the author which is not done (yet?) at this time of writing. So my apologies, but I cannot point to a public archive for that one (yet?). With regard to the performance: in the best case it is quite similar to MMLT, but only works for pure diffuse scenes." CreationDate="2018-08-26T20:14:53.480" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10695" PostId="7970" Score="0" Text="@0xbadf00d I will add tomorrow some mathematics references to Evolutionary MCMC if you want." CreationDate="2018-08-26T20:17:54.097" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10697" PostId="7972" Score="1" Text="If you don't know where to start, you can work on a simpler version of the problem: instead of projecting from a 3D space to a 2D screen, try projecting from 2D space to a 1D screen. Work one transformation at a time, and the equation in 2D becomes reasonably simple. Then you can redo it in 3D." CreationDate="2018-08-27T04:18:46.737" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="10698" PostId="7970" Score="1" Text="@0xbadf00d Added them at the end ;-)" CreationDate="2018-08-27T08:07:07.870" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10699" PostId="7945" Score="0" Text="@x-rw https://en.wikipedia.org/wiki/RGB_color_model#Additive_colors" CreationDate="2018-08-27T12:03:43.400" UserId="6092" ContentLicense="CC BY-SA 4.0" />
  <row Id="10700" PostId="7974" Score="0" Text="I accept that the math is elaborate and I will really have to put in the time to study it." CreationDate="2018-08-27T13:19:34.177" UserId="9260" ContentLicense="CC BY-SA 4.0" />
  <row Id="10701" PostId="7972" Score="1" Text="By the way, the convention is usually to name the components of a vector $\vec{V} = (V_x, V_y, V_z)$ rather than $\vec{V} = (x_v, y_v, z_v)$." CreationDate="2018-08-27T13:49:57.437" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="10702" PostId="7961" Score="1" Text="Good suggestion! I will do a write-up this week with pictures so I can help anyone who has issues in the future :)" CreationDate="2018-08-27T14:44:38.167" UserId="8893" ContentLicense="CC BY-SA 4.0" />
  <row Id="10703" PostId="7975" Score="0" Text="What is stored in the frame buffer or what is the clear color?" CreationDate="2018-08-27T17:04:25.380" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10704" PostId="7975" Score="0" Text="The clear color is black. What do you mean what is stored in the frame buffer?" CreationDate="2018-08-27T17:23:44.917" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10705" PostId="7975" Score="0" Text="You could already start with some kind of image generated during a previous pass." CreationDate="2018-08-27T17:30:25.373" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10706" PostId="7975" Score="0" Text="So you basically want to use the following blend equation `dst.rgba &lt;- src.rgba + dst.rgba`? The more non-black draws to a pixel, the larger the pixel color becomes (i.e. goes to white)." CreationDate="2018-08-27T17:32:11.590" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10707" PostId="7975" Score="0" Text="@Matthias Correct!" CreationDate="2018-08-27T17:34:54.020" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10708" PostId="7975" Score="0" Text="Then I updated my answer with the additive blend state that you need to use." CreationDate="2018-08-27T17:39:20.643" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10709" PostId="7977" Score="0" Text="Can you explain blend factors. Why is the factor one rather than source color?" CreationDate="2018-08-27T17:41:08.107" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10710" PostId="7977" Score="0" Text="Since you said in the comments above that you want the following blend equation: `dst.rgba` &lt;- `src.rgba` + `dst.rgba` which basically adds the new color to the old one (so you multiply by 1)?" CreationDate="2018-08-27T17:43:10.607" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10711" PostId="7977" Score="0" Text="So the factor is just a scalar on the RGBA values before they are added?" CreationDate="2018-08-27T17:44:46.633" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10712" PostId="7977" Score="0" Text="@J.Doe It could also be a vector: you can multiply `dst.rgba` with `src.rgba` for instance and you can also multiply with a constant color. But for additive blending, you just want to do an addition." CreationDate="2018-08-27T17:46:59.540" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10713" PostId="7978" Score="0" Text="In Metal there is a blend factor of sourceColor as well as one called &quot;one&quot; whaw would source color look like? Would that just be the color squared or something?" CreationDate="2018-08-27T17:47:34.297" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10714" PostId="7978" Score="0" Text="Yeah, exactly. There’s some interesting things you could do with that, but I haven’t seen it used much. You can make a “multiply” blend mode by setting the source blend mode to “destinationColor” and the destination one’s to “zero” (or vice versa)." CreationDate="2018-08-27T17:52:16.390" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="10715" PostId="7978" Score="0" Text="why do you set sourceRGBBlendFactor to BlendFactorSourceAlpha. Is this an error?" CreationDate="2018-08-28T06:38:45.923" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10716" PostId="7979" Score="1" Text="Asuming that they are shaded the same: Usually you would just sort them back to front using a standard quick sort (performs well, but worst case is quadratic). If your objects are moving very little from frame to frame, you can even use one or a few bubble sort passes, since they are expected to be almost sorted from the last fram (which is linear). If they do not use the same shader, you should consider settling with an &quot;almost perfect&quot; sort." CreationDate="2018-08-28T08:37:14.363" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="10717" PostId="7979" Score="0" Text="There was a method used for fast sorting polygon primitives on the Playstation 1, which involved an array of link-lists, were the table size is some division of your z-range, say 1024 entries. You can insert items into this list at O(1) efficiency as the Z value of your primitive will map directly to a table index. Rendering is simply a case of walking the table from back-&gt;front and drawing each list. This isn't 100% accurate but it is very fast." CreationDate="2018-08-28T09:06:59.933" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10718" PostId="7932" Score="0" Text="Cross-posted on the maths site: https://math.stackexchange.com/q/2891259/856" CreationDate="2018-08-28T10:07:49.037" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10719" PostId="7980" Score="0" Text="Your sphere intersection is somewhat wrong as it doesn't test, the case of spheres on the back side of camera. Second the formula is `left + (right - left)*(i+0.5)/nx` Notice that `left` doesn't get divided by `nx`" CreationDate="2018-08-28T13:21:33.230" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10720" PostId="7980" Score="0" Text="Also always aim to normalize all your direction vectors unless you have a use for their magnitude. That simiplifies a lot of things such as intersection. You won't have to divde or multiply by `a`'s" CreationDate="2018-08-28T13:30:31.923" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10721" PostId="7980" Score="0" Text="Thanks for the input, gallickgunner!&#xA;I normalized all the involved vectors and I corrected the formulas as you suggested.&#xA;I don't know what exactly of my sphere intersection test is wrong.&#xA;The result is now that the entire window gets painted green.&#xA;Here is the code: https://privatebin.net/?080012578f6fb173#mvw2yi/yV3OwIAnEHj0EX7aXxjoAj9B75DPLkgsLbuk=&#xA;Here is the screenshot: https://s15.postimg.cc/x16akk6sb/Screenshot_RT_app_03.png" CreationDate="2018-08-28T14:38:03.910" UserId="9266" ContentLicense="CC BY-SA 4.0" />
  <row Id="10722" PostId="7978" Score="0" Text="No—it allows you to modulate the amount of color being added with the pixel’s alpha value, just like changing a Screen layer’s opacity in Photoshop." CreationDate="2018-08-28T15:14:29.030" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="10723" PostId="7972" Score="1" Text="@raindrop - Check this link might come handy. https://computergraphics.stackexchange.com/questions/6365/inverse-value-in-a-perspective-matrix/6366#6366" CreationDate="2018-08-28T18:32:00.593" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10726" PostId="5694" Score="1" Text="While I was attending the University of Utah, I worked for Evans &amp; Sutherland. In 1977, I got a night shift job in the electronics assembly department. By the early 80's I was a bench test technician.&#xA;&#xA;Gonrah's colleagues are probably assuming that the graphics were driven by a single electronics board. Uhn-uhn. The low-end CT5 was comprised of about 4 full-height 19 inch racks, each with two backplanes containing up to 32 circuit boards. A low-end CT5 would set you back a few million dollars in 80's money. The high-end had 10 full height racks and cost over 10 million." CreationDate="2018-08-28T23:16:28.343" UserId="9271" ContentLicense="CC BY-SA 4.0" />
  <row Id="10727" PostId="7979" Score="0" Text="@PaulHK can you please link an article that talks about this technique? I am intrigued." CreationDate="2018-08-28T23:50:16.720" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10728" PostId="7979" Score="1" Text="Unfortunately it's a closed system so most hardware information is published from emulator developers, see the section on GPU DMA2/DMA6, I can write you a psuedo code answer of how the &quot;order table&quot; works if you need clearer details: http://www.raphnet.net/electronique/psx_adaptor/Playstation.txt" CreationDate="2018-08-29T02:49:23.533" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10729" PostId="7984" Score="1" Text="Are you trying to draw it as a single strip? You should be drawing it as a series of strips. This can still be done in 1 draw call but you will need to insert a degenerate triangle (duplicate the last vertex) at the end of each strip column." CreationDate="2018-08-29T05:09:29.913" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10730" PostId="7987" Score="0" Text="With &quot;hemisphere on the wrong side of the surface&quot; do you mean the bottom hemisphere? Why is it on the wrong side when in this scene there's no such thing like a ground?" CreationDate="2018-08-29T10:13:55.343" UserId="9274" ContentLicense="CC BY-SA 4.0" />
  <row Id="10731" PostId="7987" Score="2" Text="@yot No, I mean the hemisphere that is back-facing with respect to the surface; the opposite side from the surface normal. Just like in direct lighting, each point can only be lit from half of space." CreationDate="2018-08-29T10:35:57.877" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10732" PostId="7989" Score="0" Text="What you need is probably something along the lines of color grading (tone mapping) rather than blending. Since you wrote the question terms of CG, I won't downvote it, but imho the issue is one of &quot;using&quot; image manipulation software and thus doesn't belong here. I suggest you look into the partner stack exchange websites: https://photo.stackexchange.com/ and https://graphicdesign.stackexchange.com/" CreationDate="2018-08-29T13:08:52.213" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10733" PostId="7989" Score="0" Text="I need to do it programmatically. So using tools is not an option." CreationDate="2018-08-29T13:36:15.183" UserId="7299" ContentLicense="CC BY-SA 4.0" />
  <row Id="10734" PostId="7987" Score="0" Text="Objects are made up of flat surfaces and flat surfaces have surface normals on both side, I think. Is the &quot;front-facing&quot; side of the surface the one the 'sees' the sky and not the interior of the object it belongs to?" CreationDate="2018-08-29T13:37:16.987" UserId="9274" ContentLicense="CC BY-SA 4.0" />
  <row Id="10735" PostId="7987" Score="1" Text="If you consider a polygon mesh, it's usual for surfaces to have only one side and one surface normal: the back face is typically not drawn at all. This is because the inside of the surface isn't visible. What you said is possible too, but even then, you still have to light each side of the surface separately, otherwise surfaces could be illuminated by lights that are actually behind them. This is a much bigger discussion than just AO though." CreationDate="2018-08-29T14:13:31.167" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10736" PostId="7989" Score="0" Text="What exactly were you hoping to achieve by using Poission blending?" CreationDate="2018-08-29T14:15:11.337" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10738" PostId="7982" Score="0" Text="Part 1/3: Thanks for your input, gallickgunner! As I tried to follow your answer and correct my mistakes in code, I also consulted the article on ray tracing from scratchapixel.com[1] since the different depictions/approaches of the involved coordinate systems confused me (they are different from Wolfgang Huerst's, Shirley's).&#xA;As you pointed out, I have forgotten to map to the NDC space and from there to the screen space (as [1] puts it).&#xA;&#xA;&#xA;[1]: https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays" CreationDate="2018-08-29T16:07:54.963" UserId="9266" ContentLicense="CC BY-SA 4.0" />
  <row Id="10739" PostId="7982" Score="0" Text="Part 2/3: So I did this in the for loop where the pixels are handled (https://privatebin.net/?7546e951d5dbabd5#DeLXVUTGsVl7crnHU8D27rD54jsIuZrh/79nwplQjDo=).&#xA;The result I got is a black screen. Unexpectedly, The green sphere isn't appearing at all.&#xA;Furthermore I am not sure why the length parameter is needed. Also, what does t = -b + sqrt(disc) do? Why does the discrimant alone not suffice (the &quot;intersection detector&quot;)?" CreationDate="2018-08-29T16:08:10.757" UserId="9266" ContentLicense="CC BY-SA 4.0" />
  <row Id="10740" PostId="7982" Score="0" Text="Part 3/3: &#xA;So I need 3 spaces: world, camera, screen space.&#xA;The camera &quot;sits&quot; at the origin and shoots rays into the view plane.&#xA;Since I converted from NDC space to screen space already all I need to do is to construct a primary ray w/&#xA;the distance from the view plane and the x and y coordinates of the screen space.&#xA;Thus apparently there is no view frustum and the rays go to infinity.&#xA;Are the screen space and the camera space the same? What's the difference?" CreationDate="2018-08-29T16:08:23.557" UserId="9266" ContentLicense="CC BY-SA 4.0" />
  <row Id="10741" PostId="7982" Score="0" Text="Sorry, I forgott to disable the expiration: https://privatebin.net/?1db98377b85f2b0f#AOhpWXLhw5O9/zsYnMOsO7xT2mhdkRegbJGFGq2zTb0=" CreationDate="2018-08-29T16:13:24.357" UserId="9266" ContentLicense="CC BY-SA 4.0" />
  <row Id="10742" PostId="7991" Score="0" Text="Do 350-10=x and 360-350+10=y to get the change in degree if you go clockwise or counter clockwise. Then do 10+x/2 or (10-y/2)%360 depending on which is closest. This assumes your first degree is smaller than the second" CreationDate="2018-08-29T16:34:25.133" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="10743" PostId="7984" Score="0" Text="@PaulHK For a 3x3 plane that would be `[0, 3, 1, 4, 2, 5, 5, 8, 4, 7, 3, 6]`, right? But it still does not seem to work properly." CreationDate="2018-08-29T17:15:48.783" UserId="2485" ContentLicense="CC BY-SA 4.0" />
  <row Id="10744" PostId="7982" Score="0" Text="@user9266 - edited my answer to add some details. brief ans is use `t = -b - sqrt(disc)` (Notice the negative) and when dividing by `window_width/height` cast it into float if saved as integer." CreationDate="2018-08-29T17:52:03.040" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10745" PostId="7982" Score="0" Text="Thanks gallickgunner!&#xA;I still get a black screen though but it is porbably due to my code containing possible bugs as the c variable is always large such that the discriminant always is negative:&#xA;Engine.cpp: https://privatebin.net/?33ff3fde29662a09#OPcMukb068p0gmiXCdMwZU4vcufJjb4bEILhXratSp4=&#xA;Engine.hpp: https://privatebin.net/?aee6ed326fb190f0#JL6WE8Ofn8jzneHavld9S1qnIfmkv1Kr0gz03rXDZTo=&#xA;Ray3.hpp: https://privatebin.net/?9d8f33d689d65576#zDC7apSmzM2XmU+eeqEISGokgHtS/oRX16d9jbty4i4=&#xA;Vector3.hpp: https://privatebin.net/?4e44e7ff0cd268a5#c003Y7zoY7WOOyCYo4iZ/lqhBbNiIJrpxq27INeJmp4=" CreationDate="2018-08-29T20:15:15.227" UserId="9266" ContentLicense="CC BY-SA 4.0" />
  <row Id="10746" PostId="7982" Score="0" Text="Anyhow, thanks again for the explanation. It made the things more clear to me." CreationDate="2018-08-29T20:16:03.073" UserId="9266" ContentLicense="CC BY-SA 4.0" />
  <row Id="10747" PostId="7982" Score="0" Text="@user9266 - you have a typo in your vector class, overloaded `-` operator. You are adding instead of subtracting." CreationDate="2018-08-29T21:21:47.833" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10748" PostId="7982" Score="0" Text="gallickgunner: Wow!!! Thank you!!! Thank you very much!!! I get the sphere!!!! Thanks!!!&#xA;&#xA;Result: https://i.imgur.com/XTODdLI.png" CreationDate="2018-08-29T21:43:36.157" UserId="9266" ContentLicense="CC BY-SA 4.0" />
  <row Id="10749" PostId="7982" Score="0" Text="Good to see the result xD" CreationDate="2018-08-29T21:46:09.873" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10750" PostId="7984" Score="0" Text="It should be [Col(n), Col(n)+1, Col(n+1), Col(n+1)+1, Col(n+2), Col(n+2)+1, .... Col(n+8)+1, Col(n+8)+1]  .. So for every pair of indices we take a vertex for the current column and one from its neighbour, this is the same as your strip. At the end we insert a duplicate vertex which will create a degenerate triangle. This will cause the next set of strip indices to start a new strip without any connecting triangles to the last column." CreationDate="2018-08-30T02:13:34.747" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10753" PostId="7984" Score="0" Text="https://computergraphics.stackexchange.com/questions/3746/rendering-same-vertex-array-in-different-modes/3794#3794" CreationDate="2018-08-30T03:19:36.150" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10756" PostId="7991" Score="3" Text="Try this: https://en.wikipedia.org/wiki/Mean_of_circular_quantities" CreationDate="2018-08-30T03:55:31.760" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10758" PostId="7996" Score="0" Text="I'm not sure your rotation matrix is correct, usually the sin/cos are swapped in the second row    e.g  [ cos -sin ] [ sin cos ]" CreationDate="2018-08-30T08:59:12.127" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10759" PostId="7996" Score="0" Text="@PaulHK, silly mistake while writing it,i rewrite it, now its in the correct form." CreationDate="2018-08-30T09:12:20.940" UserId="9280" ContentLicense="CC BY-SA 4.0" />
  <row Id="10760" PostId="7992" Score="0" Text="So, there is no way to avoid multiple `glBufferData` calls?" CreationDate="2018-08-30T13:36:12.017" UserId="9270" ContentLicense="CC BY-SA 4.0" />
  <row Id="10761" PostId="7992" Score="1" Text="@trexxet: I don't know what you mean. I said &quot;Everything else is perfectly valid.&quot;" CreationDate="2018-08-30T13:51:55.493" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10762" PostId="7996" Score="0" Text="The c and d vectors do not look correct in the diagram. They should be equivalent to a and b but rotated. How are you finding those vectors? Maybe there's a bug there." CreationDate="2018-08-30T17:16:07.850" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10763" PostId="7991" Score="0" Text="@Rahul Want to post that as an answer? :)" CreationDate="2018-08-30T17:17:24.783" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10765" PostId="7989" Score="0" Text="Could you give an example of your intended result? Either an image or a description of what would be different?" CreationDate="2018-08-30T18:01:02.587" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10766" PostId="7992" Score="0" Text="I asked if it is possible to share single EBO for multiple VAOs to avoid multiple data transfers of same data. However, after your answer I assume that it's not possible." CreationDate="2018-08-30T18:01:09.393" UserId="9270" ContentLicense="CC BY-SA 4.0" />
  <row Id="10767" PostId="7992" Score="0" Text="@trexxet: &quot;***Everything else is perfectly valid.***&quot; I don't know how you can read that statement and come away with the idea that this is not possible." CreationDate="2018-08-30T18:02:28.413" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10768" PostId="7992" Score="0" Text="I misunderstood a bit, but now I get it: bind first VAO, bind EBO, load EBO, bind second VAO, bind EBO, bind third VAO etc., right?" CreationDate="2018-08-30T18:27:48.577" UserId="9270" ContentLicense="CC BY-SA 4.0" />
  <row Id="10769" PostId="7992" Score="0" Text="@trexxet: No. You replace the first half of the code you wrote with the code *I wrote*." CreationDate="2018-08-30T18:42:19.877" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10770" PostId="7996" Score="0" Text="@NathanReed isnt them correct ? There is an small error margin, but compared to difference between rotation matrix and F, its nothing." CreationDate="2018-08-30T19:07:17.413" UserId="9280" ContentLicense="CC BY-SA 4.0" />
  <row Id="10771" PostId="7996" Score="0" Text="I don't think it's a small error margin. The component of c along b, and the component of d along a, are both off by about a factor of 2. The vectors as shown in the diagram are plainly quite a bit off from the true axes of the red ellipse." CreationDate="2018-08-30T19:14:32.973" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10772" PostId="2286" Score="0" Text="I reckon this would continue from here, where the light that is reflected back internally would then hit the surface again from the inside, on the opposite side, and then again, and again, each time diminishing, onto infinity :P (or into sub surface scattering hehe)" CreationDate="2018-08-30T21:39:45.700" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10773" PostId="7989" Score="0" Text="Sorry I don't have the image for intended output. The cropped image segment looks artificial in that image. Because in this case the kid image is brighter and sharper than the destination image. That's why it doesn't look natural." CreationDate="2018-08-31T03:04:55.260" UserId="7299" ContentLicense="CC BY-SA 4.0" />
  <row Id="10775" PostId="2225" Score="0" Text="Assuming no scaling, could this be done with translation plus quaternion, eliminating the matrix multiplication?" CreationDate="2018-08-31T22:55:58.493" UserId="9268" ContentLicense="CC BY-SA 4.0" />
  <row Id="10776" PostId="2225" Score="0" Text="@jjxtra You can use (unit) quaternions to represent rotations, and you can apply quaternion multiplications to apply rotations. You can perform the latter with different operations: with or without a matrix multiplication." CreationDate="2018-09-01T06:57:31.863" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10777" PostId="6267" Score="1" Text="I did a more in depth derivation of the perspective projection matrix here: https://dovo329.github.io/DeriveOpenGLPerspectiveProjectionMatrix/" CreationDate="2018-09-01T14:56:48.167" UserId="9287" ContentLicense="CC BY-SA 4.0" />
  <row Id="10778" PostId="2225" Score="0" Text="got it working with just quaternion! Convert the rotated cube back to non-rotated coordinates and get the bounds with origin 0,0,0. The in the shader, translate ray origin by difference from cube center and then rotate raycasts by the cube rotation quaternion. Works great!" CreationDate="2018-09-01T15:37:53.227" UserId="9268" ContentLicense="CC BY-SA 4.0" />
  <row Id="10781" PostId="8011" Score="4" Text="Perhaps you could try using homogeneous coordinates with w=0.  It's sometimes used to create shadow volumes that extend off to &quot;infinity&quot;." CreationDate="2018-09-03T13:23:03.660" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10782" PostId="7994" Score="0" Text="You say that configuration 2 takes longer to render, but in your profiling screenshots, configuration 2 takes 55ms instead of 81ms. Is the sentence starting with &quot;For some reason&quot; backward?" CreationDate="2018-09-03T15:22:14.383" UserId="196" ContentLicense="CC BY-SA 4.0" />
  <row Id="10783" PostId="7495" Score="0" Text="this guy knows more about quad-mapping than everybody else ;) but nathan, you could explain the connection better. did you find a solution for that real 3d quad-mapping shader problem on your blog?" CreationDate="2018-09-03T18:43:55.557" UserId="9296" ContentLicense="CC BY-SA 4.0" />
  <row Id="10784" PostId="5805" Score="0" Text="There is a great explanation on Nathan Reeds Blog. Normally I would not suggest posting many links here, but it is to much and the only exact and good answer to this problem." CreationDate="2018-09-03T18:53:04.493" UserId="9296" ContentLicense="CC BY-SA 4.0" />
  <row Id="10785" PostId="5805" Score="0" Text="With split polys there are some situations resolving false. Whenever there are degenerated vertexes split mapping goes wrong, so there is a reason for real quad mapping. However on 99% usecases this works well enough." CreationDate="2018-09-03T18:58:54.753" UserId="9296" ContentLicense="CC BY-SA 4.0" />
  <row Id="10786" PostId="7994" Score="0" Text="It is. Edited question" CreationDate="2018-09-03T19:52:01.447" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10787" PostId="8013" Score="0" Text="Not a proper answer but regarding your second point, don't use Oren Nayar with CT - the roughness terms are not compatible. Google 'Hammon GGX' for a diffuse term that plays nice." CreationDate="2018-09-03T23:11:21.100" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="10791" PostId="8015" Score="1" Text="With your answer to question 1, I still have no idea how and for what these random numbers are used." CreationDate="2018-09-04T06:30:41.360" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10792" PostId="8015" Score="0" Text="@0xbadf00d how do you construct paths from random scattering events? When you are uniformly sampling a direction in the unit hemisphere, you need two canonical random numbers $\xi_1,\xi_2 \in (0,1)$. These are your random numbers. You need these $\xi_i$'s for any non-deterministic BRDFs." CreationDate="2018-09-04T15:13:38.797" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="10793" PostId="8022" Score="0" Text="I don't think I'll be able to understand this answer without more math/diagrams. And yes, more precision, closer, probably makes sense, but a scaling from linear by `far / z`, which is standard, doesn't make sense. It yields a depth buffer that becomes more linear the closer the two clip planes are to each other. It seems like a conflation of two concepts: screen space-linear Z, and a non-constant depth buffer mapping for a performance hack." CreationDate="2018-09-04T16:16:55.890" UserId="504" ContentLicense="CC BY-SA 4.0" />
  <row Id="10794" PostId="8015" Score="0" Text="As you wrote, I need *uniformly* distributed random numbers (for a diffuse reflection). But, for example, the small step mutations use a normal distribution to modify the $\xi_i$'s. That's what I don't understand." CreationDate="2018-09-04T16:21:25.513" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10795" PostId="8016" Score="0" Text="Thanks for the answer. I checked PBRT and they do have multi layered BRDFs such as Ashikhmin and Shirley's. They implement plastic as a mixture of diffuse and specular BRDFs where they control both through $K_d$ and $K_s$ as I talked about in the post. This is getting out of hand. I'll ask a few more separate sub questions then return to this question again." CreationDate="2018-09-04T19:03:23.303" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10796" PostId="8013" Score="0" Text="@russ - Must be why I haven't seen anybody using both with one another. I'm separating my point 3 into a different question which is more concerned with light-matter interactions and is a little unrelated to how I should pick my BSDFs." CreationDate="2018-09-04T19:40:12.593" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10802" PostId="8011" Score="0" Text="In episode &quot;[Mission Responsible](http://fairlyoddparents.wikia.com/wiki/Doug_Dimmadome/Appearances) they mention: &quot;...  80-story hat storage building&quot; - while on other pages they mention *regular* size and &quot;infinitely tall&quot;." CreationDate="2018-09-04T21:53:38.100" UserId="9258" ContentLicense="CC BY-SA 4.0" />
  <row Id="10803" PostId="8022" Score="0" Text="Specifically, it’s the, “if those are not interpolated with 1/Z” that I don’t understand. What interpolation is that?" CreationDate="2018-09-05T02:51:38.407" UserId="504" ContentLicense="CC BY-SA 4.0" />
  <row Id="10806" PostId="7984" Score="0" Text="Can you reduce the grid size to say 3x3 points, this will make visualising the problem much easier." CreationDate="2018-09-05T08:21:54.773" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10807" PostId="8027" Score="0" Text="Cool. Yeah that's exactly what I was thinking of. Three years ago, that's impressive. I wonder how it's going..." CreationDate="2018-09-05T10:53:38.043" UserId="9293" ContentLicense="CC BY-SA 4.0" />
  <row Id="10808" PostId="8022" Score="1" Text="I'll add some additional text to hopefully explain" CreationDate="2018-09-05T11:48:53.160" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10809" PostId="4369" Score="1" Text="Update link from @Alan 's comment https://developer.nvidia.com/gpugems/GPUGems/gpugems_ch01.html" CreationDate="2018-09-05T12:14:52.873" UserId="457" ContentLicense="CC BY-SA 4.0" />
  <row Id="10810" PostId="8027" Score="0" Text="@AndreasZita - I'll do some more research and see if I can improve this answer." CreationDate="2018-09-05T13:12:43.520" UserId="9258" ContentLicense="CC BY-SA 4.0" />
  <row Id="10811" PostId="8029" Score="1" Text="But then again, when using a box filter anyway, information never crosses the boundaries of a 2x2 block either, or 4x4, or 8x8,... That is to say, the same question stands when mipmapping a regular 2D texture applied in a repeating wraparound fashion, too." CreationDate="2018-09-05T13:48:28.250" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10812" PostId="8029" Score="0" Text="Given that box filters have a poor frequency response, how often are they used in production of MIP maps?" CreationDate="2018-09-05T14:21:56.270" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10813" PostId="8029" Score="0" Text="I'ts pretty common in my experience that engines will have box filter as the default way to make mips for a texture, with other options available such as kaiser windowed sync, for times when that is more appropriate." CreationDate="2018-09-05T14:58:32.440" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10814" PostId="8029" Score="0" Text="Fine way for me is using ggx importance sampling with varying roughness in 0 - 1 range, which depends on mip level. Then each iteration stores results in adequate mip level." CreationDate="2018-09-05T15:12:47.113" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="10815" PostId="8029" Score="0" Text="For the record, this has nothing to do with PBR, just regular old cube map mip maps." CreationDate="2018-09-05T15:18:40.693" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10816" PostId="8029" Score="1" Text="Well then we have just good old `glGenerateMipmap` with `glEnable(GL_TEXTURE_CUBE_MAP_SEAMLESS)`  :)" CreationDate="2018-09-05T15:23:18.347" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="10817" PostId="5146" Score="0" Text="Besides what Jarkkol said, it's common to have a blend region between cascades. The blend region is more to hide the fact that the shadow map resolution changed, and would be more of a &quot;cover up&quot; of this seam than a proper fix, but FYI in case it helps!" CreationDate="2018-09-05T15:57:50.350" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10818" PostId="8032" Score="0" Text="What's wrong with a probability density having values in $[0,+\infty)$? Perhaps you are confusing the [PDF](https://en.wikipedia.org/wiki/Probability_density_function) with the [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function)?" CreationDate="2018-09-05T18:21:44.350" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="10819" PostId="8032" Score="0" Text="@Hubble You've missed the point. The point is that the function on the left-hand side is defined on $\Omega_{p_i}$, while the function on the right-hand side is defined on $M$. And mathematically, the equation $f=g$ for functions $f,g$ only makes sense when both functions have a common domain." CreationDate="2018-09-05T18:30:08.440" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10820" PostId="8031" Score="0" Text="Also, the image must be created with `VK_IMAGE_USAGE_TRANSFER_DST_BIT`, since clearing is a transfer operation." CreationDate="2018-09-05T18:35:42.577" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10821" PostId="8031" Score="0" Text="@NicolBolas but that's not the case for implicit clears via the renderpass?  That seems like an odd distinction." CreationDate="2018-09-05T20:03:05.123" UserId="9306" ContentLicense="CC BY-SA 4.0" />
  <row Id="10822" PostId="8032" Score="0" Text="$p_A$ and $p_\omega$ are not defined on the same domain, but the Jacobian term ensures that you can go from projected solid angle to surface area, and vice versa. Read Section 5.5 of PBRT3, especially Equation 5.6 for more info." CreationDate="2018-09-05T20:51:21.010" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="10823" PostId="8031" Score="1" Text="@Jherico: Renderpass clears are often just changes to the render target caching system, so that cache-miss reads simply return the clear color, and writes of a partial cache-line fill in the other values with the clear color. So &quot;clearing&quot; is just a trick that never directly touches memory (and it's even moreso for TBR-based systems). When you clear an image directly, you're doing actual writes to the memory, rather than caching tricks or clearing tile storage." CreationDate="2018-09-05T21:23:20.197" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10824" PostId="8019" Score="0" Text="Why would it fetch things in a non sequential order? The index buffer has it go linearly through the structure. So the draw call and index buffer are the same in both scenarios. Sorry if that wasnt clear in the original question. The only way I could see this mattering is if the gpu overrides my index buffer to something that makes sense spatially for it." CreationDate="2018-09-06T00:48:42.270" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10825" PostId="7984" Score="3" Text="Ok, interesting the 3x3 version works and the 257x257 doesn't. Are you using 16bit indices by any chance? Because you can only address up to 256x256 using 16bit indices. For your case you need to use 32bit indices." CreationDate="2018-09-06T02:06:18.763" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10826" PostId="8019" Score="0" Text="@J.Doe Are you saying that the triangles end up in different strips (and thus in different draw calls) between the two configurations In that case, I'm confused by the statement in your original question stating that &quot;in memory a single strips triangles are very spread out&quot; (this is a comment about memory configuration 1)." CreationDate="2018-09-06T04:10:09.987" UserId="196" ContentLicense="CC BY-SA 4.0" />
  <row Id="10827" PostId="8032" Score="0" Text="Isn't there a bijection between points on the upper hemisphere over $p_i$ and points $p_{i+1}$ on the visible surfaces in the scene?" CreationDate="2018-09-06T05:32:20.140" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10830" PostId="3890" Score="0" Text="Multiplication." CreationDate="2018-09-06T09:08:14.910" UserId="2370" ContentLicense="CC BY-SA 4.0" />
  <row Id="10831" PostId="8027" Score="0" Text="@AndreasZita - Answer improved." CreationDate="2018-09-06T09:08:42.053" UserId="9258" ContentLicense="CC BY-SA 4.0" />
  <row Id="10832" PostId="8034" Score="0" Text="PS: I did find https://github.com/keijiro/Pcx" CreationDate="2018-09-06T09:45:57.923" UserId="9309" ContentLicense="CC BY-SA 4.0" />
  <row Id="10833" PostId="8032" Score="0" Text="@Rahul There is a bijection: Let $V_{p_i}$ denote the set of all surface points $q$ such that $p_i$ and $q$ are mutually visible and $D_{p_i}$ denote the set of all $\omega\in\Omega_{p_i}$ such that there is a $t&gt;0$ with $p_i+t\omega\in V_{p_i}$. Then there is obviously a bijection between $V_{p_i}$ and $D_{p_i}$. We can define a transformation $$T:V_{p_i}\to D_{p_i}\;,\;\;\;q\mapsto\frac{q-p_i}{\left|q-p_i\right|},$$ but the determinant of the Jacobian of $T$ is $0$ everywhere and hence the transformation formula can't be applied. Am I missing something?" CreationDate="2018-09-06T11:49:53.227" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10834" PostId="8032" Score="0" Text="@Hubble Is $T$ the transformation you're taking the Jacobian of?" CreationDate="2018-09-06T11:50:56.717" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10835" PostId="8019" Score="0" Text="1 draw call for all of the triangles" CreationDate="2018-09-06T16:53:38.387" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="10836" PostId="8033" Score="1" Text="Could you add more detail on what you're aiming to achieve? Do you want a completely new approach that is not real-time, or are you looking to redirect the real-time output so it gets recorded directly rather than going to the screen? Or something else?" CreationDate="2018-09-06T20:21:13.677" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10837" PostId="8022" Score="0" Text="Thanks! I think the problem comes down to &quot;The rendering system/hardware will linearly interpolate the screen space z&quot;. I was under the impression that NDC position would be computed as `(x, y, z) / w` per-fragment, but apparently, instead, we have to deal with a linearly-interpolated version of `(x/w, y/w, z/w)`? That doesn't seem reasonable to me in 2018, but it would be good to know if that's the hack we have to live with for now anyway!" CreationDate="2018-09-06T20:26:30.483" UserId="504" ContentLicense="CC BY-SA 4.0" />
  <row Id="10841" PostId="8022" Score="0" Text="To perform perspective correct texturing/shading/whatever, you need to linearly interpolate (Val/w) values, and then, per fragment, do a division by the linearly interpolated 1/w. It's a bit hard to explain just in a comment, but there is a little bit of an explanation in https://computergraphics.stackexchange.com/a/4799/209.  Alternatively, do a search for Jim Blinn's article &quot;Hyperbolic Interpolation&quot;" CreationDate="2018-09-07T08:01:08.263" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10842" PostId="8033" Score="1" Text="Are you asking &quot;Is there a non-realtime&quot; or &quot;slower&quot; rendering mode in Unity that will render in higher quality _OR_ are you wanting a lossless video recording approach? I don't know unity, but can you save each frame at the end of each render to create a sequence of images (e.g PNGs) and put them through a lossless or near-lossless video encoder?" CreationDate="2018-09-07T08:10:29.030" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10843" PostId="7905" Score="0" Text="@DanHulme Sometimes it's useful to know a little history.... even if it's only to give you a greater appreciation of the present ;-)" CreationDate="2018-09-07T08:20:06.157" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10847" PostId="8019" Score="0" Text="@J.Doe - I don't quite understand your statements when you say &quot;concentrated chunk of memory is updated&quot; in memory config 1 and &quot;writing is more spread out&quot; in config 2. However I think what John is trying to tell you, for memory config 2, the data for the next triangle that needs to be drawn is pulled in when the GPU accesses the data for the first one. This is because GPU accesses memory in blocks. So that whole block gets written into the cache. For the next triangle it can fetch directly from high speed cache instead of that slow access from memory, hence the speed up." CreationDate="2018-09-07T16:41:20.713" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10848" PostId="7984" Score="0" Text="@PaulHK Yes sir I did use `Uint16Array` and changing it to `Uint32Array` did solve the issue. Please submit an answer and grab the bounty. Ty!" CreationDate="2018-09-07T16:48:48.690" UserId="2485" ContentLicense="CC BY-SA 4.0" />
  <row Id="10850" PostId="8038" Score="0" Text="Are you sure to plug the result of the first alpha over into `src` of the next one? As I understand it, I would put it into `dst` instead. This lines up with some quick tests of the final equation with black `src`, white `dst` and an a&gt;0. Overlaying black over white should darken the color but ` + dst` prevents this." CreationDate="2018-09-07T20:43:27.523" UserId="6092" ContentLicense="CC BY-SA 4.0" />
  <row Id="10852" PostId="8022" Score="0" Text="That article doesn't seem to be available without paying and I can't see a preview to know if it's worth it. &#xA;&#xA;Even if linear interpolation for a reciprocal was necessary for something, it could be interpolated along with the original values, and I don't think it would ever be the right choice for storing depth. I amended the question to emphasize position." CreationDate="2018-09-08T00:30:33.760" UserId="504" ContentLicense="CC BY-SA 4.0" />
  <row Id="10854" PostId="8038" Score="0" Text="I agree that substituting into `dst` is the correct thing to do here. If you imagine doing `n` separate blends on top of each other, the `src` would be the same for all, and result of each blend would be the following blend's `dst`. I think you should be able to simplify the result to a single blend using alpha of `1 - (1 - a)^n` though. (Try defining a new variable `b = 1 - a`, and repeating the derivation you used for the first part of the answer.)" CreationDate="2018-09-08T02:37:14.697" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10855" PostId="8038" Score="0" Text="Great idea! I've updated it with your suggestion. Let me know how it works." CreationDate="2018-09-08T05:30:34.773" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10856" PostId="8038" Score="0" Text="How about `dst := (1 - b)*src + b*dst`, and then rewrite that as `src + b*(dst - src)`. Then it works out similarly to your original derivation. :)" CreationDate="2018-09-08T05:48:23.037" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10857" PostId="8038" Score="0" Text="Ah,  I see what you mean. I'll give that a try in the morning. I'm getting sleepy now and would probably screw it up." CreationDate="2018-09-08T06:18:41.850" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10858" PostId="8038" Score="0" Text="Whoa, that looks really good. Am I allowed to bracket out `src` in that sum, to avoid multiplicating with `src` n times and reduce it to once at the end? Because if so, this becomes a geometric series which can be optimized even further." CreationDate="2018-09-08T08:09:50.203" UserId="6092" ContentLicense="CC BY-SA 4.0" />
  <row Id="10859" PostId="8016" Score="0" Text="Ok so i read the slides by Naty Hoffman given here in the answer, and after reading that and your answer makes much more sense. Thanks for the help.&#xA;&#xA;https://computergraphics.stackexchange.com/questions/1513/how-physically-based-is-the-diffuse-and-specular-distinction" CreationDate="2018-09-08T08:25:55.737" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10860" PostId="8036" Score="0" Text="This is probably the best option. Thanks also for the `Time.captureFramerate` hack, I did not know that possibility." CreationDate="2018-09-08T08:48:50.043" UserId="9307" ContentLicense="CC BY-SA 4.0" />
  <row Id="10861" PostId="8038" Score="0" Text="Absolutely! If it works, go for it!" CreationDate="2018-09-08T14:48:15.747" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="10862" PostId="8032" Score="0" Text="Why do you say the determinant of the Jacobian is zero everywhere? $T$ is a bijection and a diffeomorphism, so its Jacobian matrix should exist and be invertible everywhere in the domain you've described in your comment. And yes I think you are on the right track in formalizing this with your definition of $T$. We usually leave this kind of detail implicit, for better or worse." CreationDate="2018-09-08T18:03:31.253" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10863" PostId="8032" Score="0" Text="@NathanReed A direct computation of the determinant shows that it is zero." CreationDate="2018-09-08T20:26:43.200" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10864" PostId="8032" Score="0" Text="I don't know how you're getting that, but an invertible smooth mapping between two 2D domains certainly does not have zero Jacobian determinant everywhere. That directly contradicts the premise that it is invertible!" CreationDate="2018-09-08T20:36:16.027" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10865" PostId="8041" Score="0" Text="I don’t know how to design a GPU, but it seems to me that all that’s needed is to interpolate Z instead of Z/W, for linear depth, and Z/W interpolation could still happen afterwards for anything visible. I still can’t tell if this is a matter of good reasoning or one of “nobody cares so we don’t bother updating”." CreationDate="2018-09-09T10:28:32.373" UserId="504" ContentLicense="CC BY-SA 4.0" />
  <row Id="10867" PostId="8041" Score="0" Text="Interpolating Z instead of Z/W _does not give correct results_ in screen space. Z/W does." CreationDate="2018-09-09T18:42:17.337" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10868" PostId="7984" Score="0" Text="Good to know your problem is solved." CreationDate="2018-09-10T02:11:58.147" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10869" PostId="8041" Score="0" Text="Right. But if the depth buffer is quantized to a lower precision than position, then, aside from being performant when it works, it's not a good idea to store a scaled chunk of screen space Z. If linear interpolation is all we get, then clipping needs to happen in view space. And Z needs to be interpolated before division by W, for the depth buffer, and after it, for what you've gone over.&#xA;&#xA;So is the answer to my question, &quot;because GPUs have always only interpolated in clip space because it was the only practical solution on the first GPUs, and it has worked out well enough since&quot;?" CreationDate="2018-09-10T05:00:23.203" UserId="504" ContentLicense="CC BY-SA 4.0" />
  <row Id="10870" PostId="8041" Score="0" Text="I'm not following what you mean about &quot;quantized to a lower precision than position&quot;, or &quot;store a scaled chunk of screen space Z&quot;." CreationDate="2018-09-10T05:17:12.653" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10871" PostId="8041" Score="1" Text="Also, &quot;Z needs to be interpolated before division by W, for the depth buffer&quot;—no. That's what I've been trying to explain. You get the _wrong answers_ if you interpolate Z (or anything else) in screen space without dividing it by W first. You seem to be stuck on this idea that a linear Z buffer would just work if we didn't divide by W. But it won't work—it won't interpolate in screen space properly." CreationDate="2018-09-10T05:21:10.537" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10872" PostId="8043" Score="0" Text="You might want to mention that in JavaScript/WebGL the array containing the indices has to be a `Uint32Array` instead of `Uint16Array`." CreationDate="2018-09-10T09:19:54.327" UserId="7457" ContentLicense="CC BY-SA 4.0" />
  <row Id="10873" PostId="8047" Score="0" Text="Right but in the case of the fallback to software, does DX have any software support out the box or does it rely on the DX supported drivers to implement each graphics function? If a feature isn't there - apart from WARP does DX have any implicit software implementations or just it just serve as the HAL?" CreationDate="2018-09-10T15:36:35.770" UserId="9325" ContentLicense="CC BY-SA 4.0" />
  <row Id="10874" PostId="8048" Score="0" Text="In the notation of Veach's paper, shouldn't we approximate $f$ (with $f_j=h_jf$) instead of $f_j$?" CreationDate="2018-09-10T18:16:39.490" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10875" PostId="8048" Score="0" Text="Yes absolutely. It's an abuse of notation that is commonly used to bundle the pixel reconstruction filter and the function $f$ together. You are indeed approximating $f$." CreationDate="2018-09-10T18:53:00.737" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="10876" PostId="8022" Score="0" Text="Okay, I've read the article now! From that, what I'm gathering is that it's not a problem that the division happens. Rather, the problem is that depth doesn't get written by dividing the NDC position's Z value by the interpolated clip space 1 / W value. Sound right? I don't understand why that wouldn't happen, if it happens for all the other interpolated values, such as UVs." CreationDate="2018-09-10T23:57:18.953" UserId="504" ContentLicense="CC BY-SA 4.0" />
  <row Id="10877" PostId="8041" Score="0" Text="Sorry, I meant &quot;clip space&quot;, not &quot;view space&quot; in my last comment. I'm not convinced that interpolating in clip space instead of NDC space won't work for Z, but I am convinced (as mentioned in my last comment to Simon) that interpolating 1/ZW in NDC space and then dividing the interpolated 1/Z by 1/W would work. Trouble is, that step doesn't happen. What ends up getting stored in the depth buffer is linear depth, divided by distance from the camera (which is useful as you've said), scaled  by the far clip plane – this last part seems a hack to achieve 1 at the far clip plane, to avoid division." CreationDate="2018-09-11T00:17:31.910" UserId="504" ContentLicense="CC BY-SA 4.0" />
  <row Id="10878" PostId="8047" Score="0" Text="@Charleh No, DX doesn't provide any software fallback for unsupported GPU features out of the box. That's something that would have to be implemented by the game engine." CreationDate="2018-09-11T00:29:59.403" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="10879" PostId="8043" Score="0" Text="Thanks for the suggestion, I've ammended my answer. I wanted to make the answer language agnostic as this can occur on any GL implementation." CreationDate="2018-09-11T02:59:21.527" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10880" PostId="8022" Score="0" Text="You *could* do that (there were proposals for a &quot;W&quot; buffer), but it's unnecessary for perspective depth comparisons as straight lines in world space remain straight lines in perspective space. If you ever try doing a traditional perspective drawing with pencil/ruler (e.g. using vanishing points) this may become evident.&#xA;&#xA;What *doesn't* get preserved is relative lengths but that doesn't matter as you go along a line as it's &quot;just a line&quot;. It's when you add something that does depend on distances, e.g. texturing, that you need to do the per-pixel division." CreationDate="2018-09-11T07:43:46.050" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10881" PostId="8044" Score="0" Text="An affine transformation is is any transformation that preserves points, lines and planes. This can be translation, skewing and rotation - and combinations thereof." CreationDate="2018-09-11T08:23:43.490" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="10882" PostId="8033" Score="0" Text="It looks pretty good. You understand that if you view it immediately after uploading that you'll probably be viewing it at 360P and it will be a bit fuzzy, after an hour or so a (in the case of *that* video) 1080P version becomes available." CreationDate="2018-09-11T08:32:16.500" UserId="9258" ContentLicense="CC BY-SA 4.0" />
  <row Id="10883" PostId="8044" Score="0" Text="Hi @beyond, thanks for your input. Are you able to shed some light on the code?" CreationDate="2018-09-11T09:00:40.970" UserId="9322" ContentLicense="CC BY-SA 4.0" />
  <row Id="10884" PostId="8047" Score="0" Text="or by the GPU driver, of course. Even if it were implemented by DX or the driver, how would turning *off* the feature in the game activate it? That logic is backwards." CreationDate="2018-09-11T09:53:24.180" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10885" PostId="8047" Score="0" Text="That's what I thought - it's more to settle an argument about why CPU load goes up when games are set to lower settings on the U4 engine. I'd favour the more frames = more load over anything. Someone was suggesting that the U4 engine must fall back to software for certain things but I doubt it. Maybe it does but I can't find anything in the Epic dev docs that mention that, also most hardware accelerated effects would be so slow in software they wouldn't be worth implementing." CreationDate="2018-09-11T10:13:27.937" UserId="9325" ContentLicense="CC BY-SA 4.0" />
  <row Id="10886" PostId="8044" Score="0" Text="To be honest, the code seems a bit elaborate for something as simple as applying an affine transformation to an image. The projection bit I don't get, so an example of input/expected output/actual output would be ideal." CreationDate="2018-09-11T12:57:41.527" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="10887" PostId="8048" Score="0" Text="I still don't understand how the approximation is done. I've updated the question. Hopefully it's now clearer what exactly I'm asking for." CreationDate="2018-09-11T18:01:19.577" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10890" PostId="7516" Score="0" Text="What software is this? Is it available right now?" CreationDate="2018-09-12T02:35:05.263" UserId="9335" ContentLicense="CC BY-SA 4.0" />
  <row Id="10892" PostId="8052" Score="1" Text="The first edition of PBRT (2004) used the word dielectric. Being the reference for physically-based shading, it could just be that other PBR frameworks that were developed after PBRT all tried to speak the same language. I'm sure the term dielectric was used way before that but my understanding is that this is the term that stuck." CreationDate="2018-09-12T03:02:00.220" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="10893" PostId="7516" Score="0" Text="@user2153553 this software is made by [person in twitter](https://twitter.com/hashtag/sdfmesher) I don't know Is it available right now but you can ask more about this software" CreationDate="2018-09-12T07:28:50.043" UserId="6541" ContentLicense="CC BY-SA 4.0" />
  <row Id="10894" PostId="8057" Score="0" Text="it's odd to see that in substance, the IBL color is interpreted different between the path traced and rasterized version, but that's unrelated :P" CreationDate="2018-09-12T18:35:10.703" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10895" PostId="8059" Score="1" Text="it's actually unclear if this is the whole story, and some thoughts that the path traced version may actually be incorrect. there is a twitter conversation here: https://twitter.com/Atrix256/status/1039943676552568832 ." CreationDate="2018-09-12T20:36:59.207" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10896" PostId="7516" Score="0" Text="Yes I actually have been following that guy also, he said it's coming soon." CreationDate="2018-09-13T00:10:45.490" UserId="9335" ContentLicense="CC BY-SA 4.0" />
  <row Id="10898" PostId="8055" Score="2" Text="It's because it draws a line using 1 pixel per column going from left to right, which is true for lines that are below 45 degress with respect to the horizontal axis (dx &gt; dy). If you imagine a line steeper than 45 degrees then you can no longer draw the line using 1 pixel per column, however the DDA algorithm deals this case and swaps x/y around when dy &gt; dx (line is mostly vertical)." CreationDate="2018-09-13T03:39:52.903" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10899" PostId="8059" Score="0" Text="Burley has an entire paper about the Disney Diffuse Model: https://disney-animation.s3.amazonaws.com/library/s2012_pbs_disney_brdf_notes_v2.pdf However, the Disney Diffuse BRDF is not energy conserving. This might not be a problem in the questioners case but should be mentioned.&#xA;The Titanfall approach is good, but I want to add a blog post series by Stephen Hill ( https://blog.selfshadow.com/2018/05/13/multi-faceted-part-1/)  which explains a different way to handle this problem" CreationDate="2018-09-13T11:33:19.803" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10900" PostId="8062" Score="0" Text="Is this code `vec3 colour = vec3(0.5) + normal * 0.5;` is converting RGB to XYZ ?" CreationDate="2018-09-13T13:09:44.180" UserId="9298" ContentLicense="CC BY-SA 4.0" />
  <row Id="10901" PostId="8062" Score="1" Text="@AbhinaySInghNegi no, colour is RGB and normal is XYZ, so this is converting XYZ to RGB. If you want RGB to XYZ you use `vec3 normal = (colour - vec3(0.5)) * 2.0;`" CreationDate="2018-09-13T15:55:00.507" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10904" PostId="8059" Score="0" Text="@Alan Wolfe - Although Oren-Nayar may give a closer look to the ground truth, I think the main problem is in fact the Fresnel Issue. Fresnel issue guarantees that at glancing angles, more light reflects, this would mean at glancing angles we have more specular part and very low diffuse part thus giving the black ring. Atleast that's what makes sense to me." CreationDate="2018-09-14T14:06:50.730" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="10905" PostId="8060" Score="0" Text="Filament does use energy compensation yes. Its implementation is explained here: https://google.github.io/filament/Filament.md.html#toc4.7.2" CreationDate="2018-09-14T18:33:41.757" UserId="3255" ContentLicense="CC BY-SA 4.0" />
  <row Id="10906" PostId="8011" Score="1" Text="What if you make the hat normal size but then use vertex shaders to always extend it so that it is barely larger than the screen?" CreationDate="2018-09-15T09:54:55.350" UserId="6092" ContentLicense="CC BY-SA 4.0" />
  <row Id="10907" PostId="8068" Score="0" Text="What does your data look like? What do the points represent? Do you have example images of raw data and how the final result should look like?" CreationDate="2018-09-15T10:01:55.083" UserId="6092" ContentLicense="CC BY-SA 4.0" />
  <row Id="10909" PostId="8055" Score="1" Text="@PaulHK Any reservations to fleshing that out into a proper answer?" CreationDate="2018-09-15T12:41:35.503" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10911" PostId="8068" Score="1" Text="&quot;What will you use to generate mesh of this data&quot;? I will use [MeshLab](https://meshlabstuff.blogspot.com/2009/09/meshing-point-clouds.html)." CreationDate="2018-09-16T10:00:42.343" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10912" PostId="8056" Score="0" Text="Thanks for your explanation! I think I got what you mean. So, for PCF, we first map a pixel in the camera image to a region in the scene, and then we map that region onto the shadow map. I think this is a quite complicated operation... Maybe not worthwhile to do it I guess." CreationDate="2018-09-17T11:46:52.190" UserId="8826" ContentLicense="CC BY-SA 4.0" />
  <row Id="10914" PostId="8075" Score="2" Text="&quot;*Now that I have plenty of time and am more aware of what shaders can do I'd like to resume development with the GPU in mind.*&quot; Is this terrain that the physics/AI/collision system needs to know about? Because if it is, the GPU is probably not the best place to do this." CreationDate="2018-09-18T03:23:11.310" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10915" PostId="8075" Score="0" Text="@NicolBolas Yeah it is, but since terrain never changes, chunks just need to be calculated once." CreationDate="2018-09-18T16:54:42.067" UserId="9372" ContentLicense="CC BY-SA 4.0" />
  <row Id="10916" PostId="8076" Score="1" Text="Thanks! I just mean &quot;why is it restricted to [0,1)?&quot;. I came up with this question when I was implementing shadow mapping. When using shadow maps, if the depth values stored in the shadow map are all between [0,1), we have to clamp the depth value of the pixel to [0,1) first and then do the comparison. If the shadow map stores the actual depth values as floats, we can just compare them directly." CreationDate="2018-09-18T17:52:39.947" UserId="8826" ContentLicense="CC BY-SA 4.0" />
  <row Id="10917" PostId="7967" Score="0" Text="This webpage demonstrates a ***simple*** program [MakeHuman](http://www.makehumancommunity.org/wiki/Video:Unity) which allows to easily create human models, add clothing etc. You would need to create your own custom clothing files to dress the models with specific clothes rather than the ones that are included. All of this is possible but it's not simple to do. These models can be exported to Blender and controlled with Python. Neither of us knows how you would do this with [ImageMagick](https://www.imagemagick.org/script/index.php) it doesn't have those features." CreationDate="2018-09-18T18:12:29.823" UserId="9258" ContentLicense="CC BY-SA 4.0" />
  <row Id="10918" PostId="8081" Score="0" Text="Can't think of a better way to do this, thanks. Maybe I also could get rid of functions and write all the code in a single function &quot;noise(vec3 v)&quot;? or do shader compilers do the inlining work for me?" CreationDate="2018-09-19T13:03:30.030" UserId="9372" ContentLicense="CC BY-SA 4.0" />
  <row Id="10919" PostId="8081" Score="1" Text="@Trap: I don't know how inlining works in shaders. In my case, I wrote the nodes as their own functions because I could just drop the big chunk of prewritten text at the beginning of the shader and go from there.  I also didn't have many nodes (I inlined the arithmetic, so only more complex operations had their own functions).  I think it may also be possible to use `glAttachShader` to precompile the node functions and link them to each of the created shaders to provide the functions to them, though I didn't do that." CreationDate="2018-09-19T14:20:12.780" UserId="7647" ContentLicense="CC BY-SA 4.0" />
  <row Id="10920" PostId="7967" Score="1" Text="Thanks for the update. I will have a look into this for sure." CreationDate="2018-09-19T19:13:51.530" UserId="9255" ContentLicense="CC BY-SA 4.0" />
  <row Id="10921" PostId="8057" Score="0" Text="couldn't that be a result of some tonemapping postprocessing step?" CreationDate="2018-09-20T12:45:47.277" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="10922" PostId="8083" Score="0" Text="First, how about a sample sentence showing how this term would be used?  Second, that's not ASCII art, [this is](http://users.monash.edu/~lloyd/tildeLand-Rover/LRO/ascii.art.html) (including a few of mine.)" CreationDate="2018-09-19T00:00:55.640" UserDisplayName="Roger Sinasohn" ContentLicense="CC BY-SA 4.0" />
  <row Id="10923" PostId="8083" Score="0" Text="I'm not sure how I would use it in a sentence other than searching &quot;'X' techniques&quot; where that's the thing I'm asking about. The photo above is a photo mosaic not ascii art. Making images out of something other than just individual pixels, using a real photo as input. Above is a real photo of a man, where each &quot;pixel&quot; is made of another photo, where the average color and shape match the group of pixels in the original photo in the same spot. Here's another example https://pbs.twimg.com/media/Dc2MotcW0AA3NtL.jpg" CreationDate="2018-09-19T00:10:49.163" UserDisplayName="Matt Phillips" ContentLicense="CC BY-SA 4.0" />
  <row Id="10924" PostId="8057" Score="0" Text="Nope, it doesn't have anything to do with tone mapping or any other post processing. Verified by doing other furnace tests, and comparing between different renderers which have explicit controls for those things." CreationDate="2018-09-20T14:04:59.207" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10925" PostId="8085" Score="0" Text="What are the samples and subpixels in this context? Are you ray-tracing, sampling a texture, or something else entirely?" CreationDate="2018-09-20T19:39:44.557" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10926" PostId="8085" Score="0" Text="@DanHulme I'm rendering a scene using scanline method. In order to get rid of aliasing we need to render a scene with higher resolution (2x, for example). After that we have 4 subpixels per 1 pixel. As I know, the easiest way to downsample a frame is to calculate the average color of all subpixels (regular grid algorithm?). But I can't figure out how to calculate the color of a sample which is not positioned in the center of subpixel. I think that it's calculated using 4 neighbouring subpixels,but I'm not sure." CreationDate="2018-09-20T20:02:00.570" UserId="9382" ContentLicense="CC BY-SA 4.0" />
  <row Id="10927" PostId="8057" Score="0" Text="That's strange." CreationDate="2018-09-21T15:18:39.197" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="10929" PostId="8086" Score="0" Text="Pertubated Fibonacci sphere points?" CreationDate="2018-09-21T09:13:03.090" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="10930" PostId="8086" Score="0" Text="Related (and the answer there may be relevant to a solution here): https://computergraphics.stackexchange.com/questions/51/how-can-i-generate-procedural-noise-on-a-sphere" CreationDate="2018-09-23T10:45:55.020" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10931" PostId="8086" Score="0" Text="@beyond that sounds like the beginnings of a great answer, if you could add more detail?" CreationDate="2018-09-23T10:49:19.603" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="10932" PostId="5761" Score="1" Text="I want to ask (hoping you remember the paper) if you are sure about adding cosine terms for every layer? As far as I understand, we calculate the rendering equation only on the top layer. We go downwards to determine final outgoing direction and to determine BRDF. Do I miss something?" CreationDate="2018-09-23T11:31:45.583" UserId="6698" ContentLicense="CC BY-SA 4.0" />
  <row Id="10933" PostId="8093" Score="0" Text="One practical way to do this could be to calculate screen space derivatives of world space position, with ddx/ddy and do a cross product to get a world space normal." CreationDate="2018-09-23T16:16:57.413" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="10934" PostId="8085" Score="0" Text="@PaviełKraskoŭski i need any tutorial to understand this problem" CreationDate="2018-09-23T23:29:59.837" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="10935" PostId="8097" Score="0" Text="what about the treatment of general ovaloids?" CreationDate="2018-09-24T21:23:28.853" UserId="6183" ContentLicense="CC BY-SA 4.0" />
  <row Id="10936" PostId="8093" Score="0" Text="https://www.google.ch/search?q=screen+space+derivatives+of+world+space+position&amp;oq=screen+space+derivatives+of+world+space+position&amp;aqs=chrome..69i57.410703j0j4&amp;sourceid=chrome&amp;ie=UTF-8 for the term &quot;screen space derivatives of world space position&quot;" CreationDate="2018-09-24T21:25:15.270" UserId="6183" ContentLicense="CC BY-SA 4.0" />
  <row Id="10937" PostId="8097" Score="0" Text="As in non-ellipsoidal ones? The usual way to make odd shapes like that is to warp the domain—if you shift where you’re sampling your function, you shift the appearance of the shape. Check out the “domain deformations” section of the Quílez article for some examples." CreationDate="2018-09-24T22:18:37.520" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="10938" PostId="8085" Score="0" Text="@x-rw [Here](https://mynameismjp.wordpress.com/2012/10/24/msaa-overview/) you can read about supersampling at the beginning of the article before &quot;SUPERSAMPLING EVOLVES INTO MSAA&quot;. [Wikipedia](https://en.wikipedia.org/wiki/Supersampling). [Link](https://www.everything2.com/index.pl?node_id=1028947) on the Wikipedia page to the reference article." CreationDate="2018-09-25T09:27:29.410" UserId="9382" ContentLicense="CC BY-SA 4.0" />
  <row Id="10939" PostId="8100" Score="0" Text="Can you explain what does &quot;offset the X&amp;Y screen coordinates by a sub-pixel amounts.&quot; mean ? Thank you :D" CreationDate="2018-09-25T13:24:30.323" UserId="9408" ContentLicense="CC BY-SA 4.0" />
  <row Id="10943" PostId="5761" Score="0" Text="@MustafaIşık You are right, in the paper they do not add the cosine terms to the final BRDF. Actually, I have adopted an approach a bit different from the one proposed in the paper (I will include that to my answer). In the paper, they propose the definition of an unique BRDF from the layered BRDFs. I found the derivation process a bit confusing: 1) I do not understand why they add the individual BRDFs (I think they should be multiplied);" CreationDate="2018-09-25T15:26:01.647" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="10944" PostId="5761" Score="0" Text="@MustafaIşık 2) as you have pointed out, there is no multiplication by the cosine term (I think you have to because we are considering only the energy that is incident on a differential area); 3) they do not develop the expression $t=(1−G)+T21⋅G$; 4) I do not know if they have diluted the cosine term inside the BRDF; 5) also, TIR may cause energy loss. Due those reasons, I have decided to trace each ray using Russian-Roulette to decide for refraction or reflection, at the cost of longer rendering times then those eventually obtained by them." CreationDate="2018-09-25T15:28:21.250" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="10945" PostId="5761" Score="1" Text="The paper looks straightforward but it's not since not every term nor strategy is explained clearly enough for an exact implementation. It is influential, though. Yours is more like a simulation avoided by the paper but more accurate." CreationDate="2018-09-25T18:52:21.953" UserId="6698" ContentLicense="CC BY-SA 4.0" />
  <row Id="10946" PostId="8102" Score="0" Text="Can you create an none textured specular-only and fresnel-only image to help visualise were the problem is ?" CreationDate="2018-09-26T02:12:17.823" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="10947" PostId="8086" Score="0" Text="You should be able to find the code for creating a Fibonacci sphere. These points are somewhat evenly distributed over the sphere. In order to make it look more real-life like you could pertubate the points randomly on the surface. This way you could make your grass sphere." CreationDate="2018-09-27T09:56:58.433" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="10948" PostId="8101" Score="0" Text="I can't tell exactly what you're asking, but is [OpenFab](http://openfab.mit.edu/) relevant to your question?" CreationDate="2018-09-29T18:15:44.993" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10949" PostId="8102" Score="1" Text="This PBR code contains many calculations prone to math errors like division by zero which cause undefined behaviour, i'd check that first and add some margin to prevent this like `max(1e-5, something)`. I actually implemented this tutorial some time ago and did some corrections." CreationDate="2018-09-30T18:11:53.217" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="10950" PostId="104" Score="1" Text="That comment legitimately made me sad :(" CreationDate="2018-09-30T22:07:55.317" UserId="9432" ContentLicense="CC BY-SA 4.0" />
  <row Id="10951" PostId="5223" Score="0" Text="I'm currently trying to implement the path space MLT on my own, but struggle a bit. Are you willing to share your code with me?" CreationDate="2018-10-01T14:18:42.100" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="10953" PostId="8111" Score="0" Text="Be advised that, in some cases, the same construct in GLSL can mean subtly different things in OpenGL and Vulkan. Not to mention the differences in layout qualifiers for resource binding." CreationDate="2018-10-02T20:10:28.920" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10955" PostId="8105" Score="0" Text="Have you been misreading $1-t$ as $1/t$ this whole time?" CreationDate="2018-10-03T06:44:39.953" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10956" PostId="8110" Score="1" Text="Interesting question! I don't think there is an exact analytical solution, but you could probably figure out something approximate and reasonable-looking by using a first-order approximation: Take the SDF values $\phi_i(x)$ and gradients $\nabla\phi_i(x)$ at the current point $x$, construct half-spaces $S_i=\{y:\phi_i(x)+(y-x)\cdot\nabla\phi_i(x)\le0\}$, and take the distance from $x$ to their intersection." CreationDate="2018-10-03T06:56:49.053" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10957" PostId="8105" Score="1" Text="Yeah for like 5 days" CreationDate="2018-10-03T21:49:42.427" UserId="9429" ContentLicense="CC BY-SA 4.0" />
  <row Id="10958" PostId="8113" Score="0" Text="What part is giving you trouble?" CreationDate="2018-10-03T22:04:05.627" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10959" PostId="8113" Score="0" Text="I am not trying to implement it, I want to know if something that does something like that exists already." CreationDate="2018-10-03T22:56:00.460" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="10960" PostId="8113" Score="1" Text="It sounds like something a VFX compositor like Nuke would do, but I haven't tried it myself." CreationDate="2018-10-04T11:21:13.820" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10962" PostId="8115" Score="0" Text="@NicolBolas Sorry. I have made a mistake. I have edited my question. &quot;Then the vector t will not in the plane that w and u&quot; should be &quot;Then the vector t will not in the plane that w and v&quot;" CreationDate="2018-10-05T02:01:55.243" UserId="3416" ContentLicense="CC BY-SA 4.0" />
  <row Id="10963" PostId="8115" Score="0" Text="It isn't *supposed* to be in that plane. W is the direction of the view. V is supposed to be to the right of the view. If T is up, you can't have the right of the view in the same plane as up." CreationDate="2018-10-05T02:04:53.723" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="10964" PostId="8115" Score="0" Text="@NicolBolas I have added two pictures. The v here is top of the view." CreationDate="2018-10-05T02:16:51.480" UserId="3416" ContentLicense="CC BY-SA 4.0" />
  <row Id="10965" PostId="8116" Score="0" Text="I have seen many tutorials set t as Vector.up(0,1,0) in world space. So that only means the camera is put straight. The Vector.up(0,1,0) can't set up a camera with rotation around gaze direction. Have I understood your idea properly?" CreationDate="2018-10-05T03:06:42.790" UserId="3416" ContentLicense="CC BY-SA 4.0" />
  <row Id="10966" PostId="8116" Score="0" Text="&quot;If g isn't changing, and the camera's orientation is defined by two directions, then you can only get a different orientation if t is changing.&quot;  So if I want to set up a camera with rotation around gaze direction, the t should be set other than Vector.up(0,1,0)?" CreationDate="2018-10-05T03:08:36.497" UserId="3416" ContentLicense="CC BY-SA 4.0" />
  <row Id="10967" PostId="8114" Score="1" Text="You're probably on the wrong SE side for this, as your problem is more a physical and a mathamtical one. Try https://math.stackexchange.com/ or https://physics.stackexchange.com/ or search for a fitting one here https://stackexchange.com/sites#&#xA;Furthermore, try to ask specific questions about things you don't understand. Recommendation questions usually are closed as well." CreationDate="2018-10-05T06:13:30.693" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="10968" PostId="8114" Score="0" Text="This is not my field, but there are quite a number of SIGGRAPH/Eurographics papers covering animation of elastic materials. Siggraph also runs courses - a quick search turned up this collection which might help: http://www.physicsbasedanimation.com/resources-courses/" CreationDate="2018-10-05T07:56:24.423" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="10970" PostId="8114" Score="0" Text="Do you want to simulate it in real time? If so, shape matching (Meshless Deformations Based on Shape Matching by Matthias Müller) may be useful. If you can afford offline simulation, you would get better quality but it costs extra (backward Euler+accurate constitutive model)." CreationDate="2018-10-05T16:59:56.690" UserId="120" ContentLicense="CC BY-SA 4.0" />
  <row Id="10972" PostId="8119" Score="0" Text="Your best option, if I have understood the question, is Tomas Akenine-Moller box triangle overlap algorithm. http://fileadmin.cs.lth.se/cs/Personal/Tomas_Akenine-Moller/code/" CreationDate="2018-10-06T07:07:54.590" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10973" PostId="8119" Score="2" Text="@ali From the question it seems he already *has* a good way of intersecting a box with a triangle (or simply doesn't care about exact intersection). What he's looking for is the higher level infrastructural way to sort the triangles into buckets (and a way to layout that bucket datastructure in the first place) independent of how that triangle-box intersection is computed on the lower level. I can't see anything in your link adressing the problems from the question. If it does, feel free to elaborate a little more, preferably in an answer." CreationDate="2018-10-06T12:35:22.193" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="10974" PostId="8122" Score="3" Text="(1) For a random ray, this happens with probability zero, so it doesn't really matter what you do here. (2) If the ray meets the sphere at exactly 1 point, it is tangent to the sphere, so the reflected ray will be in exactly the same direction, and it is as if there was no intersection." CreationDate="2018-10-07T07:19:01.330" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="10975" PostId="8125" Score="0" Text="You might find the [Transformations](https://learnopengl.com/Getting-started/Transformations), [Coordinate Systems](https://learnopengl.com/Getting-started/Coordinate-Systems) and [Camera](https://learnopengl.com/Getting-started/Camera) chapters of LearnOpenGL helpful." CreationDate="2018-10-08T07:45:43.490" UserId="8288" ContentLicense="CC BY-SA 4.0" />
  <row Id="10976" PostId="8114" Score="0" Text="For stability consider a different integration scheme (perhaps an implicit one), take smaller timesteps or an adaptive timestep, avoid using thin triangles (if you're using triangles), impose various dampers or caps to things like acceleration. All that stuff is just going to make performance worse. For performance consider parallelizing it and making it &quot;adaptive.&quot; Other methods like FEM, MPM, SPH, FLIP, PIC are all just other ways to implement PDEs. All of them are likely to be slower than a spring mass system." CreationDate="2018-10-08T08:57:23.740" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="10978" PostId="8131" Score="2" Text="Have you tried calculating the TBN in the fragment shader? When calculating it in the vertex shader, OpenGL will linearly interpolate the matrix for use in the fragment shader and that might cause the artifacts. Do make sure that you normalize the tangent, bitangent and normal in the fragment shader before using them (again because of  the linear interpolation)." CreationDate="2018-10-09T06:36:20.143" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="10981" PostId="8131" Score="0" Text="Yes I tried to do this in the fragment shader and normalized the vectors before, and I still get these strange artifacts" CreationDate="2018-10-09T13:55:42.450" UserId="8893" ContentLicense="CC BY-SA 4.0" />
  <row Id="10982" PostId="8131" Score="0" Text="Tangents are multiplied by the model matrix, as they dont suffer from non-uniform scale disortion as normal vector do. Also, you do need a vertex normal and a normal vector extracted from the normal map, however, I see you are applying the normalization (`normalize(normal * 2.0f - 1.0f)`) to the same vector you used to create the TBN matrix" CreationDate="2018-10-10T14:34:14.327" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="10983" PostId="8131" Score="0" Text="I thought the tangents and bitangents would also suffer from possibly being non-orthogonal to each other if scaled non-uniformly. Also I edited my fragment shader code above to showcase that I am actually sampling the normal from the normal map. My bad!" CreationDate="2018-10-10T16:51:49.270" UserId="8893" ContentLicense="CC BY-SA 4.0" />
  <row Id="10984" PostId="3969" Score="0" Text="Have a look at this thread, specifically Greg Ward answer later in the posts. I think it is related to your question. https://www.radiance-online.org:447/pipermail/radiance-general/2015-June/011053.html" CreationDate="2018-10-10T18:47:42.507" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="10985" PostId="3969" Score="0" Text="@ali do you mean it's related to my dumb addition of unweighted RGB values? I've already found this mistake and reflected on it in my answer." CreationDate="2018-10-10T19:24:05.810" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="10986" PostId="8131" Score="1" Text="Have you identified which part of the code generates the color of the artifact? Have you tried to reduce your shader to a minimal reproduce case? I wouldn't be surprised if it was due to either a normal facing away from light, or a nearly 0 but negative parameter to a `pow()` function." CreationDate="2018-10-11T04:35:56.500" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="10987" PostId="8141" Score="0" Text="What happens when someone wants to erase, and how do I approximate strokes if the user can draw whatever shape the want?" CreationDate="2018-10-12T10:12:56.377" UserId="9495" ContentLicense="CC BY-SA 4.0" />
  <row Id="10988" PostId="8141" Score="1" Text="Erasing is drawing with the background color. You can use a series of splines to approximate each stroke." CreationDate="2018-10-12T10:57:11.057" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="10989" PostId="4931" Score="0" Text="A more complete example is now available here: https://www.gsn-lib.org/docs/gallery.php?projectName=ColorMix&amp;graphName=ColorMix" CreationDate="2018-10-12T13:21:44.567" UserId="4652" ContentLicense="CC BY-SA 4.0" />
  <row Id="10994" PostId="8125" Score="0" Text="Definately check out GFLW and ImGui (super easy GUI, speedy and small footprint)." CreationDate="2018-10-08T12:55:46.307" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11000" PostId="8150" Score="0" Text="Awesome, thank you! I expect to have up to ~30 high-resolution volumes in memory at once so keeping them all as small as possible is important; interpolating between neighbour voxels instead of local corners should make that much easier." CreationDate="2018-10-16T01:58:34.557" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11001" PostId="8151" Score="2" Text="If you have a found an answer to your question, the standard thing to do is to add an answer explaining what was going on. You normally wouldn't delete a question, specially since you would need help from an admin to do that anyways." CreationDate="2018-10-17T02:06:49.887" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="11002" PostId="5535" Score="0" Text="Very precise and finely noted point!" CreationDate="2018-10-17T10:38:17.753" UserId="4504" ContentLicense="CC BY-SA 4.0" />
  <row Id="11003" PostId="8154" Score="0" Text="What do you think of [this](https://www.colour-science.org/posts/about-rendering-engines-colourspaces-agnosticism/) writeup?  If I'm reading it right, it demonstrates that calculations in different linear color spaces *do* lead to different results." CreationDate="2018-10-17T19:47:00.893" UserId="8288" ContentLicense="CC BY-SA 4.0" />
  <row Id="11004" PostId="8154" Score="0" Text="@Maxpm: That's interesting. My read of that paper is that the problem boils down to the fact that light doesn't actually fit into our RGB colorspace model. This causes divergent visual results in what ought to mathematically be the same thing. There, the only solution seems to be to stop using RGB and start using spectral rendering." CreationDate="2018-10-18T01:56:13.830" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11005" PostId="8154" Score="0" Text="@Maxpm but off course they do after all the other spaces are different. But then RGB is not color so there is that. But then there is the question ho correct do you want to be. The gains become smaller and smaller," CreationDate="2018-10-18T16:52:09.667" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="11006" PostId="8156" Score="0" Text="How is this more directly than any other method?" CreationDate="2018-10-18T17:00:27.077" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="11007" PostId="8156" Score="1" Text="Think about it like this.  If you increase 30 to 40, do you want your x,y numbers to get bigger or smaller?  I think smaller.  (wider field of view makes an objects appear smaller)  So you want a bigger number in the denominator. so you want the first one.  (z * tan(30))" CreationDate="2018-10-18T17:23:04.903" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="11008" PostId="8157" Score="0" Text="You're more likely to get good answers if you can make your question make sense to people who aren't reading the same book chapter as you." CreationDate="2018-10-18T17:23:19.000" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11009" PostId="8157" Score="0" Text="If you point out what I should expound that would help me to formulate better the question." CreationDate="2018-10-18T17:36:32.147" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="11010" PostId="8157" Score="0" Text="What is the problem you're trying to solve and what is $\Delta^k$ in this context?" CreationDate="2018-10-18T17:41:27.650" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11012" PostId="8157" Score="0" Text="Hopefully is a bit better now, please point out anything that can make it clearer." CreationDate="2018-10-19T09:26:38.600" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="11013" PostId="8157" Score="0" Text="Why not evaluate $L$ at $x(t)$ and use it in the implicit solve?" CreationDate="2018-10-19T20:22:01.040" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="11014" PostId="8157" Score="0" Text="If you had a reference that does that it would be great." CreationDate="2018-10-19T20:34:55.503" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="11016" PostId="8161" Score="0" Text="Thanks for the answer! your explanation of origin of the formula used in Stam's paper, really helps to get a deeper understanding of the math." CreationDate="2018-10-21T13:25:45.037" UserId="9528" ContentLicense="CC BY-SA 4.0" />
  <row Id="11017" PostId="8173" Score="1" Text="Without replicating the bits the LSBs will be 0, so for the maximum value of 0x1f (max for 5 bits) it would expand to 0xf8 when converted to 8 bit. What you want is 0xff so the range of 0x00-&gt;0x1f will be mapped to 0x00-&gt;0xff instead of 0x00-&gt;0xf8." CreationDate="2018-10-23T04:07:01.990" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11018" PostId="8173" Score="2" Text="@PaulHK Please post that as an answer. It's complete as it stands, but as a comment it's not searchable." CreationDate="2018-10-23T11:24:18.320" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11019" PostId="8172" Score="0" Text="If it's any help, it's an isometric projection. It's something that's been around in technical drawing since before CG." CreationDate="2018-10-23T12:51:00.927" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11020" PostId="8181" Score="0" Text="Great! How did you find this formula `abs(q)/dot(q,q) - iMouse.xy/r`? Is it empirical research or do you have a method to find formulas which have good properties?" CreationDate="2018-10-23T21:20:09.400" UserId="2173" ContentLicense="CC BY-SA 4.0" />
  <row Id="11021" PostId="8181" Score="1" Text="@arthur.sw I found this formula here: [link](http://www.fractalforums.com/new-theories-and-research/very-simple-formula-for-fractal-patterns/), and then adapted it to work for my shader." CreationDate="2018-10-24T01:57:26.180" UserId="9561" ContentLicense="CC BY-SA 4.0" />
  <row Id="11022" PostId="8173" Score="0" Text="Yes thank you @PaulHK this answers correctly my question" CreationDate="2018-10-24T03:33:49.397" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="11023" PostId="8175" Score="0" Text="What do you mean with part(s)? Like closed patches within a plane that are visible from the camera?" CreationDate="2018-10-24T07:59:54.837" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="11026" PostId="8178" Score="1" Text="Are you asking &quot;why is pixel 0,0 at the *top* left of some formats and *bottom* left of others?&quot; If so, the short answer is probably that _a)_ some formats simply mimic the way raster scanning for displays (particularly CRTs) is performed - the HW reads teh pixel data in memory in increasing address order and sends to the display from top row to bottom row, and left to right within each row.  For _b)_ I suspect some formats wanted to mimic how humans draw graphs, with (0,0) at the bottom left and the Y-axis going up the screen." CreationDate="2018-10-24T08:40:11.760" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11027" PostId="8178" Score="1" Text="It is exactly what I wanted to ask... I take your proposal of title. jpg seems to be the only one different from other formats (tiff, png, jp2000, bmp...)." CreationDate="2018-10-24T10:23:47.347" UserId="9557" ContentLicense="CC BY-SA 4.0" />
  <row Id="11028" PostId="8178" Score="0" Text="According to https://en.wikipedia.org/wiki/BMP_file_format#Pixel_array_(bitmap_data) BMPs are also bottom to top." CreationDate="2018-10-24T13:52:22.660" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11029" PostId="8178" Score="0" Text="There has to be some standard for this. I don't know why I personally feel top left to be (0,0) and bottom right as (200,200). Perhaps, due to Khan Academy's usage of `processing.js`" CreationDate="2018-10-24T17:00:10.367" UserId="9567" ContentLicense="CC BY-SA 4.0" />
  <row Id="11030" PostId="8178" Score="0" Text="MIT Scratch has a similar co-ordinate system. So, my guess is that format developers worked in different graphic environments that influenced their decision on where to place origin. I feel left corner positions appropriate because it helps people stick to positive numbers and also LTR progression (ie, 0&lt;1&lt;2&lt;3). Negative numbers take one more bit to store sometimes, or you have do some weird number inversion (ie, compliment). All in all, LOGO taught kids to move from center to different directions, assuming each new position as starting point, I feel that's where a lot of people got intuition." CreationDate="2018-10-24T17:05:30.667" UserId="9567" ContentLicense="CC BY-SA 4.0" />
  <row Id="11031" PostId="348" Score="0" Text="I like how this is simpler than the u-v form of the equation.  There's a lot to be studied in DFT on how this is good and what can improve." CreationDate="2018-10-24T17:07:50.870" UserId="9567" ContentLicense="CC BY-SA 4.0" />
  <row Id="11032" PostId="8156" Score="0" Text="I expect a simplified answer to this. I understand the question but not well enough to know what transform to apply to the vertex." CreationDate="2018-10-24T17:09:46.803" UserId="9567" ContentLicense="CC BY-SA 4.0" />
  <row Id="11034" PostId="8186" Score="4" Text="&quot;When you call a rendering function, it is given to the GPU &quot;right then&quot; and that's the end of it.&quot;&#xA;&#xA;Or more specifically, given to the driver, to give to the GPU at some point between _now_ and _never_." CreationDate="2018-10-24T20:38:23.210" UserId="9306" ContentLicense="CC BY-SA 4.0" />
  <row Id="11038" PostId="8181" Score="1" Text="If you're interested in code golfing techniques, we also have [a site for that](https://codegolf.stackexchange.com/)... They post challenges for competition, but also accept questions tagged &quot;tips&quot; for advice on golfing in general or reducing a specific piece of code." CreationDate="2018-10-24T22:00:04.180" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="11039" PostId="8179" Score="2" Text="Thank you for a detailed explanation with nice formulas. I was curious about the error introduced by the approximation so I made this graph that compares both formulas: https://www.desmos.com/calculator/cvaqofyvbf . However I prefer PaulHK's answer since it is easier to understand." CreationDate="2018-10-25T02:12:40.047" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="11040" PostId="8175" Score="0" Text="Yes, that's it. I have meanwhile realized that it may be a dumb question because it's in fact a complementary problem of shadow mapping." CreationDate="2018-10-25T08:06:24.923" UserId="9553" ContentLicense="CC BY-SA 4.0" />
  <row Id="11041" PostId="8175" Score="2" Text="I would make it simpler than shadow mapping, more close the the picking hack. I would render g-buffers for whatever info is needed of (color/pos/depth/etc.), and adding an extra render target where you would control what to render by using a uniform color for each element (using black for those you are not interested in, for example)." CreationDate="2018-10-25T08:33:55.067" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="11042" PostId="8179" Score="1" Text="Minor quibble, if m &gt;= 2n then you need to change your &quot;approximation&quot; equation.  An extreme, example, is if n=1, then you need to repeat the string 8 times (i.e. perform log2(8)=3 steps).&#xA;&#xA;Of course, if you *pad* with &quot;10...0&quot; instead of all zeros, then on average you'll have a lower error, but lose the extremes.&#xA;&#xA;&quot;However I prefer PaulHK's answer&quot; :-)  Well, there's no accounting for taste 8P." CreationDate="2018-10-25T11:54:32.073" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11043" PostId="4432" Score="0" Text="Disjoint timer queries in GL: https://www.khronos.org/registry/OpenGL/extensions/EXT/EXT_disjoint_timer_query.txt" CreationDate="2018-10-25T15:35:49.980" UserId="6442" ContentLicense="CC BY-SA 4.0" />
  <row Id="11044" PostId="8185" Score="2" Text="Regarding the second part, 'kernel' in graphics has a couple of meanings which are distinct from other uses (OS kernel etc.). A 'convolution kernel' is a 2D array of weights which describes how neighbouring pixels affect the current pixel, for doing blur, edge detection etc. A 'compute kernel' is just a chunk of code to be executed by a compute shader on GPU, using each shader instance to process a different data point. You can think of it as being the body of a parallel-for loop." CreationDate="2018-10-26T14:40:01.330" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="11045" PostId="8189" Score="1" Text="&quot;*Note that the texture parameters are always NEAREST*&quot; What exactly are you expecting this to do?" CreationDate="2018-10-27T01:56:51.660" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11046" PostId="8189" Score="0" Text="Well I have another piece of code which runs on the GPU and fills the half-size texture with every other row and column of the original. The results from that are good, with no blocky artifacts or jagged edges. I thought this would be the shader equivalent." CreationDate="2018-10-27T02:49:24.310" UserId="9577" ContentLicense="CC BY-SA 4.0" />
  <row Id="11047" PostId="8189" Score="0" Text="Did you try `GL_LINEAR` nearest neighbour is supposed to give blocked artifacts, linear will interpolate hence will look smoother imo?" CreationDate="2018-10-27T02:50:54.620" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11048" PostId="5278" Score="0" Text="`not the same size` solved my problem." CreationDate="2018-10-27T19:37:00.463" UserId="9580" ContentLicense="CC BY-SA 4.0" />
  <row Id="11049" PostId="8190" Score="0" Text="&quot;*I eventually want to convert the sampled Quaternion to Euler angles an apply the Euler rotation in some 3D shapes.*&quot; Please stop wanting to do that. Euler angles are bad, and you've taken a big step by moving away from them. Keep your orientations as quaternions; don't convert to/from Euler angles." CreationDate="2018-10-28T01:48:53.817" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11050" PostId="8191" Score="0" Text="The value of the HRESULT you're getting should shed some light on the reason." CreationDate="2018-10-28T09:42:01.980" UserId="2817" ContentLicense="CC BY-SA 4.0" />
  <row Id="11051" PostId="8190" Score="0" Text="@NicolBolas I really want to avoid using Euler angles but unfortunately I have to use Euler angles for a part of my current project. Could you also take a look at my new, relevant question [here](https://computergraphics.stackexchange.com/questions/8195/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back) and see if you can provide an answer for this one?" CreationDate="2018-10-28T16:57:44.247" UserId="9581" ContentLicense="CC BY-SA 4.0" />
  <row Id="11052" PostId="8193" Score="0" Text="Thanks Russ. Could you please take a look at my new, relevant question [here](https://computergraphics.stackexchange.com/questions/8195/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back) and see if you can provide an answer for this one as well?" CreationDate="2018-10-28T16:58:05.527" UserId="9581" ContentLicense="CC BY-SA 4.0" />
  <row Id="11053" PostId="8195" Score="2" Text="&quot;*This way I would know how I can get the correct Quaternions even if the order of rotations (XYZ --&gt; YZX etc) for applying Euler angles is changed.*&quot; That makes no sense. If you change the order of the Euler angle composition, then the &quot;same&quot; rotation angles will represent a different orientation. And therefore, you would get a different quaternion. The math you use *must* be aware of the ordering, and you would therefore have to provide that ordering in some way." CreationDate="2018-10-28T23:02:44.627" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11054" PostId="8195" Score="0" Text="@NicolBolas Sorry I'm pretty bad at stuff like Quaternions and other rotation-related stuff." CreationDate="2018-10-28T23:31:05.237" UserId="9581" ContentLicense="CC BY-SA 4.0" />
  <row Id="11061" PostId="8201" Score="1" Text="Sounds like a UI programming question" CreationDate="2018-10-30T08:23:47.043" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11069" PostId="8205" Score="0" Text="Could it be an old version of gnuplot? https://en.wikipedia.org/wiki/Gnuplot" CreationDate="2018-10-30T15:26:27.503" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11070" PostId="8201" Score="0" Text="hmm I did think it was but the issue is more on resolution of the image?" CreationDate="2018-10-30T17:21:12.067" UserId="9594" ContentLicense="CC BY-SA 4.0" />
  <row Id="11071" PostId="8202" Score="1" Text="Depth buffer and deferred shading G-buffers will also be multiples of the screen size. It's not unlikely to have a few hundred megabytes of render targets...at 4K it could even hit a gigabyte. Still, as you note that's still a small fraction of an 8GB card." CreationDate="2018-10-30T18:27:18.673" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11072" PostId="8177" Score="0" Text="I still could not figure out why choosing plane equations, not other equations?" CreationDate="2018-10-31T05:26:41.853" UserId="6407" ContentLicense="CC BY-SA 4.0" />
  <row Id="11074" PostId="8177" Score="0" Text="Because you want to express $x$ (or $y$ or $z$) in terms of $s$ and $t$ in the simplest possible way. Intuitively you would expect this to be a linear combination of $s$ and $t$, which is precisely what you get when you rearrange the plane equation. This works if the triangle is non-degenerate in $(s,t)$-space, because then the points $(x_i,s_i,t_i)_{i=1..3}$ that determine the plane are non-collinear." CreationDate="2018-10-31T11:15:09.520" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="11077" PostId="8205" Score="0" Text="Thanks for the suggestions. That particular Figure is from the 1990s, but I've seen the style in older papers too. I may ask some of my more senior colleagues what they remember using back in the old days!" CreationDate="2018-10-31T15:18:44.320" UserId="9603" ContentLicense="CC BY-SA 4.0" />
  <row Id="11078" PostId="8209" Score="0" Text="`tr(&quot;*.off&quot;)` - I'm pretty sure you don't want to translate the file extension to other languages." CreationDate="2018-10-31T17:05:44.583" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11079" PostId="8202" Score="0" Text="Good point Nathan, I overlooked deferred rendering. At 4k resolution * float RGBA you're looking at around 128mb, which is astronomical to me." CreationDate="2018-11-01T01:59:57.653" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11080" PostId="8202" Score="0" Text="A very good and detailed answer! Thank you." CreationDate="2018-11-01T13:44:38.067" UserId="9582" ContentLicense="CC BY-SA 4.0" />
  <row Id="11081" PostId="8190" Score="0" Text="@Amir Why do not you just use matrices to rotate?" CreationDate="2018-11-02T02:51:58.670" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11082" PostId="8213" Score="2" Text="This looks normal and is caused by the perspective camera being close to the spheres and having a large FOV value." CreationDate="2018-11-02T05:29:12.040" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11085" PostId="8213" Score="0" Text="Btw, this is not exclusive to ray tracing." CreationDate="2018-11-02T11:00:24.527" UserId="7750" ContentLicense="CC BY-SA 4.0" />
  <row Id="11086" PostId="8214" Score="0" Text="Nice example. It might help to have some kind of a texture (e.g. a grid) on the ground plane, though; otherwise it's hard to tell the difference between changing the view angle and just moving the camera. (See also: [dolly zoom](https://en.wikipedia.org/wiki/Dolly_zoom).)" CreationDate="2018-11-02T11:47:37.140" UserId="525" ContentLicense="CC BY-SA 4.0" />
  <row Id="11087" PostId="8214" Score="2" Text="It should also be noted that this effect typically happens with a high-FOV relative to the distance of the objects from the camera. It looks unnatural because you usually don't see objects from *that close* to your eyes. At least, not without quickly turning your eyes." CreationDate="2018-11-02T13:56:24.260" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11088" PostId="8212" Score="0" Text="thats a good note and clear explanation, Noah. thanks for this." CreationDate="2018-11-02T14:56:02.850" UserId="9594" ContentLicense="CC BY-SA 4.0" />
  <row Id="11090" PostId="8217" Score="2" Text="The problem is the `dydx` computation, which cannot distinguish between equal and opposite tangents, e.g. $(1,0)$ and $(-1,0)$. You should just use $(dy/dt, dx/dt)$ as the tangent vector and normalize it." CreationDate="2018-11-03T16:32:16.880" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="11091" PostId="8217" Score="0" Text="OK, I'll give it a shot tomorrow. Thanks for the suggestion!" CreationDate="2018-11-03T16:47:39.613" UserId="9631" ContentLicense="CC BY-SA 4.0" />
  <row Id="11092" PostId="8218" Score="0" Text="Ok, your answer gave me a better insight. One small observation, in your option 2 you said the equation isn't linear, actually the equation isn't linear even in the first case." CreationDate="2018-11-04T16:03:16.867" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="11093" PostId="8219" Score="0" Text="A full perspective **projection** matrix is in the form of $M = M_{ortho}M_{persp}$ which is a orthographic matrix applied on a perspective matrix." CreationDate="2018-11-04T20:32:14.830" UserId="9637" ContentLicense="CC BY-SA 4.0" />
  <row Id="11094" PostId="8220" Score="0" Text="Thanks! But what does my matrix do, then? Doesn't it transform from a view space (with a perspective camera at the origin looking into -z) to a &quot;clip space&quot; (rectangular view volume) where far points have larger z values?" CreationDate="2018-11-04T21:26:56.703" UserId="4650" ContentLicense="CC BY-SA 4.0" />
  <row Id="11095" PostId="8220" Score="0" Text="If your far plane is behind the near plane, far points will have smaller values, but after applying the orthographic transformation matrix it will flip back." CreationDate="2018-11-04T21:33:52.040" UserId="9637" ContentLicense="CC BY-SA 4.0" />
  <row Id="11096" PostId="8219" Score="0" Text="Also, the two sets of equations you have given are equivalent, you might have a mistake elsewhere." CreationDate="2018-11-04T21:39:40.610" UserId="9637" ContentLicense="CC BY-SA 4.0" />
  <row Id="11097" PostId="8220" Score="0" Text="I misread your equation, what you have given is indeed a projection matrix, but a very simplified one, where r = -l, t = -b and n &lt; f. Your two sets of equations are equivalent too, so the mistake is not in the matrix, it should be elsewhere." CreationDate="2018-11-04T21:42:39.370" UserId="9637" ContentLicense="CC BY-SA 4.0" />
  <row Id="11098" PostId="8219" Score="0" Text="Are they really equivalent, though? I mean, the first $B$ is $-2fn/(f-n) \neq -2fn/(n-f)$ (i.e., my $B$)." CreationDate="2018-11-04T22:07:08.950" UserId="4650" ContentLicense="CC BY-SA 4.0" />
  <row Id="11100" PostId="8219" Score="0" Text="There must be a typo somewhere, your $B = \frac{-2fn}{f-n}$ is correct. Prehaps the -1 below the $A$ is causing this problem, in my derivation below it is 1, not -1." CreationDate="2018-11-04T22:19:29.187" UserId="9637" ContentLicense="CC BY-SA 4.0" />
  <row Id="11101" PostId="8217" Score="0" Text="@Rahul Noting that, sometimes one needs to use the 2nd (or 3rd) derivatives if the 1st derivative vanishes." CreationDate="2018-11-05T09:55:24.263" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11103" PostId="8218" Score="0" Text="In the first case, it is linear in $x_{n+1}$ but not in $x_n$." CreationDate="2018-11-05T12:11:34.343" UserId="1613" ContentLicense="CC BY-SA 4.0" />
  <row Id="11104" PostId="8217" Score="0" Text="@SimonF: If the 1st derivative is zero and the 2nd derivative is not, then the curve has a cusp and the tangent is not defined. Only in the extremely rare case that both the 1st and the 2nd derivatives are zero does one have to use the 3rd derivative to obtain the tangent. (And beyond that in general, the first nonzero higher-order derivative, as long as it is of odd order.)" CreationDate="2018-11-05T12:14:06.297" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="11105" PostId="8218" Score="0" Text="I think the linearity should be tested against the $\left\{x_n\right\}_{n \in \mathbb{N}}$ as a sequence, not against single samples." CreationDate="2018-11-05T12:16:41.577" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="11108" PostId="8205" Score="0" Text="Not sure if you'd have better results on [hsm.se], but might be worth a quick search there. [retrocomputing.se] is probably more of a long shot" CreationDate="2018-11-05T20:18:31.717" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="11112" PostId="8217" Score="0" Text="@Rahul It's been a long time since I was on the OpenVG spec group so this is not exactly fresh in my memory, but what of the cubic Bezier curve with control points A, A, B, C?" CreationDate="2018-11-06T08:53:13.173" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11113" PostId="8223" Score="0" Text="Are you talking about the texel formats the GPU understands, or are you talking about the file formats that images are stored in before texture upload?" CreationDate="2018-11-06T09:20:14.870" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11114" PostId="8223" Score="0" Text="About the former" CreationDate="2018-11-06T13:03:19.783" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="11115" PostId="8223" Score="0" Text="But connecting those with details about how the data is arranged in the image files would be interesting as well Dan." CreationDate="2018-11-06T13:05:50.410" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="11120" PostId="8233" Score="0" Text="&quot;*I realized that Quaternions do not have a rotation matrix*&quot; ... what? A quaternion and a rotation matrix are ways to define an orientation (relative to another orientation). You can interconvert between them, with a 2:1 mapping (a quaternion and its negation yield the same rotation matrix). So where are you getting this from?" CreationDate="2018-11-07T23:36:43.183" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11121" PostId="8233" Score="0" Text="Sorry my bad. I'm pretty bad with rotation stuff and I'm totally new to Quaternions. Could you please help me get answer to my question?" CreationDate="2018-11-08T00:23:46.920" UserId="9581" ContentLicense="CC BY-SA 4.0" />
  <row Id="11122" PostId="8235" Score="0" Text="&quot;*Quaternions are just more compact and easier to interpolate.*&quot; And easier to re-orthonormalize after applying successive rotations." CreationDate="2018-11-08T00:44:36.920" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11123" PostId="8236" Score="0" Text="So the typical flow for something like Excel would be: Draw using GDI -&gt; OS passes those pixels to the GPU for display?" CreationDate="2018-11-08T07:32:30.413" UserId="7000" ContentLicense="CC BY-SA 4.0" />
  <row Id="11124" PostId="8235" Score="0" Text="@NicolBolas or more accurately they are never not orthogonal. They only need renormalization which you can often delay for quite a bit. Or you can embrace the length as a uniform scaling factor, though you would need to adjust the matrix a bit replacing the 1 terms with the squared length of the quaternion." CreationDate="2018-11-08T11:01:52.973" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="11125" PostId="8228" Score="0" Text="Okay thank you this has given me a good start as I did not know about gl_FragDepth. The part I am struggling with now is transforming the sprite UVs to screen UVs in order to sample the right part of the depthTexture. Any thoughts?" CreationDate="2018-11-08T11:03:23.193" UserId="9647" ContentLicense="CC BY-SA 4.0" />
  <row Id="11126" PostId="8228" Score="0" Text="@Snicklefritz To get the screen UVs, upload the screen size as uniform vec2. Then, divide gl_FragCoord.xy by the screen size and you will have them. You might have to flip the y coordinate as 1 - y" CreationDate="2018-11-08T11:21:08.327" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="11127" PostId="8228" Score="0" Text="thank you for the sanity check as that's what I am doing. Still not getting the right result though." CreationDate="2018-11-08T11:46:07.570" UserId="9647" ContentLicense="CC BY-SA 4.0" />
  <row Id="11129" PostId="8238" Score="1" Text="It's worth mentioning that for some alphas and colour combinations, the colours may come out of the equation negative e.g. if you want to turn a pure red background into pure green, there's only a positive solution if $\alpha = 1$" CreationDate="2018-11-08T12:53:16.813" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11130" PostId="8228" Score="0" Text="All sorted now, the problem was that my depth texture size was wrong, I have supersampling as a PP step so the size I was giving was wrong. I wish I could just use textureSize() in shader but it's not supported with the WebGL version Threejs uses. Thank you for your help." CreationDate="2018-11-08T15:24:06.163" UserId="9647" ContentLicense="CC BY-SA 4.0" />
  <row Id="11131" PostId="8228" Score="0" Text="@Snicklefritz Great! I believe it is better not to be supported, its a performance killer :)" CreationDate="2018-11-08T15:28:38.037" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="11133" PostId="8239" Score="0" Text="I made that post awhile back and was asking for help with my normal mapping code, which I discovered wasn't the issue. I actually located the issue now and I am asking for feedback on why this is occurring." CreationDate="2018-11-08T17:04:25.793" UserId="8893" ContentLicense="CC BY-SA 4.0" />
  <row Id="11135" PostId="8239" Score="0" Text="It's fine, just update your original question with the new information you found." CreationDate="2018-11-08T17:05:25.273" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11137" PostId="8236" Score="0" Text="Yes, typically it would happen like that." CreationDate="2018-11-08T17:13:19.020" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11140" PostId="8238" Score="0" Text="thank you this is exactly what i was looking for." CreationDate="2018-11-08T19:09:55.023" UserId="9655" ContentLicense="CC BY-SA 4.0" />
  <row Id="11141" PostId="8131" Score="0" Text="@JulienGuertault I finally figured out where it was located. It is in my specular term of my BRDF. Now the question is, why is it causing this. And why does no one else run into these issues :)" CreationDate="2018-11-08T20:29:52.047" UserId="8893" ContentLicense="CC BY-SA 4.0" />
  <row Id="11142" PostId="8242" Score="0" Text="Is there a reason why you would need to? D3D11 picks a specific index (0xFFFF or 0xFFFFFFFF) to be the restart index. So can you not just avoid using that index in your meshes? If you have a mesh with exactly 65536 indices, just use a 32-bit index." CreationDate="2018-11-09T01:23:25.357" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11143" PostId="8242" Score="0" Text="@NicolBolas I'm writing an abstract rendering API which can use multiple graphics APIs in the back-end. Strip cut imposes requirements on the way the caller generates their data, and they need to be aware of it. I'd like this feature to be under the control of the caller, so they can provide their data, and say whether strip cut is used. If I can't turn this feature off in Direct3D 11, it creates an inconsistency. Since this feature didn't exist in Direct3D 10, and it can be turned off in Direct3D 12, it strikes me as odd that I can't seem to turn it off under Direct3D 11, hence the question." CreationDate="2018-11-09T04:43:40.697" UserId="9659" ContentLicense="CC BY-SA 4.0" />
  <row Id="11144" PostId="8131" Score="0" Text="Are you using HDR? You might be burning the image by overflowing the colors" CreationDate="2018-11-09T08:17:02.243" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="11145" PostId="8131" Score="0" Text="@Nadir Yes my render targets are using HDR. They are using RGBA16F" CreationDate="2018-11-09T11:34:13.013" UserId="8893" ContentLicense="CC BY-SA 4.0" />
  <row Id="11146" PostId="8242" Score="0" Text="There will be very few instances where a mesh has exactly 65536 vertices. So it's not going to matter in practice." CreationDate="2018-11-09T12:49:46.083" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="11148" PostId="8247" Score="6" Text="What have you tried and what are you stuck on? We're not a code writing site, but we can help you figure things out if  you can be more specific about what you've tried. If you haven't tried  anything,  do that first and come back with questions about whatever isn't working." CreationDate="2018-11-10T03:41:47.577" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11149" PostId="8251" Score="2" Text="Have you added a bias to your rays? Adding a bias means that you offset the ray position by the normal of the surface multiplied by a very small number, so that the ray is just off the surface. This means that the triangle that the starting point of a ray is on, is actually a bit behind the ray meaning that $t&lt;0$. If you do not have the bias, the triangle would be at $t=0$, however we are working with limited precision and then sometimes it is below zero and sometimes above zero. It would not matter how much precision you have, you would always get this. Every render engine implements this." CreationDate="2018-11-10T21:22:48.347" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="11150" PostId="8251" Score="0" Text="@bram0101 Hi. Thanks for the advice. However I'm not sure if this solves the problem, though I didn't add the bias. This is because, in my code, the occlusion test ray starts from the point light and I don't think phenomenon is caused by the $t&lt;0$ issue. But I'll definitely add the bias to make the code robust." CreationDate="2018-11-10T23:19:26.577" UserId="7334" ContentLicense="CC BY-SA 4.0" />
  <row Id="11152" PostId="8251" Score="2" Text="Just to second bram0101's comment, this definitely looks like [shadow acne](https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/ligth-and-shadows), and shadow bias is the standard solution. Just because you're using double precision doesn't mean the floating-point error is zero! If you still have the problem after adding bias, report back." CreationDate="2018-11-11T08:21:21.553" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="11153" PostId="8252" Score="0" Text="You mention two different things: 1. Monitor's resolution; 2. An image's resolution. If an image is only 1920x1080, then between 4k and FHD monitors there won't be any difference, right? And vice versa, If an image is of 4k res', but displayed on a lower resolution monitor, then the amount of pixels to be displayed is the monitor's resolution?" CreationDate="2018-11-11T08:29:02.323" UserId="9398" ContentLicense="CC BY-SA 4.0" />
  <row Id="11156" PostId="8252" Score="0" Text="I don't think I mentioned image resolution anywhere. When I talk about resolution, I mean the full resolution of the monitor - so all pixels of the monitor. A 4K monitor has 4 times the number of pixels compared to an FHD monitor, so the GPU needs to work with, and generate, data for 4x as many pixels. &#xA;Displaying a 1920x1080 *image* on a 4K monitor will occupy only a quarter of all pixels. Displaying a 4K *image* on an FHD monitor is not possible - you can only see a quarter of the image unless you scale it down to a 1920x1080 image first.&#xA;Does that answer your question?" CreationDate="2018-11-12T10:12:20.453" UserId="4943" ContentLicense="CC BY-SA 4.0" />
  <row Id="11157" PostId="8257" Score="1" Text="Nice explanation, but I would like to add that fixing the camera isn't more or less logical than fixing the world. Both are equally valid points of view https://xkcd.com/1366/" CreationDate="2018-11-12T13:54:56.747" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="11158" PostId="8258" Score="1" Text="This site is more about description of techniques and algorithms for computer graphics - not about where to find source code. However, if you were able to give more clues about what you want maybe someone could help." CreationDate="2018-11-12T15:05:22.607" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11159" PostId="8251" Score="0" Text="+1 to Rahul.  Definitely looks like &quot;acne&quot;. With floating point, it's always worth remembering:  &quot;Floating point numbers are like piles of sand; every time you move them around, you lose a little sand and pick up a little dirt. — Brian Kernighan and P.J. Plauger&quot;" CreationDate="2018-11-12T15:15:09.903" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11160" PostId="8251" Score="0" Text="@Rahul Thank you for the reference link. I've added the bias and the phenomena seems gone away. However, one more question on how I choose the bias value. I used $1e-6$ at first and some of the &quot;acnes&quot; still exists, only after I used $1e-5$ did them disappear. But I'm afraid the bias value is too high?" CreationDate="2018-11-12T16:29:25.163" UserId="7334" ContentLicense="CC BY-SA 4.0" />
  <row Id="11162" PostId="8257" Score="0" Text="@Paul Houx, see https://www.youtube.com/playlist?list=PLzH6n4zXuckrPkEUK5iMQrQyvj9Z6WCrm in this he talks  about fixing the camera" CreationDate="2018-11-13T06:36:29.003" UserId="9661" ContentLicense="CC BY-SA 4.0" />
  <row Id="11163" PostId="8259" Score="0" Text="Do the fire flies get darker after more samples or are they stuck at white? That would suggest a NAN has been output." CreationDate="2018-11-13T07:23:09.993" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11165" PostId="8259" Score="0" Text="@PaulHK Yes they get darker but still exists. Does that suggest anything?" CreationDate="2018-11-13T14:23:28.190" UserId="7334" ContentLicense="CC BY-SA 4.0" />
  <row Id="11166" PostId="8258" Score="0" Text="If it's about the visualization, you could use a simple image format such as pgm for example to display the results (Wiki: https://en.wikipedia.org/wiki/Netpbm_format)" CreationDate="2018-11-13T14:38:47.110" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="11167" PostId="8258" Score="0" Text="@Larry &#xA;but how is the construction of those bugs" CreationDate="2018-11-13T15:42:54.127" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11169" PostId="8258" Score="0" Text="@ x-rw: Here is link that explains how these bugs are constructed. https://math.stackexchange.com/questions/2982466/binary-representation-of-pascals-triangle" CreationDate="2018-11-13T17:04:31.420" UserId="9678" ContentLicense="CC BY-SA 4.0" />
  <row Id="11170" PostId="8254" Score="0" Text="I think that my answer to your [other question](https://computergraphics.stackexchange.com/questions/8255/finding-the-projection-matrix-for-one-point-perspective/) applies here as well." CreationDate="2018-11-13T21:38:48.163" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="11171" PostId="8233" Score="0" Text="The answer by russ is correct. Unfortunately, if you are unfamiliar with quaternions you will not be able to understand it. I suggest [3Blue1Brown videos](https://www.3blue1brown.com/videos/) as a visual introduction to the topic." CreationDate="2018-11-14T01:12:00.990" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="11173" PostId="8261" Score="0" Text="If I multiply the two matrices simply I get $\begin{bmatrix} x&amp;&amp; y&amp;&amp;z&amp;&amp;\frac{z}{d}+1\end{bmatrix}$" CreationDate="2018-11-14T02:11:17.513" UserId="9661" ContentLicense="CC BY-SA 4.0" />
  <row Id="11175" PostId="8261" Score="1" Text="Sorry for the mistake! I typed the identity matrix in the editor and just changed it from there. I forgot to change the one to a zero in the last row. I edited the answer with the correction. Thanks for spotting it!" CreationDate="2018-11-14T03:28:07.470" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="11176" PostId="8261" Score="0" Text="is this derivation the same as: [this](https://computergraphics.stackexchange.com/questions/8254/perspective-projection-transformation-matrix?noredirect=1#comment11170_8254) derivation that I have done in the second derivation? What about the first derivation? Is it also only for one point perspective? Or is it more general for all point perspectives?" CreationDate="2018-11-14T05:21:05.063" UserId="9661" ContentLicense="CC BY-SA 4.0" />
  <row Id="11178" PostId="8261" Score="0" Text="Yes they are quite the same, but it is likely that your frame of reference has a different definition (one or more axes pointing at a different direction) and for sure you were using column instead of row notation. The first derivation is also similar, but for some reason the author decided to generalize the center point of the projection. It is usual in graphics programming to let the center be the origin of camera space." CreationDate="2018-11-14T21:15:02.137" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="11179" PostId="8261" Score="0" Text="just to be sure, the projection that  was being performed  in the link that I gave you in the comments above, is a one point projection,  even  if nothing was specified, right?" CreationDate="2018-11-15T06:39:32.903" UserId="9661" ContentLicense="CC BY-SA 4.0" />
  <row Id="11180" PostId="8265" Score="0" Text="thank you for answering here as well, one user pointed out that the negative sign  was due to reflection on the yz plane to correct  the reference frame.... so the x coord was reflected was what he said." CreationDate="2018-11-15T06:41:59.427" UserId="9661" ContentLicense="CC BY-SA 4.0" />
  <row Id="11181" PostId="8265" Score="0" Text="if you have some time, could you please take a look at [this question of mine](https://computergraphics.stackexchange.com/questions/8262/preserving-z-values-during-projection) , I  can't  really seem to understand the given explanation." CreationDate="2018-11-15T06:43:48.467" UserId="9661" ContentLicense="CC BY-SA 4.0" />
  <row Id="11182" PostId="8267" Score="0" Text="Being pedantic, I guess you meant &quot;uniform cubic b-spline&quot;." CreationDate="2018-11-15T08:58:56.197" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11183" PostId="8267" Score="1" Text="Yes. I forgot to mention that." CreationDate="2018-11-15T15:17:40.860" UserId="1940" ContentLicense="CC BY-SA 4.0" />
  <row Id="11184" PostId="8269" Score="0" Text="What artifacts are you seeing? Those curves look reasonable to me." CreationDate="2018-11-15T21:24:45.920" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="11185" PostId="8269" Score="0" Text="uploaded images please check" CreationDate="2018-11-15T21:39:35.377" UserId="3024" ContentLicense="CC BY-SA 4.0" />
  <row Id="11186" PostId="8267" Score="0" Text="Just to confirm, are you asking, given a set of *N* CatRom cps, {CR0, CR1, CR2, CR3 ... CRn-1}  forming a piecewise curve, what is the equivalent *N* points, {B0,B1,B2...Bn-1} for a matching piecewise uniform cubic bspline?" CreationDate="2018-11-16T10:24:40.647" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11187" PostId="8267" Score="1" Text="Exactly. That is what I'm looking for. I think conversion to cubic bezier's are easy. But I would like to know if this is even possible/doable with bsplines." CreationDate="2018-11-16T15:17:24.127" UserId="1940" ContentLicense="CC BY-SA 4.0" />
  <row Id="11188" PostId="8257" Score="0" Text="Alright, point taken about the &quot;frame of reference&quot;. Fact remains that, in OpenGL at least, a *model matrix* converts from object to world space, a *view matrix* from world to camera space and a *projection matrix* from camera to clip space. Only if you want to go the other way around should you first take its inverse." CreationDate="2018-11-16T16:02:21.413" UserId="4943" ContentLicense="CC BY-SA 4.0" />
  <row Id="11189" PostId="8276" Score="0" Text="Thanks @zeno! Probably you are right, but I don't know how to compare both without compiling blender by myself. I still think there is just something wrong in my equation so I'm not sure whether that's worth it going through that entire process setting up that thing ;) In case you are interested in how different the distortion is, you can easily compare both results using Natron and its shadertoy node (but you probably know that)." CreationDate="2018-11-17T10:09:34.123" UserId="9684" ContentLicense="CC BY-SA 4.0" />
  <row Id="11193" PostId="8280" Score="1" Text="Please include the essential parts of the solution in the actual answer rather than just linking elsewhere." CreationDate="2018-11-17T14:50:10.267" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11194" PostId="2117" Score="1" Text="@Looeee Could you pass your code please, I'm very interested in this field of graphics" CreationDate="2018-11-17T15:58:59.583" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11195" PostId="8267" Score="0" Text="I think it's a little trickier but I think there *may* be an approach. Still trying to work out the details." CreationDate="2018-11-17T16:23:41.740" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11198" PostId="2117" Score="0" Text="Yes, you can find my code here: https://github.com/looeee/blackthread-heroku/tree/master/assets/js/src/pages/experiments/escherSketch&#xA;&#xA;And the live version: https://blackthread.io/experiments/eschersketch/&#xA;&#xA;Honestly, it's so long since I looked at this that I can't remember where in the code the functions that solve this are, but if you have trouble making sense of it drop me another comment here and I'll take a look." CreationDate="2018-11-18T12:41:10.047" UserId="2775" ContentLicense="CC BY-SA 4.0" />
  <row Id="11199" PostId="8284" Score="0" Text="Thanks for the complementary info and for the link to an interesting blog post" CreationDate="2018-11-18T23:11:46.390" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="11200" PostId="7730" Score="1" Text="Haven't read through the entire question yet but note that just a view direction is underspecified for the full camera matrix as it does not account for the roll (aka the &quot;up direction&quot; of the camera." CreationDate="2018-11-19T00:13:58.173" UserId="87" ContentLicense="CC BY-SA 4.0" />
  <row Id="11203" PostId="8251" Score="0" Text="It's been a long time, but you possibly need to make the bias relative to the magnitude to the input floats." CreationDate="2018-11-19T06:42:00.807" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11211" PostId="8299" Score="0" Text="I just read your answer there" CreationDate="2018-11-20T13:27:12.713" UserId="9364" ContentLicense="CC BY-SA 4.0" />
  <row Id="11212" PostId="8299" Score="0" Text="Indeed it's a duplicate, but I have another question which directly relates to your answer given in the other post. Should I ask there?" CreationDate="2018-11-20T13:29:09.923" UserId="9364" ContentLicense="CC BY-SA 4.0" />
  <row Id="11213" PostId="8299" Score="0" Text="I cannot comment there sadly." CreationDate="2018-11-20T13:55:13.323" UserId="9364" ContentLicense="CC BY-SA 4.0" />
  <row Id="11214" PostId="8299" Score="0" Text="Since both my blogs and StackOverflow support Markdown and MathJax, see https://matt77hias.github.io/blog/2018/08/19/voxel-cone-tracing.html for Q&amp;A as well ;-)" CreationDate="2018-11-20T14:38:15.960" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="11215" PostId="8300" Score="0" Text="thanks &#xA;I would be interested in obtaining the adjacencies in text, the graphic part is not so impresindible" CreationDate="2018-11-20T14:56:15.093" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11216" PostId="8300" Score="0" Text="your answer is good that interesting the graphic part" CreationDate="2018-11-20T15:01:40.720" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11222" PostId="8297" Score="0" Text="Are rotations also involved? Further, is it like a jigsaw puzzle or can there also be overlap (e.g. image stitching for panoramas)?" CreationDate="2018-11-21T08:47:53.947" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11223" PostId="8299" Score="0" Text="@DaOnlyOwner Then [edit](https://computergraphics.stackexchange.com/posts/8299/edit) this question to focus on the actual question you have and link to the other question for context." CreationDate="2018-11-21T10:49:46.153" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="11224" PostId="8297" Score="0" Text="@SimonF &#xA;If we assume each part as a vertex, I need to know the adjacencies of each vertex" CreationDate="2018-11-21T15:33:24.663" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11225" PostId="8302" Score="0" Text="Hello there! Welcome to the CG stack! I am afraid your question is sort of hard to follow. Particularly, it doesn't quite make clear what you intend your result to be...   Do you want your code to always draw a square of fixed width and height without changing what was already there?  Do you want your code to draw a shape as similar as possible to a square using exactly cspots spots? Please clear up what you want to achieve so that we can help you out!" CreationDate="2018-11-22T00:12:58.210" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="11226" PostId="8302" Score="0" Text="@SebastiánMestre Yes I would like to draw a shape as similar to a square using exactly cspots spots for each group as cspots can contain any number. The total of all groups cspots value will equal (24*24) so as the drawing progresses, the areas will become less and less square-like but I would like to keep the semblance of a square. In this example, there are 10 groups of varying cspots values and they need to be all drawn within the 24*24 matrix as square-like as possible." CreationDate="2018-11-22T00:22:42.053" UserId="9742" ContentLicense="CC BY-SA 4.0" />
  <row Id="11227" PostId="8302" Score="0" Text="It's probably a good idea to add the explanation of cspots (or inArr) to the beginning of the question so that you don't have to read through the comments first. Nice problem by the way, it kind of reminds me of the knapsack problem." CreationDate="2018-11-22T00:43:24.440" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="11228" PostId="8302" Score="0" Text="@Chris. Sorry. I've updated it with a possible solution." CreationDate="2018-11-22T01:15:24.107" UserId="9742" ContentLicense="CC BY-SA 4.0" />
  <row Id="11229" PostId="8303" Score="0" Text="Worked very well. I need to only push a neighbour on the queue only if it was not currently a queue entry. The problem was a point (x,y) may have been in the queue and a new neighbour search would find the same (x,y) and since the touch array had not been updated, 2 entries of the same point would be in the queue and this would be replicated many times so the queue got massive and performance really suffered. Also, the multiple entries of the same point would decrement cspots more than once for the same point causing only a section of each group to be drawn." CreationDate="2018-11-22T05:18:20.473" UserId="9742" ContentLicense="CC BY-SA 4.0" />
  <row Id="11230" PostId="8303" Score="0" Text="Ohh, i just fixed the code to take that into account. good catch!" CreationDate="2018-11-22T09:42:01.023" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="11231" PostId="8233" Score="0" Text="Plus one for 3blue1brown, it's a great resource :) Also check out http://acko.net/blog/animate-your-way-to-glory-pt2/ for a good interactive explanation (the section called 'Blowing Up The Death Star')" CreationDate="2018-11-22T09:46:48.150" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="11232" PostId="8303" Score="0" Text="You may run into problems if at some point none of the remaining groups fit entirely in any of the remaining untouched components. I'm not sure if sorting the groups by size or choosing the neighbors in a particular order would solve this." CreationDate="2018-11-22T11:11:31.650" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="11233" PostId="8291" Score="0" Text="You are missing the cos(angleY) factor in the x and z position of the camera if you want the camera position to be on a sphere." CreationDate="2018-11-22T11:42:47.463" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="11243" PostId="8303" Score="0" Text="Chris, I need to do a lot of testing to determine if the algorithm creates any orphan spots/areas which does not allow additional group areas to be added." CreationDate="2018-11-22T23:17:39.363" UserId="9742" ContentLicense="CC BY-SA 4.0" />
  <row Id="11244" PostId="8303" Score="0" Text="@SebastiánMestre you can share jsfidle(code complete) please" CreationDate="2018-11-23T01:54:21.180" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11245" PostId="8302" Score="0" Text="@ImTalkingCode &#xA;What do you mean by cspots?" CreationDate="2018-11-23T08:03:16.557" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11246" PostId="8302" Score="0" Text="Cspots is the number of pixels within each group. It gets decremented with each new entry into the global grid array (gArr) until 0." CreationDate="2018-11-24T06:58:25.803" UserId="9742" ContentLicense="CC BY-SA 4.0" />
  <row Id="11249" PostId="8302" Score="0" Text="@ImTalkingCode &#xA;I'm starting in this world of graphics, can you pass your code please?" CreationDate="2018-11-26T00:16:49.007" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11250" PostId="8313" Score="1" Text="Maybe I misunderstood the question. You can just create 3d lines in the xz plane of that hexagonal pattern and set the y value of the points on the lines to the height map value at that location. The more points in the lines the better approximated it will be." CreationDate="2018-11-27T03:13:42.460" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="11252" PostId="8313" Score="0" Text="Maybe, my question was incorrect. I'm also interesting highlighting the height map area (different diffuse color or similar), based on user interaction. I'm not sure how to do all of this cleanly." CreationDate="2018-11-27T18:51:12.007" UserId="9562" ContentLicense="CC BY-SA 4.0" />
  <row Id="11253" PostId="8313" Score="0" Text="You want a transparent filling for each cell? You can do this with a second pass. Just rerender the same terrain geometry but this time without texture/shading. Just a transparent color. Alpha blend the two rendered images together. For efficiency you can construct geometry with much less detail than the terrain that achieves the same effect by sampling the height map values at the edges." CreationDate="2018-11-27T22:00:37.970" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="11254" PostId="8313" Score="0" Text="This is close, but the main point was getting transparent filling (or at least a different color) for a specific cell, similar to the highlight cell above. The highlighted cell has a slightly different shading than the rest of the cells (in addition to the god rays)." CreationDate="2018-11-28T00:21:35.973" UserId="9562" ContentLicense="CC BY-SA 4.0" />
  <row Id="11255" PostId="8319" Score="0" Text="&quot;*I also use a matrix to transform textureColor as this allows me to configure the uniform for single channel alpha textures (texture(tex, vtexcoord).r) vs 4 channel RGBA textures (texture(tex, vtexcoord).rgba).*&quot; You really should look into swizzle masks." CreationDate="2018-11-28T03:13:28.453" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11256" PostId="8319" Score="0" Text="&quot;*I read that 2x2 is required because 1x1 pixel samplers have different behaviors*&quot; Where did you read this?" CreationDate="2018-11-28T03:15:05.660" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11257" PostId="8319" Score="0" Text="@NicolBolas I saw the reference to using 2x2 vs 1x1 here &quot;[bug?] sampling of 1x1 texture just returns black&quot; &quot;Rescaling texture to the 2x2 size solves the problem.&quot; https://forums.ogre3d.org/viewtopic.php?t=52995 It could be an Ogre loader issue but it influenced my thinking towards using 2x2. Apparently it should be possible to use the so-called unbound sampler. Sampler 0 in OpenGL has defined semantics but I also read there are potentially driver bugs. I am not sure what is defined for Vulkan samplers and whether it has the equivalent of sampler 0 in OpenGL." CreationDate="2018-11-28T03:40:30.537" UserId="9763" ContentLicense="CC BY-SA 4.0" />
  <row Id="11260" PostId="8319" Score="0" Text="@NicolBolas 1x1 pixel works okay with Vulkan here. It is a power of two i.e. 2^0 = 1, so one would expect it to work, however, there is misleading information when searching regarding this topic (folk mentioning driver bugs). Unbound sampler in Vulkan on Nvidia 410.78 driver and GTX 1080 Ti returns a black pixel. Using a sampler with an incorrect descriptor config can lead to machine freezes with the current Nvidia Linux driver." CreationDate="2018-11-28T07:35:19.867" UserId="9763" ContentLicense="CC BY-SA 4.0" />
  <row Id="11261" PostId="8317" Score="0" Text="Is this a question related to programming of an FPS measurement or is it related to using tools to measure fps ?" CreationDate="2018-11-28T09:39:32.467" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11262" PostId="8322" Score="1" Text="P.S.: your question goes far beyond physically based rendering, I would even say it's not about rendering..." CreationDate="2018-11-28T09:58:28.083" UserId="9755" ContentLicense="CC BY-SA 4.0" />
  <row Id="11263" PostId="8317" Score="0" Text="@PaulHK I am looking for a tool to measure the FPS of my very simple app. I am new at this; nowhere near to instrumenting my application." CreationDate="2018-11-28T10:31:16.663" UserId="6496" ContentLicense="CC BY-SA 4.0" />
  <row Id="11264" PostId="8319" Score="0" Text="&quot;*Perhaps there is a way to specify swizzle masks when creating a VkImage or VkSampler*&quot; It's part of `VkImageView`, not of the image itself." CreationDate="2018-11-28T14:55:01.320" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11265" PostId="8295" Score="0" Text="I think the &quot;good-enough&quot; answer for most of us is that as long as you take into consideration the linearity of light and convert to the gamma curve of your display, you've done as much as most people expect you to do.  Colour calibration and monitor profiles may factor into the answer you seek.  If you just look at &quot;gamma&quot;, and in particular [sRGB](https://en.wikipedia.org/wiki/SRGB) you might get most of your questions answered.  Is that what you're looking for?" CreationDate="2018-11-28T15:45:48.487" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="11266" PostId="8317" Score="0" Text="@Vectorizer: If it's your program, you should not be using FPS as a unit of performance measurement. Use the actual frame time instead; it's far more useful." CreationDate="2018-11-28T19:13:28.587" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11267" PostId="8319" Score="0" Text="@NicolBolas Thanks for pointing out swizzling in VkImageView This is helpful as the linkage between `GL_TEXTURE_SWIZZLE_RGBA` and `components` in `VkImageView` is not immediately obvious. It is subtle. I'll edit the question. Vulkan tutorial leaves out this detail when covering VKImageView. &quot;I've left out the explicit viewInfo.components initialization, because `VK_COMPONENT_SWIZZLE_IDENTITY` is defined as 0&quot; however it is subtly implied if one reads the text closely enough https://vulkan-tutorial.com/Texture_mapping/Image_view_and_sampler" CreationDate="2018-11-28T22:21:24.903" UserId="9763" ContentLicense="CC BY-SA 4.0" />
  <row Id="11269" PostId="8322" Score="0" Text="The detail is actually not that important to me. What I need to know is what I have to store in the final framebuffer of openGL. My assumption is that the value should be proportional to luminance, because when you set it to 1, 1, 1, the output value is white. If this represents radiance than the output should be green, because when the signal radiance is the same, the pixel should be perceived as green. Am I correct?" CreationDate="2018-11-29T09:45:42.343" UserId="8565" ContentLicense="CC BY-SA 4.0" />
  <row Id="11270" PostId="8295" Score="0" Text="@Wyck If I want to use physically based light unit, should I use radiance or luminance before doing the tone mapping step and gamma correction? My assumption is luminance." CreationDate="2018-11-29T09:51:58.127" UserId="8565" ContentLicense="CC BY-SA 4.0" />
  <row Id="11271" PostId="8318" Score="0" Text="Perhaps it'd be helpful if you expanded on this and perhaps provided some links." CreationDate="2018-11-29T10:03:07.070" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11272" PostId="8295" Score="0" Text="It depends what physical properties you are trying to model, how physically correct you are trying to be, and if this is a question of modelling physical properties of a scene and then displaying an image of it (rendering), vs the radiometric properties of your display itself (calibrating to get two displays to show the same colours).  In short: radiance talks about light before it reaches your eye, so that's probably what you want to model.  Luminance talks about how a standard eye responds to various frequencies of light.  Take that into consideration late in the pipeline." CreationDate="2018-11-29T14:39:54.623" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="11275" PostId="8322" Score="0" Text="Simulating radiance or luminance is up to you. Are you simulating the energy received on solar panels? You probably want to use radiance. Do you want to know the lux on surfaces? Use luminance. Sorry, your question is not clear.. for example about your goals: what's the graphic engine for? you tagged the question pbr so it's more usual to work with radiance than with luminance." CreationDate="2018-11-29T22:04:30.423" UserId="9755" ContentLicense="CC BY-SA 4.0" />
  <row Id="11282" PostId="8329" Score="0" Text="Does that mean you could write an app in GL 4.6 and do your shaders in HLSL as well?" CreationDate="2018-12-02T03:38:12.043" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="11285" PostId="8329" Score="0" Text="@russ Maybe! I've never tried it, but that may well be possible." CreationDate="2018-12-02T05:49:47.347" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11287" PostId="8344" Score="3" Text="The painter's algorithm can also fail with nonintersecting geometry if there are cycles in the occlusion relation. For example, consider three rods arranged on the ground in a triangle such that each rod is resting on the one clockwise from it. Viewed from above, there is no rod that can be drawn first." CreationDate="2018-12-02T10:41:13.387" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="11289" PostId="8348" Score="3" Text="Honestly, this is a topic where you can find plenty of reading material with a few web searches, for instance [this](https://www.learnopengles.com/tag/mipmap/) or [this](https://graphics.ethz.ch/teaching/former/vc_master_06/Downloads/Mipmaps_1.pdf). Do you have a more focused question about mipmaps?" CreationDate="2018-12-03T01:40:32.000" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11290" PostId="8348" Score="0" Text="I've tried reading material on web searches, but can't really seem to get a solid understanding of it." CreationDate="2018-12-03T04:21:49.620" UserId="9778" ContentLicense="CC BY-SA 4.0" />
  <row Id="11291" PostId="8348" Score="4" Text="Can you be more specific about what you do and don't understand already? Right now, if I were writing an answer there wouldn't be much I could do besides reiterate what those articles say." CreationDate="2018-12-03T04:45:13.683" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11294" PostId="8348" Score="1" Text="@user9778: &quot;*can't really seem to get a solid understanding of it.*&quot; Then ask about what you *don't* understand. Not the general subject as a whole, but the specific part that you're having trouble with." CreationDate="2018-12-03T16:55:34.580" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11296" PostId="8356" Score="4" Text="&quot;*The 1-channel mipmap texture usually is divided into four parts, holding Red, Green, Blue in the first three*&quot; &#xA;It should be noted that a MIP mapped texture in hardware will not actually be done like this (at least not on any of the Hardware I've helped develop) . This scheme (as described in Williams' &quot;Pyramidal Parametrics&quot; paper, would be poor from a caching/page-break perspective.&#xA;It should also be noted that others published the idea of pre-filtered texture maps (e.g. Dungan) prior to Williams but the latter introduced Trilinear filtering." CreationDate="2018-12-04T09:11:58.080" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11297" PostId="8344" Score="0" Text="Painter's algorithm *WILL* work with overlaps etc provided you take the steps described by Newell, Newell &amp; Sancha.    https://en.wikipedia.org/wiki/Newell%27s_algorithm" CreationDate="2018-12-04T09:16:48.160" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11298" PostId="8354" Score="0" Text="This is quite a big chunk a code to debug. You will rarely find someone that will take on this kind of task here unless it's smaller snippets. That said, there should not be any link between `glReadPixels` and what you draw. To me, this hints at a possible problem with buffer bindings because that is what `glDrawArrays` uses and not the `glu` draw functions. By the way, if you're starting a new project, you should find a more modern replacement to glu if you can (like GLFW)." CreationDate="2018-12-04T09:54:49.380" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11299" PostId="8348" Score="1" Text="Perhaps you need to come at this from a different direction.  Do you know, for example, what &quot;aliasing&quot; is? i.e. insufficiently sampling a signal making it seem like it's something else?  E.g. in the spatial domain,  a diagonal edge of a triangle appearing like a staircase or, in the temporal domain, the &quot;wagon wheels&quot; in a film seemingly turning backwards?" CreationDate="2018-12-04T09:59:56.543" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11300" PostId="8361" Score="2" Text="Big up for the paper's author to register and provide an answer!" CreationDate="2018-12-04T22:01:30.503" UserId="9505" ContentLicense="CC BY-SA 4.0" />
  <row Id="11301" PostId="8354" Score="0" Text="Yeah I thought it was a lot and I honestly did not want to post any code at all as I wanted conceptual answers. It's for a course that I'm taking anyway (that's why I'm using GLEW so I just wanted to know what's wrong and learn how to fix it.&#xA;I will look more into the buffer bindings. I tried using a framebuffer object but still figuring it out." CreationDate="2018-12-04T23:16:19.303" UserId="9789" ContentLicense="CC BY-SA 4.0" />
  <row Id="11302" PostId="8354" Score="0" Text="GLEW is still totally up to date. GLEW gives access to OpenGL functions. *GLU* is out of date." CreationDate="2018-12-05T09:02:32.507" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11303" PostId="8360" Score="0" Text="Yep, and that why there are techniques like neighborhood clamping to eliminate ghosting but it works poorly with volumetrics. So I am looking for something targeted at volumetric, maybe changing weight according to prev frame and current.  &#xA;Changing weight to lower constant value won't help because trails are still noticeable and quality of volumetric is reduced." CreationDate="2018-12-05T09:26:51.093" UserId="3123" ContentLicense="CC BY-SA 4.0" />
  <row Id="11306" PostId="8362" Score="0" Text="`It's a bit hard to tell with the images, but I assume the whole viewport is distorted?` Yes, it is over the whole viewport.&#xA;&#xA;I will edit my question and add the distortion calculations, but they are pretty much the same." CreationDate="2018-12-05T11:31:02.770" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="11307" PostId="8362" Score="0" Text="As seen in the new edit, I can reproduce the issue with a minimal example that has no camera distortion implemented, just once a rendered image read out and written to the frame buffer, and once the rendered image read out with precalculated texture coordinates per pixel and then written to the frame buffer." CreationDate="2018-12-05T12:20:41.757" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="11308" PostId="8360" Score="0" Text="I’d also try decreasing resolution of volume rendertarget and just work with it and remove temporal altogether. For fast movements it should look ok." CreationDate="2018-12-05T20:13:50.223" UserId="5385" ContentLicense="CC BY-SA 4.0" />
  <row Id="11309" PostId="8352" Score="2" Text="BTW, GPU hardware does not really use scan-conversion; it uses tile-based rasterization instead. But the main point that the math is simple still holds: sample coverage is determined by checking the three edge inequalities, and attribute interpolation is evaluating a plane equation." CreationDate="2018-12-05T22:38:06.497" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11311" PostId="8367" Score="0" Text="You can have hash collissions just fine without need of major synchronization by limmiting the number of possible collisions to a known number (say 20) and using atomic operations on a flag.&#xA;&#xA;With compact oct trees like the ones generated by voxel global illumination, oct tree based algorithms you need 2 separate shaders calling each otehr in a loop to achieve memory allocation.&#xA;&#xA;And &quot;normal&quot; oct trees (the ones where you know the index based on a fixed function) create a lot of unnused memory, at that point a 3D volume is likely to be better." CreationDate="2018-12-06T01:18:33.900" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11312" PostId="8367" Score="0" Text="Also i think you swapped the complexities on your explanation of asymptotic complexity being a non-issue on modern algorithms due to mutability of hardware" CreationDate="2018-12-06T01:19:59.477" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11313" PostId="8364" Score="2" Text="Imo, the main benefit lies in the hierarchy generated by using a tree structure. This makes it so that if a ray doesn't intersect any of the upper level boxes, a large number of nodes are pruned already. In contrast, a grid need to check intersection with all of the intersected cells.&#xA;&#xA;This answer explains it pretty well.&#xA;&#xA;https://gamedev.stackexchange.com/questions/69776/when-is-a-quadtree-preferable-over-spatial-hashing" CreationDate="2018-12-06T04:45:48.257" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11314" PostId="8364" Score="0" Text="Mike Acton did a relevant presentation recently :)Check out @mike_acton’s Tweet: https://twitter.com/mike_acton/status/1062458276812451840?s=09" CreationDate="2018-12-06T05:37:50.287" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="11315" PostId="8261" Score="0" Text="Yes. Technically, the matrix in this question is a 1-point perspective. However, if you combine this matrix with a simple rotation, as it is usually the case with the view matrix, which transforms from world space to camera space, then you will easily get 2-point or 3-point perspective in the combined matrix, i.e., perspective * view." CreationDate="2018-12-06T12:23:49.983" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="11316" PostId="8265" Score="0" Text="Sorry for taking me so long to answer your comment. Another user has already answered your question, and it seems to be correct. If you just apply the perspective matrix as shown above, every z-value will map to d, losing the information about object ordering, which is needed for the Z-buffer algorithm to correctly render the scene. So, the original range of Z values is usually mapped to another range, such that after perspective projection, Z is mapped to normalized device coordinates, as expected by your graphics API of choice." CreationDate="2018-12-06T12:29:13.430" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="11317" PostId="8364" Score="0" Text="@gallickgunner That is only efficient for a one time collission to find the region in the tree where your data is stored. Most effects however require to sample multiple neighbouring areas (AO, emitance, diffuse reflections....). For which the Hash table is better.&#xA;&#xA;And you can mimick the hierarchy of the tree by using a hashed mipmap. it gives you the exact same hierarchy and the exact same asymptotic computation time." CreationDate="2018-12-06T15:43:39.750" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11318" PostId="8364" Score="0" Text="@Makogan - I don't know what a hashed mimap is, but there are other cons such as the teapot in a stadium problem which occurs with grids and spatial hashing.If you are going so far to even copy the hierarchy of the tree why not just use the k-d tree/ octree itself which are more flexible as they don't divide space in uniform grids. Overall each data structure has its own pros and cons so we can't say 1 is better than the other everywhere. As for why the octree/BVH are popular, i think they perform better overall in general." CreationDate="2018-12-06T17:01:46.513" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11321" PostId="8372" Score="0" Text="But why doesn't it work for lines with slopes greater than 1 though?" CreationDate="2018-12-07T23:26:22.433" UserId="9778" ContentLicense="CC BY-SA 4.0" />
  <row Id="11322" PostId="8372" Score="0" Text="How would we actually accomplish drawing these lines with slopes greater than 1 persay" CreationDate="2018-12-07T23:29:02.087" UserId="9778" ContentLicense="CC BY-SA 4.0" />
  <row Id="11324" PostId="8360" Score="2" Text="Unlike answers, comments don't always stay around forever, so it's worth editing the answer to include this additional useful information." CreationDate="2018-12-08T23:06:51.397" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="11325" PostId="8364" Score="0" Text="Because octrees are easier to write than hashtables I suppose?" CreationDate="2018-12-05T23:05:44.307" UserId="7234" ContentLicense="CC BY-SA 4.0" />
  <row Id="11326" PostId="8364" Score="2" Text="How is a tree easier to write than a hash table? A hash table is a one dimensional structure, and oct tree is a 4 diemnsional structure mapped onto a one dimensional memory array.&#xA;&#xA;I have written both and hash tables are so much simpler than oct trees." CreationDate="2018-12-05T23:08:02.860" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11327" PostId="8378" Score="1" Text="Are you asking this because someone else has declared it to be a cheat? If so, it might help to give the context of what they said, otherwise people can only guess what you are trying to find out." CreationDate="2018-12-08T23:26:38.600" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="11328" PostId="8372" Score="0" Text="The code only caters for the first octant by design. Well, this is easy to understand if you draw a line in the second octant. Here you will have to mirror the code in y=x, broadly speaking, or in other words step NE and N. This is the sign (dx) and sign (dy) you see in the implementations online." CreationDate="2018-12-09T00:26:10.160" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11331" PostId="8384" Score="1" Text="This worked perfectly for my situation. It's been a while since my last Linear Algebra class, and my google-fu was failing me, so this is greatly appreciated!" CreationDate="2018-12-09T17:52:44.387" UserId="9820" ContentLicense="CC BY-SA 4.0" />
  <row Id="11332" PostId="8378" Score="0" Text="Yes, someone else said it was like a &quot;cheat&quot;. Basically this cheat is better looking than using a Gouraud shaded model. But my question is how and why?" CreationDate="2018-12-10T00:01:02.923" UserId="9778" ContentLicense="CC BY-SA 4.0" />
  <row Id="11333" PostId="8378" Score="0" Text="and why is this not necessary generally applicable to any surface? Is it because it's just a &quot;cheat&quot; made for sphere approximations? not any others?" CreationDate="2018-12-10T00:01:49.567" UserId="9778" ContentLicense="CC BY-SA 4.0" />
  <row Id="11334" PostId="8385" Score="0" Text="&quot;*The alpha component seems to be affecting rest of the parts of the color and changes it at some extent.*&quot; How are you detecting those changes? That is, how do you know that the color is being changed? Have you read the pixels back? Or is this what you're seeing in some window? If it's a window, which OS and windowing compositor are you using? Is it one that supports transparency natively?" CreationDate="2018-12-10T00:30:13.997" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11335" PostId="8380" Score="0" Text="So why does it look better than the Gouraud Shaded Model specifically? and why is this not necessary generally applicable to any surface?" CreationDate="2018-12-10T01:08:19.600" UserId="9778" ContentLicense="CC BY-SA 4.0" />
  <row Id="11336" PostId="8380" Score="0" Text="Is it because it's just a &quot;cheat&quot; made for sphere approximations? not any others?" CreationDate="2018-12-10T01:08:34.277" UserId="9778" ContentLicense="CC BY-SA 4.0" />
  <row Id="11337" PostId="8385" Score="0" Text="What blend mode are you using, this looks like a case of pre-multiplied alpha." CreationDate="2018-12-10T02:23:50.477" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11338" PostId="8380" Score="1" Text="No, smooth shading works on many surfaces, not just spheres. It is very widely used. I'd call it a cheat because you are trying to draw a smooth surface without actually making the geometry smoother (more points). Of course, you only want to use it for things you want smooth (ie: any curved surface)." CreationDate="2018-12-10T04:37:34.527" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="11339" PostId="8380" Score="1" Text="Also I've edited my post to illustrate the difference between gouraud and phong providing details of why phong is better for smooth surfaces." CreationDate="2018-12-10T04:46:18.973" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="11340" PostId="8380" Score="0" Text="Oh okay I understand. Thanks a lot for clearing it up!" CreationDate="2018-12-10T04:54:19.417" UserId="9778" ContentLicense="CC BY-SA 4.0" />
  <row Id="11341" PostId="8380" Score="0" Text="The diagram in the question mentions depending on knowing the centre of the sphere, which suggests it might be a method that ignores position in the triangle and colours based on angle from the centre, which would only give meaningful results for a sphere. Not sure without more context whether this is the case though" CreationDate="2018-12-10T13:04:26.083" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="11342" PostId="8386" Score="0" Text="Thank you, sir, it works as expected. One question - why the DST equation is `GL_ONE_MINUS_SRC_ALPHA`? It looks to me that the fragment shader already performs required computation, so it should not be any further changed?" CreationDate="2018-12-10T18:17:05.820" UserId="9697" ContentLicense="CC BY-SA 4.0" />
  <row Id="11343" PostId="8387" Score="2" Text="There is no one way. Also is this a liquid or gas? With liquids we often try to acquire a polygonal surface through something like marching cubes. For gasses volume rendering is more common. Then there are completely different techniques like screen space fluids. The list goes on and on. For MPM you're free to use either but often background grids are much coarser than the particles themselves" CreationDate="2018-12-10T18:40:04.633" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="11346" PostId="8386" Score="0" Text="The destination is for blending the results of your fragment shader with whatever is already in the framebuffer. So it happens after your fragment shader runs. Your fragment shader isn't touching the destination in any way." CreationDate="2018-12-11T02:11:47.220" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11348" PostId="8387" Score="0" Text="Have a look at Cubical Marching Cubes --- a good alternative to MC when visualising liquids." CreationDate="2018-12-11T10:26:09.823" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11349" PostId="8390" Score="1" Text="Wow! That's an answer.:) This helps a lot. Thank you so much my friend!:D" CreationDate="2018-12-11T11:24:40.080" UserId="9825" ContentLicense="CC BY-SA 4.0" />
  <row Id="11350" PostId="8392" Score="0" Text="A screenshot would be helpful. It sounds like the problem is the triangles are much longer along u than v (or vice versa)? Are you able to set two different sampling rates for u and v, or do they need to be the same rate?" CreationDate="2018-12-11T23:41:43.850" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11351" PostId="8392" Score="0" Text="I can't take pictures of this :p" CreationDate="2018-12-11T23:54:08.697" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11352" PostId="8392" Score="1" Text="@NathanReed Sampling for u and v can be any integer number, independently of one another. The main issue however is finding a procedure to sample the vertices that doesn't add too many extra samples but rather uses the samples more effectively to avoid thin geometry." CreationDate="2018-12-11T23:56:20.870" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11355" PostId="8392" Score="0" Text="When you say &quot;thicker&quot; triangles, do you just mean that the triangles are more &quot;equilateral&quot; in shape (albeit smaller in area?).&#xA;Could you instead generate an initial mesh with a large number of samples and then do a post process to reduce the number of triangles such that you discourage the choice of long/thin triangles?" CreationDate="2018-12-12T09:14:56.727" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11356" PostId="8392" Score="0" Text="Yes I mean trying to find more &quot;equilateral&quot; triangles. Post processing is a possibility, but since ti would add algorithmic complexity and runtime it's less ideal." CreationDate="2018-12-12T14:54:48.477" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11357" PostId="8392" Score="0" Text="Just to get it clear in my head you want a) a simple grid-like subdivision of the parametric space so that there is minimal calculation overhead but b) want to avoid (if possible) long-thin triangles that can result from the mapping of parametric UV positions to 3D space?  Do you have the (partial) derivatives of the mapping function?" CreationDate="2018-12-12T16:11:03.603" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11358" PostId="8392" Score="0" Text="@SimonF I do not have the partial derivatives but they could be numerically approximated" CreationDate="2018-12-12T16:20:00.680" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11359" PostId="4654" Score="0" Text="I implemented a CUDA version of RAISR and am working on an OpenGL implemention. The hashing step isn't much of a problem -- just cache things in shared memory. You'll get garbage performance on older hardware, but on newer low-end cards, it's totally fine. The tricky part is the filter lookup, and there's quite a few methods available to deal with that problem (shared memory, some compression tricks, etc). But you'll probably get decent performance if you just store the filters in an RGBA texture." CreationDate="2018-12-12T20:35:28.617" UserId="6380" ContentLicense="CC BY-SA 4.0" />
  <row Id="11360" PostId="8386" Score="1" Text="The Dreams Wind: The source.rgb * source.alpha is already computed in your texture. You still need to blend it with your destination. The full blending formula becomes   frag = source.rgb + dest.rgb * (1-source.a); because source.rgb is &quot;pre-multipled&quot; already, saving your GPU some calculation. That is what the glBlendFunc is doing." CreationDate="2018-12-13T03:58:03.620" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11361" PostId="8395" Score="0" Text="It's probably easier to do it with 3 copies of each vertex - with the appropriate colour on each copy." CreationDate="2018-12-13T13:31:54.107" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11362" PostId="8396" Score="2" Text="I'm not familiar with javascript, but it looks to me like that is using a separate transformation per face. It might be ok for a trivial example, but *please* don't try to scale that to larger models." CreationDate="2018-12-14T09:56:42.327" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11363" PostId="8395" Score="0" Text="Welcome to the site. Please elaborate on your specific problem and show what you have so far. A code dump is not a proper way to do this. Extract the minimal and meaningful parts and post them in the question." CreationDate="2018-12-14T14:23:27.103" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11364" PostId="8397" Score="0" Text="&quot;with the spatial hash were the ones with thousands of particles and no static geometry&quot;&#xA;&#xA;Awesome that's exactly what I am working on, that means my intuition about this was correct!" CreationDate="2018-12-14T22:36:02.503" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="11365" PostId="8404" Score="2" Text="Can you give more context to your question? Is this a quote from some documentation? Can you link where you read it?" CreationDate="2018-12-17T23:25:10.147" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11369" PostId="1755" Score="0" Text="Have you tried contacting the authors?" CreationDate="2018-12-18T04:06:35.900" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11372" PostId="8401" Score="0" Text="Your question looks interesting, but it's missing a key element. What does your MOCAP data actually use then instead of &quot;simple joint coordinates&quot;?" CreationDate="2018-12-18T16:07:39.513" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11373" PostId="8401" Score="0" Text="I have edited the question to provide this information, I can give little explanations about the format if needed." CreationDate="2018-12-18T16:18:10.770" UserId="9864" ContentLicense="CC BY-SA 4.0" />
  <row Id="11374" PostId="8401" Score="0" Text="I'm not familiar with that format and I bet many others are not either. If you explain what it contains perhaps someone who knows mocap in general will be able to answer." CreationDate="2018-12-18T16:39:47.060" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11379" PostId="8410" Score="1" Text="An intuitive rotation depends on the view direction. Multiply your change in transformation by the inverse view matrix. Then apply the change in transformation to the overall transformation of the object. Think of it like undoing whatever rotation you did with the view. If you have a translation in your camera you'll have to deal with that also." CreationDate="2018-12-20T00:25:52.303" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="11381" PostId="8410" Score="0" Text="Ok. Now I'm making progress.  I got the camera rotation working properly:  I created a temporary matrix from the camera, translated the position out of it, inverted it, then rotated the screenAOR about it, and Boom!  It worked! Yahoo." CreationDate="2018-12-20T05:54:07.397" UserId="9886" ContentLicense="CC BY-SA 4.0" />
  <row Id="11383" PostId="8410" Score="0" Text="I added the working code above as an EDIT." CreationDate="2018-12-20T06:10:07.200" UserId="9886" ContentLicense="CC BY-SA 4.0" />
  <row Id="11384" PostId="8401" Score="0" Text="@bernie ASF/AMC files are the [Acclaim motion capture format](http://research.cs.wisc.edu/graphics/Courses/cs-838-1999/Jeff/ASF-AMC.html) which is used, for example, in the well-known [CMU Motion Capture Database](http://mocap.cs.cmu.edu/)." CreationDate="2018-12-21T07:40:05.407" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="11385" PostId="8413" Score="0" Text="By the way, the ASF files mentioned in the question are exactly the same format as the .asf file you use as an example in your answer :)" CreationDate="2018-12-21T14:38:39.727" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="11386" PostId="8410" Score="0" Text="I'm still having trouble getting the model to rotate properly.  I translate and invert the camera into a temporary matrix (as in the view rotation section), then I translate the invert the model instance (in a similar manner) into another temporary matrix, then I multiply them together and transform the screen AOR into a model AOR, then rotate the model instance, and lastly translate the model back to it's position." CreationDate="2018-12-22T05:16:53.117" UserId="9886" ContentLicense="CC BY-SA 4.0" />
  <row Id="11388" PostId="8410" Score="0" Text="The model seems to be rotating properly, except that it is as though it didn't get translated to the origin to be rotated, because as it rotates it is also orbiting the origin. Dang! @Andrew can you see anything I'm missing? I'll add the code in another EDIT above." CreationDate="2018-12-22T05:58:37.900" UserId="9886" ContentLicense="CC BY-SA 4.0" />
  <row Id="11389" PostId="5153" Score="0" Text="@RichieSams Am I missing something or is your code sample not casting any shadow rays?" CreationDate="2018-12-23T00:32:44.167" UserId="9900" ContentLicense="CC BY-SA 4.0" />
  <row Id="11390" PostId="5153" Score="0" Text="The first code block doesn't have shadow rays. However, the following code blocks add light sampling, which are a form of shadow rays" CreationDate="2018-12-23T01:12:15.977" UserId="310" ContentLicense="CC BY-SA 4.0" />
  <row Id="11392" PostId="1903" Score="1" Text="After discussion, the consensus seems to be that questions about plotting are off-topic. https://computergraphics.meta.stackexchange.com/questions/205/are-questions-about-plotting-graphs-on-topic" CreationDate="2018-12-23T17:35:47.990" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11393" PostId="8420" Score="0" Text="the p1 is position to ray?, also this should resolve with crossProduct ?" CreationDate="2018-12-23T19:16:39.013" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11394" PostId="8420" Score="0" Text="p is the ray direction and p1 is the vector describing the position to the square. No need for cross product in this method" CreationDate="2018-12-23T19:19:01.120" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11396" PostId="8420" Score="0" Text="how i get the normal (n)  to this t=(p1−e).n/d.n" CreationDate="2018-12-23T20:13:29.407" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11399" PostId="8420" Score="0" Text="I'm new, I do not know much about graphics, can you explain how I get the normal please?" CreationDate="2018-12-24T01:22:13.257" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11400" PostId="8420" Score="1" Text="The normal is for the square. you need to define it when initializing the square. It's a necessary information required in the Square structure. `Normal` is a vector that is perpendicular to the plane containing the square. If for example your square is located at `1,0,0` looking left, the normal is gonna be `-1,0,0`. The normal can be caluclated by taking the cross product of the two edge vectors e1 and e2." CreationDate="2018-12-24T08:41:41.000" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11401" PostId="8422" Score="0" Text="not my cup of tea but I would expect that for solids Fresnell equation will give you  ratio between how much energy is reflected/refracter/absorbed so you just configure the pseudo random generator that decide between reflect/refract to match the probability distribution. The frequency of light is not changing so nor the energy quantum should have any change ... with gases its different as there you got absorbtion/excitation instead which affects also the energy/frequency output I think..." CreationDate="2018-12-17T08:24:59.170" UserDisplayName="Spektre" ContentLicense="CC BY-SA 4.0" />
  <row Id="11402" PostId="8422" Score="1" Text="From &quot;A Practical Guide to GI using PMs&quot;, organized by Henrik Jensen: &quot;Photon tracing works in exactly the same way as ray tracing except for the fact that photons propagate flux whereas rays gather radiance. This is an important distinction since the interaction of a photon with a material can be different [...]. A notable example is refraction where radiance is changed based on the relative index of refraction — **this does not happen to photons**&quot; - so, I would be careful using Fresnell equations as it depends on refractive indices. That's why I want to understand the underlying equation." CreationDate="2018-12-20T15:35:06.313" UserDisplayName="CygnusX1" ContentLicense="CC BY-SA 4.0" />
  <row Id="11403" PostId="8423" Score="0" Text="Thank you for your answer, but this is not exactly what I asked for. I know how to find radiance out from flux. I am asking for the underlying Rendering Equation formula that is &quot;hidden&quot; behind the Russian Roulette step. The RR is one of possible many algorithms to approximate some underlying physical formula and I am asking what that underlying physical formula is." CreationDate="2018-12-19T08:14:04.093" UserDisplayName="CygnusX1" ContentLicense="CC BY-SA 4.0" />
  <row Id="11404" PostId="8423" Score="0" Text="Hmm my bad, I misunderstood your question but I still don't get what you mean by  &quot; The RR is one of possible many algorithms to approximate some underlying physical formula&quot;. Isn't RR just a way to terminate paths with negligible wieghts without introducing bias. What sort of &quot;underlying physical formula&quot; are you talking about?" CreationDate="2018-12-19T09:22:03.930" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11405" PostId="8423" Score="0" Text="It's similar to the relation of a regular path tracer (it often uses RR too) to a Rendering Equation. The former is a way to approximate the latter." CreationDate="2018-12-19T14:23:04.743" UserDisplayName="CygnusX1" ContentLicense="CC BY-SA 4.0" />
  <row Id="11406" PostId="8423" Score="0" Text="Ok it seems you think that RR is a sort of approximate solution to something when it isn't afaik. RR was introduced as early as in 1960s and was used to terminate paths in MC methods used to simulate Neutron Transport without introducing bias. Arvo and Kirk were the first ones to adapt RR being used in Physics and engg. to here in monte carlo path tracing etc.&#xA;&#xA;Try moving this over to the graphicsexchange as it's mostly a theoretical question and maybe someone there can answer better." CreationDate="2018-12-20T12:41:39.243" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11407" PostId="8423" Score="0" Text="RR and MC methods are - by their nature - an approximate, a way to compute infinite light paths in finite time. I will follow your suggestion asking it at graphicsexchange. (Is there a way to *move* a question, or should I re-ask it?)" CreationDate="2018-12-20T15:23:07.553" UserDisplayName="CygnusX1" ContentLicense="CC BY-SA 4.0" />
  <row Id="11408" PostId="8423" Score="0" Text="you can either flag it describing it needs to be moved or maybe just re-ask it as you said." CreationDate="2018-12-21T06:29:15.560" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11409" PostId="8423" Score="0" Text="Thank you; already flagged, awaiting response." CreationDate="2018-12-21T11:40:11.413" UserDisplayName="CygnusX1" ContentLicense="CC BY-SA 4.0" />
  <row Id="11410" PostId="3890" Score="0" Text="I realize this question has been solved in the comments but has no &quot;formal answer&quot;, so I am copying my comments as an answer." CreationDate="2018-12-24T10:50:39.413" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11412" PostId="8420" Score="0" Text="i edited my question, &#xA;I can not graph the square, I can only graph spheres" CreationDate="2018-12-24T14:30:55.497" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11413" PostId="8420" Score="1" Text="Well for sphere the intersection procedure is different altogether. These are basic intersection procedures given in almost any good book for graphics and games. I'd recommend you to read &quot;Fundamentals of Computer Graphics by Peter Shirley&quot; or &quot;Essential maths for games by Van Verth&quot;." CreationDate="2018-12-24T14:38:03.047" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11415" PostId="8422" Score="0" Text="I thought the Fresnel equation told the probability for a photon to get reflected (instead of refracted). Is that not the case?" CreationDate="2018-12-25T03:28:39.167" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11417" PostId="8420" Score="0" Text="thanks for the recommendation, I will leave my project to understand more about graficacion, &#xA;could you recommend more books please, I am very interested in this topic." CreationDate="2018-12-25T16:30:01.040" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="11418" PostId="8420" Score="1" Text="That's a great step to be honest. Most of peter shirley books include raytracing stuff, I have personally read the above one so i can tell it's a good book. These two are more than enough for now. For more advanced stuff there is &quot;Physically Based Rendering&quot;" CreationDate="2018-12-25T16:50:18.870" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11419" PostId="8422" Score="0" Text="@JulienGuertault Unless we touch quantum effects and discuss individual real photons, there is no randomness in physics of light. Any randomness is our algorithmic approximation to reduce the complexity of computation. Fresnell equations are expressed in terms of irradiance (applicable to radiance as well) and account for the fact that parallel beams of light, after refraction (using Snell's law) are closer to each other (or further apart, depending on the refraction index). This changes radiance, but not flux. Obviously, Fresnell is more, e.g. it also depends on polarisation..." CreationDate="2018-12-25T19:55:34.867" UserId="9911" ContentLicense="CC BY-SA 4.0" />
  <row Id="11420" PostId="1691" Score="0" Text="Do you have materials that can change the frequency? i.e. the outgoing radiance at frequency f1 depends on incoming radiance at a *dfferent* frequency f2? Because if you do, reducing the spectrum to 3 values may significantly reduce the quality of that effect." CreationDate="2018-12-25T22:00:08.073" UserId="9911" ContentLicense="CC BY-SA 4.0" />
  <row Id="11422" PostId="8428" Score="0" Text="Thanks for a great answer. Instinctively you will still be ok if any non-uniform scaling happens before any rotation, is this the case? Do you know how general you can go with this?" CreationDate="2018-12-26T16:56:38.533" UserId="9909" ContentLicense="CC BY-SA 4.0" />
  <row Id="11423" PostId="8428" Score="0" Text="@derekdreery Yep, that's right, as long as all non-uniform scaling happens before any rotation, you can still put it in TRS form. In the general case, with no restrictions on non-uniform scaling, you can apply [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition). This doesn't yield TRS form but rather a TRSR form: two rotations, one before and one after the scale." CreationDate="2018-12-27T01:25:10.143" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11425" PostId="8428" Score="0" Text="Ooh so that is a geometric understanding of SVD. TIL! Will the SVD reduce to the TRS form in the case that a TRS is possible?" CreationDate="2018-12-27T14:36:27.967" UserId="9909" ContentLicense="CC BY-SA 4.0" />
  <row Id="11429" PostId="8428" Score="1" Text="@derekdreery I think so, yes, but I haven't tried it. (BTW, just to be clear: I'm talking about doing SVD on just the 3×3 submatrix, which gives you the RSR part of the decomposition; the translation part is read off from the fourth column like usual.)" CreationDate="2018-12-27T17:48:06.347" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11430" PostId="8428" Score="0" Text="Yep that was my understanding. Because the bottom row is (0, 0, 0, 1), the translation neatly decomposes from the other parts of the transformation. So when I say TRS, read RS. :)" CreationDate="2018-12-27T18:18:19.153" UserId="9909" ContentLicense="CC BY-SA 4.0" />
  <row Id="11442" PostId="8416" Score="0" Text="Wahoo!  That did it!  It took a little trial-and-error to get the Java translation correct, but it works perfectly.  Thanks a million!!!  I've added the working code as another answer below.  it won't let me up-vote your answer, but this is it." CreationDate="2018-12-30T07:24:47.907" UserId="9886" ContentLicense="CC BY-SA 4.0" />
  <row Id="11447" PostId="8436" Score="0" Text="also somewhat obvious, but still: any point on the AB edge will project onto the ab edge, and any point in the plane will project onto the line" CreationDate="2018-12-30T23:38:54.260" UserId="9934" ContentLicense="CC BY-SA 4.0" />
  <row Id="11449" PostId="8436" Score="0" Text="Hi Elena, Thanks a lot!. Your explanation really clear up some of my confusion. But I still don't fully don't understand the algorithm yet. Is there any relation between the plane's normal with the projected line's normal. I have updated my question to reflect this additional confusion. Please have a look at it." CreationDate="2018-12-31T02:48:23.607" UserId="8565" ContentLicense="CC BY-SA 4.0" />
  <row Id="11452" PostId="8436" Score="0" Text="Okay, now I can see that the plane normal is the line normal as well. But I am still confused as how we can substract the w component of the normal to get the outer triangle." CreationDate="2018-12-31T10:42:08.563" UserId="8565" ContentLicense="CC BY-SA 4.0" />
  <row Id="11454" PostId="8437" Score="0" Text="CMYK colorspace? Anyway you could be more clear whet is the other picture from" CreationDate="2019-01-01T09:48:17.123" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="11455" PostId="8437" Score="0" Text="@joojaa my laptop uses MacOS. The interface to the file system is called Finder. In that window I took a screen shot that included a file name that contained the word &quot;color&quot;: https://i.stack.imgur.com/r6kGH.png" CreationDate="2019-01-01T09:54:51.597" UserId="5846" ContentLicense="CC BY-SA 4.0" />
  <row Id="11457" PostId="8438" Score="1" Text="Yes, sure, I wasn't clear enough, I'll change the wording a bit. The displayed image is manipulated in anticipation of the subpixels, so that the results will be as I described. The actual color variations are simply due to relation of the mathematically calculated outline to the physical position of the pixels. If it crosses a red subpixel, the red subpixel will be lit and you'll see a red color fringe, that's all." CreationDate="2019-01-01T12:58:38.993" UserId="7555" ContentLicense="CC BY-SA 4.0" />
  <row Id="11459" PostId="8438" Score="0" Text="*an inevitable consequence of the existence of subpixels* sums up your answer to &quot;why color&quot; for me very nicely, thank you very much!! I think that &quot;why gray in some places, color in others: is better asked in Apple SE since I have a hunch it's going to be platform-specific and not really a topic for Computer Graphics SE." CreationDate="2019-01-01T13:07:49.320" UserId="5846" ContentLicense="CC BY-SA 4.0" />
  <row Id="11460" PostId="8437" Score="0" Text="Yeah thats better. Microsoft owned the patent for subpixel rendering on a OS. Now this patent restriction has been lifted up and the browser window is free to implement whatever technique they wish." CreationDate="2019-01-01T13:09:07.640" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="11461" PostId="8438" Score="1" Text="Check this out first: http://osxdaily.com/2014/10/27/change-font-smoothing-text-os-x-yosemite/, there is a description of how you can tweak it somewhat in macOS." CreationDate="2019-01-01T13:14:09.673" UserId="7555" ContentLicense="CC BY-SA 4.0" />
  <row Id="11462" PostId="8438" Score="0" Text="[Why does some text on my laptop have gray-scale sub-pixel rendering and some has color?](https://apple.stackexchange.com/q/347234/143729) oops too late. If that answers this question, please feel free to post it there!" CreationDate="2019-01-01T13:15:33.240" UserId="5846" ContentLicense="CC BY-SA 4.0" />
  <row Id="11463" PostId="8438" Score="1" Text="I didn't mean &quot;prior to asking&quot;, just before you decide what to use. Some other setting might be more pleasing to you. :-)" CreationDate="2019-01-01T13:17:16.553" UserId="7555" ContentLicense="CC BY-SA 4.0" />
  <row Id="11464" PostId="8437" Score="0" Text="@joojaa just fyi I've just asked [Why does some text on my laptop have gray-scale sub-pixel rendering and some has color?](https://apple.stackexchange.com/q/347234/143729) separately, since it's platform-specific. Thanks for your help!" CreationDate="2019-01-01T13:30:05.140" UserId="5846" ContentLicense="CC BY-SA 4.0" />
  <row Id="11465" PostId="8438" Score="1" Text="I added a paragraph about why gray in some cases and colored in others." CreationDate="2019-01-01T13:51:19.357" UserId="7555" ContentLicense="CC BY-SA 4.0" />
  <row Id="11467" PostId="8437" Score="0" Text="Well the answer is simple each app on the globe has a choice on what architecture they choose to use in their gui frameworks. Some frameworks inherit other methods. For example the antialasing adobe does is different from all other applications. Every app can choose for arbitrary reasons do it  differently." CreationDate="2019-01-01T13:56:25.440" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="11468" PostId="4015" Score="0" Text="Looks like that link is gone now. What title were you referring to as the best 3d primer you’ve read?" CreationDate="2019-01-01T23:53:24.233" UserId="9943" ContentLicense="CC BY-SA 4.0" />
  <row Id="11469" PostId="1770" Score="0" Text="@Tara if you have a floor of tiles for example, you would make them repeat rather than have a specific texture for every single tile on the floor. Now you can use the remaining texture space for other things or save texture being send to the graphics card.&#xA;@RichieSams `Texture atlases became popular due to the fact that changing textures causes a full pipeline flush on the GPU.` Does this still hold true?" CreationDate="2019-01-02T07:23:42.313" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="11472" PostId="8436" Score="1" Text="@kevinyu When moving of the plane happens, you already operate in the window space, as far as I understand.&#xA;(a, b) · **x** + c = 0 is the general form of line equation in 2D, a b and c are just coefficients and **x** is a 2D point [x, y]. [a, b] is its normal, and c is the distance from [0, 0, 0] to the plane along the normal. Maybe take a line like 2x + y + 1 = 0. Draw a vector [2 cm, 1 cm], move along it from the center 1 cm and draw a perpendicular to it. And then try some points from that line, they will all satisfy the equation. So if you add more to the c - you'll move the line further." CreationDate="2019-01-03T00:43:27.103" UserId="9934" ContentLicense="CC BY-SA 4.0" />
  <row Id="11473" PostId="8436" Score="1" Text="So you take **.xy** component of the plane to get the normal of the line. ( it is a bit incorrect to say that the normal of the plane is the normal of the line btw. It's rather the _projection_ of the plane normal is the line normal).&#xA;now your .xy is exactly the [a, b], the normal of the line, and you need to move along the normal hPixel times." CreationDate="2019-01-03T00:44:28.683" UserId="9934" ContentLicense="CC BY-SA 4.0" />
  <row Id="11474" PostId="8436" Score="0" Text="Actually I have figure it out by myself after thinking for some time. But still thanks for clearing things up. It would be nice. If you include the explanation above into your answer. So, it is more complete. I have accepted your answer." CreationDate="2019-01-03T09:38:55.493" UserId="8565" ContentLicense="CC BY-SA 4.0" />
  <row Id="11478" PostId="8438" Score="1" Text="Regarding browser behavior: in most modern browsers, the algorithm used can be affected via the [CSS `text-rendering`](https://developer.mozilla.org/en-US/docs/Web/CSS/text-rendering) property. Mind you, the exact behavior of these pretty high-level values is still vendor-specific, as they are explicitely not meant to be more than just a hint." CreationDate="2019-01-03T20:03:12.537" UserId="9953" ContentLicense="CC BY-SA 4.0" />
  <row Id="11484" PostId="8443" Score="0" Text="This makes sense, but is there any way to do this without computing the world-space position in the fragment shader? It seems kind of brute-force, but I understand if that's the only realistic solution here. _Edit: I ask because I've been able to generate some symmetric patterns on the plane already without sending any additional data to the shaders._" CreationDate="2019-01-06T04:23:57.240" UserId="9952" ContentLicense="CC BY-SA 4.0" />
  <row Id="11485" PostId="8443" Score="0" Text="I believe the reason there are four tris is because there are four &quot;vanishing&quot; points to contend with with regards to the infinite plane. If you only have one triangle, it would not be possible to cover the entire space?" CreationDate="2019-01-06T04:26:29.020" UserId="9952" ContentLicense="CC BY-SA 4.0" />
  <row Id="11487" PostId="8447" Score="1" Text="Ah, thanks. I didn't think of checking D3D12, the fact is present there seems to suggest that its omission in D3D11 was just because it just never got added, rather than being deliberately omitted." CreationDate="2019-01-07T04:10:15.090" UserId="9962" ContentLicense="CC BY-SA 4.0" />
  <row Id="11488" PostId="8444" Score="0" Text="You shouldn't include the cosine foreshortening term in your `brdf_pdf`." CreationDate="2019-01-07T06:26:54.650" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="11489" PostId="1770" Score="0" Text="@Tare Yes, but what's the point of using virtual texturing then? The idea is to not make every tile look the same, without resorting to &quot;hacks&quot; like placing decals everywhere. Also instead of using &quot;repeat&quot; addressing, you can just adjust the texture coordinates.&#xA;Regarding texture atlases: Yes, changing textures is very expensive on the CPU. That's in fact currently one of the most time consuming things my renderer does. Particles for example are traditionally done with texture atlases exactly because you don't want to swap textures all the time, which can be very expensive." CreationDate="2019-01-07T07:20:47.280" UserId="5909" ContentLicense="CC BY-SA 4.0" />
  <row Id="11490" PostId="1770" Score="0" Text="@Tara i don't think virtual textures are used (only) for that purpose, but for all textures you can currently see in your scene. have a look at this: http://www.adriancourreges.com/blog/2016/09/09/doom-2016-graphics-study/ there is a small part about the virtual texture: `Of course depending on where the player is looking at, this set is going to change: new models will appear on screen`" CreationDate="2019-01-07T07:25:48.957" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="11491" PostId="8453" Score="0" Text="If only one direction light is taken into account, no further processing such as half-lambert, is there any need to use the max func?" CreationDate="2019-01-07T07:54:29.123" UserId="3416" ContentLicense="CC BY-SA 4.0" />
  <row Id="11493" PostId="8444" Score="0" Text="Im using cosine weighted hemispherical sampling so the BRDF PDF is supposed to be `cos(theta)/pi`" CreationDate="2019-01-07T09:57:05.943" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11494" PostId="8453" Score="3" Text="If your application does indeed not rely on this value not being negative (only a single light source, just rendering straight to a color target in a way that will lead to the output being clamped to black anyways, etc.) then, by all means, go ahead and skip the `max()` if it makes you feel better. But do so, knowing that this will merely produce a result indistinguishable from the correct result in the most narrow circumstances and incorrect results in any other case. Meanwhile, any useful explanation of how shading works will continue to present the formula that is correct in general… " CreationDate="2019-01-07T10:00:25.917" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11495" PostId="8449" Score="0" Text="Since the transformation that is asked for includes a translation (the camera is located at a position other than the origin), you will indeed have to use homogeneous coordinates to describe this transformation in matrix form. I'm not sure how much help you want beyond this simple hint. I'll happily explain in more detail below, but I wouldn't wanna take away from you the opportunity to find an answer yourself…" CreationDate="2019-01-07T11:37:52.920" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11496" PostId="8453" Score="0" Text="@MichaelKenzel Actually, it will makes me feel better  :) knowing it could be safely skipped. Because I am afraid there is sth beyond my knowledge happens under the hood. Thanks." CreationDate="2019-01-07T11:46:46.490" UserId="3416" ContentLicense="CC BY-SA 4.0" />
  <row Id="11497" PostId="8449" Score="0" Text="@MichaelKenzel Thank you. I'd appreciate if you could offer an explanation, as right now I have no clue and maybe I could use this as a reference to further my study on the subject." CreationDate="2019-01-07T11:48:30.883" UserId="9966" ContentLicense="CC BY-SA 4.0" />
  <row Id="11498" PostId="8454" Score="0" Text="Thank you very much. I understood the part about setting up the matrix but I have some doubts about the initial steps. What does the fact that the camera is looking at the origin actually implies? Why did you start with e_3 and concluded that e_1 had to point in the opposite direction of the world-space z-axis? (sorry to be a pain)" CreationDate="2019-01-08T01:59:05.143" UserId="9966" ContentLicense="CC BY-SA 4.0" />
  <row Id="11500" PostId="8458" Score="3" Text="I'm guessing a bit, so you will have to try around. However, you probably don't need the `core` annotation, i'm not even sure it exists for version 130. I couldn't find anything on the fly that would confirm this though.&#xA;Some older versions don't support `texture` for access of textures, so you may need a `texture2D` here. &#xA;All that being said, are your sure your glsl version is 130? That is extremely old. What kind of graphics card are you using?" CreationDate="2019-01-08T12:57:49.923" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="11501" PostId="8458" Score="0" Text="@Tare thank you. I will try your suggestion. I am sure about glsl version, confirmed both from line code glGetString(GL_SHADING_LANGUAGE_VERSION) and from terminal (glxinfo |grep version). Not sure about the graphics card, because the core runs under a virtual machine." CreationDate="2019-01-08T13:16:56.240" UserId="9982" ContentLicense="CC BY-SA 4.0" />
  <row Id="11502" PostId="8460" Score="0" Text="That answers my point mentioned in the edit portion. The main problem still remains though." CreationDate="2019-01-08T16:08:54.213" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11510" PostId="8459" Score="0" Text="Thanks, the provided syntax has been very useful. I checked about version, under the virtual machine the version number of GLSL is 1.30 and 99% i will not allowed to update the drivers, even if it is possible. (yes, I am forced to work under this virtual machine)" CreationDate="2019-01-09T13:57:50.437" UserId="9982" ContentLicense="CC BY-SA 4.0" />
  <row Id="11511" PostId="8459" Score="0" Text="Well, if this is a virtual machine which you can't change and are forced to use, then I guess 1.30 is, unfortunately, indeed what you're stuck with…" CreationDate="2019-01-09T14:06:54.767" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11514" PostId="8466" Score="0" Text="But theta is the angle between the half-vector and normal right?  How can we use theta as a microfacet normal directly? Do we need to compute half-vector based on theta?" CreationDate="2019-01-10T17:49:06.763" UserId="9987" ContentLicense="CC BY-SA 4.0" />
  <row Id="11515" PostId="8466" Score="0" Text="@TIANLUNZHU Yes, compute half-vector based on theta (and phi) using the spherical coordinates transformation." CreationDate="2019-01-10T17:55:14.143" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11517" PostId="1770" Score="0" Text="@Tare I don't know what you are getting at. Yes, if you look around, new tiles become visible and have to be streamed in. Yes, tiling a texture (by repeating texture coordinates or maybe by using texture views) is going to use less tiles. The point of virtual texturing is to be able to have massively more high-res textures by having only what's visible resident in memory. It also makes rendering more efficient, as less texture binding operations are required for rendering (at the cost of texture streaming and decoding). Id used mega textures in Rage in order to give their artists more freedom." CreationDate="2019-01-11T03:06:16.950" UserId="5909" ContentLicense="CC BY-SA 4.0" />
  <row Id="11518" PostId="1770" Score="0" Text="What I was aiming at, was that you seem to think of virtual texturing as being used only for static environment, whereas I understand it to be used for all objects in the world. And then (to get back to your original &quot;repeat&quot; question) you could use your resources for streaming new textures in more efficiently for these other objects like enemies rather than streaming the same tile over and over again." CreationDate="2019-01-11T07:20:43.947" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="11519" PostId="6115" Score="0" Text="I would have thought for indexed meshes that array-of-structures would be more efficient as the vertex shader will be fed all vertex attributes so having them grouped together in memory is going to be cache friendlier." CreationDate="2019-01-11T07:33:40.950" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11520" PostId="8444" Score="0" Text="May be a coincidence but are your diffuse surfaces exactly PI times darker ?" CreationDate="2019-01-11T07:38:52.690" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11521" PostId="6115" Score="0" Text="Depending on the data types in structure, vec4s and floats pack together nicely, other types not so well" CreationDate="2019-01-11T07:41:33.653" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11522" PostId="8444" Score="0" Text="@PaulHK - What would that imply? I don't think it's actually PI times darker, its just `mis_weight` times darker/brighter. Because as I said as soon as i remove mis_wieght and just divide by `light_pdf` I get the brighter image." CreationDate="2019-01-11T10:33:19.567" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11523" PostId="8471" Score="0" Text="Ok I think i get what you are telling me. So basically the color obtained on any pass can be greater than 1. And this extra information is lost when i clamp. I don't think it'd make such a great deal as the main problem is still averagin with samples giving 0 color. But anyway, how are you supposed to handle that in a progressive PT?" CreationDate="2019-01-11T20:14:39.137" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11524" PostId="8471" Score="0" Text="In a progressive path tracer you'd still need to store the total value and number of samples so far per pixel, otherwise there is no way to calculate the average when you take new samples. You'd only clamp it for display purposes" CreationDate="2019-01-13T09:47:18.377" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="11525" PostId="8471" Score="0" Text="I'll try storing the total color in the w coordinate since I'm using a float4 and pass a single variable for number of iterations to the kernel. Will post resutls shortly." CreationDate="2019-01-13T10:05:10.303" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11526" PostId="8471" Score="0" Text="Ok i totally forgot it's three channels worth of information :s How the heck am I suppose to store 3 channel worth information. That's basically an extra image passed to the kernel just for storing un clamped values?" CreationDate="2019-01-13T10:13:25.947" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11527" PostId="8471" Score="0" Text="Many thanks for pointing it our bro. I was at it for days." CreationDate="2019-01-13T13:58:07.913" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11528" PostId="8472" Score="0" Text="Yeah, that's basically the solution that I was talking about. The specific thing that I don't understand is how can you write p=v1 + lambda_1*v2 + lambda_2*v3" CreationDate="2019-01-13T20:42:42.043" UserId="9994" ContentLicense="CC BY-SA 4.0" />
  <row Id="11529" PostId="8472" Score="0" Text="@stackmann0 that's how u/v coordinates (or any attribute really) are interpolated over a triangle. General barycentric coordinates are not unique. So we turn to affine coordinates instead, which are just a normalized form of barycentric coordinates. Every linear combination of a triangle's vertices where all the coefficients sum to one uniquely identifies a point on the triangle's plane. If we know two of the coefficients, the third one is given by 1 - the rest. If you think about it: a triangle/plane is a 2D entity, so there's inherently only two degrees of freedom to fix on any point on it." CreationDate="2019-01-13T21:12:12.423" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11530" PostId="8472" Score="0" Text="Furthermore, any point *inside* a triangle is uniquely identified by a [convex combination](https://en.wikipedia.org/wiki/Convex_combination) of the vertices, which is just a further constrained version of the affine coordinates from before where, in addition to summing to 1, all coordinates are nonnegative." CreationDate="2019-01-13T21:12:14.933" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11531" PostId="8472" Score="0" Text="Also, this all makes a lot of sense if you just think about it geometrically: Basically, you start at any of the three vertices, walk some distance along the direction of one edge, and then some distance along the direction of another edge. Any unique combination of distances along two of the edges will take you to a unique point in the triangle's plane. And you'll never be able to leave this plane by just walking along triangle edges." CreationDate="2019-01-13T21:37:18.973" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11532" PostId="8472" Score="0" Text="And if you walk both distances in the direction that each edge is pointing, and you never walk all the way to the next vertex along the first edge and you always stop short of the distance remaining to the third edge along the second edge from there, then you can never leave the triangle…" CreationDate="2019-01-13T21:38:22.213" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11533" PostId="8472" Score="0" Text="@stackmann0 sorry, I somehow overlooked that I actually had a mistake in that formula you mentioned in your comment above. Of course $\vec v_1$ should be weighed by $1 - \lambda_1 - \lambda_2$ to compute $\vec p$. fixed…" CreationDate="2019-01-13T21:51:12.947" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11534" PostId="8444" Score="0" Text="A shot in the dark really, I have seen other question on here were the solution was surprisingly that." CreationDate="2019-01-14T02:07:38.130" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11535" PostId="8454" Score="0" Text="@JohnDoe sorry, totally overlooked your comment here. I updated my answer to give some more detail on how $\vec e_1$, $\vec e_2$, and $\vec e_3$ are chosen." CreationDate="2019-01-14T08:35:47.200" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11536" PostId="8474" Score="1" Text="It will not be guaranteed that your sampled partions will be convex, so your mapping will be a crude approximation. Maybe you could provide an illustration of what you have in mind?" CreationDate="2019-01-14T09:11:40.050" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11537" PostId="8474" Score="1" Text="Sure. Go to: https://www.katasterjam.si/Caves/Details?id=10904#dokumenti, click on 3D button. Then go to options icon, and tick checkbox &quot;Splay legs&quot;. You will see gray splay legs around the main polygon. You can rotate the cave system." CreationDate="2019-01-14T09:14:58.743" UserId="10006" ContentLicense="CC BY-SA 4.0" />
  <row Id="11538" PostId="8474" Score="0" Text="Interesting. Well, you could generate convexe hulls for each of the sampling points and find the union of them. A rough approximation :) but maybe a better visualisation than the one you got already. There are several 3D QuickHull implementations available on the 'net." CreationDate="2019-01-14T09:21:11.210" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11539" PostId="8474" Score="0" Text="I see. That would probably be better then the one we use now. I'll try to investigate more. tnx" CreationDate="2019-01-14T09:22:51.323" UserId="10006" ContentLicense="CC BY-SA 4.0" />
  <row Id="11540" PostId="8474" Score="1" Text="Search for QuickHull and 3D, this should lead you to some code you can use :)" CreationDate="2019-01-14T09:24:53.683" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11541" PostId="8472" Score="0" Text="Thank you for the detailed explanation, but the way I see it is that you got two triangles: the one in UV space (with which we're getting lamba_1 and lambda_2) and the one we have in 3D space (part of the mesh). lambda_1 and lambda_2 are barycentric coordinates for a point inside a triangle in UV space, I don't see how they can be used for the 3D triangle without talking about the relationship between the two triangles.&#xA;I'm probably not seeing things clearly or I'm confusing things.." CreationDate="2019-01-14T10:56:35.773" UserId="9994" ContentLicense="CC BY-SA 4.0" />
  <row Id="11542" PostId="8472" Score="0" Text="@stackmann0 Each 3D triangle has a corresponding triangle in UV space. What you're looking for is any triangle that contains the point $(0,0)$ in UV space. To find this triangle, you only have to consider the UV space triangles. It's completely irrelevant where the corresponding triangles are located in 3D space. Only once you found a triangle that contains $(0,0)$ in UV space, you are interested in what point on the corresponding 3D triangle the UV point $(0,0)$ maps to. The mapping of the UV space triangle to the 3D triangle works as described above and can be inverted as described above…" CreationDate="2019-01-14T11:59:47.010" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11543" PostId="8472" Score="0" Text="Barycentric coordinates are independent of whatever coordinate system the triangle is embedded in. Think about barycentric coordinates as a way of describing a point on the plane of a triangle relative to the triangle itself. For example, the barycentric coordinates $(1, 1, 1)$ always identify the centroid of a triangle and $(1, 0, 1)$ will always be the point half way along the third edge, no matter what the coordinates the triangle's vertices may be…" CreationDate="2019-01-14T12:13:55.543" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="11544" PostId="8444" Score="0" Text="What do you get if you take out your light source choosing heuristic?" CreationDate="2019-01-14T22:38:21.970" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="11545" PostId="5408" Score="0" Text="What happens in glutMainLoop? Is there any animation/rotation/etc. or is this all a static picture for a given set of inputs? Btw, the question's title totally hooked me! I can't believe people actually launch OpenGL within Excel!" CreationDate="2019-01-15T07:45:20.547" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11546" PostId="8444" Score="0" Text="@trichoplax - If i removed it, this gets a little messy. This is how I do it. I'm now accumulating color from two light sources but it's a single sample ( we don't want to divide by 2). However for every light source I also take a `brdf_sample`. This accounts as 2 BRDF samples. Hence I use MIS with the formula accounting for 1  light sample and 2 BRDF samples. Result seem almost same to me. Note if i assume those 2 brdf samples as 1, results are very bright, like way bright so I think the first approach is right. Updated github so you can see how I did it." CreationDate="2019-01-15T09:34:48.350" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11551" PostId="8472" Score="0" Text="I understand now.. Thank you for you explanation :)" CreationDate="2019-01-15T15:21:52.030" UserId="9994" ContentLicense="CC BY-SA 4.0" />
  <row Id="11562" PostId="8481" Score="1" Text="First question: do you understand what perspective-correct texture mapping is trying to *accomplish*? I'm not being snide; I need to know where to start in explaining the process, because if you say &quot;yes&quot;, I can basically summarize about half of what I would otherwise have to spell out." CreationDate="2019-01-17T00:39:21.650" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11563" PostId="8481" Score="1" Text="Well the points used for interpolation when filling a triangle are not the points of the actual triangle. They're the points of the projected triangle which has been flattened. The triangle that has been z divided. IE: It's point x,y,z is basically now x/z, y/z, 1. (not quite because the typical perspective transform has a bit more too it than that, but that's the general idea). Also z is not interpolated. 1/z is interpolated and 1/(1/interpolated z) is used in the divide." CreationDate="2019-01-17T02:56:44.157" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="11564" PostId="8481" Score="0" Text="Hi @AndrewWilson, your comment looks like an answer to me. I'd suggest you move it as such." CreationDate="2019-01-17T05:16:51.307" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11565" PostId="8482" Score="2" Text="Please describe what your intentions are, what are you going to use it for - it might affect your answer." CreationDate="2019-01-17T07:46:08.480" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11566" PostId="8481" Score="1" Text="Perhaps you should look at these posts:&#xA;1) https://computergraphics.stackexchange.com/questions/4798/what-happened-to-my-texture-mapping &#xA;&#xA;2)https://computergraphics.stackexchange.com/questions/4079/perspective-correct-texture-mapping" CreationDate="2019-01-17T09:42:28.923" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11568" PostId="8481" Score="0" Text="Ok guys, i've just added an edit section in my original message to explain what i understood from the perspective texture mapping. Can you guys read it and tell me if i'm right about it ?" CreationDate="2019-01-17T16:39:23.637" UserId="10022" ContentLicense="CC BY-SA 4.0" />
  <row Id="11570" PostId="8483" Score="0" Text="So subdivision surfaces sound best. Wondering if you have a reading recommendation to learn the depths, you seem to know a lot about the details which I haven't heard about. Will check out OpenSubDiv." CreationDate="2019-01-18T01:53:31.823" UserId="8632" ContentLicense="CC BY-SA 4.0" />
  <row Id="11572" PostId="8488" Score="0" Text="The *eye position* is known, but not the *pixel's position in the eye coordinate system* which is what you're looking for." CreationDate="2019-01-18T09:54:14.253" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11573" PostId="8488" Score="0" Text="ok but the problem is that I not use OpenGL explicity and don't have that stuff described in wiki likes clip space, projection matrix etc. :( I have only simplified javascript used by [gpu.js](https://github.com/gpujs/gpu.js) library which allows me to perform any calculations on gpu - the kernel function (fragment shader written in simplidied javascript) takes on input i,j as this.thread.x and this.thread.y coordinates and thats all (plus input parameters which I describe on question)." CreationDate="2019-01-18T09:59:57.370" UserId="9881" ContentLicense="CC BY-SA 4.0" />
  <row Id="11574" PostId="8488" Score="0" Text="Updated my answer..." CreationDate="2019-01-18T12:23:21.953" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11575" PostId="8488" Score="0" Text="Can you explain what is the meaning of $(W, 0) * \vec {P'_{ij}}$ and $[P_{ij}]_e$ and $(\vec b, \vec v, \vec t)$ and $[P_{ij}]_{std}$ (and how to get $r_{ij}$ from it) because I don't understand notation" CreationDate="2019-01-18T12:36:54.237" UserId="9881" ContentLicense="CC BY-SA 4.0" />
  <row Id="11576" PostId="8488" Score="0" Text="Sorry that's just too much to explain. $(W, 0) * \vec {P'_{ij}}$ means multiply the x component of $\vec {P'_{ij}}$ by W. Read up on linear algebra and change of basis for more information. Also, you did not specify in which basis you want $r_[ij]$." CreationDate="2019-01-18T13:24:57.093" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11577" PostId="8488" Score="0" Text="you not explain meaning of all symbols - sorry I dont understant. I dont indroduce any basis - because I need result in natural/canonical basis $\left[\begin{matrix} 1 &amp;0 &amp;0 \\ 0 &amp;1 &amp;0 \\ 0 &amp;0 &amp;1\end{matrix}\right]$ - which means that $E, T, w$ and $r_{ij}$ shoud not be transformed to any basis" CreationDate="2019-01-18T13:40:21.050" UserId="9881" ContentLicense="CC BY-SA 4.0" />
  <row Id="11578" PostId="8488" Score="0" Text="you also not provide formula for viewport center $C=P_c$ which you use in your calculations" CreationDate="2019-01-18T13:47:29.747" UserId="9881" ContentLicense="CC BY-SA 4.0" />
  <row Id="11579" PostId="8486" Score="0" Text="I think perhaps it might be &quot;constraint-based&quot; or &quot;rule-based&quot; modelling. &#xA;&#xA;Ivan Sutherland's sketchpad program (https://en.wikipedia.org/wiki/Sketchpad) perhaps may have been the first." CreationDate="2019-01-18T16:46:48.617" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11580" PostId="8494" Score="0" Text="thanks, think this will work in OpenGL ES 3 but not 2. I forgot to mention I'm interested in the latter. Perhaps i may have to switch to OpenGL ES 3 though." CreationDate="2019-01-18T20:32:35.580" UserId="9851" ContentLicense="CC BY-SA 4.0" />
  <row Id="11581" PostId="8494" Score="0" Text="On the other hand ES 3.1 only covers 50% of the market, so i will probably look for an alternative way." CreationDate="2019-01-18T21:00:59.067" UserId="9851" ContentLicense="CC BY-SA 4.0" />
  <row Id="11583" PostId="8494" Score="0" Text="Might be possible in ES 2: https://www.khronos.org/registry/OpenGL/extensions/NV/NV_draw_instanced.txt" CreationDate="2019-01-18T22:12:49.873" UserId="9851" ContentLicense="CC BY-SA 4.0" />
  <row Id="11584" PostId="8483" Score="0" Text="Wondering if you could elaborate on [this problem you mentioned with subdivision surfaces](https://computergraphics.stackexchange.com/questions/8489/if-you-can-use-subdivision-surfaces-for-2d-curves/8493#8493): &quot;Subdivision curves do not suffer from the problem that subdivision surfaces have around extraordinary points and therefore all subdivision&quot;" CreationDate="2019-01-18T22:55:10.237" UserId="8632" ContentLicense="CC BY-SA 4.0" />
  <row Id="11585" PostId="8487" Score="0" Text="This looks very close to what I was looking for thank you! Question though, is there a related formula on how to achieve the rectangular effect for each node? From what I've looked into it seems like all examples have voronoi diagrams with polygons. The weight-proportional paper had a rectangular image on google but this paper is paywalled." CreationDate="2019-01-18T23:30:13.483" UserId="10028" ContentLicense="CC BY-SA 4.0" />
  <row Id="11586" PostId="8482" Score="0" Text="I would like a general answer if possible." CreationDate="2019-01-19T01:06:19.763" UserId="8632" ContentLicense="CC BY-SA 4.0" />
  <row Id="11587" PostId="8499" Score="0" Text="Ahh..I don't understand why t_12 = t_21..." CreationDate="2019-01-20T05:03:54.850" UserId="9987" ContentLicense="CC BY-SA 4.0" />
  <row Id="11588" PostId="8500" Score="0" Text="Could you clarify whether this is for a physical camera or the camera view in a rendering setting? Are you looking to apply this transform to modelled objects or real world scenes?" CreationDate="2019-01-20T19:20:51.097" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="11589" PostId="8500" Score="1" Text="Idk if this answers your question, but as I mentioned on the post I'm modding games to intercept the camera properties (including this matrix) and later modify them. The point of this is to create a ``.lib`` camera manipulator to emulate camera modes like nvidia ansel or other ingame implementations, but open source, generic and without dev support. &#xA;&#xA;I'd guess games don't use physical models to render scenes, so I'd think this is ``the camera view in a rendering setting``, I'm modding binaries so I don't know _precisely_ what I'm altering." CreationDate="2019-01-20T22:35:01.630" UserId="10039" ContentLicense="CC BY-SA 4.0" />
  <row Id="11590" PostId="8502" Score="0" Text="I didn't know I could build a frustum with negative left, right, top or bottom values. Now that I'm reading the docs on ``glFrustum`` more carefully I notice that ``GL_INVALID_VALUE`` is only returned under more reasonable conditions (like right and left being the same value). Would you mind clarifying what I should do with the  M11,M13,M22,M23 elements of the matrix? All the resources I was able to find about building a perspective projection matrix conclude on a 4x4 one, while the ones I'm working with are only 3*3. Thanks for the help!" CreationDate="2019-01-20T22:59:15.820" UserId="10039" ContentLicense="CC BY-SA 4.0" />
  <row Id="11591" PostId="8502" Score="0" Text="Uh, your matrices really can't be 3x3 if you're doing perspective projection in 3D." CreationDate="2019-01-20T23:12:13.180" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11592" PostId="8502" Score="0" Text="Anyway, as to the actual formulas, it's really not all that involved. Just look at the values you'd need to put into `frustum_matrix` which are quite straight forward, and then how the matrix you have as input was constructed. It all comes down to scaling the diagonal entries and offseting the translation entries. I'll not provide the exact computations since 1) I'm on mobile and 2) I'll ultimately make a small mistake and you'd have to figure it out yourself anyway. ;-) It's one of these small computations best figured out on a sheet of paper in a minute." CreationDate="2019-01-20T23:28:49.083" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11593" PostId="8499" Score="0" Text="@TIANLUNZHU It's because of the law of reciprocity, a fundamental principle of optics. It says if you swap the incoming and outgoing rays, the BSDF (reflectance or transmittance) doesn't change. Note from the diagram that the $t_{12}$ and $t_{21}$ cases are exactly symmetrical with each other, only with the rays going in the opposite directions." CreationDate="2019-01-21T05:38:29.620" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11600" PostId="8483" Score="1" Text="Around extraordinary vertices subdivision surfaces will generate an infinite number of patches and have only tangent plane continuity at the extraordinary vertex." CreationDate="2019-01-21T09:37:28.347" UserId="7724" ContentLicense="CC BY-SA 4.0" />
  <row Id="11621" PostId="8491" Score="0" Text="Yes. you can see my open-source path tracer as a reference. It contains the createRay function.&#xA;&#xA;https://github.com/gallickgunner/Yune/blob/master/Yune-1.x/progressive-PT.cl" CreationDate="2019-01-21T11:32:37.837" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11624" PostId="8502" Score="0" Text="@Cristian I didn't realize I was looking at the _rotation matrix_ for the camera, totally confused. I'm going to start looking for the projection matrix in the game now. I'll investigate some more and mark this as the solution if it ends up working." CreationDate="2019-01-21T17:38:08.133" UserId="10039" ContentLicense="CC BY-SA 4.0" />
  <row Id="11627" PostId="8502" Score="1" Text="This has the solution for extracting the frustum values https://stackoverflow.com/a/12926655/5538719" CreationDate="2019-01-23T00:11:48.587" UserId="10039" ContentLicense="CC BY-SA 4.0" />
  <row Id="11628" PostId="8510" Score="1" Text="The reason is that when drawing transparent faces, draw order matters. It is simpler to have rough draw order guarantees when you split space into chunks and draw them far to near. Or that is what i think. I might be wrong, which is why I didn't post this as an answer." CreationDate="2019-01-23T03:49:47.220" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="11630" PostId="8511" Score="0" Text="Thanks for the link explaining Clustered Forward Rendering, it is very clear indeed. Then, what is the benefit of still rendering opaque objects using 2D-Tiled Shading? Would it not be easier to use 3D-Clustered Rendering for all objects, like in Doom 2016?" CreationDate="2019-01-23T11:34:57.720" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="11631" PostId="8511" Score="2" Text="I added a quote. It's a performance optimization especially effective with complicated lighting equations and/or many light sources. There is no silver bullet. Each rendering method has benefits and trade-offs which make them suitable in certain contexts or not." CreationDate="2019-01-23T12:36:08.623" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11633" PostId="344" Score="0" Text="What I'm not clear on is what the difference is between EDL vs ED*L? Is EDL not multiplicative?" CreationDate="2019-01-23T17:55:32.867" UserId="101" ContentLicense="CC BY-SA 4.0" />
  <row Id="11635" PostId="8505" Score="0" Text="Hello and welcome to the site. You are using quite a few equations there which means not many people know all of them well enough to answer. I suggest you flesh out the question more by adding the relevant equations and/or code." CreationDate="2019-01-23T19:54:24.597" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11637" PostId="8505" Score="0" Text="@bernie Thank for that comment! Added code to illustrate what I've tried so far." CreationDate="2019-01-24T10:53:03.003" UserId="2960" ContentLicense="CC BY-SA 4.0" />
  <row Id="11639" PostId="8515" Score="0" Text="Path tracing and rasterisation are quite different. Path tracing is basically sending out rays in world space from the camera position through its pixel and hoping for it to hit a light source while bouncing about the scene. Rasterisation is transforming all scene triangles into screen space and paint them on top of each other, using all the tricks in the trade to make it look real, including ambient occlusion. Reflections are easy to get correct with path tracing and &quot;impossible&quot; in rasterisation. Writing up how this is done in different commercially available game engines is a huge task..." CreationDate="2019-01-25T12:36:53.853" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11640" PostId="8515" Score="0" Text="I know most of the things you said. I'm not talking about VSD but the shading/lighting part of both techniques which are somewhat similar I guess. For example in rasterization based engines we are still using BRDFs which require an outgoing and incoming direction. I can't think of any other way but using ray-casting to compute this BRDF. This means it's simialr to what we do in path tracing. And what you said -`&quot;impossible&quot; in rasterization- This is not quite true. Reflections can be done in rasterization based engines it's just hard to do." CreationDate="2019-01-25T12:42:11.637" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11641" PostId="8515" Score="0" Text="Well, I wrote &quot;impossible&quot; because it's almost true - I'm sure you get it. You can hack reflections so that it will fool most people while the trained eye will hurt. Even today getting reflections on a non-trivial sphere-flake is hard using rasterisation. Semi-glossy and refractive materials are also a difficult to (m/f)ake." CreationDate="2019-01-25T13:04:10.097" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11643" PostId="8517" Score="0" Text="How do you get the view direction aka the ray from hit point  to the eye in a raytracer?  Do they just do `view_dir = eye - hitpoint` and `light_dir = light_pos - hitpoint` to get these vectors and just plug in to the BRDF?" CreationDate="2019-01-25T20:07:53.447" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11644" PostId="8517" Score="1" Text="@gallickgunner In a ray-tracer, it's just the direction of the incoming ray. You don't need to calculate it again, you already have it stored. In a rasteriser where there is no incoming ray, it's calculated the way you describe." CreationDate="2019-01-26T10:59:43.883" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11645" PostId="8517" Score="0" Text="Yeah that's what I wanted to know. The direction of the rays are calculated so the process of direct lighting is almost the same as in raytracing. Thanks for the answer." CreationDate="2019-01-26T11:36:23.943" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11648" PostId="8523" Score="0" Text="A method (though probably not a great one) would be to represent one of the frusta as a 6 sided polyhedron and then do a 3D clip of it with the other's planes. There are *probably* free libraries out there to do the clipping/intersection....but I haven't looked." CreationDate="2019-01-28T16:40:39.753" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11649" PostId="8523" Score="0" Text="Thanks @SimonF, I can easily retrieve the info for each of the six planes (12 if accounting for two cameras), so yes, I am already representing the frusta as 6 sided polyhedrons. I'm not sure what you mean by doing a 3D clip of it with the other's planes. Do you mean to check if any of the planes in set 1 intersects any of the planes in set 2?" CreationDate="2019-01-28T16:46:38.020" UserId="10074" ContentLicense="CC BY-SA 4.0" />
  <row Id="11651" PostId="8524" Score="1" Text="Do you have an oscilloscope or logic analyzer you can use to capture the signal and see if it looks right?" CreationDate="2019-01-28T18:26:37.997" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11653" PostId="8525" Score="0" Text="I'll try to find time to answer later, but in the mean time, Naty Hoffman's course is an excellent primer (I'm not sure those are the notes you are referring to). Video: https://www.youtube.com/watch?v=j-A0mwsJRmk Course material: https://blog.selfshadow.com/publications/s2013-shading-course/" CreationDate="2019-01-29T08:39:08.783" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11654" PostId="8525" Score="2" Text="Regarding your third point, I asked a similar question here: https://computergraphics.stackexchange.com/questions/2285/how-to-properly-combine-the-diffuse-and-specular-terms" CreationDate="2019-01-29T08:41:41.237" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11655" PostId="8523" Score="0" Text="If you just consider, say, a cube and imagine a plane as a knife, you can slice off a portion of the cube to leave a new (likely more complex) polyhedron. Repeat with the remaining planes or until there is nothing left of the original. This involves some tricky coding (FWIW I needed to do this when writing the original PowerVR graphics API and I'm in no hurry to do it again!) so it might be more convenient to search for an existing library." CreationDate="2019-01-29T08:48:38.400" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11656" PostId="8525" Score="0" Text="Yeah I actually read the whole course notes by Naty. It was an eye opener but I still couldn't get an answer to above problems. Infact that's where my problem arose. If the Fresnel reflectance is an RGB triple then what's the origin of the color, the absorption by atoms or the frensel reflectance? Maybe the two are linked but I can't seem to figure out how. &#xA;&#xA;I read your question but didn't quite understand what the guy was trying to say in the end. So basically the gist is, the fresnel terms are actually baked inside the diffuse BRDF?" CreationDate="2019-01-29T09:32:07.907" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11657" PostId="8524" Score="0" Text="@DanHulme luckily I got access to a CRO, I had a look at the outputs from the sample bit-stream and my code, turns out the hardware did not generate any signal from my code, which is not what the test bench showed. I tested out the signal individually by assigning it to a switch and toggling manually, that worked, so the ucf constraints were ok. Beyond that I have no clue as to where the issue resides." CreationDate="2019-01-29T11:11:08.353" UserId="10075" ContentLicense="CC BY-SA 4.0" />
  <row Id="11658" PostId="8524" Score="2" Text="Well, it sounds like you have an FPGA problem, not a graphics problem. [electronics.se] might be more help to you." CreationDate="2019-01-29T11:13:12.070" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11659" PostId="262" Score="0" Text="Link borken, the new one here: https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch27.html" CreationDate="2019-01-30T07:52:28.233" UserId="10089" ContentLicense="CC BY-SA 4.0" />
  <row Id="11660" PostId="8529" Score="3" Text="[This related question](https://computergraphics.stackexchange.com/q/4422/6) and its answers should get you a little further. DirectX 12 is actually more similar to Vulkan than to OpenGL, but with the help of those tables you should be able to figure out the further DirectX keywords you should be looking for. But...rest assured that something to the effect of loading/storing data in aribtrary memory locations of arbitrary textures is definitely possible in DirectX 12." CreationDate="2019-01-30T13:18:14.300" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11661" PostId="8523" Score="1" Text="Thank you very much @SimonF I'm relatively new to this, would you happen to have any suggestion to what keywords to look for in a library? is this computational geometry?" CreationDate="2019-01-30T14:06:37.520" UserId="10074" ContentLicense="CC BY-SA 4.0" />
  <row Id="11662" PostId="8523" Score="0" Text="Yes or &quot;Constructive Solid Geometry&quot;  AKA  CSG https://en.wikipedia.org/wiki/Constructive_solid_geometry" CreationDate="2019-01-30T14:26:05.340" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11663" PostId="8523" Score="1" Text="@SimonF I really appreciate your help" CreationDate="2019-01-30T14:29:59.820" UserId="10074" ContentLicense="CC BY-SA 4.0" />
  <row Id="11664" PostId="8531" Score="1" Text="What surface are you talking about? It's not obvious what you want from your question." CreationDate="2019-01-30T22:10:43.400" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11665" PostId="8531" Score="0" Text="@lightxbulb Added some extra information, and tweaked the title. I'm looking to &quot;unwrap&quot; a torus in four dimensions to create a two-dimensional tile, this tile (or surface) will contain noise values which could be used for any form of procedural texture (such as a height map). It is possible to also retrieve the gradient at each point on this surface, however since they're 4D I don't know how to translate that to 2D (or even if it is possible)." CreationDate="2019-01-30T22:58:24.770" UserId="6406" ContentLicense="CC BY-SA 4.0" />
  <row Id="11666" PostId="8531" Score="1" Text="Can you define the map that you use to transform it into 2 dimensions? Because currently you have a surface in 4 dimensions, which you can't even embed in 3d. It's also not obvious what you want to do with the gradient vector. It simply gives you the direction in which your function increases the fastest, and there are infinitely many mappings to 2D." CreationDate="2019-01-30T23:16:15.150" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11667" PostId="8523" Score="0" Text="Could you give a bit of context as to why you want to do that? My gut feeling is that there is probably a closed form solution (compute the volume that satisfies all 12 plane equations) but it's probably very tedious to write. So my reaction would be to look at the bigger picture and see if it's possible to solve the problem differently, in a simpler way." CreationDate="2019-01-31T02:44:03.840" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11668" PostId="4994" Score="0" Text="Wow, wonderful and thorough answer! And I could even understand it!" CreationDate="2019-01-31T08:33:22.720" UserId="9792" ContentLicense="CC BY-SA 4.0" />
  <row Id="11669" PostId="8533" Score="0" Text="I actually know most of the things you're trying to tell me. In CG nothing is accurate and we are dealing with approximations even at the highest level of Physically based rendering/shading. The main point is the origin of the color. As you said albedo represents the amount light energy gets reflected in that specific wavelength or in-other words get absorbed. Where does the RGB triple for Fresnel reflectance ties in as this also represents the amount of energy reflected in each of the specific wavelengths?" CreationDate="2019-01-31T09:54:08.443" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11670" PostId="8533" Score="0" Text="gallickgunner: You don't use Fresnel with ideal diffuse materials. RGB are not specific wavelengths either, you do (weighted) integration over wavelengths.&#xA;@simon F Please do not edit my post over nothing. 'realize' is a perfectly valid spelling. And I would appreciate it if you do not remove my thoughts out of my post. Most of what was written was wrong in case you did not notice." CreationDate="2019-01-31T10:16:18.453" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11671" PostId="8533" Score="3" Text="I edited your answer WRT your opinion on the OP's mental state. IMO that was inappropriate. (As for the spelling that was simply because my the browser flagged it as incorrect)" CreationDate="2019-01-31T11:15:37.150" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11672" PostId="8531" Score="0" Text="@lightxbulb [This page](http://ronvalstar.nl/creating-tileable-noise-maps) might help convey the coordinate conversion. I've also added more information to the question. My end goal is something similar to [this picture](https://catlikecoding.com/unity/tutorials/simplex-noise/02-04-3d.png). I'm essentially trying to produce a matching heightmap and normal map?" CreationDate="2019-01-31T11:40:30.973" UserId="6406" ContentLicense="CC BY-SA 4.0" />
  <row Id="11673" PostId="8534" Score="0" Text="Not sure exactly what you are doing, but do the surfaces have the same alpha value? That *might* generate the same coverage mask for two surfaces and, hence, only the closer one would be seen." CreationDate="2019-01-31T11:55:56.030" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11674" PostId="8534" Score="1" Text="@SimonF good idea! yes that is in fact the case, especially with the back faces." CreationDate="2019-01-31T12:04:57.107" UserId="10099" ContentLicense="CC BY-SA 4.0" />
  <row Id="11675" PostId="8534" Score="1" Text="@SimonF After creating my own randomized alpha to coverage shader (writing gl_SampleMask[0]) I can confirm that this solves the issue." CreationDate="2019-01-31T12:51:13.157" UserId="10099" ContentLicense="CC BY-SA 4.0" />
  <row Id="11676" PostId="8534" Score="0" Text="No worries - I had suspicions as, in the past, I once was trying to propose better hardware for &quot;screen door&quot; translucency to address this sort of thing." CreationDate="2019-01-31T12:55:54.803" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11678" PostId="8533" Score="2" Text="@Simon F Yeah, after googling it doesn't seem like it means what I thought it did (at least in English, I  translated a phrase from my language), I apologise. What I was trying to say is that he has too many wrong notions mixed up with correct ones. My point being that it's probably best that he reads a good book/article on the subject from scratch, rather than keep adding on top of that." CreationDate="2019-01-31T13:19:19.893" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11679" PostId="8531" Score="1" Text="Ok, I believe that article clarified what you're doing: you're taking a 2-manifold slice out of some 4-dimensional scalar field. What is still unclear is what you want to do with that gradient, because it is indeed 4D. Assuming that you want the gradient of the image, then there are a few possibilities. The obvious is to take finite differences from your image, then you get the 2D gradient. Another possibility is to try and taking the derivative analytically. Let $f(x,y,z,w)$ be your scalar field (the noise function), the I take it that image space for you is really $\theta, \phi$, that is..." CreationDate="2019-01-31T13:36:14.540" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11680" PostId="8531" Score="1" Text="You can take the partial derivatives with respect to $\theta, \phi$: $\frac{\partial f}{\partial \theta} = -\frac{\partial f}{\partial x}\sin\theta + \frac{\partial f}{\partial y}\cos\theta$ and similarly:  $\frac{\partial f}{\partial \phi} = -\frac{\partial f}{\partial z}\sin\phi + \frac{\partial f}{\partial w}\cos\phi$ where I have used the chain rule for taking a partial derivative of a multivariable composite function (the last two terms for $\theta$ are $0$, same for the first two of $\phi$). If this answers your question I can formulate it as an answer." CreationDate="2019-01-31T13:41:10.097" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11681" PostId="8523" Score="0" Text="@Francesco You would have to find the polyhedron defined by the intersection and then integrate (it's integrable) to get the volume, then you have to divide the volume by the union of the two volumes I guess. This last part really depends on how you want to setup your percentages, since there are many possibilities which result in 100% when coinciding and 0% when there's no intersection at all. If you're fine with a numerical solution, you can run a monte carlo simulation. Basically generate random points inside a tight volume around both. If a point falls within both count it, otherwise not." CreationDate="2019-01-31T13:52:53.417" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11682" PostId="8523" Score="0" Text="Let the counter for points inside the intersection be $a$, also count the number of points that fall only in one of the two volumes (but not in both at the same time), denote it by $b$, let $n$ be the total number of points generated, and let $V$ be the volume of the tight volume in which you generate the points. Then $\frac{a}{n}V$ gives you an approximation of the volume of the intersection and $\frac{a+b}{n}V$ an approximation of the volume of the union. If you're happy with this I can code you a C++ variant." CreationDate="2019-01-31T13:59:05.347" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11683" PostId="8520" Score="0" Text="I think that it does depend on it, it just does not depend on $\phi$. Where's the reference from?" CreationDate="2019-01-31T14:00:37.833" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11684" PostId="8533" Score="0" Text="I'm not concerned whether we are talking about ideal diffuse or real diffuse. (atleast for point 1). When I say specific wavelengths, I don't mean to think of R,G,B as a single number assosciated with wavelength. I fully comprehend R,G,B are more of a range of spectrum. If it seemed that way then I apologize for stating it that way. That aside your answer here completely misses the core of my question and points 1,2,3. I'm simply confused about the relation between the fresnel reflectance and the energy getting absorbed. Both imply the origin for the color of the surface." CreationDate="2019-01-31T14:16:19.840" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11685" PostId="8533" Score="0" Text="For points 2 and 3 assume a physically based diffuse surface. That's why I asked the question. If this was ideal diffuse there would be no point to use microfacet based models and we'd be just using lambertian model. Point 2 is just for personal understanding. For a physically based diffuse surface it'd seem more correct to use a diffuse brdf + a specular brdf, but where ever I see, I find only diffuse brdfs like oren nayar only." CreationDate="2019-01-31T14:20:24.787" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11686" PostId="8531" Score="0" Text="@lightxbulb I'm aiming for the analytical solution, your partial derivatives certainly look like the solution! Thank you!" CreationDate="2019-01-31T14:21:42.270" UserId="6406" ContentLicense="CC BY-SA 4.0" />
  <row Id="11687" PostId="8523" Score="1" Text="Thanks everyone for the help, it's pretty clear to me how this is out of my reach with my current geometry and math knowledge. @lightxbulb isn't the montecarlo simulation very computationally expensive? I think I am approaching the problem from the wrong angle: I need to calculate an image overlap without using feature matching, but using the viewing frusta of the images. I assumed that the intersection of the two frusta would be proportional to the image overlap, but I might try to use the canvas planes and project one onto the other, instead of going the intersecting frusta way" CreationDate="2019-01-31T14:35:00.397" UserId="10074" ContentLicense="CC BY-SA 4.0" />
  <row Id="11688" PostId="8523" Score="0" Text="@Francesco Depends what you consider computationally expensive and what's considered an acceptable error threshold in your case. I didn't get the rest really. You have two 2d images and are simply trying to find whether they overlap? Isn't that just checking whether the rectangles overlap?" CreationDate="2019-01-31T14:42:54.610" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11689" PostId="8523" Score="0" Text="@lightxbulb which rectangles? you mean the projection planes?" CreationDate="2019-01-31T14:49:33.220" UserId="10074" ContentLicense="CC BY-SA 4.0" />
  <row Id="11690" PostId="8523" Score="0" Text="@Francesco Let's take it to a chatroom if you're online right now: https://chat.stackexchange.com/rooms/89078/frusta" CreationDate="2019-01-31T14:52:11.047" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11691" PostId="8523" Score="0" Text="@lightxbulb the chatroom stopped working" CreationDate="2019-01-31T15:20:11.683" UserId="10074" ContentLicense="CC BY-SA 4.0" />
  <row Id="11692" PostId="8531" Score="0" Text="I had forgotten the $r_0$ and $r_1$ terms from the differentiation, refer to the answer for the correct expression." CreationDate="2019-01-31T15:45:40.683" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11693" PostId="8523" Score="1" Text="For future users: we have clarified that the problem is so vague since it is a definition from a paper that was not explained in details, as it stands it's not clear what exactly the 'correct' formulation should be." CreationDate="2019-01-31T15:54:24.403" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11694" PostId="8533" Score="0" Text="@gallickgunner Well you should be concerned with whether you're talking about ideal diffuse or simply diffuse. You are referring half of the time to the diffuse term of the Cook-Torrance BRDF without taking into account that it models ideal labertian reflectance while ignoring any Fresnel effect, since $f_0$ is set to $1$ in that case. In fact if you go to other models and do not concern yourself only with Cook-Torrance, you'll see that Fresnel effects enters the diffuse term too: https://www.cs.utah.edu/~shirley/papers/jgtbrdf.pdf&#xA;As I said, Cook-Torrance is an approximate model of a class.." CreationDate="2019-01-31T16:02:48.533" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11695" PostId="8533" Score="0" Text="of different materials. Regarding your first point: it depends on the material, you'll see a lot more more Fresnel effect (affecting the color obviously) for materials with a specific refraction index, than compared to materials close to diffuse (where the albedo matters a lot more). As for your second point: what people refer to as diffuse is simply ideal diffuse. Sure if you want more complex materials, you'll use more complex models. There's no proper way either, unless you measure the BRDF from real-world data and use that (and it's still a discretisation), the rest are approximations." CreationDate="2019-01-31T16:07:25.813" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11696" PostId="8533" Score="0" Text="For your third point, except for the linked paper, consider that you can explain this in a number of ways. You can obviously say that it's taken into account inside the term, or you can also say that it is constant such that it produces $1$ (for the Schlick approximation $f_0=1$). If you want to be (almost) entirely accurate you should start using LUTs for brdf functions that were measured from the real-world, and you should do spectral tracing, and you should use the actual Fresnel formula and not Schlick's approximation, and you you may have to take polarisation into account, etc." CreationDate="2019-01-31T16:11:09.250" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11698" PostId="8520" Score="0" Text="https://drive.google.com/file/d/0BzvWIdpUpRx_d09ndGVjNVJzZjA/view, from page 4." CreationDate="2019-01-31T20:47:00.017" UserId="9987" ContentLicense="CC BY-SA 4.0" />
  <row Id="11699" PostId="8520" Score="0" Text="The GGX distribution term they are referring to doesn't take $l$ as argument. However you can see that they do have other terms that take $l$ into account." CreationDate="2019-01-31T21:39:16.407" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11700" PostId="8533" Score="0" Text="Then you mean to say both Fresnel and absorption by electrons are the reason for color. Take the example of the metal gold then. According to the paper by Naty Hoffman, you set the fresenel reflectance (FR) as the specular color. What about the albedo here? By albedo I mean the concept of electrons abosrbing inverse of what was reflected." CreationDate="2019-01-31T23:56:25.477" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11701" PostId="8539" Score="0" Text="How is mat[3] calculated ? I notice your diffuse shading is also broken so I would start by looking at your normals. Maybe outputting the 'norm' variable as your fragment shader output so you can visualise if they are correct." CreationDate="2019-02-01T08:16:35.247" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11713" PostId="8544" Score="0" Text="You could try the complementary colors. So if you have $c \in [0,1]^3$ then $(1,1,1) - c$. You can use luma to control luminance." CreationDate="2019-02-02T23:24:05.097" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11714" PostId="8547" Score="0" Text="Giving a quick look I see a few problems. You are missing the negative signs ($-u,-v$) in the matrix $A$ during matrix composition. Also since you want to do transformations around the point $u,v$. You correctly move $u,v$ to origin by doing $-u,-v$ but you forgot to move back to the origin since $x,y$ are translating from origin to $x,y$ not from $u,v$ to $x,y$. So in your matrix composition before the last matrix $T$ you need to add another matrix which scales from $u,v$ to origin." CreationDate="2019-02-03T11:43:25.747" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11716" PostId="8547" Score="0" Text="@gallickgunner you are completely right. I am just now confused where the scale matrix should go or if maybe I need to make a new A matrix that considers the scale? Nonetheless with this implemented I still have the result of a rectangle that is either not moving or off screen depending on whether I pass in the transpose or not." CreationDate="2019-02-03T19:41:28.883" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="11717" PostId="8547" Score="0" Text="Ok I assume the vector you are multiplying with is a column vector since you do `l * p` i.e. the vector comes afterwards. Then shouldn't the above composition be backwards since thinking about it in components, you should have $TRSA * v$. I don't know if that's what you meant when you said you had tried changing the order of multiplications, though.  Also try breaking your composite matrix in components, maybe first check each of your matrices separately to see if the individual operations are working perfectly, then start composing them 1 by 1." CreationDate="2019-02-03T21:36:37.843" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11718" PostId="8547" Score="0" Text="T R S A * v was just what I needed thanks!" CreationDate="2019-02-03T22:32:28.233" UserId="2308" ContentLicense="CC BY-SA 4.0" />
  <row Id="11719" PostId="8549" Score="1" Text="Perhaps working in HSV space would give results more like in the image?" CreationDate="2019-02-04T04:44:08.670" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11720" PostId="8548" Score="0" Text="Do you need to render to screen and generate the movie at the same time? If you can do a separate pass for the movie, you could draw to an offscreen and call `getBytes()` on the offscreen once it's finished before moving on to the next frame, maybe?" CreationDate="2019-02-04T04:48:20.417" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11721" PostId="8549" Score="0" Text="I tried with HSL without much luck, but I might have missed something. The linked jsbin has the functions to transform to/from hsl space. What should I try?" CreationDate="2019-02-04T05:28:56.610" UserId="7512" ContentLicense="CC BY-SA 4.0" />
  <row Id="11723" PostId="8549" Score="0" Text="What does this have to do with a gradient?" CreationDate="2019-02-04T08:52:10.100" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="11724" PostId="8549" Score="0" Text="It is just some white or blue noise remapped to some narrow range added to color probably. Same value added to all rgb components gives first picture result." CreationDate="2019-02-04T11:56:26.677" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="11725" PostId="8539" Score="1" Text="Please copy and paste the text into the question instead of adding screenshots of text. Screenshots aren't searchable so it's harder for people to find and answer your question. They're also not useful to blind people who use a screen-reader to use the site." CreationDate="2019-02-04T13:10:32.417" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11726" PostId="8546" Score="1" Text="This is a great way, but as i get closer to 127 in R or G or B values, lower is the difference between colors" CreationDate="2019-02-04T15:34:12.840" UserId="10110" ContentLicense="CC BY-SA 4.0" />
  <row Id="11727" PostId="8550" Score="0" Text="Thanks, looks like adding the same value to each channel worked: https://i.imgur.com/1OydskY.png" CreationDate="2019-02-04T16:52:12.293" UserId="7512" ContentLicense="CC BY-SA 4.0" />
  <row Id="11728" PostId="8548" Score="0" Text="@user1118321 I have been trying to do that (render just to an offscreen buffer) but have been going around in circles with how to set the Descriptor. Apparently I don't fully understand how to use the colorAttachments. I get an error if I try to attach a buffer without the usage set to include .rendertarget. But if I do that then I get an error when trying to use getBytes. Is there any sample code on the web I could study?" CreationDate="2019-02-05T04:19:08.427" UserId="10117" ContentLicense="CC BY-SA 4.0" />
  <row Id="11730" PostId="8546" Score="0" Text="@Strader I don't understand what mean. The difference is a constant 128." CreationDate="2019-02-05T06:47:24.717" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11731" PostId="8547" Score="0" Text="Np. Adding this as an answer to make it clear the question was solved." CreationDate="2019-02-05T08:40:27.357" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11733" PostId="8546" Score="1" Text="&quot;component = 0x80 ^ component;&quot; should do the trick more succinctly, with a trivial extension to all three components in parallel via 0x808080." CreationDate="2019-02-05T11:30:39.950" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11734" PostId="8553" Score="0" Text="Thank you for the answer, its greatly appreciated!" CreationDate="2019-02-05T15:16:07.430" UserId="10120" ContentLicense="CC BY-SA 4.0" />
  <row Id="11736" PostId="8546" Score="0" Text="Found it to be better way to manually lower the alpha of background color for better contrast and calculate inverted one using full alpha" CreationDate="2019-02-05T15:56:23.053" UserId="10110" ContentLicense="CC BY-SA 4.0" />
  <row Id="11739" PostId="8550" Score="0" Text="Out of curiosity, how did you determine that it was luminance noise?" CreationDate="2019-02-06T03:11:18.297" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="11740" PostId="8550" Score="0" Text="@JulienGuertault Just look at it." CreationDate="2019-02-06T08:31:25.557" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11741" PostId="8559" Score="0" Text="Read [this](http://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/) blog post." CreationDate="2019-02-06T21:56:44.440" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="11742" PostId="8555" Score="0" Text="Try substituting $1/\alpha^2$ with $\cos^2\phi/\alpha^2_x + \sin^2\phi/\alpha^2_y$." CreationDate="2019-02-07T03:11:47.937" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11744" PostId="8560" Score="0" Text="Thank you for pointing the wikipedia article about the Mach bands. It is very interesting" CreationDate="2019-02-07T10:25:08.887" UserId="10136" ContentLicense="CC BY-SA 4.0" />
  <row Id="11747" PostId="8563" Score="0" Text="I don't quite understand your question. In the local coordinate system (of each object) the object is assumed to be at the origin. This local origin can be any point in the world space though. You then need to construct a transformation matrix to go between world-local. Are you perhaps asking how to construct this matrix?" CreationDate="2019-02-07T12:39:25.810" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11748" PostId="8563" Score="0" Text="Thanks for the comment. Yeah ! I would want to know that ! and also I meant if I wanted to put `m2` in (1,1,1) of `L` how would I do that ?" CreationDate="2019-02-07T13:16:46.087" UserId="9739" ContentLicense="CC BY-SA 4.0" />
  <row Id="11749" PostId="8564" Score="0" Text="Thanks for the answer ! my question would be then how to say where is `m2` wrt `L` where L is the local coordinate system of `m1`, or if I wanted to put `m2` in (1,1,1) of `L` How would I proceed ?" CreationDate="2019-02-07T13:18:45.080" UserId="9739" ContentLicense="CC BY-SA 4.0" />
  <row Id="11750" PostId="8564" Score="0" Text="What coord system is $m_2$ define in to begin with?" CreationDate="2019-02-07T13:22:48.410" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11751" PostId="8564" Score="0" Text="in the World coordinate system `W`" CreationDate="2019-02-07T13:23:27.787" UserId="9739" ContentLicense="CC BY-SA 4.0" />
  <row Id="11752" PostId="8564" Score="1" Text="Then you transform it the same way as you do $m_1$. You need $W$'s basis vectors expressed in $L$, and then build a matrix with the columns being those vectors. Then multiply the points of $m_2$ with it. You may also need to translate in case the origins of $L$ and $W$ are not the same." CreationDate="2019-02-07T13:29:59.537" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11753" PostId="8566" Score="0" Text="Thanks, that's what I'm doing atm. Your second link is pointing to RWTexture3D, I tihnk you meant [link](https://docs.microsoft.com/en-us/windows/desktop/direct3dhlsl/sm5-object-rwtexture2d)" CreationDate="2019-02-07T23:09:07.930" UserId="10086" ContentLicense="CC BY-SA 4.0" />
  <row Id="11754" PostId="8555" Score="0" Text="Thanks for your comment, @lightxbulb. That does indeed seem to make the two formulas equal if anisotropy is 0. Before I can investigate further, can you please clarify what ϕ stands for in your formula?" CreationDate="2019-02-08T06:44:20.143" UserId="10125" ContentLicense="CC BY-SA 4.0" />
  <row Id="11756" PostId="8555" Score="0" Text="It's the azimuthal angle (the one accounting for anisotropy). Go to the course notes of &quot;Physically based shading at Disney&quot;. Specifically at the end of the pdf you have derivations in the appendix: https://blog.selfshadow.com/publications/s2012-shading-course/" CreationDate="2019-02-08T11:40:16.160" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11757" PostId="8569" Score="0" Text="What about moving a shape around with the keyboard? In this case, there is no way of knowing where the next 'keyframe' is going to be and therefore you can't interpolate anything to get a consistent speed." CreationDate="2019-02-09T17:42:41.603" UserId="10149" ContentLicense="CC BY-SA 4.0" />
  <row Id="11758" PostId="8574" Score="0" Text="Are the last 8 bits the exponent?" CreationDate="2019-02-10T18:58:01.977" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11763" PostId="8575" Score="0" Text="Could there be [shear in the matrix](https://en.wikipedia.org/wiki/Shear_matrix)?" CreationDate="2019-02-11T05:53:23.887" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11765" PostId="8575" Score="0" Text="There definitely is a shear involved. Seems like a shear in the X-coordinate." CreationDate="2019-02-11T08:54:47.067" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11767" PostId="8578" Score="0" Text="Removing the π and using 1/2 and cos(theta) respectively appears to work. I wish I understood why, though." CreationDate="2019-02-11T09:05:14.903" UserId="5180" ContentLicense="CC BY-SA 4.0" />
  <row Id="11768" PostId="8560" Score="0" Text="@lightxbulb *&quot;Your monitor can reproduce a finite amount of grey levels&quot;*. One could also argue that if you're only storing the image with 8bits/channel, it's not just the monitor's fault!  &#xA;&#xA;*David* : One suggestion would be to compute your gradient with 9 or 10 bits/channel and use, say, ordered dither when reducing to 8bpc precision." CreationDate="2019-02-11T09:19:13.977" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11771" PostId="8560" Score="0" Text="@SimonF You're sending off 8bit buffers for displaying precisely because of the limited capabilities of your monitor. Most (budget) monitors are not even 8bit but 6bit+FRC. You may as well be using doubles on your app side, you still need to display the colours on your monitor, so you will convert to 8bit at some point if your monitor doesn't support higher. Obviously dither can help a little." CreationDate="2019-02-11T10:06:46.527" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11772" PostId="8575" Score="0" Text="Yes, one of the matrices is Shear, but I don't understand when to use it." CreationDate="2019-02-11T10:13:20.960" UserId="10160" ContentLicense="CC BY-SA 4.0" />
  <row Id="11775" PostId="8575" Score="0" Text="Try Shearing &gt; Scaling &gt; Rotating &gt; Translatng. As long as the scaling is uniform it doesnt matter if you do shearing or scaling first. Translation wouldn't affect shearing though, not quite sure about rotation. Why don't you try shearing before and after rotation. And post the results. Would be a nice find :)" CreationDate="2019-02-11T11:25:18.843" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11776" PostId="8575" Score="1" Text="You can find the complete transform matrix T in one step minimizing a linear least squares error." CreationDate="2019-02-11T12:35:09.053" UserId="7957" ContentLicense="CC BY-SA 4.0" />
  <row Id="11792" PostId="8575" Score="0" Text="You don't need least squares. The mapping is obviously linear, so it can be computed from 2 different points." CreationDate="2019-02-11T16:51:05.000" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11794" PostId="8560" Score="0" Text="@lightxbulb Ha! You youngsters with your digital input monitors. I feel you have put the cart before the horse. Shall I send you a picture of a graphics card with its separate DAC chip set to drive an *analogue* input CRT monitor? ;-)  It's more a historical, memory cost reason for the choice of precision and that 8-bits (with gamma) is *almost* enough for certain use cases: https://poynton.ca/notes/colour_and_gamma/GammaFAQ.html#smoothly_shade" CreationDate="2019-02-12T08:40:32.450" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11795" PostId="8575" Score="0" Text="@lightxbulb You'll need 3 pairs of points because there will be 6 unknowns in the transformation matrix (4 for rotation/scaling +2 for translation)" CreationDate="2019-02-12T09:09:24.613" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11796" PostId="8560" Score="0" Text="@SimonF I don't think I understood your point." CreationDate="2019-02-12T11:18:55.447" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11797" PostId="8575" Score="0" Text="@SimonF You can see the translation from the image directly. If anything your solution is needlessly complex." CreationDate="2019-02-12T11:28:24.867" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11800" PostId="8560" Score="1" Text="What I mean is, display devices from, say, the 70s through to late 90s, had analogue input, so the precision (arguments about noise etc, limiting precision excepted) of these devices could be said to be &gt; 8bpc. It was the graphics subsystem memory + DAC which then determined the effective image depth. Since memory was typically rather expensive, 8bpc was usually the limit. (Some systems did use higher precision, e.g. some Silicon Graphics work stations, but then they were probably also rendering in linear space and so would need greater precision in order to obtain the required dynamic range.)" CreationDate="2019-02-12T15:18:16.947" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11802" PostId="8584" Score="1" Text="I don't quite understand this question. The &quot;spherical coordinates representation&quot; of *what*? And what does &quot;without calculations&quot; mean here? Are you just supposed to draw the sphere on the left side into polar space on the right side? Ultimately, though, I'm not even sure this question is particularly computer graphics related to begin with. It seems more like a general mathematical question." CreationDate="2019-02-12T19:00:57.770" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11803" PostId="8584" Score="0" Text="Your question is not clear, however I believe that this image will help you: https://en.wikipedia.org/wiki/Spherical_coordinate_system#/media/File:3D_Spherical.svg&#xA;In cartesian coordinates you represent a point, by its offset along $x,y,z$ in spherical coordinates you represent a point in terms of 2 angles and a length. The equations that you do not want to use are what actually links these two." CreationDate="2019-02-12T19:36:00.840" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11804" PostId="8585" Score="0" Text="Whether you pick one or the other, both have an error compared to using spectral radiance and corresponding materials. I think it's really about how you prefer to have your pipeline. I would stick to trichromatic rendering approximating radiometry, simply because it's easier to derive and reason about. You can always integrate the radiance in the end. I'll emphasize though, that if you want to be accurate you should use neither, and rather opt for spectral radiance." CreationDate="2019-02-12T21:42:28.400" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11806" PostId="8584" Score="1" Text="I believe the question is what would be the 3d shape of the equivalent sphere on the left, if the coordinates where R/theta/phi instead of XYZ. Probably without calculation means coming up with the shape intuitively. The answer is pretty easy, it looks like a curved sheet from 0,[0-360],0 to 2r,[0-360],90. You can think of it like this: the sphere is a bunch of circles around Z. They start at R=0,phi=0 and go to R=2r,phi=90. Each complete circle around Z is a line from theta=0 to theta=360. So, bunch of circles translates to bunch of lines. We have the start/end, and you can guess the curve." CreationDate="2019-02-13T05:24:39.320" UserId="6442" ContentLicense="CC BY-SA 4.0" />
  <row Id="11807" PostId="5725" Score="0" Text="It might be worth saying that `GL_TEXTURE_RECTANGLE_ARB` doesn't use normalized coordinates, but that `GL_TEXTURE_*D` textures do." CreationDate="2019-02-13T05:42:28.293" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11809" PostId="8587" Score="0" Text="&quot;but I am not sure if it is what I am looking for&quot; - why do you expect that others would know what you're looking for? It's not obvious what you want from the way you formulated it. There's infinitely many ways to cover a polygon with rectangles. The simplest is to just take max and min $x,y$ vertices and create a rectangle based on that." CreationDate="2019-02-13T13:14:34.873" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11810" PostId="8587" Score="0" Text="Yes if the number of rectangle is 1. I need one algorithm that does that but choosing the number of rectangles" CreationDate="2019-02-13T13:17:31.180" UserId="9125" ContentLicense="CC BY-SA 4.0" />
  <row Id="11811" PostId="8587" Score="0" Text="For not sure what I am looking for is the high number of papers that I am not able to understand properly as non-graphic guy." CreationDate="2019-02-13T13:19:53.680" UserId="9125" ContentLicense="CC BY-SA 4.0" />
  <row Id="11812" PostId="8587" Score="0" Text="As I said, this has infinitely many solutions. If you want n rectangles, you could just partition that 1 rectangle into however many you want. The issue here is that as you formulated your problem it's underconstrained." CreationDate="2019-02-13T13:20:12.500" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11813" PostId="8587" Score="0" Text="In that way it won’t grant me minimum area coverage. If you take a triangle and apply that how it will work?" CreationDate="2019-02-13T13:22:35.457" UserId="9125" ContentLicense="CC BY-SA 4.0" />
  <row Id="11814" PostId="8587" Score="0" Text="You never mentioned anything about minimal area in your question. Edit your question to provide additional details." CreationDate="2019-02-13T13:24:55.617" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11815" PostId="8587" Score="0" Text="You should also include the form of your polygon (rectilinear, convex, with holes,self intersecting, etc), as well as whether the covering rectangles need to be axis aligned or not." CreationDate="2019-02-13T13:53:59.887" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11816" PostId="8587" Score="0" Text="As you have formulated your problem it is NP. (also do add whether the rectangles are axis-aligned). Considering that the problem is NP you might want to look at approximations to the actual solution. https://link.springer.com/article/10.1007%2Fs10589-009-9258-1" CreationDate="2019-02-13T14:06:56.237" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11824" PostId="8589" Score="2" Text="What's vt? Also uv coordinates just tell you how to map the texture on the triangle, not hold to load textures. You can use any texture." CreationDate="2019-02-13T19:26:41.357" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11825" PostId="8579" Score="0" Text="Comments are not for extended discussion; this conversation has been [moved to chat](https://chat.stackexchange.com/rooms/89698/discussion-on-answer-by-lightxbulb-how-to-set-equivalent-pdfs-for-cosine-weighte)." CreationDate="2019-02-13T22:28:37.243" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11827" PostId="8589" Score="1" Text="What part are you having trouble with? Have you done texture mapping in OpenGL before? Do you have code loading the geometry already?" CreationDate="2019-02-14T02:15:56.953" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11830" PostId="8589" Score="0" Text="@user1118321 I have not done texture mapping in OpenGL before." CreationDate="2019-02-14T06:32:18.327" UserId="10174" ContentLicense="CC BY-SA 4.0" />
  <row Id="11831" PostId="8589" Score="0" Text="@lightxbulb an obj file contains vt - which apparently are the texture coordinates." CreationDate="2019-02-14T06:45:53.247" UserId="10174" ContentLicense="CC BY-SA 4.0" />
  <row Id="11832" PostId="8589" Score="2" Text="@Trial Have you successfully loaded the vertex co-ordinates from your file? What code do you have working now?" CreationDate="2019-02-14T09:48:55.720" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11837" PostId="8589" Score="0" Text="@DanHulme I have successfully loaded the vertex coordinates from the obj file, so I am able to load the model and render it, but not it's texture, so it comes only of a uniform color. I don't know how to get the texture." CreationDate="2019-02-14T12:26:31.463" UserId="10174" ContentLicense="CC BY-SA 4.0" />
  <row Id="11839" PostId="3976" Score="0" Text="What is *explicit light sampling* and *sky light*? https://computergraphics.stackexchange.com/questions/8596/how-to-implement-explicit-light-sampling-when-implementing-path-tracing" CreationDate="2019-02-15T10:08:15.197" UserId="7114" ContentLicense="CC BY-SA 4.0" />
  <row Id="11840" PostId="8596" Score="1" Text="Explicit light sampling aka Next Event Estimation. Basically at each bounce you shoot a ray towards light source and 1 ray elsewhere to compute direct and indirect illumination separately. Richie provided a very good answer here&#xA;https://computergraphics.stackexchange.com/questions/5152/progressive-path-tracing-with-explicit-light-sampling?noredirect=1&amp;lq=1" CreationDate="2019-02-15T13:22:07.933" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11841" PostId="2123" Score="4" Text="@MartinEnder it's not GLM who's guilty: the GLSL spec prescribes this confusion to exist: there's `length(vecN)` free function returning the norm of `vecN`, and `vecN.length()` method, returning dimension of `vecN`. See §5.5 _Vector and Scalar Components and Length_ of GLSL 4.60 spec." CreationDate="2019-02-15T16:16:34.223" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="11849" PostId="8596" Score="0" Text="@gallickgunner I didn't read the thread you linked in detail but what got my attention was that the advice on MIS is bad, if not outright wrong.&#xA;Also zwcloud: yes you can regard direct light sampling as an estimator from BDPT (in fact it is, an estimator with k bounces from the camera with a direct connection)." CreationDate="2019-02-15T19:50:55.433" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11850" PostId="8606" Score="2" Text="Try posting the relevant section where you calculate reflection. Are you using Monte-Carlo methods to simulate GI as well? The grey sphere is getting a greenish tint from the floor. Seems like color bleeding effect due to GI but simple raytracers don't do that. It could be that the reflection function is working partially." CreationDate="2019-02-16T13:40:43.293" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11851" PostId="8606" Score="0" Text="You'll have to post the relevant parts of your code. Shirley's code is available on github, and it works correctly if you compile it, thus even if you say that the code is the same, it most certainly is not." CreationDate="2019-02-16T15:09:23.743" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11852" PostId="8606" Score="2" Text="Note that it's not just the metal reflection that's wrong in your image, it's obvious that the middle (lambertian) sphere is also wrong." CreationDate="2019-02-16T17:34:14.277" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11853" PostId="8606" Score="0" Text="It seems like the normals on your spheres are wrong." CreationDate="2019-02-17T01:31:50.447" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="11854" PostId="8610" Score="0" Text="Ah, that makes more sense. Thanks for the quick answer :)" CreationDate="2019-02-18T00:00:48.247" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11855" PostId="8609" Score="1" Text="It's been a long while I've done any image processing but f I understand correctly you are talking about the convolution operation applied to images with a kernel matrix? In that case the kernel is applied on every pixel. Your local workgroup size is (kernel width, kernel height,1). If you apply the operation to every pixel in this group, then it'd equal to just looping every pixel in the entire image and applying the kernel. Why would there be aliasing?" CreationDate="2019-02-18T04:09:54.683" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11856" PostId="8609" Score="0" Text="I'm thinking because no information is shared between groups, so pixels at the edges of a group will catch much more influence from pixels on one side than from pixels on the other. Shouldn't that bias the image somehow? Aliasing maybe isn't the right word" CreationDate="2019-02-18T04:17:19.713" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11857" PostId="8609" Score="2" Text="Ah if you are thinking it like that sure. I had a different picture in mind. You don't necessarily need to share information. If you pass the image as image2D you could just calculate the indices for the pixels required by the edge pixel in a group. Although half of those will lie in the other groups, you can still read from the image at those pixels assuming you are not reading and writing from the same image. You can't anyway i think there should be 2 images, one for reading and other for the filtered one." CreationDate="2019-02-18T04:28:43.867" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11858" PostId="8609" Score="0" Text="Mm, either two images or some careful memory barriers with separated texel collection/convolution blocks :). Admitting texels outside the kernel works, but it also means sampling a few more texels...I wonder if it's possible to modify convolution so the edges are naturally accounted for without any extra samples." CreationDate="2019-02-18T05:58:02.400" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11859" PostId="8609" Score="0" Text="@PaulFerris Convolution becomes multiplication in the Fourier domain." CreationDate="2019-02-18T11:46:45.167" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11865" PostId="8609" Score="0" Text="@lightxbulb which means aliasing doesn't occur because the multiplications can be applied discretely?" CreationDate="2019-02-18T19:03:55.490" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11866" PostId="8609" Score="1" Text="@PaulFerris I have an issue with your definition of aliasing (it means something entirely different in image processing than what you have in mind). As far as the fourier domain goes, it will let you convolve your image efficiently with large filters by simply multiplying two fourier transformed images. There's a tradeoff obviously of going to the fourier domain and back. This can be done through FFT though which is $O(n\log n)$. So it really depends on your kernel size whether it's better/more efficient." CreationDate="2019-02-18T19:10:41.710" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11867" PostId="8609" Score="0" Text="That's interesting, thanks. I'm using small kernels (up to 5x5), do you think the Fourier domain would be efficient at that scale?" CreationDate="2019-02-18T19:35:46.883" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11868" PostId="8609" Score="2" Text="@PaulFerris Not really, 5x5 is small enough for you to do it directly, just use a texture for input and a texture for output. The other thing that you can look into (for more specialized cases) is summed area tables/integral images, which let you perform arbitrary region box filtering in constant time. There's generalizations to other filters foo, but it does require preprocessing of the image (can be done in parallel though)." CreationDate="2019-02-18T20:00:35.557" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11869" PostId="8609" Score="0" Text="mmm...I feel like preprocessing will wipe out any savings from using the tables (this is for a game engine, so there could be a different image every frame)." CreationDate="2019-02-18T23:29:51.667" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11870" PostId="8609" Score="0" Text="I'll definitely consider them if blurring edge texels in like gallickgunnner suggested is too expensive, though ^_^. Would it be reasonable to interpolate between kernel regions after applying the filter? Given e.g. two patches you'd weight each separately, then compute the overall filter density for each and interpolate pixels towards one or the other depending on their position in the image." CreationDate="2019-02-18T23:46:31.653" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11871" PostId="8609" Score="1" Text="@PaulFerris I don't think it would make sense for what you want to do. Just do it normally, I don't think you can get rid of the fact that for each pixel you have to sample a different subset of the image. Just implement it and profile." CreationDate="2019-02-19T10:05:14.860" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11874" PostId="8609" Score="0" Text="Right, will do; I'm almost definitely over-optimizing anyway. Thanks for the help :)" CreationDate="2019-02-19T20:05:09.060" UserId="7868" ContentLicense="CC BY-SA 4.0" />
  <row Id="11876" PostId="8444" Score="0" Text="I need to update this. Should I edit out the previous stuff or leave it as it is so people can follow the changes?" CreationDate="2019-02-20T09:46:20.497" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11877" PostId="8612" Score="0" Text="output.positionSV = mul(input.position, worldMatrix).xyww;  &lt;&lt; Should this not be xyzw ?" CreationDate="2019-02-20T12:23:13.167" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11878" PostId="8612" Score="0" Text="Unfortunatelly it isn't helpful. I believe that xyww component is needed because we are forcing 'z position' to be equal to 1 to make sure that object is a indefinetely far skybox which isn't moving." CreationDate="2019-02-20T19:14:19.473" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="11879" PostId="8612" Score="0" Text="These are 3d coordinates and I am guessing you are projecting 6 sides of a cube, so Z=1 is only true for one of the faces ? Or am I misunderstanding the vertex inputs ? One thing you could try is to output the world space coordinates of each pixel as a RGB colour (e.g. R=X, G=Y, B=Z) to test at least the 3d inputs are correct" CreationDate="2019-02-21T02:00:26.577" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11881" PostId="8622" Score="0" Text="I did wonder about that, but it seems unnecessarily computationally intensive to need to run the function twice. This is for a real-time application, so I want it to be as fast as possible ideally. But I will definitely investigate it, thanks!" CreationDate="2019-02-21T13:04:47.523" UserId="7023" ContentLicense="CC BY-SA 4.0" />
  <row Id="11882" PostId="8622" Score="0" Text="Perlin noise is already a very efficient means to obtain noise. It is not uncommon to see multiple evaluations of Perlin noise per pixel for procedural textures. The performance loss will not be severe." CreationDate="2019-02-21T13:08:15.960" UserId="7724" ContentLicense="CC BY-SA 4.0" />
  <row Id="11884" PostId="8624" Score="0" Text="It's possible, however if you have an issue with the interpolation part specifically, then you'll just need to sample the temporal domain more finely." CreationDate="2019-02-21T14:09:08.323" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11886" PostId="8629" Score="0" Text="Yes, that's my intuition. Note 1), when does this translation happen? I am shooting rays from the origin into each pixel, and at some point I 'move' my origin and shoot rays from the new origin. Does this happen only when I hit the class &quot;translation&quot;? Note 2) yes, but since I only offset when both original ray and new ray hit the object, only a small portion of the object will be moved. If the object goes from x=1 to x=4, and I move my origin from 0 to -2, the only portion of the object moved is between x=2 and x=4 (for x&lt;2 the new ray does not hit the object)" CreationDate="2019-02-22T16:11:00.223" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="11887" PostId="8629" Score="0" Text="@MauroComi I don't get what you mean by 'hit the class translation'. You don't offset only a part of the object, you offset the ray origin, which is the same as offsetting the object. Note that if you translate an object, rays that previously intersected it, will not anymore - it's the same thing. What you should take away from this is that coordinate system translation by $-\vec{t}$ and object translation by $\vec{t}$ are the same thing. If you formulate it in terms of matrices, you can consider even more general transformations." CreationDate="2019-02-22T16:42:31.590" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11888" PostId="8629" Score="0" Text="Thanks for our explanation. With &quot;hit the class translation&quot; I mean that I offset the origin inside the function `translate::hit()`, which means that this function is called when the original ray hits a Hitable object inside the class `translate`?" CreationDate="2019-02-22T17:36:52.543" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="11889" PostId="8629" Score="0" Text="@MauroComi Yeah as far as I can see it's just an interface to apply the transform before passing the transformed ray to the primitive hit function. People use a similar thing in sdf raymarching to distort the domain (they apply a nonlinear map to the basis)." CreationDate="2019-02-22T18:42:00.010" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11890" PostId="8630" Score="0" Text="You are supposed to use gamma correction if that is your question. You are free to play with $L_{white}$, and the scene materials/lights but if that doesn't yield the results you are looking for, then I would recommend trying a different operator." CreationDate="2019-02-24T12:20:36.007" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11891" PostId="8630" Score="0" Text="No I know that I'm supposed to use it, it's just the result look better without it. Doing Gamma Correction just makes it too whitish. It feels unnatural. That's what confuses me" CreationDate="2019-02-24T12:29:15.117" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11892" PostId="8630" Score="0" Text="I do not understand why it confuses you. Applying gamma obviously brightens up the image, due to $a^{\gamma}\geq a^1, a \in [0,1], \gamma \in [0,1]$, for example $\sqrt{1/100}=1/10 &gt; 1/100$. That leaves more precision for darker regions, while lighter regions get compressed, this is done in order to compensate for our imperfect monitors. If you do not apply gamma your falloff would in general look wrong (non-quadratic), and you'll have a number of other issues. If you're unhappy with the image, you should tweak materials/lighting, $L_{white}$, or just use a different operator." CreationDate="2019-02-24T13:26:11.227" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11893" PostId="8630" Score="0" Text="On an unrelated note, do use brdf sampling + NEE MIS, if you aren't already doing so, in order to reduce the fireflies." CreationDate="2019-02-24T13:28:38.640" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11894" PostId="8630" Score="0" Text="I'm actually using NEE but having some issues with MIS check my other question on it. https://computergraphics.stackexchange.com/questions/8444/multiple-importance-sampling-in-path-tracer-produces-dark-images&#xA;&#xA;I'm gonna study about Gamma in detail in the meantime, I don't understand it correctly enough I guess." CreationDate="2019-02-24T13:55:19.807" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11895" PostId="8444" Score="0" Text="Could you add a small LaTeX derivation as to how you compute your weights, and at what events you do so, and the the final estimator you use. Basically a small explanation as to when and how you do direct light sampling, and brdf sampling and then the way you combine those. I can also link you some code of mine with MIS in glsl if you want." CreationDate="2019-02-24T14:17:55.280" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11896" PostId="8444" Score="0" Text="@lightxbulb - I actually posted the link to the whole code incase you overlooked it.  Check from line 471. Didn't want to add here since it's gotten messy and long as it is. The weights are computed using power heuristic (1 line function at the end of the file). If you still want I can try posting here as well." CreationDate="2019-02-24T14:45:49.233" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11897" PostId="8444" Score="0" Text="@gallickgunner I saw that you linked the code, however, I'd rather see the theoretical explanation of what you're doing rather than go through the code. Just a high level overview is ok." CreationDate="2019-02-24T15:08:49.153" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11898" PostId="8630" Score="1" Text="I would recommend comparing your results with some real world images. I have a bright red wall next to a white wall in my room and i can tell that your non gamma corrected image LOOKS WRONG. It might look better to you but it does not look natural. It really comes down to whether you want physically accurate results or you prefer going with some other aesthetic." CreationDate="2019-02-24T15:11:31.990" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="11900" PostId="8444" Score="0" Text="Note that I have gone through your code, and I believe there are a number of mistakes. I think that if you do a small overview of the relevant parts then: 1) I'll be certain it's not me misunderstanding what your code does, 2) I'll be able to explain why theoretically what you are doing is incorrect (without talking about the implementation specifically). I just think you've misunderstood some parts of the derivation of MIS. If we can clarify which parts, then I can do a detailed writeup." CreationDate="2019-02-24T15:25:08.277" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11901" PostId="8444" Score="0" Text="@lightxbulb - Check now." CreationDate="2019-02-24T15:49:35.353" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11902" PostId="8630" Score="0" Text="Ok, supposing I use color information from an RGB color picker online to set the values for my red/green wall. Then this means I need to un-correct them first before using in my calculations right? Since the colors I'm picking are actually Gamma-corrected?" CreationDate="2019-02-24T16:04:03.727" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11903" PostId="8630" Score="1" Text="Yes, that is correct. Colors you feed into you equations need to be converted to linear first." CreationDate="2019-02-24T17:06:01.620" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11905" PostId="8631" Score="0" Text="Thanks for the detailed write-up. So in short (1) I need to compute both light and brdf samples using integral over area not solid angle. And (2) I also need to compute the BRDF PDF w.r.t to the area. I was actually doing (1) but changed it to integral over solid angle and noticed images getting brighter. Well anyways, will report back shortly." CreationDate="2019-02-24T18:33:40.167" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11906" PostId="8631" Score="0" Text="You can decide on one of the two formulations: either area or solid angle. That means that all of your pdfs (even outside of the weights) should match this formulation, as well the rendering equation formulation that you use and consequently the estimator that you will compute. The good thing is that nothing except for the pdfs in your weights needs to change. Since I think that most of your stuff is already in solid angle formulation (your brdf pdf and the estimators), then just convert the light pdf wrt solid angle (mult by $r^2/\cos\theta_y$)." CreationDate="2019-02-24T18:47:44.437" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11909" PostId="8631" Score="0" Text="Hey thanks for the answer. I think most of the problems have been resolved. Check the update images. The fireflies seem no less though or it's just that there are too many of them to make a difference. I'll try with different scene parameters. I also don't get what you said about the first ray should not use MIS. I think you meant for naive path tracing. I'm using NEE." CreationDate="2019-02-24T19:43:53.237" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11910" PostId="8631" Score="0" Text="The first ray (emanating from the camera) should not use NEE, since you will hit anything visible either way. The fact that the fireflies do not even diminish is strange (it's not impossible though), so I still have my doubts." CreationDate="2019-02-24T20:02:04.853" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11912" PostId="8631" Score="0" Text="About fireflies diminishing, I think the real reason for fireflies in my scene is because the reflective sphere shows an image of bright light source. This is very small as compared to the actual size of the light sources. Hence when the rays hitting the walls bounce, only a very few of them end up striking the light source image on the reflective sphere giving an incredibly high color in some samples. This just sounds like I am double dipping lights (1 from NEE, 2nd from reflective sphere) Sounds wrong but never seen such a case where people ignore the reflection of light in GI." CreationDate="2019-02-24T21:07:54.687" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11913" PostId="8631" Score="0" Text="I don't really understand how MIS is gonna help me reduce fireflies in this case? Since it's acutally supposed to reduce variance when shooting at small light sources and for direct illumination if I got the gist correct." CreationDate="2019-02-24T21:11:24.977" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11914" PostId="8631" Score="0" Text="MIS is not only there to help you reduce variance from sampling small light sources through brdf sampling, it works the other way around too - giving more weight when sampling a large light through a tight brdf as opposed to sampling the light by direct light sampling (which can be outside the cone of the brdf). Note that MIS can in fact also increase variance (if both estimators are bad at some point). I can't tell why your scene has so many fireflies, since the illumination seems smooth for the most part. I am assuming a phong with a power that's too high, or something's wrong." CreationDate="2019-02-24T21:59:57.103" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11915" PostId="8631" Score="0" Text="That's actually true. The reflective sphere isn't modeled as a specular surface but a glossy one with a high phong exponent, About the MIS part, I still think it wouldn't help much here since all the surfaces are diffuse except for the reflective one. And diffuse surfaces have a very generic brdf, So there won't be any noticable difference. The thing you are talking about (tight brdfs) would be in case of glossy and near mirror surfaces. But the fireflies here are appearing on diffuse surfaces." CreationDate="2019-02-24T22:05:45.687" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11916" PostId="8631" Score="0" Text="The fireflies are appearing on diffuse surfaces due to the reflection off the high exponent sphere. As you can see it is riddled with fireflies, and I am unsure whether this is just due to a mistake in the implementation." CreationDate="2019-02-24T22:21:48.073" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11917" PostId="8631" Score="0" Text="Yea that's what I said earlier. Anyways I'll try to debug it again and see If I did any sort of mistakes. Thanks for the help on MIS tho." CreationDate="2019-02-24T22:32:35.680" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11918" PostId="8593" Score="4" Text="Possible duplicate of [Gaussian blur with transparency](https://computergraphics.stackexchange.com/questions/5509/gaussian-blur-with-transparency)" CreationDate="2019-02-25T07:00:00.583" UserId="10217" ContentLicense="CC BY-SA 4.0" />
  <row Id="11920" PostId="8612" Score="1" Text="@PaulHK is right, you are discarding `z` far too early. View and projection matrix don't apply correctly due to that. Also, you have a numerical issue, `up` and `normal` are parallel for the center of two of the surfaces." CreationDate="2019-02-25T08:16:55.507" UserId="10217" ContentLicense="CC BY-SA 4.0" />
  <row Id="11921" PostId="8633" Score="0" Text="the object is a texture with opacity? if so, just use Shobel with opacity as source value" CreationDate="2019-02-25T19:23:16.840" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="11922" PostId="8633" Score="0" Text="Are many of these images the same? (Same camera angle) If the topology is roughly consistent it would be pretty easy to simply fill in the details on the latter image with some template. Wouldn't help much with the graphics on the tshirts though." CreationDate="2019-02-26T04:15:34.257" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="11923" PostId="8636" Score="1" Text="Can you try it out and profile it? Just write a loop that binds textures over and over and draws a quad with  the currently bound texture. See how long it takes on average for 1 loop. Then bind each texture to a different texture unit and run the loop again, incrementing which texture unit you're using and draw  the quads without rebinding each time and see if the average time per loop is different. I suspect the answer will be hardware, OS, and OpenGL version dependent, and that for your use case it will make no difference, but you don't know until you try it." CreationDate="2019-02-26T05:45:28.093" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11925" PostId="6115" Score="2" Text="@user1118321 While individual benchmarking experiments are a commendable practice, there certainly is more long-term value in a broader theoretical discussion and maybe resulting establishment of general practices based on how vertex pulling hardware works." CreationDate="2019-02-26T09:31:31.997" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11927" PostId="8639" Score="0" Text="&quot;*it has the brackets behind it*&quot; Do you mean in front of it?" CreationDate="2019-02-26T18:40:21.670" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11928" PostId="8639" Score="0" Text="You are right. I have it reversed for some reason. Can youbtell me" CreationDate="2019-02-26T18:42:18.700" UserId="10235" ContentLicense="CC BY-SA 4.0" />
  <row Id="11929" PostId="8641" Score="0" Text="Great answer. You look young, how are you so smart :P" CreationDate="2019-02-26T20:14:55.530" UserId="10235" ContentLicense="CC BY-SA 4.0" />
  <row Id="11931" PostId="8640" Score="0" Text="With the texture atlas approach would there be issues at tile edges were bilinear sampling in some cases pulls in texels from neighbouring tiles ?" CreationDate="2019-02-27T03:57:14.360" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11932" PostId="8640" Score="0" Text="@paulhk yes, there would be. So you need padding between the source textures, ideally extrapolating based on the desired tiling behavior (so either clamp to edge or repeat), and choose the number of mip levels such, that in the lowest level no sample comes from outside the padding." CreationDate="2019-02-27T05:38:26.243" UserId="10217" ContentLicense="CC BY-SA 4.0" />
  <row Id="11935" PostId="104" Score="0" Text="The project is open source and would really love help modernizing it. There's no other tool that does what it did." CreationDate="2019-02-27T13:50:56.877" UserId="363" ContentLicense="CC BY-SA 4.0" />
  <row Id="11936" PostId="8639" Score="1" Text="I'm voting to close this question as off-topic because this is a fundamental question about the C++ programming language entirely unrelated to the problem of computer graphics." CreationDate="2019-02-27T18:50:53.303" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11937" PostId="8639" Score="0" Text="I will delete it sorry. I didn't mean to piss anybody off, im just a newbie, the negative votes are a bit harsh :(" CreationDate="2019-02-27T18:56:50.197" UserId="10235" ContentLicense="CC BY-SA 4.0" />
  <row Id="11942" PostId="8643" Score="1" Text="Right. It was originally done by hand by artists who designed the images around the palette constraints. I'm asking about an algorithm to do it." CreationDate="2019-02-28T04:14:12.583" UserId="10243" ContentLicense="CC BY-SA 4.0" />
  <row Id="11943" PostId="8643" Score="0" Text="Ah, I see. Sorry for my confusion!" CreationDate="2019-02-28T04:14:56.473" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11944" PostId="8644" Score="0" Text="A piecewise linear curve is not smooth by standard definitions. Smoothness usually implies $C^k$ or $G^k$, $k&gt;0$. It may look smooth, when seen at a specific scale, but you can always fund a scale such that it is not. If you want true smoothness use a spline for example." CreationDate="2019-02-28T08:32:39.070" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11945" PostId="8645" Score="1" Text="If I understand the proposal, this sounds like something I was considering initially, but I wasn't sure how to do VQ on aggregate vectors as the aggregate vectors could have many permutations of the same set of colors (which would prevent finding the proper centroids). E.g. for two color aggregates (11, 21, 31, 41, 51, 61) would be the same as (41, 51, 61, 11, 21, 31)." CreationDate="2019-02-28T10:24:09.110" UserId="10243" ContentLicense="CC BY-SA 4.0" />
  <row Id="11946" PostId="8645" Score="0" Text="That's why I thought assigning the count to the &quot;tile pixel&quot; would get around the ordering problem. Of course, having  lots of &quot;similar&quot; colours in the original palette might work against this hence the uncertainty on the initial choice for *M*" CreationDate="2019-02-28T10:37:15.557" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="11947" PostId="8645" Score="0" Text="That's definitely a way of imposing a definite order. Though the fact that the distance between two points in the x-D index vector space will not necessarily be proportionate to the actual different between the colors for the two subpalettes means you could potentially end up with a centroid that has wildly different colors that the points away from the centroid." CreationDate="2019-02-28T10:57:04.293" UserId="10243" ContentLicense="CC BY-SA 4.0" />
  <row Id="11948" PostId="8645" Score="1" Text="Actually, that might not be true since we're just using the counts of each palette entry, and the actual colors for each subpalette could be recomputed once similar blocks are found." CreationDate="2019-02-28T11:06:43.637" UserId="10243" ContentLicense="CC BY-SA 4.0" />
  <row Id="11951" PostId="8644" Score="0" Text="@lightxbulb For drawing your spline will, at the end of the day, be discretized into linear pieces too, the amount of which is again a difficult decision to make. What the asker is after, though, is some kind of quality measurement for determining *how* linear is *too* linear based on human perception of the result. That sure is dependent on a lot of things, but saying &quot;use a smooth curve&quot; is only shifting away the problem." CreationDate="2019-03-01T00:51:56.277" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11953" PostId="8644" Score="0" Text="@ChristianRau There's a very big difference between a piecewise linear curve and a smooth curve even if you discretize the smooth one. The point is that  at a specific scale your piecewise linear curve will not be smooth, whereas the same does not hold for the smooth curve as long as you discretize correctly, thus his requirement is not achievable unless he specifies that he cares only about a specific scale. Considering the fact that he mentioned GIS, I assumed he cares about things like the curve remaining smooth under different scales, I just wanted to point out that it is infeasible then." CreationDate="2019-03-01T01:06:42.397" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11954" PostId="8644" Score="0" Text="I would just assume they want to know how to determine what will look smooth at a given scale. Presumably when the user zooms in or out, the scale will change and they will want to recalculate the curve." CreationDate="2019-03-01T03:54:02.453" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11955" PostId="8646" Score="1" Text="Please don't [cross post to different stack exchange sites](https://gamedev.stackexchange.com/questions/168510/show-the-edges-between-the-clipping-plane-and-clipped-objects)." CreationDate="2019-03-01T03:58:37.083" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11956" PostId="8644" Score="1" Text="Yes, I'm trying to come up with a definition of smooth for a line generalisation algorithm in GIS.  Beziers are drawn in Postscript and PDF renderers by repeatedly  dividing them up into straight line segments (using the flattenpath operator) until the maximum distance error of the approximation is small enough.  The problem with this distance-based approach is that curves with jagged fine detail can remain jagged after flattening.  Splines are more complex and GIS software tends to handle only polylines.  And splines are converted back to polyines for rendering anyway." CreationDate="2019-03-01T10:07:40.153" UserId="10244" ContentLicense="CC BY-SA 4.0" />
  <row Id="11957" PostId="8644" Score="0" Text="@JamesAshton So I take it you're using splines after all and not piecewise linear curves. In that case smoothness of the discretized curve can be achieved by simply making sure that the discretisation has subpixel precision. If you want to relax that you can use additionally curvature information to decide how densely to discretize, then you can even quantify the error of your approximation." CreationDate="2019-03-01T12:06:19.023" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11958" PostId="8644" Score="0" Text="Having a point on the curve that maps to screen coordinates $(x,y)$ find the closest intersection with the lines $x+1, x-1, y+1, y-1$, thus you will get pixel precision, you can even go as far as subpixel precision, this is obviously unnecessary where the curvature is low enough (curvature = 0 - you can do a straight line). So it's a mix of screen space length and curvature (note that curvature changes at different scales, as does length)." CreationDate="2019-03-01T12:15:33.747" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11959" PostId="8649" Score="5" Text="Unity is a real-time engine, it doesn't do ray tracing for you out of the box." CreationDate="2019-03-02T07:17:36.403" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="11960" PostId="8644" Score="1" Text="@lightxbulb&#xA;I'm still using polylines.  This is printed graphics, so very high res, up to 2540ppi.  Think printing coasts, roads, or contours with curves that are smooth to human vision.  Often the source polyline is very detailed, but GIS &quot;generalization&quot; means removing unnecessary detail with minimum inaccuracy." CreationDate="2019-03-02T08:04:23.190" UserId="10244" ContentLicense="CC BY-SA 4.0" />
  <row Id="11961" PostId="8646" Score="0" Text="As user1118321 said, don't do that. It results in information and answers being split up between different places, making it harder for people to find information, and wasting the effort of answerers. You can already see this because your question on [gamedev.se] has been clarified but this one has not. Since your question there now has more information, I'm going to close this copy." CreationDate="2019-03-02T09:57:09.857" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11962" PostId="8646" Score="0" Text="While this question is on-topic, I'm closing because it's a cross-post of a question on another site, which has received more edits and clarification on the other site." CreationDate="2019-03-02T09:57:56.920" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11963" PostId="8644" Score="0" Text="@JamesAshton So you simply want to remove vertices/edges such that you minimize the error? Take a look at this: https://www.sciencedirect.com/science/article/pii/S092577210800028X" CreationDate="2019-03-02T19:11:54.147" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11964" PostId="8648" Score="1" Text="I can't really put a finger on the moment when the christening happened, but I'm under the impression that it came about solely because it never needed a name before, but now it did, in order to be distinguished from &quot;deferred&quot;.&#xA;&#xA;I never understood, though, why something more linguistically consistent wasn't chosen instead (e.g. &quot;immediate&quot;). I guess the industry somehow aligned on &quot;forward&quot; and that was that." CreationDate="2019-03-02T22:51:44.580" UserId="2817" ContentLicense="CC BY-SA 4.0" />
  <row Id="11965" PostId="8648" Score="0" Text="I don't actually know why it's called forward shading but I would speculate it's because the shaders used to draw polygons need forward knowledge of the lighting setup ? As opposed to deferred, where lighting is integrated in a separate pass." CreationDate="2019-03-03T07:20:13.917" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="11966" PostId="8648" Score="0" Text="I'd say it's because the process is more or less linear. So in essence all you have to do is move forward in the pipeline like geometry -&gt; vertex shader ... -&gt; fragment shader -&gt; screen and you are done. Nothing could be more simpler than that xD  Deferrred got it's name from the meaning. Deferred means to put off something to a later time and that's what we do." CreationDate="2019-03-03T19:48:53.957" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11967" PostId="8648" Score="0" Text="It's my understanding that forward rendering (rasterisation) is opposed to backwards rendering (ray tracing), while deferred rendering simply is a specialisation of forward rendering. Forward as geometry is pushed out, on top of eachother (with regard to the z-buffer, of course) onto the image plane (as a pointer would), while ray tracing is projecting the light back to the image plane (as a kind of photography)." CreationDate="2019-03-04T09:31:14.400" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="11969" PostId="8648" Score="0" Text="@beyond - raytracing is also forward (light tracing) and backwards. I don't think the above terms has to do anything with raytracing/rasterization" CreationDate="2019-03-04T12:07:50.093" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11970" PostId="8648" Score="0" Text="@beyond In the rasterization world &quot;forward&quot; is definitely used as an antonym for &quot;deferred&quot;. I can't think of any other term but &quot;rasterization&quot; I've heard used to contrast rasterization with raytracing." CreationDate="2019-03-04T17:20:36.067" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="11971" PostId="8652" Score="1" Text="Do you care what happens to geometry that doesn't affect a pixel? e.g. it's degenerate or end-on to the camera" CreationDate="2019-03-04T20:49:45.580" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="11972" PostId="8652" Score="0" Text="Unless parts of my geometry happen to be outside of clip space, every fragment from geometry should end up being rendered to FBO. Geometry from different submissions may and will overlap. What I need a rectangular area within my FBO that covers pixels &quot;affected&quot; by rendering. This area may well cover whole FBO viewport or cover zero area (in case all geometry happen to miss clipspace of FBO)" CreationDate="2019-03-04T21:12:27.467" UserId="10267" ContentLicense="CC BY-SA 4.0" />
  <row Id="11973" PostId="8652" Score="0" Text="Well, what *is* available to you? Since you got FBOs I assume an old-school GPGPU ping-pong reduction with a fragment shader would be viable? In that case, either you can distinguish a &quot;rendered&quot; pixel from a &quot;background&quot; pixel in the color buffer or you use the stencil buffer and then just reduce it computing the minimum and maximum of all &quot;set&quot; fragment positions." CreationDate="2019-03-04T22:20:01.267" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="11974" PostId="8652" Score="0" Text="@lhog: &quot;*read back the minimal screen-space bounding box of all drawn geometry after the rendering to FBO is finished.*&quot; Did you pass the bounding box? If not, how do you expect to read back something you didn't provide?" CreationDate="2019-03-05T01:59:34.643" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11975" PostId="8655" Score="7" Text="It's unclear exactly what you're referring to here. You conflate lighting models (Blinn-Phong vs. more capable &quot;BRDF approximations&quot;) with rendering algorithms (ray tracing) and presumably anti-aliasing (monte-carlo). What exactly are you looking for here? Also, list-questions of this sort are not a good fit for Stack Exchange sites." CreationDate="2019-03-05T19:36:42.790" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="11976" PostId="8652" Score="0" Text="@NicolBolas I am assuming what he means here is the minimal rectangle that contains all rasterized pixels. The obvious algorithm is to iterate over all pixels and find the rasterized ones with minimum x, minimum y, maximum x, maximum y. The thing is that he wants something efficient. The only thing I can think of is doing a vertical and horizontal pass to compute those (which can be parallelized), or alternatively use a divide and conquer approach (once again parallelized) which will yield $O(n\log n)$." CreationDate="2019-03-05T20:01:09.013" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11977" PostId="8652" Score="0" Text="Yes @lightxbulb got it correct. I don't know beforehand what part of my FBO the rendered geometry will take, so I need to obtain bounding rectangle containing all pixels that have been changed/rendered to by geometry draw call afterwards. glReadPixels() is used for this currently, but it's rather slow and I was hoping to catch some trick to get minmax rect from GPU." CreationDate="2019-03-05T20:20:08.577" UserId="10267" ContentLicense="CC BY-SA 4.0" />
  <row Id="11978" PostId="8652" Score="0" Text="@lhog I don't know about a trick from the gpu. The best I can think of is using the stencil buffer and using a procedure similar to the ones used to build summed-area tables on the gpu (if you look for fast SAT building you'll find those), but not with addition but rather with min/max." CreationDate="2019-03-05T21:41:22.290" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11979" PostId="8652" Score="1" Text="Please don't [cross-post](https://gamedev.stackexchange.com/questions/168646/convey-screenspace-bounding-box-of-rendering-result) questions to multiple Stack Exchange sites." CreationDate="2019-03-06T04:31:11.067" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="11980" PostId="8654" Score="0" Text="Thank you very much for your answer.Is the light probe used to collect the indirect illumination of objects around a point, but the surrounding objects need information on the light probe to calculate the illumination, is this not contradictory?Or I understand it wrong？" CreationDate="2019-03-06T04:58:05.170" UserId="10269" ContentLicense="CC BY-SA 4.0" />
  <row Id="11981" PostId="8624" Score="0" Text="I'm quite sure you have seen full CG characters in some movies especially vfx heavy ones, without even realizing that the creatures are CG. In most cases you only recognize the bad ones. And very often your reception is biased if you know that the creatures are not real, e.g. the Hulk, or Arnold in Terminator Genisys." CreationDate="2019-03-06T17:32:13.190" UserId="8219" ContentLicense="CC BY-SA 4.0" />
  <row Id="11982" PostId="8654" Score="0" Text="@jacksparowll the light probe data is gathered from information generated with some other offline lighting technique (such as ray tracing). You then use that light probe to fake the lighting effects of that technique on a scene that uses cheaper real time graphics." CreationDate="2019-03-07T03:04:29.797" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="11983" PostId="8658" Score="0" Text="Shouldn't the return value be a float3? Also, why do you assume fresnel to be constant 1? That would only be true for an incident angle of $90°$. Have a look at Natty Hoffman's Siggraph Physically Based Shading Course, he displays some Fresnel values plotted for different angles (p. 15) https://blog.selfshadow.com/publications/s2013-shading-course/hoffman/s2013_pbs_physics_math_notes.pdf" CreationDate="2019-03-07T07:44:29.337" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="11984" PostId="8658" Score="1" Text="You are confusing brdf with probabilities.&#xA;https://gamedev.stackexchange.com/questions/62438/ggx-specular-brdf-is-way-over-1&#xA;&#xA;https://www.gamedev.net/forums/topic/257668-brdf-range----greater-than-01-/" CreationDate="2019-03-07T10:59:04.963" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="11986" PostId="8658" Score="1" Text="A brdf can be larger than $1$ and be energy conserving. Consider the ideal specular reflection/refraction brdf which can be considered to go to infinity at a single point. It's the integral of the brdf multiplied by the cosine for all directions that needs to be $\leq 1$." CreationDate="2019-03-07T12:30:15.377" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11987" PostId="8655" Score="0" Text="While the question is not very precise I don't think the right way to go about it is to downvote everyone. That's directed to whoever's having a field day with the downvote button." CreationDate="2019-03-07T14:55:22.037" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11988" PostId="8658" Score="0" Text="Thank you very much. I understand now. If I do importance sampling on brdf, then my brdf/pdf can rescale my value back to [0,1] right? But if I do importance sampling on the light source, my brdf/pdf can still has some value greater than 1 because the pdf is the pdf of sampling my light source, not the pdf of sampling brdf. Am I correct?" CreationDate="2019-03-07T15:07:28.020" UserId="9987" ContentLicense="CC BY-SA 4.0" />
  <row Id="11991" PostId="8661" Score="0" Text="What do you refer to as vertices? Triangle vertices? So you are using a triangle mesh to draw this? As opposed to simply a texture? Can't you just generate a suitable 2d texture that you apply on a single quad? By this I mean a texture which has this circle with the radials in the middle. If you do this your problem with reduce to simply generating the correct texture fro your data (you do something similar currently from what I gather anyways but very inefficiently)." CreationDate="2019-03-07T16:41:25.013" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11992" PostId="8661" Score="0" Text="One approach generates a mesh for each radial (a triangle strip), where each range gate is described by a quad (really, two triangles). The second approach attempts to describe range gates by mapping a 1d texture onto each radial (a single quad)." CreationDate="2019-03-07T16:52:50.557" UserId="10286" ContentLicense="CC BY-SA 4.0" />
  <row Id="11993" PostId="8661" Score="0" Text="Yes but why? Both of these generate a lot of triangles. Why not just make a quad and map a texture on it? Surely that will be more efficient. All you would have to do is to generate that texture. The second variant is not so terrible though, even though you generate a bunch of triangles. It really comes down to what your requirements are (in terms of resolution/zooming) and what your initial data looks like. I assume your initial data is resolution limited. What would be that initial resolution per radial and how many radials do you have." CreationDate="2019-03-07T16:57:07.200" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11994" PostId="8661" Score="0" Text="The problem is that given the data we're expecting, there isn't really a trivial way to generate the circular texture you described in real time. As data arrives, it will need to be mapped to a radial at some arbitrary angle. That's the crux of the problem." CreationDate="2019-03-07T17:03:22.070" UserId="10286" ContentLicense="CC BY-SA 4.0" />
  <row Id="11995" PostId="8661" Score="0" Text="Join the chatroom if you want so we can clarify in real-time what you mean: https://chat.stackexchange.com/rooms/90733/trying-to-optimize-texture-mapping" CreationDate="2019-03-07T17:07:04.547" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11996" PostId="8661" Score="0" Text="I can't use the chat room, I don't have enough reputation." CreationDate="2019-03-07T17:15:25.620" UserId="10286" ContentLicense="CC BY-SA 4.0" />
  <row Id="11997" PostId="8661" Score="0" Text="Oh ok. Then we'll have to go back and forth over in the comments. From what you clarified last, it seems that your data arrives in the form of radial segments with some resolution say $w$ and you have a maximum of $h$ such radial segments (the last two are my assumptions), is that correct? And you're asserting that it is not efficient enough. How many such segments do you have? Basically how large can $h$ grow? Also how large can $w$ grow? And additionally at what resolution are you supposed to present the result?" CreationDate="2019-03-07T17:19:36.807" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="11999" PostId="8661" Score="0" Text="I tried giving you write access to the chat room but I couldn't since: &quot;Please note that you can only add users that have already visited chat.stackexchange.com.&quot; Try visiting chat.stackexchange.com" CreationDate="2019-03-07T17:32:33.607" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12000" PostId="8661" Score="0" Text="Unfortunately I still can't use the chat. [1]: Data arrives as a 1D array of intensity values, 1 for each range gate within a radial. These are colored via a colormap. [2]  We can't make many assumptions about how big h can grow; we want to maximize the # of draw-able radials while maintaining adequate performance. Currently this limit is defined by the size of the vertex buffer, which can store a maximum of 100,000 vertices. [3]  The plot must render with a varying resolution, as it's part of a map which can be panned and zoomed. (just visited chat.stackexchange.com)" CreationDate="2019-03-07T17:36:12.687" UserId="10286" ContentLicense="CC BY-SA 4.0" />
  <row Id="12001" PostId="8661" Score="0" Text="Still can't add you for some reason. You say you can have $h$ grow arbitrarily large, does that mean that you remake your mesh every time? Since you triangles will need to form a fan with equidistant vertices, so a new triangle will have to change the angular distance between vertices on the circle, no? You could render more vertices with more draw calls but that seems inefficient. Maybe you can do a compute shader that directly samples your data which you feed in as a texture? That is, you feed in a texture where each row represents a radial (possibly empty such are simply $0$)." CreationDate="2019-03-07T17:47:30.967" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12002" PostId="8661" Score="0" Text="And then transform each pixel position on the unit disk onto the unit square in order to sample that texture. Note that you can even do some fancy (non-hardware) filtering this way." CreationDate="2019-03-07T17:48:23.783" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12003" PostId="8662" Score="1" Text="It is not uncommon to simulate each braid with the same physical model of a single strand that is used as a skeleton for a hair model wich could maybe be made up of splines or not" CreationDate="2019-03-07T18:07:59.497" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="12004" PostId="8662" Score="0" Text="I am not asking about the animation/simulation, only about the modelling/mesh" CreationDate="2019-03-07T18:21:33.497" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="12005" PostId="8661" Score="0" Text="Here's an example of what I mean: https://www.shadertoy.com/view/3dfXDB&#xA;Basically you can send in your radials as the rows of your texture. If some row is still empty (you still haven't computed the radial) you can for example set it to the background color. Use W/S for zoom, and mouse click to see the original texture used for this." CreationDate="2019-03-07T18:59:17.063" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12006" PostId="8661" Score="0" Text="Thanks for the demo; another requirement that complicates things (should have mentioned this in the question) is that each radial can have an arbitrary width. This is another reason we're generating the graphics using a mesh." CreationDate="2019-03-07T19:18:07.150" UserId="10286" ContentLicense="CC BY-SA 4.0" />
  <row Id="12008" PostId="8661" Score="0" Text="As long as you're happy with texel precision, you can make the width arbitrary by repeating a row multiple times. Or you can use a look up table for the conversion of coordinates. In the shadertoy demo I use simply $\theta=atan(y,x)$, but you can have a mapping that shrinks/stretches space based on an arbitrary function, then you can have arbitrary width. I update the example to have a monotonically increasing function applied at the end so that the range is stretched based on it. Note that you can use a discrete lookup table for this to have an arbitrary width." CreationDate="2019-03-07T19:27:52.437" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12009" PostId="8661" Score="0" Text="Your function to stretch/squeeze the range would be a monotonically non-decreasing function $f:[0,1]\rightarrow [0,1]$ such that $f(0) = 0, f(1) = 1$. Then you can construct $f$ to work similarly as what you were doing already." CreationDate="2019-03-07T19:30:46.010" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12010" PostId="8661" Score="0" Text="As an example if I had two rows (two radials) the original mapping would map them to both hemicircles, so that the first row would correspond to $[0,\pi]$ and the second to $[\pi,2\pi]$. Now if you want the first to be only in $[0,a]$ and the second to be only in $[a,2\pi]$ all you have to do is construct a piecewise linear function mapping $[0,\pi]$ to $[0,a]$ and $[\pi,2\pi]$ to $[a,2\pi]$. Note that the function can be more complex, but it needs to be non-decreasing and $f(0)=0,f(1)=1$. For more points the function would obviously have more 'pieces' but the idea is the same." CreationDate="2019-03-07T19:35:47.730" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12011" PostId="8662" Score="0" Text="What level of fidelity are we talking about here? Are we talking about indie-game, AAA game, CG animated series, or feature film?" CreationDate="2019-03-08T14:47:24.683" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12012" PostId="8662" Score="0" Text="Static mesh on a mobile game" CreationDate="2019-03-08T16:27:05.867" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="12013" PostId="8662" Score="0" Text="For a static mesh on a mobile game, you could model each braid as a cylinder with appropriate bump mapping, and maybe parallax mapping, depending on how close the camera can get to it. Would that be sufficient?" CreationDate="2019-03-09T04:07:03.357" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12014" PostId="8624" Score="0" Text="@haggikrey Cats are real, but the end credits scene where a cat from that recent movie coughs out that cube thing still looks fake. It's especially noticeable if the creatures *are* real, in fact, because you're more familiar with real things than something completely made up like say giant alien robots. The only CG I never noticed was CG of inanimate things, like in The Wolf of Wall Street." CreationDate="2019-03-09T06:09:47.423" UserId="10212" ContentLicense="CC BY-SA 4.0" />
  <row Id="12016" PostId="8548" Score="0" Text="Definitely would like to see a code sample" CreationDate="2019-03-09T00:52:19.703" UserId="10291" ContentLicense="CC BY-SA 4.0" />
  <row Id="12018" PostId="8666" Score="0" Text="Are you talking about refraction or total internal reflection?" CreationDate="2019-03-09T13:06:39.987" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12019" PostId="8661" Score="0" Text="Any updates on this?" CreationDate="2019-03-09T13:12:40.900" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12020" PostId="8662" Score="0" Text="If i do  cylinder however, would I not end up using a really large amount of triangles for the overall head?&#xA;&#xA;My main issue is I need to send the model over a network and this needs to run in a broad number of devices, so cheap and drity that minimizes triangle use trumps quality." CreationDate="2019-03-09T16:49:47.893" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="12021" PostId="8666" Score="2" Text="A little more info would be useful. Is there a picture online of the effect you are after?" CreationDate="2019-03-11T09:15:49.140" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12022" PostId="8664" Score="1" Text="How is g_lightViewMatrix &amp; g_lightProjectionMatrix calculated ? I would guess it has a inverse view-projection matrix to convert from camera to world space in there too ?" CreationDate="2019-03-13T08:25:52.553" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12030" PostId="8672" Score="2" Text="Is there some reason why you can't simply learn what that &quot;bloated&quot; &quot;opengl code&quot; is doing? Learning Vulkan without any graphics knowledge is a Herculean task; better to start with an API that has training wheels." CreationDate="2019-03-14T13:53:21.523" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12031" PostId="8677" Score="0" Text="Without matrix form just rotate the points 45 degrees around the y axis. Then divide x and y by z and get rid of the z if you want perspective. How much you divide by z controls the fov (but is not formally the fov)." CreationDate="2019-03-16T11:32:22.873" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="12032" PostId="8657" Score="0" Text="Could you summarise what you found there? Links like this have a tendency to just die, and then your answer becomes completely useless. Also people don't generally like to download a PDF with no idea of the contents." CreationDate="2019-03-18T08:25:25.427" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12035" PostId="8681" Score="0" Text="For Question 1 you should try reading up on Weber-Fechner Law and Just Noticeable Difference. In short Afaik there is no set value for that. It depends on our perception. and we might be more sensitive to intensity shifts in some colors more than others. As for (2) I don't understand, 1 unit change in integer equals 0.004 change in the floating point. So whatever you do the change is same. If you are trying to ask how big of a change would be detectable then again that depends on the perception I guess." CreationDate="2019-03-18T14:17:14.597" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12036" PostId="8681" Score="0" Text="Also found an interesting site https://www.strollswithmydog.com/just-noticeable-difference-color/" CreationDate="2019-03-18T14:19:43.733" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12037" PostId="8681" Score="0" Text="Why only 255^3 and not 256^3 combinations?" CreationDate="2019-03-18T16:03:51.523" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12038" PostId="8681" Score="0" Text="RE your last paragraph - are you aware of the differences between non-linear, e.g. sRGB encoded (https://en.wikipedia.org/wiki/SRGB),  and linear colour? It might make a difference to your fade-out choices" CreationDate="2019-03-18T16:07:48.740" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12052" PostId="8683" Score="0" Text="I see. What I meant by ends up as floating point was that's how it ends up in the shader. But what you're saying is two things basically as I understand it. The framebuffer, when written to, stores values as integers in most cases anyway, so any extra precision from a 0-1 floating point in the shader is rounded off, but in any case at the end of the day your display (screen) displays intensities based on integer values anyway, correct? Edit: what I mean by integer values is discrete values of say 2^8 or 2^10 if it's 10 bit, as you said." CreationDate="2019-03-19T01:40:40.540" UserId="10333" ContentLicense="CC BY-SA 4.0" />
  <row Id="12057" PostId="8684" Score="1" Text="I notice in Nathan's version there is a linear-&gt;sRGB conversion which may account for his version looking brighter &amp; flatter. I would suggest looking at stratifying the samples too  to lower graininess, unless your sampler.Random() already does that ?" CreationDate="2019-03-19T06:08:51.940" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12058" PostId="8683" Score="0" Text="@Zebrafish That's exactly right." CreationDate="2019-03-19T08:25:12.927" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12061" PostId="8681" Score="0" Text="@SimonF Yes, I do, well sorta. Many times I've understood it and many times I've forgotten exactly which way it goes. I know that the gist behind it is that that our eyes perceive intensity in a non linear way, so twice the physical intensity isn't a twofold perception in brightness. So I think the point is to spread out the color space so more values are available within the lower end of the intensity (or was it higher). I definitely messed up when I loaded a normal map in sRGB instead of linear, so instead of the normal pointing straight out, 128, 128, 255, it went in a wrong direction." CreationDate="2019-03-19T09:52:16.367" UserId="10333" ContentLicense="CC BY-SA 4.0" />
  <row Id="12062" PostId="8684" Score="0" Text="Yes I have tried stratified samples too which improves it but nothing like Nathan's. Also I wonder the second formula in Wiki page should be modified to &quot;a**2&quot; in the numerator instead of &quot;a&quot;?" CreationDate="2019-03-19T10:10:28.180" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12063" PostId="8684" Score="1" Text="Ideally a minimum of 256 samples should be used so that it can converge on the full 0-255 RGB output range accurately. As most of your samples are going to be either black or white in your input image, 16 average samples are going to give you a none-smooth / quantised range. Although your filter is going to negate most of that, I didn't study too closely how your filter is working. I still think the linear-&gt;sRGB is responsible for most of Nathans 'flatness'" CreationDate="2019-03-19T10:19:02.987" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12064" PostId="8684" Score="0" Text="Thank Paul. Could explain a bit more how  linear-&gt;sRGB works? Do you refer to tone-mapping?" CreationDate="2019-03-19T13:59:14.527" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12065" PostId="8684" Score="2" Text="BTW, the factor $a/\pi^2$ in the weights is not needed because the final image is normalized by the sum of weights anyway, so any constant factor will cancel out." CreationDate="2019-03-19T16:31:24.770" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="12066" PostId="8684" Score="2" Text="Re: sRGB, no it's not tone-mapping, it's a conventional nonlinear encoding of pixel values in images to obtain better precision in the dark values where our eyes are more sensitive. You can read here for more: https://en.wikipedia.org/wiki/SRGB#The_sRGB_transfer_function_(%22gamma%22)" CreationDate="2019-03-19T16:36:59.303" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="12067" PostId="8684" Score="0" Text="Paul, your guess was correct. After applying the conversion to sRGB it soomths out the image. Thanks." CreationDate="2019-03-19T22:30:44.710" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12068" PostId="8684" Score="0" Text="Thanks Nathan for having look at this." CreationDate="2019-03-19T22:31:25.147" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12069" PostId="8683" Score="0" Text="There *are* high dynamic range monitors that will use more than 24 bits per pixel, but it is debatable how much that is really needed." CreationDate="2019-03-20T15:55:05.453" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="12070" PostId="8683" Score="0" Text="@ratchetfreak Well, you can see more than 256 shades of grey, so it's needed if you want to display every visible colour. Also, if your monitor actually has more dynamic range, it needs more precision too; e.g. imagine only having 256 brightness values between &quot;no light at all&quot; and &quot;as bright as the sun&quot;" CreationDate="2019-03-20T16:50:39.487" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12071" PostId="8685" Score="2" Text="You can't cheat the sampling theorem unfortunately." CreationDate="2019-03-20T23:48:49.620" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12072" PostId="8685" Score="0" Text="If you can integrate it analytically on the other hand, then your issue is solved at the price of perf." CreationDate="2019-03-20T23:57:34.833" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12073" PostId="8685" Score="1" Text="Your statement *&quot;A slightly better technique is to, in addidition to mipmapping and anisotropic filtering, multisample a disk around the uv coordinate and look for white texels. If the result is white we discard the previous sample in favour of the white texels we just got.&quot;*  is curious. Are you taking gamma into account?  You can't correctly average black &amp; white in sRGB space as &quot;128&quot; is *not* &quot;half as bright&quot;.  That, *IIRC*, is actually around 186 in sRGB.  You thus really need to do your filtering in linear colour space." CreationDate="2019-03-21T09:16:05.037" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12074" PostId="8679" Score="0" Text="Thank you for your answer. Two comments: the way rays are scattered changes when he makes a mixture pdf in chapter 8, which I haven't implemented yet, but before that (when we are playing around with different pdf) this doesn't seem to change. Second, so the contribution of each ray changes both 1) for the sampling, which makes rays with lower theta more likely to be sampled, 2) and for the multiplication term which depends on the probability. Is this correct?" CreationDate="2019-03-21T10:32:44.743" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12075" PostId="8679" Score="0" Text="@MauroComi If you're talking about chapter 7 only (which you should have mentioned explicitly) then he doesn't do brdf sampling at all, as obvious by the code and the images. You can see that he returns the contribution after only one bounce. In chapter 8 brdf and light sampling are combined. The contribution changes since you're not sampling uniformly anymore - I have derived why this is necessary in my answer above, and in this case - yes, directions closer to the normal are more likely to be sampled, the pdf weight 'compensates' for this. I don't get what mult term are you talking about." CreationDate="2019-03-21T14:36:30.120" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12076" PostId="8614" Score="0" Text="Can you put the examples in the answer itself, rather than as video links?" CreationDate="2019-03-21T15:36:28.223" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12081" PostId="8691" Score="1" Text="Scaling is multiplicative, translation is additive. Also are you translating along the geometric or shading normal? You could also check the precision of the fp numbers you use and the precision of the operations. Finally, fixed point, or even rational numbers are a thing: https://www.iquilezles.org/www/articles/floatingbar/floatingbar.htm&#xA;Do you keep your ray and normal normalized?" CreationDate="2019-03-22T15:09:56.550" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12083" PostId="8692" Score="0" Text="In my case i'm not using adjacent pixels, but i do a lot texture sampling, im curious if a texture sample is more expensive than just access the float array." CreationDate="2019-03-22T22:06:06.277" UserId="10332" ContentLicense="CC BY-SA 4.0" />
  <row Id="12084" PostId="8692" Score="0" Text="If you’re using texture filtering—i.e. reading from somewhere other than the center of any given pixel—then using the actual texture-sampling hardware for the interpolation will be faster than reading individual values and interpolating them yourself. Otherwise, I’d use an array." CreationDate="2019-03-23T00:00:37.257" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="12085" PostId="8691" Score="1" Text="How about raytracing in model space coordinates ? There will still be some inprecision when converting the ray from world-&gt;model but the triangle intersections will be done in a higher precision coordinate space." CreationDate="2019-03-25T02:45:07.470" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12086" PostId="8691" Score="0" Text="Looking at your tank/gbuffer picture, are you using 2 sided polygons or is front-face culling used ?" CreationDate="2019-03-25T02:46:38.453" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12087" PostId="8693" Score="0" Text="Does the renderer use NEE aka Next Event Estimation or Explicit Direct Light Sampling? Because from the looks of it, it doesn't. And that's prolly the biggest reason. If you use NEE, you don't have to worry about the probability of hitting a light source based on the BRDF. What you heard might possibly be true for NEE renderers. Since I can't think of any way a NEE implemented renderer will produce fireflies in a completely diffuse scene" CreationDate="2019-03-25T09:41:33.333" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12088" PostId="8694" Score="0" Text="Why don't you use something like a frustrum? https://threejs.org/docs/#api/en/math/Frustum" CreationDate="2019-03-25T14:45:18.323" UserId="7724" ContentLicense="CC BY-SA 4.0" />
  <row Id="12089" PostId="8693" Score="0" Text="@gallickgunner Hi thanks for the comment. Yes I spent lot time reading some materials yesterday and might have figured out some thing.. But first could you explain what is the difference between NEE and Explicit Direct Light Sampling? Because from my understanding I think NEE is just doing DI explicitly, instead of waiting for the renderer to find a hit on emitters.." CreationDate="2019-03-25T16:13:47.327" UserId="10365" ContentLicense="CC BY-SA 4.0" />
  <row Id="12090" PostId="8693" Score="0" Text="Yeah you understood correctly. Both are the same thing." CreationDate="2019-03-25T16:16:11.663" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12091" PostId="8693" Score="0" Text="@gallickgunner What I learned is that, for diffuse surfaces, we only consider BRDF sampling in intermediate vertices, and do light sampling at the last path vertex. If we use NEE, even if we use a point light, the pdf of light sampling would be 1.0, therefore, no small pdf would occur along the entire path and thus there will not be extremely highlight (fireflies). So I summarize a bit: fireflies will only appear when there are glossy materials. (please correct me if wrong!)" CreationDate="2019-03-25T16:20:34.283" UserId="10365" ContentLicense="CC BY-SA 4.0" />
  <row Id="12092" PostId="8693" Score="0" Text="The pdf would be zero for any direction choosen randomly. It's 1 for the direction that is specifically shot at the point light source. And yes afaik, there won't be any fireflies in a diffuse scene if you use NEE." CreationDate="2019-03-25T17:31:14.697" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12093" PostId="8693" Score="0" Text="Can you post an image to illustrate what you mean and some code/pseudo-code for the relevant part to sampling/scattering?" CreationDate="2019-03-25T20:42:59.777" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12098" PostId="1537" Score="0" Text="I think this answer isn't upvoted as highly because it's too technical for us newbies.  Maybe a simple example with simple wording would illustrate the principles better" CreationDate="2019-03-26T19:20:22.190" UserId="10376" ContentLicense="CC BY-SA 4.0" />
  <row Id="12100" PostId="8691" Score="0" Text="@lightxbulb The scaling vs translations makes sense! We are using the shading normals, we don't have geometric ones handly available so this prototype used the shading normals, though I doubt that would have caused all of the issues because for example in the [scaling output](https://i.stack.imgur.com/pGGSN.png) the top planes of the tank are intersecting even though the normal map has barely any detail in that area. What do you mean with the precision of the FP, how do I check the specific cases. And we should be using normalized normals indeed." CreationDate="2019-03-27T13:53:39.220" UserId="10354" ContentLicense="CC BY-SA 4.0" />
  <row Id="12101" PostId="8691" Score="0" Text="@PaulHK Thanks for the suggestion but I doubt moving the intersections to model space will work, the reason is two-fold. Firstly the raytracing is largely abstracted by DXR which uses pre-calculated acceleration structures containing all geometry in world space we can't simply take matters in our own hands and do the AO in model space due to this. Secondly, it sounds like moving to model space will rule out occlusion between models in which case we would be better off using pre-calculated AO maps and use the benefit of raytracing. We are using backface culling but struggle to see the relation." CreationDate="2019-03-27T14:02:09.190" UserId="10354" ContentLicense="CC BY-SA 4.0" />
  <row Id="12102" PostId="8691" Score="0" Text="The code you're using is for geometric normals though. Try to use the geometric normals just to check whether there's a difference. By precision of FP (floating point) numbers I mean the precision that the hardware you use supports. For example on some mobile devices no 32bits is supported or it requires an explicit ``precision highp float``. Another idea I just had is offseting by the scaled normal - this way if your models are at a similar scale originally and you rescale, you'll get a rescaled offset too." CreationDate="2019-03-27T14:31:08.250" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12103" PostId="8700" Score="0" Text="Thanks! The vulkan spec doesn't clearly state the std140 rules and i must have missed that. Is this better  documented in the opengl spec instead? Or doea glsl have its own spec?" CreationDate="2019-03-27T16:01:50.697" UserId="6331" ContentLicense="CC BY-SA 4.0" />
  <row Id="12104" PostId="8700" Score="1" Text="@Temp4890: &quot;*The vulkan spec doesn't clearly state the std140 rules*&quot; It isn't supposed to. `std140` layout is not a part of Vulkan or SPIR-V; it's a part of *OpenGL and GLSL*. Vulkan *requires* that the SPIR-V it consumes provides explicit byte offsets and array/matrix strides; it doesn't allow you to use `std140` layout. It is the GLSL-to-SPIR-V compiler that creates these offsets&amp;strides by applying the OpenGL `std140` rules. It's GLSL/OpenGL that decides what `std140` means." CreationDate="2019-03-27T16:05:35.353" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12105" PostId="8700" Score="0" Text="@Temp4890: That being said, the [Vulkan specification section on layout](https://www.khronos.org/registry/vulkan/specs/1.1/html/chap14.html#interfaces-resources) does make it clear that UBOs require that the ArrayStride must be a multiple of the size of the extended alignment. And the extended alignment of an array/struct is the alignment of its member(s), rounded up to 16." CreationDate="2019-03-27T16:16:53.657" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12106" PostId="8700" Score="0" Text="Thanks for the clarification. I'll go grab myself a copy of the glsl spec" CreationDate="2019-03-28T00:41:01.283" UserId="6331" ContentLicense="CC BY-SA 4.0" />
  <row Id="12109" PostId="8701" Score="1" Text="First place I would look is at offset.xy used in textures[0].Sample, it looks like its has a high scale factor? Do you use texture wrapping or clamping when sampling the depth buffer, wrapping would explain the tiled small versions of your scene. Also note the values stored in the depth buffer are not in view space, so comparing sampleDepth to sample.z may not work as expected (did I see anothre post from you involving shadow mapping, that also makes the same assumption) , that is my understanding with GLSL, I haven't used HLSL before so I may be wrong." CreationDate="2019-03-28T03:17:51.207" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12111" PostId="8704" Score="3" Text="See [homogeneous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates)." CreationDate="2019-03-29T02:55:34.053" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="12113" PostId="8704" Score="0" Text="Also see  [What is the use of Homogeneous Divide](https://computergraphics.stackexchange.com/questions/6271/what-is-the-use-of-homogenous-divide/6275#6275)" CreationDate="2019-03-29T10:51:28.673" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12114" PostId="8706" Score="0" Text="`p` is gonna be zero for light sources. Light sources are  defined by emission variable and color is zero for them. Hence the reflectance parameter calculated previous to this line (`p`) would be zero giving us `obj.e`" CreationDate="2019-03-29T15:41:04.517" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12115" PostId="8706" Score="0" Text="Getting a bounce off a lightsource is normal - imagine that your light source actually reflects light (like in the real world). However as @gallickgunner mentioned in the specific scene that you have the light source does not reflect any light (this doesn't need to be the case however)." CreationDate="2019-03-29T19:27:35.140" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12116" PostId="8710" Score="0" Text="I gave it a quick look and I don't see any checks that account for double dipping the light source contribution. Like the answer said, since you are sampling light sources explicitly now, you either need to divide by 2 if you are double dipping or simply just ignore the contribution when you hit a light source by random sampling the hemisphere" CreationDate="2019-03-30T17:38:45.533" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12117" PostId="8710" Score="0" Text="I only add the light emission if the ray hits a light on the first bounce. Otherwise, the light emission only comes from the direct lighting part." CreationDate="2019-03-30T18:07:15.730" UserId="8925" ContentLicense="CC BY-SA 4.0" />
  <row Id="12118" PostId="8710" Score="0" Text="okay, btw did you took both images at the same samples? cuz then it wouldn't be a fair comparison. You said something about 30 bounces and `bounces-1` I don't understand why `-1` The NEE version is substantially higher (in image quality) So you should be comparing something like a 4k sample naive image with a maybe 100 sample NEE image." CreationDate="2019-03-30T23:01:49.013" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12119" PostId="8706" Score="0" Text="@lightxbulb - wouldn't the contribution by the reflected light in that case be negligible unless the light's emission is very dim. I've always wondered that we could model real world light sources by an outer non-emissive shell and an inner emissive part. That way we can safely say that we stop when ever we hit a light souce or more specifically the core emissive part." CreationDate="2019-03-30T23:20:49.147" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12120" PostId="8706" Score="0" Text="@gallickgunner What does the lightsource being dim have to do with its reflectivity? On what basis do you say it's &quot;negligible&quot;? And why do you believe you can throw away &quot;negligible&quot; contributions? You can always model a light the way you said, but that's only if that model is accurate to your use case. Note that the proposed model also ruins NEE." CreationDate="2019-03-31T06:32:40.600" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12121" PostId="8708" Score="0" Text="One model is simply more expressive than the other, there's no injection from Cook Torrance to Phong though, so your last requirement cannot be satisfied unambiguously." CreationDate="2019-03-31T06:37:12.067" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12122" PostId="8706" Score="0" Text="I mean if the light source isn't dim, you aren't going to see the outer shell anyway. The emission part will be much stronger than an indirect reflection off the surface of the light source. It's gonna be &quot;negligible&quot; in the sense that it's not gonna add much to what's already there due to the emission part." CreationDate="2019-03-31T07:12:38.827" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12123" PostId="8706" Score="0" Text="@gallickgunner That's wrong. Assume the light is however strong you want (not infinitely though), now assume it perfectly reflects light from one direction only, stick a light 100x stronger than your light there, you would have reflected radiance 100x stronger than your non dim light's radiance. In general such arguments like &quot;negligible&quot; do not work unless you can define robustly what &quot;negligible&quot; is and formally prove that it is indeed &quot;negligible&quot;. My example clearly illustrates a case where the error can be arbitrarily large, thus not negligible by any informal meaning of the word." CreationDate="2019-03-31T10:32:14.543" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12124" PostId="8706" Score="0" Text="Ok but there's another problem, if the scene consists of 10 lights. Upon striking a light source we'd have to loop over all the remaining ones and gather all the contributions then. That'd be endless. I'll ask a separate question about it though" CreationDate="2019-03-31T11:05:43.810" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12125" PostId="8706" Score="0" Text="@gallickgunner It will not be endless in general, it will be endless only if your operator is not a contraction and there is no way for enough energy to escape at the same time - then yes, in that case you won't have a steady state." CreationDate="2019-03-31T15:24:40.753" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12126" PostId="8706" Score="0" Text="@gallickgunner Thank you for your answer, it's what I was looking for. If you write it in the answer, I can give you the best answer. Also, I have a question on how MC importance sampling works in smallpt, I'm going to ask it in a different question." CreationDate="2019-03-31T18:51:24.753" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12130" PostId="8712" Score="0" Text="Can you narrow down what you want to know about the rendering performance? There can be many metrics and they too will depend on so many factors in the scene (e.g. complexity of shading models)" CreationDate="2019-04-01T08:39:25.027" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12131" PostId="8716" Score="0" Text="[this](https://computergraphics.stackexchange.com/questions/7933/path-tracer-sampling-dimensions-confusion) might help," CreationDate="2019-04-01T11:21:50.093" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12132" PostId="8716" Score="0" Text="@gallickgunner It helped indeed, thanks! It is exactly what I asked. So, the theory states that we need to shoot x number of rays for each intersection, but in practice, it would be too computationally expensive." CreationDate="2019-04-01T11:41:17.983" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12133" PostId="8716" Score="0" Text="Yes try reading Cook's paper on &quot;Distributed Ray Tracing&quot; 1984. There he clearly explains that instead of adding X number of rays for every dimension (shadows, light etc) we distribute them pre hand in space. That way we don't need to shoot multiple rays as they are already distributed in space." CreationDate="2019-04-01T11:45:56.457" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12134" PostId="8716" Score="0" Text="@gallickgunner I am reading it right now, thank you. I also noticed that, in the smallpt, the cos(theta) of the BRDF is not considered. The color is calculated as `Vec f = obj.c`, while according to the literature it should be multiplied by cos(theta). Is it something that you noticed as well?" CreationDate="2019-04-01T12:09:31.793" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12135" PostId="8716" Score="0" Text="That might be because cosing weighted hemispherical sampling has a pdf of cos. The cos in the rendering equation and the pdf gets cancelled. It's micro optimization process. So don't take smallpt as a reference. Smaller isn't always better. Go check up on PBRT or some other site like scratchapixel." CreationDate="2019-04-01T12:26:51.210" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12136" PostId="8694" Score="0" Text="@Reynolds That's could be interesting, do you have any idea about how to manipulate the camera frustum in that way?" CreationDate="2019-04-01T14:20:35.080" UserId="10368" ContentLicense="CC BY-SA 4.0" />
  <row Id="12137" PostId="8717" Score="0" Text="Thank you so much for your amazing answer, it went even beyond my original question providing the mathematical framework." CreationDate="2019-04-01T14:35:45.377" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12138" PostId="8717" Score="0" Text="I don't think your question can really be answered (in a satisfactory manner) without mathematics, unless one just states that it's less efficient without explaining the basis for this. The basis is that you assume energy conservation of the brdf (otherwise the operator won't be a contraction), then with each bounce the contribution will be attenuated more and more, so you'd expected that bounces later down the bounce tree have a very small weight, thus you don't want to split there. The other obvious reason is that splitting results in exponential ray count growth." CreationDate="2019-04-01T14:40:27.470" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12139" PostId="8717" Score="0" Text="One cannot really talk about using Monte Carlo to solve the rendering equation without understanding how this is done. What my answer does is explain (albeit not in very many details) to what reformulation of the problem Monte Carlo is applied, which naturally address the misconception you had about the necessity of splitting. Note that both Whitted and Cook (and some other researchers), has the exact same idea you had, it was ultimately Kajiya that showed that this idea is not optimal (nor is it necessary for things to work out). My point is that your misunderstanding was in some way natural." CreationDate="2019-04-01T14:44:11.920" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12140" PostId="8717" Score="0" Text="It is perfectly clear now, thank you. Regarding the canceling of the cosine term in the BRDF, is there a source you know that provide the math for this micro-optimization? I have just calculated on paper the x,y and z coordinates both for uniform sampling, and for `p(w) = cos(theta) / pi` depending on two random uniform generators, and I obtained the same equations that smallpt already implements for the second case." CreationDate="2019-04-01T15:28:50.140" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12141" PostId="8717" Score="0" Text="There's not much math, you have the integrand piece for the current bounce: $f(x_{i+1}\rightarrow x_{i} \rightarrow x_{i-1})L_e(x_{i+1}\rightarrow x_i)\cos\theta$ which you must divide by the pdf as I have explained here https://computergraphics.stackexchange.com/questions/8676/monte-carlo-importance-sampling/8679#8679&#xA;Since you have the cosine in the integrand and the cosine in the pdf they cancel out. In fact you should cancel them out otherwise you may get numerical imprecision or NaNs especially for the cosine equal or close  to $0$ (remember computers have finite precision)." CreationDate="2019-04-01T15:58:17.920" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12142" PostId="8717" Score="0" Text="Great, so if I understand correctly, the returned reflectance should be multiplied in each bounce for `pi`, since we are dividing the integrand you have just written by `cos(theta)\pi`. I mean, in the Diffusion case: `return obj.e + f.mult(radiance(Ray(x,d),depth,Xi)) * pi`;" CreationDate="2019-04-01T16:21:10.643" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12143" PostId="8717" Score="0" Text="The $\pi$ (it is a constant after all) can be absorbed into the albedo depending on how you define your albedo, which I think is what they did in smallpt. But yes, your idea is correct if you consider $\pi$ to not be absorbed. You should also try to rewrite your raytracer with a while loop rather than a recursion to gain further insight into what actually happens (I would recommend also checking out the path formulation in vech's thesis, or the same thing in Iliyan's thesis: http://www.iliyan.com/publications/ThesisPhD)." CreationDate="2019-04-01T16:35:34.487" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12144" PostId="8717" Score="0" Text="I multiplied for π as I wrote above, but the image returned is almost completely white. As you said, π is probably already absorbed in the albedo. Thank you for those sources!" CreationDate="2019-04-01T17:34:09.827" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12147" PostId="8717" Score="0" Text="If you multiplied it in smallpt, then as I explained there it is accounted for, so another multiplication simply yields a non-energy conserving brdf most likely." CreationDate="2019-04-01T18:16:16.217" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12148" PostId="8710" Score="0" Text="I tested it with 10k samples with the naive version and 400 with the explicit one. Don't mind the -1 thing, it was just a test." CreationDate="2019-04-01T20:30:17.937" UserId="8925" ContentLicense="CC BY-SA 4.0" />
  <row Id="12149" PostId="8707" Score="1" Text="Next thing I would check is if your samples are being generated as you expect. Try reducing it to 1 sample that is somewhere at the edge of your sampling disc and output the view-space position of that sample. Would be useful to have side-by-side with your original view-space position rendering, they should look similar with the offset version looking slightly shifted in terms of position. How large are the disc sampling vectors compared to the size of your mesh ?" CreationDate="2019-04-02T02:11:27.467" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12150" PostId="8707" Score="0" Text="I've already made subtle modifications and tests and it turned out that by using around 30 samples and substracting by 10 I am getting best results (https://github.com/komilll/LEngine/blob/master/LEngine/ssaoShader.ps).&#xA;&#xA;However it was only a result of personal tests on given model and might not be general rule. I will try side-by-side comparision.&#xA;&#xA;Also does mesh size vs sampling size matters? Shouldn't there be a universal best sample size that doesn't collide with any mesh size? Could you elaborate on that, please?" CreationDate="2019-04-02T06:40:08.360" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="12151" PostId="8707" Score="1" Text="Your sampling offsets could be so large that you could miss your model completely so yes, it is related to the mesh size. If my understanding is correct the offsets are added in view coordinate space? I wanted to eliminate the possibility that you're taking samples from somewhere near or beyond the edge of your depth texture." CreationDate="2019-04-02T06:46:37.747" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12152" PostId="8717" Score="0" Text="Just for the sake of completeness: I don't think the `π` is absorbed in the albedo (at least, in the diffusion case). I think that, since the BRDF for the Lambertian surface is `reflectivity  / π `, and `pdf = cos(theta) / π`, `π ` cancels out along with `cos(theta)`." CreationDate="2019-04-02T09:04:13.990" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12153" PostId="8717" Score="0" Text="The point is that the brdf is $f = r/\pi$, so when you write $f * L$ you're really writing $r/\pi * L$ thus it hasn't really been canceled. If one is to be entirely precise if he considers the $\pi$ to be absorbed, then he must call it $f_{\pi}$ or something. This becomes especially problematic when you use other sampling strategies where the constant factor is not $\pi$, and then people often make mistakes because of this. I recommend things being explicit, to make it explicit in smallpt you'll just have to divide the albedo by $\pi$ where the material is defined, then you can mult by $\pi$." CreationDate="2019-04-02T10:17:32.853" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12154" PostId="8718" Score="0" Text="Thanks, I will take a look. In the meantime I have made use of https://developer.mozilla.org/en-US/docs/Web/API/OffscreenCanvas with decent results, offloading the heavy work into a WebWorker Thread." CreationDate="2019-04-02T20:27:54.113" UserId="10372" ContentLicense="CC BY-SA 4.0" />
  <row Id="12156" PostId="8721" Score="0" Text="Are you looking for an existing application to do it or do you want to understand the algorithm so you can write the code yourself?" CreationDate="2019-04-03T03:22:19.927" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12157" PostId="8724" Score="0" Text="And you can try it for free interactively with the free Houdini appentice version." CreationDate="2019-04-03T12:14:53.940" UserId="8219" ContentLicense="CC BY-SA 4.0" />
  <row Id="12158" PostId="8724" Score="0" Text="Can I mix this with voronoi patterns?" CreationDate="2019-04-03T14:04:43.977" UserId="10418" ContentLicense="CC BY-SA 4.0" />
  <row Id="12159" PostId="8720" Score="0" Text="homogenious xD&#xA;Contrary to your belief 3x4 and 4x3 matrices can have a left and right inverse, and you can make it work in terms of CG. Not only that but you can make it work even without that as long as you realize that your matrices are products only of $S,R,T$ matrices. To add to this, you can in fact go with 3x3 matrices which are invertible and keep a translation vector which can be equivalent. Or even use a quaternion, scale, translation representation. The real answer is that you do not have to use 4x4 matrices, it's just one way of going about things." CreationDate="2019-04-03T14:38:20.953" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12160" PostId="8726" Score="0" Text="I'm aware that it doesn't work in the general case without some other adjustments, such as splitting up transparent solids into convex bodies and potentially doing a pass for each. I'm happy to accept that limitation and it seems like even with such a limitation it would be a useful general-purpose algorithm." CreationDate="2019-04-03T14:38:44.953" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12161" PostId="8726" Score="0" Text="Sure if you know that you have only a front face and a back face between the camera and the far plane, then you can draw both and then simply compute the distance and use that for attenuation, but that's not a realistic scenario. Your best bet is to look into recent papers on volume rendering, here's something: https://www.slideshare.net/DICEStudio/physically-based-and-unified-volumetric-rendering-in-frostbite" CreationDate="2019-04-03T14:44:26.900" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12162" PostId="8720" Score="0" Text="@lightxbulb yes off course but then they are not general matrices. And yes you can decompose and have pseudo inverses too but they dont fulfill classical matrix algebra.  And yes you CAN assume that the last line is given. I gave you a example where it has been done. I just said it leaves a lot to be desired because i can now not use the data as a general matrix. It turns it has many properties. But then you dont need to have any matrices to do 3D. Its just convenient to have a general purpose container that has clearly define math. AS for SRT well yes but having skew is useful too" CreationDate="2019-04-03T15:27:17.383" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12163" PostId="8720" Score="0" Text="@lightxbulb is it somehow better if i point out that one could calculate the inverses by other means. (such as by negating the last row and transposing the rotate part and inverting scale part)." CreationDate="2019-04-03T15:37:32.033" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12164" PostId="8720" Score="0" Text="My issue is really that you presented this as the main reason as to why 4x4 matrices are used, when it simply is not, and you can have well defined inverses for 3x4 and 4x3 matrices that you just waved off (when there are actual benefits when using those such as bandwidth and ops). Not only is it not the main reason, but there are various other representations that are better in some sense depending on what you're doing (within computer graphics mind you). None of the answers here is really satisfactory, the most upvoted one touches upon a more important aspect, but it's obviously missing..." CreationDate="2019-04-03T15:47:30.350" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12165" PostId="8720" Score="0" Text="...a lot too. I would've written an exhaustive answer if it wouldn't have taken at least a few hours, and several pages, and there's the fact that most of the major bits and pieces that this answer would've containted are already available in some for or another throughout books and blogs on the subject. The other reason is that I don't believe that many people will appreciate the details that would go into an exhaustive answer, most look for something similar to the answer referring to projections." CreationDate="2019-04-03T15:50:39.053" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12166" PostId="8720" Score="0" Text="@lightxbulb yes well if you only model rigid objects you obviously only need 6 values and standard campera fov maybe 7. But even so the homogeneous  explanation does not really describe why opengl does have to use 16 values it could just use 15. Having a general entity is kind of beneficial enough" CreationDate="2019-04-03T15:50:43.380" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12167" PostId="8720" Score="0" Text="I can tell you one thing for sure - the 15 vs 16 point isn't really supported by the invertibility that you talked about, nor is it a major factor. Homogeneous coordinates are an important reason, but they are neither the only reason, nor the most important such, nor are they always necessary. As explained this topic goes quite a bit deeper than what all of the answers here combined amount to. On a side note this has nothing to do with rigid vs non-rigid objects." CreationDate="2019-04-03T15:53:11.603" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12169" PostId="8720" Score="0" Text="@lightxbulb Yeah i understand that i was just giving a example of way they could go into this. But, thing is opengl was done by scientists and the 4 by 4 matrix was a bit of standard notation in science. So they chose that some of the benefits came later because its widely used." CreationDate="2019-04-03T15:58:48.877" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12170" PostId="8721" Score="0" Text="Sorry probably it is not clear from the question. I don't think there is any existing application to do it, so I'm interested in understanding a possible algorithm approach to code myself." CreationDate="2019-04-03T16:26:16.013" UserId="10415" ContentLicense="CC BY-SA 4.0" />
  <row Id="12172" PostId="8730" Score="0" Text="How do you imagine that divide and conquer algo? How would you prune branches? There could be a single dot anywhere in the volume." CreationDate="2019-04-03T18:13:31.970" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12173" PostId="8730" Score="0" Text="Well as a simple thing if you split your voxel field into two you can end up having either closed objects on both fields in that case you won't do anything, otherwise you'll end up with objects divided by the common side of the two voxel fields. Depending how you represent your mesh it would be a matter of joining the two pieces into a single one. This is what I've imagined." CreationDate="2019-04-03T19:12:23.943" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12174" PostId="8730" Score="0" Text="But you still have to evaluate every cell don't you? What do you gain by your proposed method. On the other hand I found this: http://www.zib.de/visual-publications/thesis/anders/anders_dipl_main.pdf" CreationDate="2019-04-03T19:29:36.393" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12176" PostId="8726" Score="0" Text="As I said at the outset, most volume rendering is about scattering and participating media. That paper is about volume lighting and shadows, and real-time smoke and flame effects. That's not relevant to me at all." CreationDate="2019-04-04T08:25:53.150" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12177" PostId="8726" Score="0" Text="The &quot;smoke&quot; is a (possibly isotropic) heterogeneous density field, so I believe it is quite relevant to what you were looking for (namely: &quot;My solids are heterogeneous and isotropic&quot;). If it is not, please elaborate." CreationDate="2019-04-04T13:18:42.530" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12178" PostId="8726" Score="0" Text="Oops, my solids are actually **homogeneous** and isotropic. Silly me. No wonder we were talking past each other a bit. I'll fix my question now. Sorry for the confusion." CreationDate="2019-04-04T13:45:10.540" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12179" PostId="8726" Score="0" Text="@DanHulme See the update. I have no idea how to deal with the ordering except for painters' algorithm, but that doesn't always work. You can of course encode distances for the different volumes in different bits, then you don't necessarily need an ordering, but that seems a bit more complex." CreationDate="2019-04-04T16:18:09.760" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12180" PostId="8737" Score="0" Text="Yes, this is correct. The iteration is happening when you multiply a camera matrix with every point you want to project. In short, this is because you ultimately need to transform to screen coordinates. You could certainly project points/things to another plane in 3d space without applying the inverse of the camera to the objects. Positioned elsewhere. But ultimately need it back in screen resolution." CreationDate="2019-04-05T04:01:00.773" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="12181" PostId="8730" Score="0" Text="Flying edges (2015) is the fastest variation of it I know of. It runs in parallel. Better for large datasets though. Check out the paper. You can find an implementation in the VTK library. https://vtk.org/doc/nightly/html/classvtkFlyingEdges3D.html#details" CreationDate="2019-04-05T04:04:04.473" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="12182" PostId="8737" Score="0" Text="@AndrewWilson: &quot;*But ultimately need it back in screen resolution*&quot; That's only true if you're doing rasterization. You don't need &quot;screen resolution&quot; if you're doing raytracing; you merely need to know the ray direction for a particular pixel, which isn't difficult either way." CreationDate="2019-04-05T04:24:43.100" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12183" PostId="8739" Score="0" Text="Thanks for the correction for the gamma. About the answer number two. If I store light as color and luminous intensity. Then after I convert to radiometry unit, It is possible that the color will change right? Because each color contribute different amount of luminous intensity. Or should I just convert the intensity part by assuming efficiency is 100%?. I ask about tonemapping because I am not sure what unit should I deliver to the tonemapping operation. Radiance or luminance? Tonemapping will convert linear HDR to linear LDR. But not sure what unit is this linear HDR." CreationDate="2019-04-05T10:00:31.987" UserId="8565" ContentLicense="CC BY-SA 4.0" />
  <row Id="12184" PostId="8739" Score="0" Text="The way I understand it, the color should not change, just your way of expressing the light. The same way that 2.54cm is the same as 1 inch. Thus, you have the same input for tonemapping, regardless of how you calculate your light." CreationDate="2019-04-05T12:14:49.243" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="12188" PostId="8248" Score="0" Text="What do you mean with &quot;multiple vbos contributing to a single attribute&quot;? This does *never* work, even with multiple VAOs this wouldn't work. A single attribute can only ever be sourced from a single VBO. So it seems you mean something different when you say &quot;contributing to a single attribute&quot;. If you mean sourcing different attributes from different VBOs, this is very much possible, even with just a single VAO." CreationDate="2019-04-09T09:37:58.847" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12190" PostId="8746" Score="0" Text="Great thank you!" CreationDate="2019-04-10T12:13:56.830" UserId="10447" ContentLicense="CC BY-SA 4.0" />
  <row Id="12191" PostId="8751" Score="0" Text="Sorry i was referring to real-time. For example when it comes to algorithms for AO, have the latest achieved the best result or they have a huge research area for improvement. I am just pointing it out as an example it can be anything." CreationDate="2019-04-10T14:08:30.050" UserId="10468" ContentLicense="CC BY-SA 4.0" />
  <row Id="12192" PostId="8751" Score="0" Text="First you need to define what &quot;the best result&quot; means here. Also SSAO is quite different from actual (raytraced) AO. &quot;The best&quot; in terms of accuracy is simply raytracing AO until convergence. But since performance is a factor you usually only get screen space AO, last time I checked Scalable ambient obscurance was one of the best in terms of perf/accuracy. While working on RTX some researchers developed a hybrid AO algorithm that combines SSAO and raytraced AO to get both acceptable performance and quality. I believe that's currently the cutting edge." CreationDate="2019-04-10T14:50:40.767" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12194" PostId="8752" Score="0" Text="Hi and welcome. Slightly offtopic but still worth knowing. Line drawing algorithms may be intuitive. But they tend to lead people astray when you move up this way to try to implement antialiasing. For these kinds of reasons antialiasing is essentially broken in mostly all 2D vector rendering engines." CreationDate="2019-04-11T05:53:22.683" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12195" PostId="8755" Score="0" Text="To be honest it looks more like moire and insuficent shadow bias" CreationDate="2019-04-11T06:12:12.253" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12196" PostId="8754" Score="0" Text="Seems like either numerical error or you have a mistake in your impl." CreationDate="2019-04-11T06:38:24.300" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12197" PostId="8758" Score="0" Text="I was looking to create something like this https://vimeo.com/125511537. Didn't know how it's done. I kinda assumed the workflow I mentioned in the question and hypothesised that's how it's done." CreationDate="2019-04-11T06:44:49.297" UserId="10478" ContentLicense="CC BY-SA 4.0" />
  <row Id="12198" PostId="8754" Score="0" Text="@lightxbulb You are right! The numerical error caused self intersection. I have fixed it now. https://i.imgur.com/mGwTjGA.png" CreationDate="2019-04-11T09:00:04.093" UserId="10476" ContentLicense="CC BY-SA 4.0" />
  <row Id="12199" PostId="8758" Score="0" Text="As far as you have the depth map, I guess the shadow in that video is rendered with [shadow mapping](https://en.wikipedia.org/wiki/Shadow_mapping). The main object may be rendered in many ways. &#xA;The easiest technique (if scanner resolution is not too high) consist on generate a quad per pixel in your depth map and set the axis coordinate according to depth values in the vertexes. If you want to do it in a more efficient way or scanner resolution may vary with time, the quads can be generated by a geometry shader from a more basic mesh." CreationDate="2019-04-11T10:50:36.550" UserId="9908" ContentLicense="CC BY-SA 4.0" />
  <row Id="12202" PostId="8755" Score="0" Text="Interesting! I've not seen that happen before. Very cool." CreationDate="2019-04-11T16:02:19.893" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12203" PostId="8752" Score="0" Text="@joojaa That's really interesting. Do you have a link where I could read more about that?" CreationDate="2019-04-12T02:29:50.057" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12204" PostId="8754" Score="2" Text="@YuChang you should explain what the problem was and how you fixed it in an answer" CreationDate="2019-04-12T13:25:40.983" UserId="8680" ContentLicense="CC BY-SA 4.0" />
  <row Id="12205" PostId="8764" Score="3" Text="It should be noted that, on the hardware-side, the picture will look different. GPU cores are purpose-built for floating point because they are there to run shaders and float is what shaders do. In fact, it's only recently that we're seeing integer performance on-par with float. So when we're talking about software on the GPU, float is king. I'm not a hardware engineer, but based on all I know, the cost of floating point in terms of area and power is much, much higher. So when you're building a hardware rasterizer, you will want to stick with fixed-point (i.e. integer) arithmetic if you can…" CreationDate="2019-04-13T01:42:51.783" UserId="9812" ContentLicense="CC BY-SA 4.0" />
  <row Id="12206" PostId="8765" Score="0" Text="`ray.o` almost always denote the origin, for what it's worth." CreationDate="2019-04-13T17:20:27.400" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="12207" PostId="8766" Score="0" Text="Dividing by the 4th coordinate would lead to conversion from [homogeneous projective coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates) to [Carthesian coordinates](https://en.wikipedia.org/wiki/Cartesian_coordinate_system) in Euclidean space. The clipping test then computes intersection with a plane in Euclidean space, with that plane just happening to be axis-aligned.&#xA;&#xA;If you need to do the same thing in world-space, the only difference is that the plane is no longer axis-aligned, but ends up being the (not-axis-aligned) planes bounding your viewing frustum." CreationDate="2019-04-14T09:20:52.760" UserId="10497" ContentLicense="CC BY-SA 4.0" />
  <row Id="12209" PostId="2441" Score="0" Text="(Know it's a bit late to contribute to this post). You can also try to discretize the convolution integral, but in tangent space. Namely if $U$ is the patch of your mesh $$g = \int_U K f dU \approx  \sum_{v_i} K(v_i)f(v_i) \Delta A_i, $$&#xA;where $\Delta A_i$ is the differential area that should come out from your discretization, few approaches of this discretization are given in the link mentioned in the previous comment." CreationDate="2019-04-15T12:35:31.833" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12210" PostId="8755" Score="0" Text="I also think it may be a shadow bias issue, as I've seen this happening on some of my renders. Just bias the ray starting position an epsilon away from the surface when tracing the reflected rays." CreationDate="2019-04-15T15:02:32.933" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="12211" PostId="8765" Score="0" Text="@Hubble Thank you, so the first point is clear." CreationDate="2019-04-16T12:42:39.667" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="12212" PostId="8707" Score="2" Text="I finally got time to test it. THANK YOU. Everything works fine. I tweaked bias and radius values and after getting pleasable values I copied this values into my code and it works great." CreationDate="2019-04-16T17:29:03.553" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="12215" PostId="8774" Score="3" Text="What you're doing is quite similar to *UV unwrapping*, for which there are a few good algorithms. If you search for that, you'll probably find some existing code that could be adapted to your use case." CreationDate="2019-04-18T11:33:05.873" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12216" PostId="8770" Score="0" Text="Do you just require the outcome to be a circle (any size circle), or does the diagram indicate that you require specifically that size circle from that size ellipse?" CreationDate="2019-04-18T20:40:57.170" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="12218" PostId="8770" Score="0" Text="The circle size can be arbitrary size. I just want the transformed into complete circle.&#xA;thank you very much." CreationDate="2019-04-19T01:33:59.533" UserId="10500" ContentLicense="CC BY-SA 4.0" />
  <row Id="12220" PostId="8285" Score="0" Text="You might also check out [LEAN Mapping](http://www.cse.chalmers.se/edu/year/2011/course/TDA361/Advanced%20Computer%20Graphics/LEANpres.pdf)." CreationDate="2019-04-19T05:15:13.187" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12221" PostId="5952" Score="0" Text="@MikkelGjoel Unfortunately, your belief is incorrect due to a bug in your code. You re-used the same RNG for both `dithertri` and `dithernorm` instead of an independent one. Once you work through all the math and cancel all the terms, you'll find that you're not lerping at all! Instead, the code acts like a hard cutoff at `v &lt; 0.5 / depth || v &gt; 1 - 0.5/depth`, instantly switching to the uniform distribution there. Not that it takes away from the nice dithering you have, it's just needlessly complicated. Fixing the bug is actually bad, you'll end up with a worse dither. Just use a hard cutoff." CreationDate="2019-04-19T07:10:20.903" UserId="10515" ContentLicense="CC BY-SA 4.0" />
  <row Id="12222" PostId="8774" Score="0" Text="Do you want the geometry of the unfolding to be arbitrary? In many problems, one seeks a geometrically very well structured unfolding. Often the choice is a type of discrete conformal mapping (also called conformal parametrization, or conformal uniformization), a discrete version of the continuous analytic conformal parametrization (uniformization). There are several different way of defining it in the case of meshes (see for example https://www.discretization.de/media/filer_public/2015/11/16/a01-10-bss.pdf)" CreationDate="2019-04-19T13:52:22.233" UserId="8851" ContentLicense="CC BY-SA 4.0" />
  <row Id="12229" PostId="8778" Score="0" Text="Hi, thank you for your response! The overlaps will/should be removed by an greedy algorithm I plan to implement later, but for now I just want to unfold it to get a better feeling of what is happening (so no blind programming needs to be done).&#xA;&#xA;I understand how I get to the 2D - triangle with your solution, but I'm not sure how I can &quot;stick&quot; the triangle together again with affine transformation, since here I lose all information of the real place of the triangle, do I?" CreationDate="2019-04-20T05:41:28.513" UserId="8757" ContentLicense="CC BY-SA 4.0" />
  <row Id="12230" PostId="8785" Score="0" Text="Hi, thank you for your response. I'm not sure if I can follow your solution. For matrix m I don't know any of these points?" CreationDate="2019-04-20T05:43:44.613" UserId="8757" ContentLicense="CC BY-SA 4.0" />
  <row Id="12232" PostId="8785" Score="0" Text="@Anima I thought you had those. In your question you are asking about the transformation matrices. So do you need a method for calculating the coordinates of the vertices in the plane?" CreationDate="2019-04-20T13:34:32.037" UserId="8851" ContentLicense="CC BY-SA 4.0" />
  <row Id="12233" PostId="8785" Score="0" Text="The coordinates on the plane, do not matter for me (it can be transformed to any position on the plane), I'd only need to do this transformation to the plane of the first triangle and then accordingly transform all other triangles too. I'm sorry if i was unclear :(" CreationDate="2019-04-20T14:19:43.090" UserId="8757" ContentLicense="CC BY-SA 4.0" />
  <row Id="12234" PostId="8785" Score="0" Text="@Anima Do you want each triangle from the 3D mesh to be transformed isometrically, i.e. the original triangle and the triangle in the plane to be congruent (to be identical copies of each other)? In this case the unfolding may self-intersect. Or is there any other geometric requirement? And what matrices are you asking about, because an infolding does not necessarily require matrices (angles or edge-lengths are enough), unless you want to transform points inside the triangles." CreationDate="2019-04-20T14:53:10.697" UserId="8851" ContentLicense="CC BY-SA 4.0" />
  <row Id="12236" PostId="8785" Score="0" Text="@Anima Are you trying to rotate the whole 3D mesh so that one triangle ends up on the plane, then rotate it again, for the next triangle to land on the plane and so on? Like rolling the 3D mesh on the plane?" CreationDate="2019-04-20T15:03:13.540" UserId="8851" ContentLicense="CC BY-SA 4.0" />
  <row Id="12237" PostId="8785" Score="0" Text="The overlaps/intersects I would handle separately. A greedy algorithm will try to figure out a unfolding, where there are no overlaps. &#xA;The unfolded triangles should be identical copies (i.e. after unfolding I want to be able to recreate the original model)&#xA;I thought for unfolding I'd need matrices, how could I do it through the angles and length?" CreationDate="2019-04-20T15:42:19.363" UserId="8757" ContentLicense="CC BY-SA 4.0" />
  <row Id="12238" PostId="8787" Score="3" Text="BTW, the term you're looking for is CSG: [Constructive solid geometry.](https://en.wikipedia.org/wiki/Constructive_solid_geometry)" CreationDate="2019-04-20T19:59:30.250" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12239" PostId="8785" Score="0" Text="@Anima I changed my post, so now you can see a sketch of an algorithm of unfolding using edge-lengths." CreationDate="2019-04-20T21:09:48.600" UserId="8851" ContentLicense="CC BY-SA 4.0" />
  <row Id="12241" PostId="8785" Score="0" Text="Thank you very much. I accept this answer, it's really well explained. Again, thank you very much!" CreationDate="2019-04-21T07:31:54.610" UserId="8757" ContentLicense="CC BY-SA 4.0" />
  <row Id="12243" PostId="8772" Score="0" Text="Thank you very much for your kind answer." CreationDate="2019-04-22T00:07:34.793" UserId="10500" ContentLicense="CC BY-SA 4.0" />
  <row Id="12244" PostId="8790" Score="0" Text="MRI produces slices from volumetric data, your surface to slice conversion will just produce a flat slice with nothing remarkable inside the boundary. Granted, you can do that - just find the intersection of a slicing plane with all of your triangles, then assemble the resulting polygon." CreationDate="2019-04-22T10:20:05.973" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12245" PostId="8721" Score="0" Text="Is this what you mean by equirectangular projection: https://en.wikipedia.org/wiki/Equirectangular_projection ? Are you looking for the transformation from  the points on one of the pictures to the points on the other picture, knowing the change in 3D position of the camera (center of projection)? Or is it that you know where the second center of projection is in 3D relative to the coordinate system of the first one? I honestly do not understand what geometric information you have and what you are trying to obtain." CreationDate="2019-04-22T16:24:16.727" UserId="8851" ContentLicense="CC BY-SA 4.0" />
  <row Id="12246" PostId="5952" Score="0" Text="After digging even deeper I've found another issue in your shadertoy where you don't do gamma-correcting while averaging samples (you average in sRGB space which isn't linear). If you handle gamma appropriately we find that unfortunately we are not done yet. We must shape our noise to deal with the gamma correction. Here is a shadertoy displaying the issue: https://www.shadertoy.com/view/3tf3Dn. I've tried a bunch of things and couldn't get it to work, so I posted a question here: https://computergraphics.stackexchange.com/questions/8793/noise-shaping-for-dithering-with-gamma." CreationDate="2019-04-23T01:52:56.427" UserId="10515" ContentLicense="CC BY-SA 4.0" />
  <row Id="12247" PostId="8692" Score="1" Text="Textures are optimised for accessing neighbouring pixels and are usually encoded in some space filling curve like Morton order. If you are accessing the array in a linear fashion then this optimisation will lower cache efficiency." CreationDate="2019-04-23T02:36:50.980" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12248" PostId="8795" Score="0" Text="You mean, besides the fact that one of these is self-explanatory and the other appears to be gibberish?" CreationDate="2019-04-25T13:42:06.533" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12249" PostId="8795" Score="0" Text="@NicolBolas comparison with self is the usual way to test for nan with standard conforming floats and glsl is, as far as I know, required to abide by them. Therefore I consider adding another function as superfluous (and confusing), unless (or especially if) it has a different behavior." CreationDate="2019-04-25T14:49:42.043" UserId="10547" ContentLicense="CC BY-SA 4.0" />
  <row Id="12250" PostId="8795" Score="0" Text="&quot;Usual&quot; is in the eye of the beholder. C99 has `isnan`; C++11 has `std::isnan`. Java has `Java.lang.Double.isNan`. C# has `Double.IsNan`. IEEE-754 even has a specific predicate called `isNaN`. It seems to me that those relying on `!(x==x)` are due to using standard libraries that are deficient, not because it is &quot;usual&quot;. An explicit function is always easier to understand than an idiom that must be learned." CreationDate="2019-04-25T14:57:49.790" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12251" PostId="8795" Score="0" Text="I did not know that such functions were so common. Thanks for mentioning them.&#xA;Can I conclude that the behavior is indeed the same?&#xA;&#xA;By one's personal experience, is it possible that the two methods behave differently on some devices?" CreationDate="2019-04-25T15:09:57.807" UserId="10547" ContentLicense="CC BY-SA 4.0" />
  <row Id="12252" PostId="8796" Score="0" Text="Great!&#xA;So other languages actually have a difference between the two approaches.&#xA;And glsl has the function just to mimic those languages, even if there is no difference (apart from the readability point).&#xA;Thanks for the explanation :D" CreationDate="2019-04-25T15:25:44.623" UserId="10547" ContentLicense="CC BY-SA 4.0" />
  <row Id="12253" PostId="8587" Score="0" Text="How about considering the answer from other SE such as this:&#xA;https://mathoverflow.net/questions/105164/covering-a-polygon-with-rectangles" CreationDate="2019-04-26T05:34:05.333" UserId="10552" ContentLicense="CC BY-SA 4.0" />
  <row Id="12259" PostId="8806" Score="1" Text="When you say *&quot;cover it with 2 (at least)&quot;* and *&quot;The amount of the parallelograms should be the least possible and the square of the parallelograms should be the least possible&quot;* which takes priority? If the former, then surely it must always be 2 parallelograms?&#xA;&#xA;Further, do any of the sides have to be aligned with a major axis or can they be entirely arbitrary?" CreationDate="2019-04-29T10:52:36.907" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12262" PostId="8806" Score="0" Text="@SimonF thanks! Edited question. what is &quot;major axis&quot;? Can you expand?" CreationDate="2019-04-30T07:52:06.417" UserId="10564" ContentLicense="CC BY-SA 4.0" />
  <row Id="12264" PostId="8806" Score="0" Text="By &quot;major axis&quot; I just meant do you want to constrain all the parallelograms so that 2 sides are always horizontal (or vertical)? I was just wondering if you were trying to do, say, some kind of conservative rasterisation. (Also quite a few polygon triangulation algorithms first divide the data into trapeziums (*for those in USA, &quot;trapezoids&quot;* :-/ ) with horizontal  top and bottom edges. I was just thinking maybe you wanted something similar)" CreationDate="2019-04-30T11:06:48.310" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12265" PostId="8806" Score="1" Text="@SimonF I understood that I myself need to know more about the task so I could specify it better. I am going to understand the thing deeper, draw some images of input polygon and desired output and will come back." CreationDate="2019-04-30T13:37:23.893" UserId="10564" ContentLicense="CC BY-SA 4.0" />
  <row Id="12266" PostId="8794" Score="0" Text="wow, thanks a ton for this answer. it is of huge help. at first glance that document seems like just what I need. there's much to learn" CreationDate="2019-04-30T20:14:24.517" UserId="10512" ContentLicense="CC BY-SA 4.0" />
  <row Id="12270" PostId="8793" Score="0" Text="I think now I understand your question. I'm not familiar with the language you implemented the function you mentioned, could you maybe write down $n(s,r)$ explicitly in &quot;math notation&quot;? :) (btw: I assume $r$ is uniform on $[0,1]$, right?)" CreationDate="2019-05-01T09:13:32.170" UserId="1981" ContentLicense="CC BY-SA 4.0" />
  <row Id="12271" PostId="8793" Score="0" Text="@flawr It's only `dither_quantize` you need to look at. There `c` is $s$, `rng` is $r$, `depth` is $m$, `ci` is $s\cdot (m-1)$ and `d` is $n(s, r)\cdot (m-1)$. `a ? b : c` means `b` if `a` else `c`, and `clamp(x, lo, hi)` is `min(max(x, lo), hi)`. Finally `uint(x)` is basically $\lfloor x \rfloor$." CreationDate="2019-05-01T09:49:27.887" UserId="10515" ContentLicense="CC BY-SA 4.0" />
  <row Id="12272" PostId="8793" Score="0" Text="@flawr One thing that isn't particularly clean here is that we must not forget $n(s, r)$ also depends on $m$, as fewer quantization steps means the noise must increase in size. It is $n(s, r) \cdot (m-1)$ that doesn't change as $m$ does." CreationDate="2019-05-01T09:54:55.200" UserId="10515" ContentLicense="CC BY-SA 4.0" />
  <row Id="12273" PostId="8793" Score="0" Text="@flawr There is another way you can view it, which I posted here: https://math.stackexchange.com/questions/3200249/maintaining-probability-distribution-under-transform. I convolved the above noise function with a unit square to find the probability that a signal $s$ gets quantized to $[s]$ after adding noise. Then you can view the whole quantizing operation as summing a couple random variables (e.g. one for the chance at $[s] -1$, one for $[s]$ and one for $[s] + 1$)." CreationDate="2019-05-01T10:26:21.840" UserId="10515" ContentLicense="CC BY-SA 4.0" />
  <row Id="12274" PostId="8438" Score="0" Text="@Gábor For some reason I didn't get notified of your last comment about the edit, I just stopped by today and I see it now. I wish I could up vote and accept twice! This is an excellent and really thorough answer! Thank you again for your time and effort." CreationDate="2019-05-03T00:02:43.937" UserId="5846" ContentLicense="CC BY-SA 4.0" />
  <row Id="12275" PostId="5697" Score="0" Text="I'm not clear on what shape you're trying to parameterize. Is it an oblique triangular prism? What's a refracted edge? Are the triangles parallel? Could you draw a picture of the shape?" CreationDate="2019-05-03T07:45:29.243" UserId="7647" ContentLicense="CC BY-SA 4.0" />
  <row Id="12277" PostId="8815" Score="1" Text="Depends on which space are you shooting your ray in. If you are shooting rays in camera space then you'll have to transform all the vertices. The other way would be to just shoot the ray in world space and multiply your ray with the inverse of the world-to-view transform." CreationDate="2019-05-03T10:36:38.040" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12283" PostId="8737" Score="0" Text="For raytracing it's not 'easier'. But it might have some precision benefits." CreationDate="2019-05-06T05:02:34.650" UserId="7057" ContentLicense="CC BY-SA 4.0" />
  <row Id="12284" PostId="8823" Score="0" Text="I can live with clamping. My current solution is to extend the near clipping plane outside of the global bounding box, but that looses precision, which can be quite visible with large scenes. Yet, if you see the pictures they are sill clipped." CreationDate="2019-05-06T07:36:05.677" UserId="9387" ContentLicense="CC BY-SA 4.0" />
  <row Id="12285" PostId="8823" Score="0" Text="@rioki: So your solution is to scale the depth range to fit in [0,1]. But that doesn't loose precision if you have a floating point depth buffer. This is the entire point behind reverse-Z and floating point buffers. Also I'm sorry, but it's hard to understand what's that in the posted picture. Using a simpler scene can help." CreationDate="2019-05-06T18:54:10.707" UserId="7057" ContentLicense="CC BY-SA 4.0" />
  <row Id="12286" PostId="8771" Score="0" Text="I think I've figured how it works at last. I'll try to draw some proper diagrams to explain how it works." CreationDate="2019-05-06T19:17:29.770" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12287" PostId="8824" Score="0" Text="Perhaps something is wrong (or set oddly) in Gimp? On macOS in the Finder and in Preview.app, it shows up as solid red, as expected." CreationDate="2019-05-07T05:16:18.190" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12289" PostId="8825" Score="1" Text="[Why is it twice as expensive to make a noise function that can be tiled?](https://computergraphics.stackexchange.com/questions/250/why-is-it-twice-as-expensive-to-make-a-noise-function-that-can-be-tiled) contains a possible answer to your question, but the accepted answer there explains the big disadvantage of that approach." CreationDate="2019-05-07T07:32:23.533" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12290" PostId="8824" Score="0" Text="I couldn't see a link to the binary file to check myself, but can you open it in a hex editor just to make sure you haven't shifted the data by a byte?" CreationDate="2019-05-08T13:40:19.020" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12292" PostId="8824" Score="0" Text="@user1118321 Thanks for pointing this out. How could you manage to open the file with the Preview.app? I'm trying but the app doesn't show any window." CreationDate="2019-05-08T18:06:22.413" UserId="8807" ContentLicense="CC BY-SA 4.0" />
  <row Id="12293" PostId="8824" Score="1" Text="@SimonF I added a screenshot of the binary file with an hex editor, and as far as I can tell, it should be fine... Don't you think?" CreationDate="2019-05-08T18:15:47.443" UserId="8807" ContentLicense="CC BY-SA 4.0" />
  <row Id="12294" PostId="8830" Score="0" Text="I'm not too familiar with c++ file i/o , but in c maybe it's the difference between creating the file with textmode, fopen(fname, &quot;w&quot;),  VS binary, fopen(fname, &quot;wb&quot;). It needs to be a binary file." CreationDate="2019-05-09T08:06:57.413" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12295" PostId="8830" Score="1" Text="Yeah, I think that would work." CreationDate="2019-05-09T15:58:06.487" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12296" PostId="8812" Score="0" Text="Nice attempt, but the question isn't about how to use 3ds Max, but rather **how** 3ds Max works and **why** this changes makes the z-fighting go away. Could you [edit] your answer to answer that question?" CreationDate="2019-05-09T20:11:17.827" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12298" PostId="8562" Score="1" Text="If you are having difficulties on the algebra side, I would like to suggest that you see 3Blue1Brown videos on linear algebra that are available on Youtube. They will give you all the geometric intuition you need to understand transformation matrices." CreationDate="2019-05-09T22:34:02.047" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="12303" PostId="8823" Score="0" Text="If you look at https://paroj.github.io/gltut/Positioning/Tut05%20Depth%20Clamping.html You will see clipping vs clamping. With clipping all values that fall behind the near plane are rejected, but with clipping they are treated in the depth test as having the value of the near plane. The pictures are the the shadow maps and you can see they are clipped. not clamped..." CreationDate="2019-05-10T12:23:14.970" UserId="9387" ContentLicense="CC BY-SA 4.0" />
  <row Id="12304" PostId="8830" Score="2" Text="Thanks to both of you. Changing the opening the file with `ofs.open(&quot;./example.ppm&quot;, std::ofstream::binary);` did it." CreationDate="2019-05-10T15:44:24.550" UserId="8807" ContentLicense="CC BY-SA 4.0" />
  <row Id="12306" PostId="8836" Score="0" Text="looks as if you are doing gouraud/phong shading for anywhere without reflections and flat shading for anywhere with reflections. Why using 2 techniques, just use one. Afaik, phong shading is the standard or best looknig method." CreationDate="2019-05-12T10:38:27.753" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12307" PostId="8839" Score="4" Text="What formula &quot;should&quot; be used depends on what you're trying to achieve. What's the situation you want the light to model?" CreationDate="2019-05-12T12:52:31.717" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12310" PostId="8841" Score="0" Text="Looks to me like that contains the results of &quot;shaded&quot; a bump map. I suspect you'd need to (a) assume an incoming light direction (b) determine a height map (or a normal map) that then produces that shading given the assumed light direction.  That would appear to be a rather underconstrained problem, but I guess you could start by assuming that the largest contiguous area of &quot;constant&quot; colour (well anything in the range 123~127), was of &quot;middle height&quot;, and try &quot;flood-filling&quot; outwards and use weighted sums and multiple passes to get it to converge to *a* solution." CreationDate="2019-05-13T10:14:18.290" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12311" PostId="8843" Score="1" Text="I probably won't find the answer, I find your abbreviations a bit hard to read. However, I noitced you calculating `float Fr = D * G;` but never using it. Perhaps you forgot something?" CreationDate="2019-05-14T08:51:58.753" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="12317" PostId="8843" Score="0" Text="Thanks Tare, that's just an artifact of me trying to debug the code. I'll edit the post." CreationDate="2019-05-15T01:11:11.910" UserId="10270" ContentLicense="CC BY-SA 4.0" />
  <row Id="12319" PostId="8843" Score="0" Text="@polyrhythm I second that, just because dolt researchers like to use abhorrent abbreviations doesn't mean *you* have to.   There's honestly no reason you can't name things properly here.  Your names are misleading as well, NoL to me would have translated to &quot;NormalOfLight&quot; not &quot;N dot L&quot;, regardless *I would have just named it what it was* (and no, N dot L is not an appropriate name, what is N? What is L?, what does the combination of the two represent? That's how you name these things." CreationDate="2019-05-15T15:40:06.743" UserId="6530" ContentLicense="CC BY-SA 4.0" />
  <row Id="12320" PostId="8843" Score="0" Text="Thanks for your comment! I think my abbreviations are pretty industry-standard, though. A quick google search turns up a ton of high quality codebases using similar nomenclature, here is an example: https://github.com/google/filament/blob/master/shaders/src/brdf.fs&#xA;&#xA;I have never seen `NoL` to mean &quot;normal of light&quot;..." CreationDate="2019-05-16T01:22:08.347" UserId="10270" ContentLicense="CC BY-SA 4.0" />
  <row Id="12322" PostId="8842" Score="0" Text="Thank you very much for the time you took to write this explanation." CreationDate="2019-05-16T07:13:08.170" UserId="10633" ContentLicense="CC BY-SA 4.0" />
  <row Id="12323" PostId="8848" Score="1" Text="I'll just a comment until I can find the source paper/notes, but a while ago I had a similar problem where I wanted to recreate a height map from the normal map. I tried to use some work published by (I think) Nvidia. It wasn't perfect, but it sounds like you would need this as a first step.  I'll see if I can find the link" CreationDate="2019-05-16T16:09:56.160" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12324" PostId="8848" Score="2" Text="I'm not sure if these were the ones we looked at, but perhaps they may be of use:&#xA;Dave Eberly: https://geometrictools.com/Documentation/ReconstructHeightFromNormals.pdf &#xA;Siggraph 2011: http://developer.download.nvidia.com/assets/gamedev/docs/nmap2displacement.pdf" CreationDate="2019-05-16T16:35:25.810" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12325" PostId="8848" Score="1" Text="What do you mean with normal map associated to the mesh? How did you generate the normal map? Also how did you generate the low poly mesh?" CreationDate="2019-05-16T17:55:55.033" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12331" PostId="8848" Score="0" Text="Thanks SimonF - very helpful. &#xA;@user8469759 - I tried to provide additional clarification." CreationDate="2019-05-18T13:44:58.777" UserId="7622" ContentLicense="CC BY-SA 4.0" />
  <row Id="12333" PostId="6393" Score="0" Text="One issue with using a sinc filter for mip-mapping has to do with the hardware texture units. Mipmaps are usually built in a way that the next level sample falls 'between' the previous level samples. That is as if the samples are located at half-integer coordinates and the origins of all the levels coincide. Sinc filter downsampling, on the other hand, filters the data and then throws away every other pixel. This results in a visible half pixel shift. If you try to average those pixels instead of subsampling, then you arrive at a 2x2 box filter essentially." CreationDate="2019-05-19T05:27:19.447" UserId="7057" ContentLicense="CC BY-SA 4.0" />
  <row Id="12335" PostId="8848" Score="1" Text="Depending how much info you can share we can give an exact solution. It is also possible that your low poly mesh is already the best approximation of the original high poly one. If not you can try to minimize some cost function of the form $$ \int_{\mathcal{M}} \lVert n(p) - n_{map}(p) \rVert^2 \mu(dp) $$, you can minimize such integral by using gradient descent, the process is analogous to what @SimonF proposed, however gradient descent on the integral above will take into account arbitrary topology. But you'll need at least boundary conditions." CreationDate="2019-05-20T14:59:26.777" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12336" PostId="8854" Score="4" Text="What are &quot;advanced/special&quot; rasterization algorithms? How are they different from the normal kind of rasterization?" CreationDate="2019-05-21T10:14:28.640" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12337" PostId="8854" Score="0" Text="The ones specialized for correcting or playing around limitations of displays (anti aliasing measures, various small shape/position adjustments to avoid strat pixel defects and so on)" CreationDate="2019-05-21T10:39:10.867" UserId="10674" ContentLicense="CC BY-SA 4.0" />
  <row Id="12339" PostId="8854" Score="2" Text="Requests for books, papers, etc. are considered off-topic here. If you have a specific question about the topic, feel free to ask that, though." CreationDate="2019-05-22T02:39:08.510" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12340" PostId="8852" Score="0" Text="Very interesting, Adam! I hadn't heard about this function." CreationDate="2019-05-22T05:51:06.250" UserId="6596" ContentLicense="CC BY-SA 4.0" />
  <row Id="12341" PostId="8854" Score="1" Text="*&quot;The ones specialized for correcting or playing around limitations of display&quot;* Do you mean just like font-hinting tricks, or are you interested in filtering/reconstruction/coping with Nyquist limits/human visual systems?" CreationDate="2019-05-22T08:48:36.557" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12342" PostId="8854" Score="0" Text="@Simon_f mostly the second" CreationDate="2019-05-22T09:30:06.513" UserId="10674" ContentLicense="CC BY-SA 4.0" />
  <row Id="12343" PostId="8854" Score="1" Text="In that case, perhaps Andrew Glassner's &quot;Principles of Digital Image Synthesis&quot; *might* be useful. You may want to borrow from a library, though, as it comes in two large volumes and I have a feeling that the 1st volume might be the more relevant for this. Unfortunately, my copy &quot;walked off&quot; some time ago and I've not seen it since, so I can't check. :-(" CreationDate="2019-05-22T15:58:20.103" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12344" PostId="392" Score="0" Text="I keep re-reading this every once in while and I can't quite tell, but I might have the skew transforms described wrongly. Skews are confusing. If anyone sees this and want to have a go at editing, please help clarify that part!" CreationDate="2019-05-23T01:32:17.080" UserId="482" ContentLicense="CC BY-SA 4.0" />
  <row Id="12346" PostId="8856" Score="0" Text="The first cosine is for the direction of incoming light, to get the &quot;true&quot; irradiance or whatever. E.g. $E_0$ coming in at a $90^\circ$ angle doesn't light up the surface at all. I think my discrepancy comes from the fact you integrated $L\cos(\theta)$ instead of just integrating $L$." CreationDate="2019-05-23T03:59:06.423" UserId="10685" ContentLicense="CC BY-SA 4.0" />
  <row Id="12347" PostId="8856" Score="0" Text="1). I know. But if you are putting in the cosine then you have to do it properly. $dE$ will change into $dL_f$.  Because as shown below $dE = dL \cos(\theta) d\omega$ 2) Why would you just integrate $L$. You get irradiance by integrating $L \cos(\theta)$." CreationDate="2019-05-23T06:34:54.730" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12348" PostId="8856" Score="0" Text="Ah I thought I had already taken care of the $\cos\theta$ when I was calculating solid angle for my actual problem (not a flat plane). But that's just it, I needed $\cos^2\theta$ in total, and now I get $E=E$." CreationDate="2019-05-23T08:23:48.977" UserId="10685" ContentLicense="CC BY-SA 4.0" />
  <row Id="12350" PostId="8847" Score="0" Text="Looks wrong honestly, you cannot get to (1) by importance sampling in any sane manner that I can think of. They are using ggx sampling and do not divide by the pdf, but rather by some sum. Now if they are using a biased estimator of some form, then that's another matter, but I do not believe this was mentioned in the article. I am inclined to believe it's just wrong, try contacting the author." CreationDate="2019-05-24T22:15:54.390" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12351" PostId="8847" Score="0" Text="They do mention in the article they divide on purpose for the total sum though." CreationDate="2019-05-24T22:27:16.127" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12352" PostId="8847" Score="0" Text="Also if you, like me, think it's wrong... What would be the right montecarlo estimator then?" CreationDate="2019-05-24T22:28:07.873" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12353" PostId="8847" Score="0" Text="The shader is a cut and paste of the one shown at page 6 here: https://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_notes_v2.pdf ." CreationDate="2019-05-24T22:57:52.483" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12354" PostId="8847" Score="0" Text="&quot;As shown in the code below, we have found weighting by coslk achieves better results1. 1This weighting is not present in Equation 7, which is left in a simpler form&quot; - from the article you linked. They have not provided a reasoning beyond &quot;achieves better results&quot;. Or at least I cannot find the part where they formally motivate this." CreationDate="2019-05-25T04:42:24.577" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12355" PostId="8847" Score="0" Text="As for the correct estimator (not split sum) and code, you can find it just above the copy pasted part in the self shadow article you linked, in the Image based lighting section. Note that what they are estimating is not even  L weighted by some cosine term, since they do not divide by the GGX's importance sampling pdf, then it's implicitly in the estimated integral." CreationDate="2019-05-25T04:51:31.330" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12356" PostId="8847" Score="0" Text="I meant the &quot;correct estimator&quot; for the factor in question in the split and sum." CreationDate="2019-05-26T12:25:47.520" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12357" PostId="8857" Score="2" Text="The algorithm as written is only for the case $\Delta x &gt; \Delta y &gt; 0$. In your case you have $\Delta y &lt; 0$, which is why it does not work. See the complete algorithm under &quot;All cases&quot; instead." CreationDate="2019-05-26T20:30:00.537" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="12358" PostId="8847" Score="0" Text="It's also in the article, you can see it where they split the sums, they just don't write code for it." CreationDate="2019-05-27T07:41:21.290" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12360" PostId="8858" Score="0" Text="If you don't want sequential interpolation, but actual interpolation between 100 poses with 100 parameters, then you have to define a 100 dimensional manifold which would serve as a metric of &quot;how close&quot; two poses are, then you simply take a convex linear combination of the parameters on that manifold and find the corresponding point. It's a non-trivial problem without further constraints." CreationDate="2019-05-28T07:10:29.790" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12361" PostId="8859" Score="0" Text="That...is entirely a matter of taste and how you define your image data layout. There is no universal &quot;first&quot; or &quot;last&quot;. 50% of formats store them bottom-up and 50% top-down. The first line in the file *is* the first scanline because it's...the first scanline in the file's order. What you *could* argue is that Windows normally prefers a top-down coordinate system in the rest of its 2D operations (although, that's also largely configurable). In light of *that context* this layout might indeed seem unusual. But don't fall in the trap of thinking there's a universal order the file ought to have." CreationDate="2019-05-28T09:40:00.990" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12362" PostId="8863" Score="2" Text="Figure out how the image is mapped to the sphere (you can test with an image of a grid) and then work backwards" CreationDate="2019-05-28T10:36:55.097" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="12363" PostId="8863" Score="1" Text="It doesn't sound dissimilar to the problem of coping with the distortions needed when rendering for VR glasses, e.g. barrel/pincushion&#xA;https://www.imgtec.com/blog/speeding-up-gpu-barrel-distortion-correction-in-mobile-vr/" CreationDate="2019-05-28T13:09:31.743" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12364" PostId="8863" Score="1" Text="Check out the projections used for maps (globe vs planar): https://en.m.wikipedia.org/wiki/Map_projection&#xA;Then choose the one that preserves the properties that you want (area, angles...)" CreationDate="2019-05-28T17:58:27.040" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12367" PostId="8847" Score="0" Text="Well anyway, I'd really like to get in touch with the author of that shader, but I can't find any e-mail address. I'm not either sure that was actually published in some scientific paper." CreationDate="2019-05-29T10:20:25.930" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12368" PostId="8867" Score="0" Text="So, if I have two poses like &quot;head bow&quot; and &quot;hands up&quot;, they are not really close and `nlerp` will produce bad results?" CreationDate="2019-05-29T11:39:18.000" UserId="2750" ContentLicense="CC BY-SA 4.0" />
  <row Id="12369" PostId="8867" Score="0" Text="manually create intermediary poses that look better and path the weights between them." CreationDate="2019-05-29T11:58:48.683" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="12370" PostId="8063" Score="0" Text="Tangent spaces are not uniquely parametrized, yet I find a lot of mesh+normal-map-in-tangent-space images around. Is there a conventional way to define the tangent space basis?" CreationDate="2019-05-29T15:34:07.990" UserId="10714" ContentLicense="CC BY-SA 4.0" />
  <row Id="12371" PostId="8871" Score="3" Text="&quot;*If you're on a desktop architecture where device local memory is typically not mappable, then you have to create a transfer buffer that is host visible.*&quot; Even implementations that offer split memory may allow you to use host-visible memory for vertex data. In cases where you're frequently uploading data (like a GUI), this would be a good thing to use." CreationDate="2019-05-30T04:55:06.250" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12372" PostId="8871" Score="1" Text="Also, synchronization is really important when updating memory." CreationDate="2019-05-30T04:59:11.333" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12373" PostId="8847" Score="0" Text="Don't think too hard about it. A large part of real-time techniques have no physical basis and were usually tweaked by hand. You can regard the weighting as some kind of biased estimator I believe, but I don't believe it will make much sense physically even if I write out the analytical formulation." CreationDate="2019-05-30T10:56:59.407" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12374" PostId="8847" Score="0" Text="What do you mean with biased estimator?" CreationDate="2019-05-30T10:58:51.840" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12375" PostId="8874" Score="1" Text="Try reading &quot;Raytracing Complex Scenes&quot; by Kajiya." CreationDate="2019-05-31T09:25:33.507" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12376" PostId="8874" Score="0" Text="@gallickgunner Assuming you meant Kay and Kajiya's paper, I'm not sure it suggests using Octrees, but they do cite earlier works by Glassner and Kaplan which do, e.g.  Glassner's *&quot;Space Subdivision for Fast Ray Tracing,&quot;* IEEE Computer Graphics&#xA;and Applications, Oct 1984" CreationDate="2019-05-31T11:22:20.360" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12378" PostId="8847" Score="0" Text="An estimator with non-zero bias. Look up bias of an estimator, it's a math term." CreationDate="2019-05-31T17:04:24.177" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12379" PostId="8879" Score="1" Text="You're conflating energy conservation, with light additivity, with the film's sensitivity function. You can have an energy conserving bsdf and still get &quot;overexposed&quot; images. The second paragraph of your answer is simply confusing in the context of the question and the &quot;problem&quot; of overexposure mentioned." CreationDate="2019-05-31T17:09:30.390" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12380" PostId="8882" Score="0" Text="I thought about it a bit, maybe they multiplied the diffuse, added the rest, averaged it out." CreationDate="2019-05-31T21:08:29.187" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="12381" PostId="8882" Score="0" Text="@AnnoyinC well, in that case the product would be much darker." CreationDate="2019-05-31T21:12:47.437" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="12382" PostId="8874" Score="0" Text="@SimonF - Yeah, Kajiya doesn't explain what an octree is but defines a whole algorithm (using a heap) how to go about using it in raytracing. And I think that's what the OP wanted, that's why." CreationDate="2019-06-01T00:00:12.463" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12383" PostId="8882" Score="0" Text="Since the lit areas look identical in your result with addition vs. the reference image, but the shadow areas are darker in the latter, my guess is that they forgot to clamp the diffuse component to nonnegative values before performing the addition." CreationDate="2019-06-01T11:16:37.037" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="12384" PostId="8882" Score="0" Text="@Rahul that might be indeed. But why would they get negative values in the first place? From some dot product?" CreationDate="2019-06-01T12:49:55.067" UserId="4647" ContentLicense="CC BY-SA 4.0" />
  <row Id="12385" PostId="8882" Score="0" Text="Yes, the diffuse component of illumination is $\max(n\cdot l,0)$ where $n$ is the surface normal and $l$ is the unit vector towards the light source. If you forget to do the $\max$, you get negative illumination on surfaces facing away from the light." CreationDate="2019-06-01T13:24:16.673" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="12386" PostId="8881" Score="0" Text="Im packing it into a GLB file to gain the advantage of packing everything into a single file. I was using GLTF files with separate textures until now." CreationDate="2019-06-01T14:35:48.427" UserId="10720" ContentLicense="CC BY-SA 4.0" />
  <row Id="12387" PostId="8881" Score="0" Text="In what language?  There's a TypeScript solution with a NodeJS npm package available for this:  https://github.com/najadojo/gltf-import-export" CreationDate="2019-06-02T00:50:56.750" UserId="1908" ContentLicense="CC BY-SA 4.0" />
  <row Id="12388" PostId="8881" Score="0" Text="Im using that very package to generate my glb files from the command line. My only issue is now to figure out how to pack a bunch of images into the .bin file programmatically with images." CreationDate="2019-06-02T14:47:06.827" UserId="10720" ContentLicense="CC BY-SA 4.0" />
  <row Id="12389" PostId="8881" Score="0" Text="So, images can go in a `.glb` file, but they do not go in the `.bin` file, nor in the binary mesh portion of the `.glb` file.  So I'm having trouble understanding what you're trying to do here, can you explain it more?" CreationDate="2019-06-02T17:54:28.063" UserId="1908" ContentLicense="CC BY-SA 4.0" />
  <row Id="12390" PostId="8881" Score="0" Text="OK here is my understanding: GLB files contain everything packed into one single file. But to generate a GLB file, one needs : a .BIN file containing all the image data and the .GLTF file containing the mesh.&#xA;&#xA;Basically I have a bunch of 3D models which need to be programmatically generated. The GLTF file for these doesnt change at all since the base mesh is the same. How these models differ is via the textures applied on them. Since in my understanding, the BIN file contains all the textures, Im trying to ask how I can generate the BIN file dynamically." CreationDate="2019-06-03T04:42:26.017" UserId="10720" ContentLicense="CC BY-SA 4.0" />
  <row Id="12392" PostId="8881" Score="0" Text="To generate a GLB file, the `.gltf` and `.bin` are not enough, you need the `.jpg` and/or `.png` files that hold the textures.  The `.bin` file does not contain any images.  Take a look at [DamagedHelmet](https://github.com/KhronosGroup/glTF-Sample-Models/tree/master/2.0/DamagedHelmet/glTF) for example.  It references [jpgs separately from the bin](https://github.com/KhronosGroup/glTF-Sample-Models/blob/77f1a295e65c3a59c7131e6a15552c69817c9449/2.0/DamagedHelmet/glTF/DamagedHelmet.gltf#L92-L114).  All of the texture data is in the JPGs, not the bin file." CreationDate="2019-06-03T13:41:56.510" UserId="1908" ContentLicense="CC BY-SA 4.0" />
  <row Id="12393" PostId="8848" Score="0" Text="@user8469759 thanks. Is there any software you'd recommend that can do the gradient descent on a mesh as you described? In terms of the additional info, could you let me know what other information I can provide? I've updated the post to include a screenshot of two meshes that may provide additional clarification." CreationDate="2019-06-04T16:28:33.730" UserId="7622" ContentLicense="CC BY-SA 4.0" />
  <row Id="12394" PostId="8848" Score="1" Text="Can you share two 3d models?" CreationDate="2019-06-04T17:54:26.330" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12395" PostId="8848" Score="1" Text="Also I don't know your background, but I would look up either CGAL or OpenMesh (two geometry libraries). I'm more familiar with the second one. The method I suggested is a variational method, if you have the normals of the original high poly mesh (normal per vertex) then the Euler-Lagrange equation should provide the iteration you need to implement to solve the problem. You can use openmesh to load your low poly mesh, adding more vertices and refine their position using the Euler-Lagrange equations. Let me know if you need help with that." CreationDate="2019-06-04T19:11:37.863" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12397" PostId="8848" Score="0" Text="@user8469759 Thanks for the offer of help! This may take me a while but I will try. In the meantime I have included two 3d models as requested. They are the swords from the screenshot." CreationDate="2019-06-06T16:59:40.837" UserId="7622" ContentLicense="CC BY-SA 4.0" />
  <row Id="12398" PostId="8848" Score="0" Text="That's ok. I'll probably learn new stuff as well." CreationDate="2019-06-06T20:19:15.907" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12399" PostId="7485" Score="0" Text="I'm a bit late tot he game but I had a similar issue, I have no idea how to simplify the Cone but I rewrote the Torus function in a way that seemed more natural to me below.&#xA;&#xA;float SdTorus(float3 position, float innerRadius, float outerRadius)&#xA;{&#xA;    float thickness = .5 * (outerRadius - innerRadius);&#xA;    float2 q = float2(length(position.xz) - outerRadius + thickness, position.y);&#xA;    return length(q) - thickness;&#xA;}" CreationDate="2019-06-07T08:47:12.230" UserId="10743" ContentLicense="CC BY-SA 4.0" />
  <row Id="12400" PostId="3569" Score="0" Text="You have the answer, you have solved for u, after you have that n x u = v" CreationDate="2019-06-10T23:29:56.220" UserId="10734" ContentLicense="CC BY-SA 4.0" />
  <row Id="12401" PostId="8895" Score="0" Text="You should probably implement a push pull graph if you are cpu bound" CreationDate="2019-06-11T21:24:16.517" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12402" PostId="8897" Score="0" Text="Is batching outdated? It uses a queue after all. I was thinking about abusing the batching queue for state updates?" CreationDate="2019-06-12T04:24:11.607" UserId="10734" ContentLicense="CC BY-SA 4.0" />
  <row Id="12403" PostId="8895" Score="0" Text="Never heard of it. Are there examples or articles?" CreationDate="2019-06-12T04:25:50.117" UserId="10734" ContentLicense="CC BY-SA 4.0" />
  <row Id="12404" PostId="8895" Score="0" Text="A push pull graph does not actually update on changes. It pushes a flag that marks the graph element as needing to update (maya for example calls this flag a dirty bit, since the node attribute is dirty and needs washing). Instead the actual computation is pulled from the final nodes if needed. Causing the nodes to compute only when the downstream thing is needed. Thisway the graph self optimizes since it does not evaluate stuff that does not need to be updated." CreationDate="2019-06-12T05:16:19.680" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12405" PostId="8897" Score="0" Text="Batching is still recommended, however a scenegraph hinders efficient batching more than it helps. With a flat array of objects you can sort it so similar render commands end up next to each other." CreationDate="2019-06-12T09:54:35.053" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="12406" PostId="8900" Score="0" Text="&quot;*Compared to DX12, the resource binding procedure seems a little bit complicated.*&quot; To be honest, D3D12's seems more complicated. Vulkan is just &quot;descriptors&quot; and &quot;sets of descriptors&quot;; it's all very simple and *regular*. I still haven't found out what a &quot;root descriptor&quot; is, or how that relates to non-&quot;root&quot; descriptors." CreationDate="2019-06-12T13:40:02.770" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12407" PostId="8900" Score="0" Text="&quot;*I am not considering push constants because I want a universal approach that I can apply to other kinds of uniform buffers as well (i.e. per pass, per frame, per scene and etc.).*&quot; And why can't push constants be used for those too? Also, if efficiency matters so much, you should use the tool that gives you the best performance for the task at hand. Not the one that is the most &quot;universal&quot;." CreationDate="2019-06-12T13:41:44.690" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12409" PostId="8899" Score="0" Text="yes it's necessary to check whether `dot(light_dir, norm)` is positive or not. If negative we don't need to include specular component." CreationDate="2019-06-12T14:48:45.320" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12410" PostId="8899" Score="0" Text="I see, I wonder why the majority of tutorials don't bring up this caveat. Thank you very much @gallickgunner . Feel free to add your comment as an answer as it is exactly what I was looking for." CreationDate="2019-06-12T15:01:10.057" UserId="10759" ContentLicense="CC BY-SA 4.0" />
  <row Id="12412" PostId="8903" Score="0" Text="I will have a look at the reading you recommended. Very interesting topic. Thanks." CreationDate="2019-06-12T15:32:59.287" UserId="10762" ContentLicense="CC BY-SA 4.0" />
  <row Id="12413" PostId="8906" Score="0" Text="This is exactly what I need. Thanks so much!" CreationDate="2019-06-13T03:47:14.520" UserId="9901" ContentLicense="CC BY-SA 4.0" />
  <row Id="12414" PostId="8905" Score="0" Text="You need a function that maps a colour to a value. Possibly what you have at the bottom right but in the form of a function." CreationDate="2019-06-13T12:21:11.767" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12415" PostId="8905" Score="0" Text="i'm not really sure what to do i'm stuck :(" CreationDate="2019-06-13T15:05:40.840" UserId="10767" ContentLicense="CC BY-SA 4.0" />
  <row Id="12416" PostId="8908" Score="0" Text="Thanks! Solved it with FileIncluder from shaderc/glslc/src/file_includer.h and shaderc/glslc/src/file_includer.cc" CreationDate="2019-06-14T02:22:33.160" UserId="9901" ContentLicense="CC BY-SA 4.0" />
  <row Id="12417" PostId="8839" Score="0" Text="smoothstep(0,35,x) and ( 1.0 - smoothstep(65,100,x) )" CreationDate="2019-06-14T14:18:09.737" UserId="10774" ContentLicense="CC BY-SA 4.0" />
  <row Id="12418" PostId="8905" Score="0" Text="It's going to be very hard for us to help you if we don't have more information. What are you stuck on? Do you know how to sample the pixels of an image? Do you know how to read an image into memory? Do you have any existing code you can show us?" CreationDate="2019-06-16T01:37:38.547" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12419" PostId="8905" Score="1" Text="if you cant figure out a function you can maybe just hard code the given values in the bottom right in the form of a map (key value pair) and just linearly interpolate if a value is in between. Don't know how accurate will that be though." CreationDate="2019-06-16T10:54:56.960" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12421" PostId="8911" Score="1" Text="&quot;*I'd like to give the object a pitch of 60 degrees, yaw of 20 degrees and roll of 70 degrees.*&quot; Why? Why do you want to do this thing?" CreationDate="2019-06-17T05:36:32.573" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12422" PostId="8913" Score="0" Text="[Xybots](https://en.wikipedia.org/wiki/Xybots) is an example of a C64 game with the movement model you describe. Not sure what rendering technique it used, though." CreationDate="2019-06-17T09:55:49.640" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12423" PostId="5059" Score="0" Text="Sure, but do Teslas even support (reasonably modern) OpenGL to begin with? Afterall, they're *deliberately* non-graphics." CreationDate="2019-06-17T11:07:45.300" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12424" PostId="8911" Score="0" Text="OpenGL has *nothing* to do with that. GLM might very well regarding the implementation (but in turns has nothing to do with OpenGL either). But ultimately that's a purely mathematic problem, the *practical* implementation of which depends on a lot more things than if you're using GLM. And even the mathematical theory of which depends on a few other unclarities, e.g. what &quot;pitch&quot;, &quot;yaw&quot; and &quot;roll&quot; even mean in your context and what *exactly* you want your object to behave like." CreationDate="2019-06-17T11:17:24.973" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12425" PostId="8911" Score="0" Text="@Nicol Bolas Because those are all the three rotation values that an object can take at a time. Such thing is needed for example, in games with planes and helicopters, since they can turn around these 3 axis all at the same time." CreationDate="2019-06-17T14:27:12.840" UserId="10784" ContentLicense="CC BY-SA 4.0" />
  <row Id="12426" PostId="8912" Score="0" Text="Refer to: http://www.songho.ca/opengl/gl_transform.html" CreationDate="2019-06-17T16:42:55.853" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12427" PostId="8911" Score="0" Text="@Nicol Bolas Nevermind, I just realised that the answer is simpler than I first thought. I think that simply creating a rotation matrix for each axis and then multiplying them would do the trick." CreationDate="2019-06-17T17:37:45.717" UserId="10784" ContentLicense="CC BY-SA 4.0" />
  <row Id="12428" PostId="8911" Score="1" Text="And then you'll be back tomorrow asking about [Gimbal Lock](https://stackoverflow.com/search?q=gimbal+lock) or other similar [Euler angle](https://stackoverflow.com/search?q=Euler+Angle) problems." CreationDate="2019-06-17T17:39:16.987" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12429" PostId="8911" Score="0" Text="@Nicol Bolas I've heard about that Gimbal Lock issue. I heard that Quaternions can help fix it, but I'm not sure how to use them yet." CreationDate="2019-06-17T20:15:57.630" UserId="10784" ContentLicense="CC BY-SA 4.0" />
  <row Id="12430" PostId="8916" Score="0" Text="Being pedantic, the compilation looks fine, but you are getting a linker error.  Presumably you'll need a &quot;-l&quot; (lower case L) linker option on the gcc line that points to the library that contains the SDL_xxxx functions. &#xA;This, https://stackoverflow.com/a/17886993/626644 , suggests you need -lSDL  (noting again the first char is a lowercase L)" CreationDate="2019-06-18T08:09:04.047" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12431" PostId="8916" Score="0" Text="You might need to add a search path as well. (FWIW I've not used this library)" CreationDate="2019-06-18T08:21:44.887" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12432" PostId="8915" Score="1" Text="Are you saying that you simply took an 'instructional' image which portrays a highly magnified view of an LCD screen showing text when subpixel rendering is is used, and then simply downsized that 'instructional image' in an image editor?  That's unlikely to achieve anything useful.  Unfortunately, I don't have time at the moment to give an explanation but, if no one else answers, I'll try to add a reply later today or early tomorrow." CreationDate="2019-06-18T08:33:36.007" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12433" PostId="8915" Score="0" Text="What did you do use to resize the image? You will get very different results depending on the tool (algorithm) used. It looks like there is a slight rotation too. Which would account for the various colors, most likely generated by aliasing." CreationDate="2019-06-18T13:43:00.960" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="12434" PostId="8910" Score="0" Text="Thanks for your response. Is it correct that, I will need to build a separate pipeline for ImGUI; it will only share the surface/swapchain with the actual app?" CreationDate="2019-06-18T20:20:30.497" UserId="6496" ContentLicense="CC BY-SA 4.0" />
  <row Id="12435" PostId="7516" Score="0" Text="really interesting software indeed, it seems the author will be selling it on [steam](https://github.com/funparadigm/SdfMesher_Models/issues/1#issuecomment-503458497)" CreationDate="2019-06-19T08:40:47.617" UserDisplayName="user4801" ContentLicense="CC BY-SA 4.0" />
  <row Id="12436" PostId="8920" Score="0" Text="Ok, I will try this asap and mark as answer if it works; thanks a lot for the help in deciphering the code!" CreationDate="2019-06-19T09:38:10.500" UserId="10798" ContentLicense="CC BY-SA 4.0" />
  <row Id="12437" PostId="8920" Score="0" Text="This only seems to work if the original surface is convex; if it is not, this creates &quot;folds&quot; in the resulting sphere, which effectively corrupts the neighbourhood information from the original mesh." CreationDate="2019-06-19T10:45:54.770" UserId="10798" ContentLicense="CC BY-SA 4.0" />
  <row Id="12439" PostId="8917" Score="0" Text="I don't know what &quot;the standard canvas&quot; is, but it sounds like the term you want to search for is &quot;antialiasing&quot;, e.g. https://en.wikipedia.org/wiki/Xiaolin_Wu's_line_algorithm" CreationDate="2019-06-19T15:44:16.367" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12440" PostId="8917" Score="1" Text="yes this is exactly what I was looking for!" CreationDate="2019-06-19T21:21:15.473" UserId="10787" ContentLicense="CC BY-SA 4.0" />
  <row Id="12441" PostId="5059" Score="0" Text="@ChristianRau: I was referring to the Tesla microarchitecture, not the workstation GPU brand (blame nVidia for the confusion). Tesla is also used from the GeForce 8XXX series up to the 3XX series." CreationDate="2019-06-19T21:33:58.740" UserId="6567" ContentLicense="CC BY-SA 4.0" />
  <row Id="12442" PostId="8917" Score="1" Text="@SimonF if you want to formulate your comment as an answer I will accept?" CreationDate="2019-06-20T20:38:06.523" UserId="10787" ContentLicense="CC BY-SA 4.0" />
  <row Id="12443" PostId="8915" Score="1" Text="Sub pixel images like these only work at one scale so they are 1:1 aligned with the RGB elements of your display. If you want to rescale them you will need to convert the image back to greyscale, scale it, and then convert back to RGB subpixels." CreationDate="2019-06-21T04:16:16.787" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12444" PostId="8913" Score="0" Text="While this question is on-topic here, I wonder if you might get a better or faster response asking on [RetroComputing](https://retrocomputing.stackexchange.com/)? (If you do post there, please delete this question so it's not cross-posted.)" CreationDate="2019-06-22T22:34:18.697" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12445" PostId="8913" Score="0" Text="@user1118321 it is not on topic over there. That's a shame, but there were too many &quot;did this ever happen&quot; questions that consensus was to make that site about things which actually existed only." CreationDate="2019-06-23T01:24:54.480" UserId="10771" ContentLicense="CC BY-SA 4.0" />
  <row Id="12446" PostId="8913" Score="0" Text="Bummer. Sorry for the misleading suggestion!" CreationDate="2019-06-23T01:31:52.667" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12447" PostId="8913" Score="0" Text="@user1118321 no problem at all, I thank you for it :-)" CreationDate="2019-06-23T05:40:20.130" UserId="10771" ContentLicense="CC BY-SA 4.0" />
  <row Id="12448" PostId="8891" Score="0" Text="Do you have a requirement that all possible polyhedra have a non-zero probability of being generated? Do you require equal probability for each possible polyhedron? Your specific requirements may affect the range of solutions that are applicable." CreationDate="2019-06-23T08:24:53.993" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="12449" PostId="8891" Score="0" Text="For example, would combining 2 or more convex polyhedra to give a non-convex one be sufficient for your purposes?" CreationDate="2019-06-23T08:26:30.350" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="12451" PostId="8933" Score="1" Text="Do you mean orthographic projection? If that is the case then find by what rotation matrix do you need to transform the xy plane for example (normal=(0,0,1)) to get to the plane in your problem. Then multiply each point with the transposed rotation matrix and drop the third coordinate." CreationDate="2019-06-24T11:03:25.210" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12452" PostId="8933" Score="0" Text="@lightxbulb I think so but I'm not sure. Could you give me an example for say projecting a point A(2,1, 3) which I need to project to plane with  60 degrees polar angle and 20 degrees  azimuth angle." CreationDate="2019-06-24T11:10:56.260" UserId="10826" ContentLicense="CC BY-SA 4.0" />
  <row Id="12453" PostId="8933" Score="0" Text="Compute it yourself and see if that is what you want." CreationDate="2019-06-24T11:47:41.640" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12456" PostId="8936" Score="1" Text="This is a fairly broad question. The problem is that the answer varies wildly depending on which hardware, OS, and application is doing the displaying, not to mention which format the image is in, etc. You should think about narrowing down the question because as it is it may attract down votes or close votes." CreationDate="2019-06-26T04:23:49.380" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12458" PostId="8941" Score="0" Text="So it would be safe to rely on this behavior across different implementations and GPUs for example when rendering texture overlays in game programming?" CreationDate="2019-06-26T09:43:28.903" UserId="10840" ContentLicense="CC BY-SA 4.0" />
  <row Id="12459" PostId="8941" Score="0" Text="I would say its down to the internals of the GPU, I've seen some mobile SOC GPUs with terrible interpolation precision. I'm not sure, but I don't think this precision is exposed to openGL either, given that it could be performed in floating point units of fixed point integer units inside the GPU silicon." CreationDate="2019-06-26T09:47:13.230" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12460" PostId="8941" Score="0" Text="That comment is casting doubt on whether or not I should use this in my game.  My alternative is to generate alpha masks for the textures I want to apply overlays to, but I would like to avoid this to save space on the texture atlas if possible. I am only concerned with desktop GPUs, should it be safe for those?" CreationDate="2019-06-26T09:53:18.120" UserId="10840" ContentLicense="CC BY-SA 4.0" />
  <row Id="12462" PostId="8941" Score="2" Text="@Quantum64 No, this doesn't come down to the internals of the GPU alone, the OpenGL standard actually makes certain guarantees for invariance and this is one of that. For the same inputs the same calculations will return the same results. This holds for pipeline transformations and interpolations as much as it holds for any deterministic program..." CreationDate="2019-06-26T12:26:13.110" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12463" PostId="8941" Score="0" Text="...What could result in differences here would be if you have different vertex transformation operations in the vertex shaders (i.e. an MVP multiply vs. a separate P * MV multiplication). Make sure you use the same computations. Even then there could *theoretically* be different results (practically not, though), but even that is resolvable with declaring the vertex position `invariant`. So yes, drawing your mesh multiple times for stuff like texture layering is a perfectly reasonable and common use-case that, when handled properly, can be done in a 100% standard compliant and reliable way." CreationDate="2019-06-26T12:27:17.103" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12464" PostId="8941" Score="0" Text="At the end of the day, this comes down to the same issue as the common misunderstandings about floating point computations. Lack of precision or inaccurate interpolations is not the same as *randomly* noisy results. As long as your computations are deterministic the results will be the same, no matter how they differ from any possible (but entirely irrelevant) exact ground truth. And the OpenGL standard, under the above constraints, guarantees this determinism." CreationDate="2019-06-26T12:35:41.117" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12465" PostId="8941" Score="0" Text="@Christian Rau Thank you for your very helpful and informative answer." CreationDate="2019-06-26T20:29:35.787" UserId="10840" ContentLicense="CC BY-SA 4.0" />
  <row Id="12466" PostId="8936" Score="0" Text="I realise it's broad and you're absolutely right. The reason (and I should probably have included this in the question) is that I am an educator and it's exactly this broad overview I want to convey to learners. I need the details to improve my own understanding and make sure I am accurate and I don't introduce misconceptions, but it's the general idea that interests me." CreationDate="2019-06-27T07:27:58.427" UserId="10834" ContentLicense="CC BY-SA 4.0" />
  <row Id="12467" PostId="8939" Score="1" Text="Thank you so much for your answer, it was extremely useful." CreationDate="2019-06-27T08:11:02.600" UserId="10834" ContentLicense="CC BY-SA 4.0" />
  <row Id="12468" PostId="8944" Score="1" Text="It looks to me like you're using software emulation and if you want to use your GPU you need to install a different graphics driver. Installing new drivers really is a question for Ask Ubuntu though." CreationDate="2019-06-27T09:03:15.290" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12469" PostId="8947" Score="2" Text="It entirely depends on what type of renderer you're talking about. Techniques appropriate to ray-tracing aren't necessarily appropriate to triangle rasterisation, and vice-versa. If you're using scanline rasterisation, are you asking from the POV of someone designing a GPU, implementing OpenGL (or similar) in software, or writing an application that sends triangles to an OpenGL implementation?" CreationDate="2019-06-27T10:46:07.283" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12470" PostId="8946" Score="4" Text="Well, a cube has 6 faces to begin with, no? That's about the whole point of a cube map. However, I'm afraid your question is a little unclear. The &quot;advantages of using 3D directional vector&quot; as compared to what? That's just how cubemaps work. This largely comes down to *how* you learned about cubemaps and *what* you learned about them. Understandably just learning about their *existence* might leave you baffled what they are, but a proper source would tell you how they work and what they're for." CreationDate="2019-06-27T11:22:17.670" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12471" PostId="8947" Score="1" Text="@DanHulme I have edited the post with more specific details. (Rasterisation when using a graphics API like OpenGL or Vulkan)" CreationDate="2019-06-27T13:09:27.627" UserId="9902" ContentLicense="CC BY-SA 4.0" />
  <row Id="12472" PostId="8939" Score="0" Text="You're very welcome. :-)" CreationDate="2019-06-27T16:42:49.570" UserId="10837" ContentLicense="CC BY-SA 4.0" />
  <row Id="12473" PostId="8944" Score="0" Text="How can you tell? Is it the simple fact that the version string and core profile version don't match?" CreationDate="2019-06-27T16:46:37.807" UserId="10837" ContentLicense="CC BY-SA 4.0" />
  <row Id="12474" PostId="8944" Score="0" Text="No, it's because Mesa is an open-source OpenGL implementation. Usually you'd expect the GPU vendor's name to appear there." CreationDate="2019-06-27T18:47:18.390" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12476" PostId="8944" Score="0" Text="So, you're saying that Mesa is being my OpenGL driver instead of my Intel chip? I thought Mesa was a utility for getting OpenGL info from my hardware." CreationDate="2019-06-27T20:00:27.473" UserId="10837" ContentLicense="CC BY-SA 4.0" />
  <row Id="12477" PostId="8952" Score="0" Text="Great thanks, do you know where could I find information on some common partition methods?" CreationDate="2019-06-27T22:13:31.070" UserId="9902" ContentLicense="CC BY-SA 4.0" />
  <row Id="12478" PostId="8949" Score="1" Text="This looks like JPEG artefacts so I guess the image went through a JPEG compression step at one stage and that permanently introduced noise into the image. The red channel is blurred stronger because JPEG applies chroma subsampling." CreationDate="2019-06-28T06:16:58.960" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12479" PostId="8949" Score="0" Text="That's really useful and sounds like a very possible cause. If you add it as an answer I'll upvote it .&#xA;Is there any way to deconvolute this? I know JPEGs are lossy but we know a lot about the original image." CreationDate="2019-06-28T09:15:08.923" UserId="10849" ContentLicense="CC BY-SA 4.0" />
  <row Id="12480" PostId="8949" Score="1" Text="You can mostly clean it up. You can &quot;despeckle&quot; it in photoshop to get rid of the high frequency / random dots. The chroma subsampling is a bit trickier as some parts of the colour space are at lower resolution. As your image is monochrome we could cheat, and just capture the highest resolution colour channel (Green) and reconstruct the monochrome image from that. An even better way would be to convert RGB back to YCrCb space, and then keep only the Y/Luminance channel as these are usually encoded at 1:1 scale unlike the Cr/Cb colour channels." CreationDate="2019-06-28T09:28:53.893" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12481" PostId="8949" Score="0" Text="Great suggestions. BTW I am doing this automatically (using https://boofcv.org/ - a great Java library similar to OpenCV.) Maybe I should open a new question for this?" CreationDate="2019-06-28T09:43:00.090" UserId="10849" ContentLicense="CC BY-SA 4.0" />
  <row Id="12482" PostId="8949" Score="0" Text="Go for it, would be interesting to see other ideas." CreationDate="2019-06-28T09:51:08.477" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12484" PostId="8956" Score="0" Text="I don't think it has been clipped from a screen as it's common to many publishers and (I assume) subpixel rendering is device specific. It seems more likely that it't been automatically rendered to JPEG by a PDF production company (there are only ca 3 main ones) and they probably all use similar JPEG conversions." CreationDate="2019-06-28T11:26:56.937" UserId="10849" ContentLicense="CC BY-SA 4.0" />
  <row Id="12485" PostId="8957" Score="0" Text="[Triangle](https://www.cs.cmu.edu/~quake/triangle.html) is easy to use (if you're comfortable with calling command-line programs) and generates high-quality triangulations in 2D." CreationDate="2019-06-29T16:58:21.580" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="12486" PostId="8961" Score="1" Text="It took me a good 2 hours to understand this simple idea. Reading back and forth from your answer to your referenced documentation I finally understood what you meant by &quot;..indices of the points in the first list&quot;. I was thinking in the context of math here, and what indices could possibly mean in that context. But it's actually just indices in the programming context, i.e. indices of an array f.e.&#xA;In my case that means f.e. that the first i-value (7) just means &quot;this vertex points to the x-y-z-point with index 7 (which is the last one since I only have 8 points) in the x-y-z-list&quot;." CreationDate="2019-07-01T09:49:32.857" UserId="10863" ContentLicense="CC BY-SA 4.0" />
  <row Id="12487" PostId="8963" Score="1" Text="Ray marching simply solves for the intersection numerically. So it's more logical to compare it to ray casting as a technique. You can obviously use a combination of both for path tracing - you'll evaluate some points/intersections through ray marching, and others analytically." CreationDate="2019-07-01T18:44:27.753" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12488" PostId="8956" Score="0" Text="I was wondering where the red halo was coming from. For a monochrome input this image would only exist in the Y plane so there should be nothing in the chroma channels to cause that artefact. It's possible the image came from a scanner source and there was some chromatic aberration added in from that." CreationDate="2019-07-02T03:11:51.683" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12489" PostId="8963" Score="0" Text="^^ This. There are tons of other sources which imply that Ray marching is more of a an algorithm used for volume rendering, even wikipedia suggest that. Where as after getting to know it, it feels just, as you said, an algorithm to solve the intersection problem. I mean the core part of ray marching lies in using variable/fixed steps to keep marching and find the intersection point along that ray, the sampling along that ray part is more related to volumetric rendering and not to ray marching in general. Is that correct?" CreationDate="2019-07-02T07:07:58.827" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12490" PostId="8963" Score="0" Text="The articles and wikipedia are not wrong per se. The issue is that based on the context it can mean different things. You use ray marching both for volume rendering (since you march along the ray to integrate over space) and for numerical surface intersection.  It's just how some &quot;terms&quot; work in CG - they've been overloaded with various meanings in multiple papers." CreationDate="2019-07-02T07:28:14.170" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12491" PostId="8963" Score="1" Text="Okay but if we are using progressive Monte Carlo methods, we don't necessarily have to take `n` samples or steps along the ray to integrate over the space, right? We'd just probabilistically select a sample point along the ray and weigh the whole thing by the probability of selecting that point." CreationDate="2019-07-02T08:40:17.260" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12492" PostId="8963" Score="0" Text="Not always, you may want to march in probabilistic steps. Due to the attenuation law (or for anisotropic media) you may not want to just pick sample randomly over the ray, but rather march along it, so when the contribution becomes too small you can exit early. Just take the term as is - a very general term. The same way ray tracing is a very general term." CreationDate="2019-07-02T09:24:59.050" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12493" PostId="8964" Score="2" Text="&quot;*I cant imagine these libraries(QT,OpenGL,Vulkan) written by C*&quot; Well, Qt is written in C++ predominantly, so obviously that. It's also not a graphics library. Overall, this question is very unclear as to exactly what it is you're trying ask about. What do you mean by &quot;computer graphics library&quot; or &quot;image library&quot; and so forth?" CreationDate="2019-07-02T19:12:07.850" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12495" PostId="8965" Score="1" Text="I edited your question according to my understanding. Please check whether everything is still according to your intentions and [edit] if needed." CreationDate="2019-07-02T20:38:14.670" UserId="394" ContentLicense="CC BY-SA 4.0" />
  <row Id="12496" PostId="8965" Score="2" Text="Ideally you would run a fourier transform on the image and then filter those. As you would save some steps. But the skin frequency technique you see used in PS can be done recursively. Just  separate very high form rest i then separate high ones from rest etc  etc. The reason youd youse fourier is that this task gets easier and has a lower computationalb complexity (and you get a sinc basis instead of a gaussian one) I usually split my images to 2-4 bands when retouching." CreationDate="2019-07-02T20:52:46.170" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12497" PostId="8963" Score="1" Text="I dont think the term is overloaded. Its  just describing  how its done. I mean i can describe traveling to work as taking the bus, driving  car or walking. But that  doea not man my mode of transportation tells where  im going. So its valid to do other things with those modes of transportation." CreationDate="2019-07-02T23:14:55.690" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12498" PostId="8957" Score="0" Text="If you need something  quick and dirty to bolt in look up ear clipping. Though dicretisizing a random svg is not  so easy so find a tool that  does this." CreationDate="2019-07-02T23:19:09.030" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="12499" PostId="8957" Score="0" Text="Triangle looks exactly like it provides the needed control, but there is no fast and easy way to use it with svg files - need to convert it to a special triangle file format first - probably with a separate script." CreationDate="2019-07-03T08:09:17.757" UserId="10860" ContentLicense="CC BY-SA 4.0" />
  <row Id="12500" PostId="8968" Score="0" Text="Quads especially but (arbitrary) Polygons in general could be used in older versions (of OpenGL anyway). I am not exactly sure, but I think there also a hardware optimizations for the triangle related maths you do in rendering." CreationDate="2019-07-03T08:40:46.290" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="12502" PostId="8956" Score="0" Text="It would be very useful to know the source so I'll include an ImageJ ColoInspector3D output which might give some cludes" CreationDate="2019-07-03T08:55:09.597" UserId="10849" ContentLicense="CC BY-SA 4.0" />
  <row Id="12505" PostId="8965" Score="0" Text="This may be useful reading: https://en.wikipedia.org/wiki/Discrete_cosine_transform" CreationDate="2019-07-04T04:06:57.643" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12506" PostId="8965" Score="0" Text="Another alternative to a Fourier transform would be to use a wavelet decomposition, say either (the very easy to code) Haar Wavelet (https://en.wikipedia.org/wiki/Haar_wavelet) or, probably better , say a linear or cubic wavelet. (eg see http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.5600 )" CreationDate="2019-07-04T07:24:52.877" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12507" PostId="3848" Score="0" Text="`Even people who do not know about gamma tend to do it linear for bump/displacement for some reason`&#xA;Likely because those maps are generated." CreationDate="2019-07-05T08:44:56.573" UserId="5909" ContentLicense="CC BY-SA 4.0" />
  <row Id="12508" PostId="8971" Score="0" Text="Actually, OpenGL is only the specification. Mesa is only one implementation of the OpenGL specification. This just to be extra pendantic." CreationDate="2019-07-05T13:58:06.310" UserId="6647" ContentLicense="CC BY-SA 4.0" />
  <row Id="12509" PostId="8573" Score="1" Text="&quot;*Shaders is probably the biggest evolution of graphic cards in the last 10 years in my opinion.*&quot; Shaders date back to [the GeForce 3](https://www.khronos.org/opengl/wiki/History_of_Programmability#Enter_Register_Combiners) *at least* (you could charitably call the register combiners of the GeForce 1 &quot;shaders&quot;). So you're talking 2001. That's rather farther back than &quot;the last decade&quot;. Also, Tessellation shaders are not a required feature of Vulkan. And while indirect commands are, bundling multiple indirect commands is not." CreationDate="2019-07-05T13:59:58.003" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12510" PostId="8971" Score="1" Text="@Paul92 That's why I described Mesa as &quot;a software implementation of OpenGL&quot; in my answer. I don't think the distinction between the Khronos specs and single-implementation libraries like Qt and Gtk is really the crucial distinction in the context of the OP's question, so I've tried to write an answer that's correct without going off on that tangent. If you have a specific clarification you'd like to make, feel free to edit." CreationDate="2019-07-05T14:14:30.320" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12511" PostId="8958" Score="0" Text="OK. Let me reference this answer while poking around a bit with the Qt documentation and my program. I'm not creating the context myself, but am creating a Qt OpenGL Widget and trying to specify which level of functions I'd like loaded. Unfortunately I have no idea how Qt is actually creating the context behind the scenes and adhering it to the widget surface." CreationDate="2019-07-05T21:27:23.317" UserId="10837" ContentLicense="CC BY-SA 4.0" />
  <row Id="12512" PostId="8944" Score="0" Text="I've updated my drivers fully using Intel's update tool and, assuming [this article](https://www.gamingonlinux.com/articles/an-explanation-of-what-mesa-is-and-what-graphics-cards-use-it.9244) is correct, I'm using the integrated drivers for my hardware." CreationDate="2019-07-05T21:29:54.807" UserId="10837" ContentLicense="CC BY-SA 4.0" />
  <row Id="12513" PostId="8958" Score="1" Text="It seems that you have to set correct `QSurfaceFormat` (major and minor version numbers and profile to `Core`). By default, surface format is initialized for OpenGL 2.0, so you won't be able to use GL 4+ with Mesa drivers." CreationDate="2019-07-06T13:33:07.907" UserId="8796" ContentLicense="CC BY-SA 4.0" />
  <row Id="12514" PostId="368" Score="0" Text="You wouldn't need a stack to perform tail recursion, since tail-recursive functions can be converted to iterative functions. Does the OpenCL compiler not do this automatically?" CreationDate="2019-07-06T18:46:35.430" UserId="7553" ContentLicense="CC BY-SA 4.0" />
  <row Id="12515" PostId="8966" Score="1" Text="Triangles are the basic building block in geometry (mathematics) as well because they are a simplex - the smallest number of points needed to make a shape in the given dimension." CreationDate="2019-07-07T04:28:37.367" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="12516" PostId="8970" Score="0" Text="While I'm not sure it will help, I'm pretty sure this will be an interesting read for you: https://medium.com/@alen.ladavac/the-elusive-frame-timing-168f899aec92" CreationDate="2019-07-07T09:12:05.913" UserId="5215" ContentLicense="CC BY-SA 4.0" />
  <row Id="12517" PostId="8979" Score="3" Text="Do you know if this is coming from the specular or diffuse contribution. I would start by making your shader output one contribution at a time so you can narrow down the source of the problem." CreationDate="2019-07-08T07:57:21.467" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12518" PostId="8979" Score="0" Text="I'm not sure I understand how the textures are being read, it looks like you read normals from a texture in the geometry shader, and then use another texture (whose origin I don't understand) in the pixel shader. Can I see your varying and attribute declarations too, as well as the code which sets up the textures so I can see their format ?" CreationDate="2019-07-08T08:03:05.140" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12519" PostId="8979" Score="0" Text="@PaulHK No problem, I've updated the above code. Since this is deferred rendering, everything is flowing from top to bottom. The geometry pass outputs to multiple render targets in an off screen framebuffer then switching to the on screen framebuffer to combine all the gBuffers then render onto a single quad. Btw the geometry pass fragment shader is reading a normalmap (aka bump map) in tangent space then transformed to world space to be sent through a gBuffer to the lighting pass." CreationDate="2019-07-08T16:47:22.707" UserId="10886" ContentLicense="CC BY-SA 4.0" />
  <row Id="12520" PostId="8979" Score="0" Text="Also, it seems to be coming from the diffuse component." CreationDate="2019-07-08T16:56:24.740" UserId="10886" ContentLicense="CC BY-SA 4.0" />
  <row Id="12521" PostId="8982" Score="0" Text="Look at samples online, find youtube videos that talk about the code behind them, etc." CreationDate="2019-07-08T18:04:12.743" UserId="10816" ContentLicense="CC BY-SA 4.0" />
  <row Id="12522" PostId="8983" Score="1" Text="The cosine on rendering equation is due to foreshortening effect of the solid angle, not the Lambert law." CreationDate="2019-07-08T18:24:12.643" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12523" PostId="8983" Score="1" Text="These two cosine are basically same. If you consider the radiance(on radiance formula) to be the incoming, reordering the formula gives you: dPhi/dA = L dw cos(theta); in which dPhi/dA is dE." CreationDate="2019-07-08T18:29:26.377" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12524" PostId="8982" Score="0" Text="http://in1weekend.blogspot.com/2016/01/ray-tracing-in-one-weekend.html?m=1" CreationDate="2019-07-09T00:25:28.477" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12525" PostId="8986" Score="0" Text="There is 1 cosine term in the solid angle formulation of the rendering equation and it's due to Lambert's law. If you take the area formulation you get a second cosine term, but this time it's from the relationship of differential area element and solid angle." CreationDate="2019-07-09T09:56:19.093" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12526" PostId="8988" Score="0" Text="That's the third cosine :)" CreationDate="2019-07-09T12:58:14.213" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12527" PostId="8988" Score="0" Text="@ali There's no third cosine. There's only the one from Lambert's law, and the one I showed here which is from the projection of the differential area element onto the hemisphere. Other cosines may appear only in the brdf, but that is an entirely different matter." CreationDate="2019-07-09T13:15:34.370" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12528" PostId="8984" Score="0" Text="PS: Sorry about the changed notation (ωi,ωo vs. ω′,ω). I find this one more practical. I can rewrite it if it confuses you." CreationDate="2019-07-09T15:12:50.087" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="12529" PostId="8988" Score="0" Text="@eclmist is right, there are two cosine terms: 1.cosine(theta) which is the angle between receiving patch normal and incoming radiance. The second cosine(different from area solid angle) is implicit in the definition of Li which here is incoming radiance. In path tracing this is either a light source with a known radiance or unknown indirect radiance which is solved recursively.so no need to multiply by second cosine. If the incoming radiance weren't known and you had to calculate it you need to use radiance definition from point of view of the sender. https://en.wikipedia.org/wiki/Luminance" CreationDate="2019-07-09T15:47:02.163" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12531" PostId="8975" Score="0" Text="descTex.MipLevels = 0; Does that have anything to do with it ?" CreationDate="2019-07-10T05:47:55.130" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12533" PostId="8969" Score="2" Text="Non-planar quads would also be painful in that they can have 2 interection points per pixel, i.e. have an implicit silhouette edge." CreationDate="2019-07-10T09:21:15.757" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12534" PostId="8975" Score="0" Text="The doc. says: &quot;The maximum number of mipmap levels in the texture. See the remarks in D3D11_TEX1D_SRV. Use 1 for a multisampled texture; or 0 to generate a full set of subtextures.&quot; I guess I may an explicit number to see whether it changes anything..." CreationDate="2019-07-10T09:28:33.647" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="12535" PostId="8886" Score="0" Text="This question was effectively cross posted by you on stack overflow and has answer [here](https://stackoverflow.com/a/56457127/128511)" CreationDate="2019-07-10T14:54:16.410" UserId="4766" ContentLicense="CC BY-SA 4.0" />
  <row Id="12536" PostId="8395" Score="0" Text="[This page](https://webglfundamentals.org/webgl/lessons/webgl-3d-orthographic.html) describes drawing faces in different colors." CreationDate="2019-07-10T15:01:49.667" UserId="4766" ContentLicense="CC BY-SA 4.0" />
  <row Id="12538" PostId="8958" Score="0" Text="Thank you! I sat down today to pick this project back up and that last comment helped me get everything up and running. I'm now rendering under the 4.5 core profile. :-)" CreationDate="2019-07-11T22:17:40.833" UserId="10837" ContentLicense="CC BY-SA 4.0" />
  <row Id="12539" PostId="8958" Score="0" Text="I just realized we haven't talked specifically about the shading language version string. Given what's been said so far, can I assume they are read the same way? If I use a compatibility profile than the max shading language version I can use is 1.3, and if I use a core profile the max version is 4.5?" CreationDate="2019-07-11T22:28:13.490" UserId="10837" ContentLicense="CC BY-SA 4.0" />
  <row Id="12540" PostId="8970" Score="0" Text="The GPU buffering step would be gone in a software renderer, but you still have latency from double buffering. If you were to render directly to the &quot;front buffer&quot; you could get rid of that and have latency below 1 frame, although you need to race the scan out hardware." CreationDate="2019-07-12T02:33:44.320" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12541" PostId="8995" Score="0" Text="&quot;*Is there any way to leverage device caches directly in vertex shader?*&quot; Leverage them to do... what? I mean sure, it's obvious that your shader is attempting to compute the minimum and maximum values for the depth of the vertices in the rendering operation(s), but how exactly would &quot;device caches&quot; help here?" CreationDate="2019-07-12T17:33:08.110" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12543" PostId="8995" Score="1" Text="@NicolBolas, from my understanding, atomics on local memory are meant to be faster, so I hoped to just use them." CreationDate="2019-07-15T12:33:11.530" UserId="10806" ContentLicense="CC BY-SA 4.0" />
  <row Id="12544" PostId="9002" Score="0" Text="`discrim = (np.dot(rayDirection, eminc))**2 - ((np.dot(rayDirection, rayDirection))*(np.dot(eminc, eminc))-obj.radius**2)`, use `discrim = b * b-4*a*c`" CreationDate="2019-07-15T13:34:24.510" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12545" PostId="9002" Score="0" Text="@lightxbulb discrim is equivalent to b^2-4ac with the values for a,b and c substituted in." CreationDate="2019-07-15T16:26:11.760" UserId="10912" ContentLicense="CC BY-SA 4.0" />
  <row Id="12546" PostId="9002" Score="0" Text="It is not. It would be if you actually wrote `(np.dot(rayDirection, eminc))**2 - 4*((np.dot(rayDirection, rayDirection))*(np.dot(eminc, eminc)-obj.radius**2))`. So just put in `b * b - 4 * a * c`, and check whether it works." CreationDate="2019-07-15T18:37:41.120" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12547" PostId="8965" Score="0" Text="Is this what you want? https://david.li/filtering/&#xA;If it is, check out the code (lower right corner)." CreationDate="2019-07-15T18:39:59.303" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12548" PostId="9005" Score="1" Text="Can you try rendering the same thing with the code that goes with the book on github? Also post your scatter and random functions." CreationDate="2019-07-16T17:18:13.917" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12549" PostId="9005" Score="0" Text="@lightxbulb I hadn't checked the light's color beforehand. I just assumed it's 1.0f, but it had been 18.0f. I changed that and clamped all colors, and the issue is solved." CreationDate="2019-07-16T17:57:16.027" UserId="9391" ContentLicense="CC BY-SA 4.0" />
  <row Id="12550" PostId="9005" Score="1" Text="If you find the answer to your problem yourself, [self answers are encouraged here](https://computergraphics.stackexchange.com/help/self-answer)." CreationDate="2019-07-16T21:21:55.717" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="12551" PostId="9005" Score="1" Text="@trichoplax I have added an answer, but I can't mark it as an answer within 48 hours of posting the answer. I will mark it when I am able." CreationDate="2019-07-17T07:33:46.687" UserId="9391" ContentLicense="CC BY-SA 4.0" />
  <row Id="12553" PostId="9013" Score="0" Text="What I meant by the comment you copied is that the sensitivity function I have to work with is a function of Watts.   Radiance has units of W*m^-2*sr^-1.  So if I multiply the radiance value by the solid angle represented by the ray, then I would get an irradiance value, with units W*m^2.  I then multiply that by the surface area of the pixel that the ray originated from the get the Watts received, which I can then use in the sensitivity calculation." CreationDate="2019-07-18T17:42:09.853" UserId="10932" ContentLicense="CC BY-SA 4.0" />
  <row Id="12554" PostId="9013" Score="0" Text="But the part that is confusing me is that the solid angles of the BRDF and of the ray appear (at least to me) to be opposite one another.  The BRDF deals with solid angles whose vertex is at the point of intersection on the surface.  The rays on the other hand have a solid angle with its vertex at the camera (more specifically, somewhere on the pixel that it was sent from.  If no super sampling is done, then it originates just from the pixel center).  So at the intersection point, the ray forms a cone in one direction, while the BRDF forms a cone in the exact opposite direction" CreationDate="2019-07-18T17:45:01.117" UserId="10932" ContentLicense="CC BY-SA 4.0" />
  <row Id="12555" PostId="9013" Score="0" Text="You're not multiplying radiance by a solid angle, to get a different measure you're integrating out the steridian part. Granted you're integrating over the solid angle, but multiply the radiance with the solid angle doesn't make much sense. The BRDF is not something related to the camera, its simply a function that describes the scattering properties of a surface at some point. It just tells you how ray coming from direction A scatters (and vice-versa)." CreationDate="2019-07-18T17:49:38.960" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12556" PostId="9013" Score="0" Text="With graphics:  [Here is a simple diagram of a ray] (https://upload.wikimedia.org/wikipedia/commons/b/b2/RaysViewportSchema.png).  It is diverging away from the camera, so the solid angle of incoming light to the sensor represented by the ray has its vertex at the camera origin.  [This diagram of a BRDF](https://www.researchgate.net/profile/David_Delafosse/publication/256444960/figure/fig1/AS:297976355409923@1448054507535/Geometry-for-the-definition-of-the-BRDF_W640.jpg) shows that the solid angle diverging off the surface.  Which is opposite how the ray looks at that point" CreationDate="2019-07-18T17:50:41.910" UserId="10932" ContentLicense="CC BY-SA 4.0" />
  <row Id="12557" PostId="9013" Score="0" Text="I understand it has nothing to do with the camera, but the rays reflected off the object are diverging away from the point they reflected off of.  The camera rays however, are diverging from the camera, not the point they intersect." CreationDate="2019-07-18T17:52:01.383" UserId="10932" ContentLicense="CC BY-SA 4.0" />
  <row Id="12558" PostId="9013" Score="0" Text="It doesn't matter, BRDFs are usually symmetric (at least if they are to be physically based). Also a common convention is to have both directions point out in the BRDF: https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function&#xA;I still don't get what your question is really. Can you try and formalize this." CreationDate="2019-07-18T17:53:51.017" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12559" PostId="9013" Score="0" Text="Also, I'm not entirely sure what you mean that the steradian is being integrated out.  Each ray has some associated solid angle with it which if not accounted for, would mean that adding more and more rays per pixel would increase the brightness per pixel.  Giving each ray an appropriately scaled solid angle allows you to ensure adding more rays doesn't increase the actual signal, rather just increases sub pixel resolution.  At least, that is how it was explained to me." CreationDate="2019-07-18T17:54:07.007" UserId="10932" ContentLicense="CC BY-SA 4.0" />
  <row Id="12561" PostId="9013" Score="0" Text="[Here is a drawing of what I mean](https://i.imgur.com/OEUrblI.png).  Tracing the light forward makes perfect sense.  It is reflected in some manner, and is now diverging away from the reflection point (indicated by the green arrows).  The red line is the traced intersection point of the ray, which also makes sense.  However, it is my understanding that the rays are diverging from the camera, and so I represented them with the blue arrows.  This means that they are diverging in opposite directions which leaves me confused as to how the reflection value can be transfered to the ray." CreationDate="2019-07-18T18:02:47.287" UserId="10932" ContentLicense="CC BY-SA 4.0" />
  <row Id="12562" PostId="9013" Score="0" Text="@ChrisGnam See the edit. A possibly easier to understand analogy is through acceleration, speed, and distance. If you have the acceleration function for an object over time, you can find its speed at each point in time by integrating the acceleration (which integrates out 1/s, so from m/s^2 you get m/s). You can also integrate speed over time to get distance traveled (that gets rid of the other 1/s and you get from m/s to m)." CreationDate="2019-07-18T18:03:21.417" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12563" PostId="9013" Score="0" Text="@ChrisGnam The Bi-directional part in BRDF stands for the fact that it is symmetric. It shouldn't matter in which direction the radiance travels, since both are equivalent under the assumptions made." CreationDate="2019-07-18T18:05:31.493" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12564" PostId="9013" Score="0" Text="Thank you that does help somewhat.  Do you have any recommendation on textbooks or papers or anything that covers the math used here in rigorous, yet understandable terms?" CreationDate="2019-07-18T18:05:53.483" UserId="10932" ContentLicense="CC BY-SA 4.0" />
  <row Id="12565" PostId="9013" Score="0" Text="@ChrisGnam I recommend Advanced Global Illumination. But depending on your background, you may not find it rigorous enough, in which case you can refer to Eric Veach's thesis, Mathias Lang's thesis, or Christian Lessig's thesis (this one is the most abstract that I know of and the hardest). Refer to the acknowledgements section here for more resources: https://vchizhov.github.io/resources/ray%20tracing/ray%20tracing%20tutorial%20series%20vchizhov/index.html" CreationDate="2019-07-18T18:08:15.553" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12566" PostId="9018" Score="0" Text="What exactly do you want to plot?" CreationDate="2019-07-19T20:23:47.457" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12567" PostId="9018" Score="0" Text="any single variable or multi variable function,and I want to create my own data visualization tools." CreationDate="2019-07-19T20:26:56.277" UserId="10871" ContentLicense="CC BY-SA 4.0" />
  <row Id="12568" PostId="9018" Score="0" Text="I can recommend using graphics APIs like OpenGL, DirectX, Vulkan, WebGL. If you want resources on those just elaborate on which exactly and I can provide some. For a GUI, depending on which you pick from the above, there are multiple solutions, I can recommend Dear ImGUI for simplicity, or dat.GUI for WebGL." CreationDate="2019-07-19T20:30:08.763" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12569" PostId="9019" Score="0" Text="Please consider marking this as the accepted answer." CreationDate="2019-07-22T08:34:10.907" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="12570" PostId="9013" Score="0" Text="thank you for the help.  I've ordered the textbook and I'm looking forward to reading it!  I hate to ask one more question, but I must know.  Having calculated the reflected light off of the object, do I apply the inverse square law to that irradiance to obtain the irradiance received by the camera?  Or is that accounted for by the fact that the object is so far away that it takes up a small pixel area?" CreationDate="2019-07-22T13:01:49.317" UserId="10932" ContentLicense="CC BY-SA 4.0" />
  <row Id="12571" PostId="9013" Score="0" Text="@ChrisGnam Depends on whether you use the area or the solid angle formulation of the rendering equation. See my answer here: https://computergraphics.stackexchange.com/questions/9015/rendering-equation-in-terms-of-paths-rather-than-directions" CreationDate="2019-07-22T13:05:47.183" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12572" PostId="8966" Score="0" Text="not only tris, pathtracing/raytracing techniques does not require triangles https://codepen.io/strangerintheq/pen/OKVVOW" CreationDate="2019-07-22T13:06:26.703" UserId="6887" ContentLicense="CC BY-SA 4.0" />
  <row Id="12573" PostId="8992" Score="0" Text="Have you considered weighting the vertices so that the interpolation is skewed towards the higher values?" CreationDate="2019-07-24T21:08:26.137" UserId="10874" ContentLicense="CC BY-SA 4.0" />
  <row Id="12574" PostId="8992" Score="0" Text="@PixelArtDragon Not sure I got what you mean, but the interpolation is done within a triangle. Since the triangles can be extremely small (detailed mesh), I'll get one tiny patch colored." CreationDate="2019-07-24T21:41:56.077" UserId="6647" ContentLicense="CC BY-SA 4.0" />
  <row Id="12575" PostId="8992" Score="0" Text="Ah, I think I misread the first time. Is it possible to render to a much higher resolution and then apply some smoothing using another step? Also, given the large variation, maybe when storing the values use a logarithmic scale?" CreationDate="2019-07-24T22:24:55.253" UserId="10874" ContentLicense="CC BY-SA 4.0" />
  <row Id="12576" PostId="8992" Score="0" Text="Storing the values is not a problem. Let's imagine a more concrete example. In a room scene, with a lot of triangles, there is something very interesting in a table corner. I get this as a value associated with the vertex. I can draw with a texture based on this value, but the problem is that only the exact corner will be highlighted (due to small triangles). I'd like to highlight a quarter of the table, to be easier to see. This is a qualitative visualization, for human inspection, so the exact location is not really important." CreationDate="2019-07-24T22:31:48.110" UserId="6647" ContentLicense="CC BY-SA 4.0" />
  <row Id="12578" PostId="9027" Score="0" Text="Have you tried changing the workgroup size? Just a hunch but sometimes if you have a bigger WG size then your local memory can't hold all that data for the workgroup if you are using too many variables. Try reducing your workgroup size." CreationDate="2019-07-25T08:03:38.860" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12579" PostId="9027" Score="0" Text="Thank you for the suggestion, but sadly reducing the work group size doesn't help." CreationDate="2019-07-25T08:31:43.547" UserId="10952" ContentLicense="CC BY-SA 4.0" />
  <row Id="12580" PostId="9027" Score="1" Text="Did you check glGetError output?" CreationDate="2019-07-25T09:58:28.947" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="12581" PostId="9027" Score="0" Text="Try adding more and more until it occurs. Also be careful with while loops in glsl." CreationDate="2019-07-25T10:07:49.447" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="12582" PostId="9027" Score="0" Text="I have a  `glDebugMessageCallback` that reports all errors and I manually checked with `glGetError`, but no errors occur." CreationDate="2019-07-25T10:34:27.400" UserId="10952" ContentLicense="CC BY-SA 4.0" />
  <row Id="12583" PostId="9027" Score="0" Text="Can you pinpoint a little more exactly where it stops working? There's a lot of room between 100k and 1M." CreationDate="2019-07-25T12:29:52.620" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12584" PostId="9027" Score="0" Text="@ChristianRau it is quite inconsistent. I measure around 600k +/- 100k" CreationDate="2019-07-25T12:42:43.933" UserId="10952" ContentLicense="CC BY-SA 4.0" />
  <row Id="12585" PostId="9027" Score="0" Text="Simplify your loop and check if it works for simpler tasks for 1M and less, then gradually add more advanced stuff and check etc.." CreationDate="2019-07-25T12:54:55.350" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="12586" PostId="9027" Score="0" Text="Yeah, as I wrote above removing certain logic does indeed result in the shader working with 1M photons, but that doesn't solve the problem, because I need that logic. My current suspicion is that the NVIDIA driver or the OS has a timeout condition that kicks in. I am on SUSE Enterprise 12." CreationDate="2019-07-25T13:46:42.883" UserId="10952" ContentLicense="CC BY-SA 4.0" />
  <row Id="12587" PostId="9027" Score="0" Text="On Windows 7 timeout is detected after default two seconds of stall and generates a message. I actually turned if off for some reason. Experienced infinite while loop crash few times." CreationDate="2019-07-25T14:12:38.557" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="12588" PostId="9029" Score="0" Text="&quot; I collect all the color information from the voxel and blend together&quot;  Can we see code for this part ?" CreationDate="2019-07-26T06:26:05.423" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12589" PostId="221" Score="0" Text="That completely depends on the use case. Imagine your calculations take an hour to run. Wouldn't it be nice to just pregenerate a lookup texture with the results and use it on your GPU at runtime?&#xA;Some more simple things might be faster to do with ALU instructions nowadays (certain color transforms for example), but anything that requires heavy computation is always going to be faster using lookup textures." CreationDate="2019-07-26T08:47:53.070" UserId="5909" ContentLicense="CC BY-SA 4.0" />
  <row Id="12590" PostId="9029" Score="0" Text="Thanks, but I think I was able to find the issue. The texture being used for rendering and storing the entry/exit position of the ray had only limited precision. I am using the Tao framework for C# and the internal format being used was RGB instead of RGB32f, simply swithing to RGB32f improved my rendering considerably." CreationDate="2019-07-26T14:52:53.467" UserId="10955" ContentLicense="CC BY-SA 4.0" />
  <row Id="12591" PostId="9032" Score="0" Text="Can you be more specific? Are you looking to convert from something like Y'CbCr to RGB (or the opposite)? Or HSV or La*b*? It's certainly possible to convert between color spaces on the GPU without going to the CPU, but it's not clear what you're asking." CreationDate="2019-07-27T03:41:55.587" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12592" PostId="9032" Score="0" Text="@user1118321 hi, thank you for your answer. Well, I heard that each GPU decodes to a different color format. So what I need is to convert from this unknown color format to RGB8. I know it's a hard task to do it for all color formats, but I plan to support, for now, only the format outputted from the Jetson Nano board from NVIDIA. I don't know its output because the ffmpeg support for this board is not ready yet but I guess it's NV12 (the color format for CUDA). Can you tell more about how to do these color conversions?" CreationDate="2019-07-27T04:03:25.480" UserId="10964" ContentLicense="CC BY-SA 4.0" />
  <row Id="12593" PostId="9032" Score="0" Text="[This article on NV12](http://paulbourke.net/dataformats/nv12/) says, &quot;The NV12 image format is commonly found as the native format from various machine vision, and other, video cameras.&quot; The article has CPU code for converting from NV12 to Y'CbCr and then to RGB. I don't see why you couldn't modify that code to work on the GPU." CreationDate="2019-07-27T21:25:38.563" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12594" PostId="9032" Score="0" Text="@user1118321 thanks, that's a starting point. But how should I do this conversion in GPU? I have no idea, and I searched about rendering from GPU memory but couldn't find anything. Could you give me a starting point?" CreationDate="2019-07-27T22:37:21.833" UserId="10964" ContentLicense="CC BY-SA 4.0" />
  <row Id="12595" PostId="9032" Score="0" Text="OK, I've added an answer that goes into more detail." CreationDate="2019-07-27T23:23:53.340" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12596" PostId="9035" Score="0" Text="Thanks, I'm learning about frameBuffers and I kinda understood all you said. I think I can follow it after some days of study. There's just one thing that I didn't understand. After calling `ret = avcodec_receive_frame(avctx, frame);`, ffmpeg stores my decoded frame somewhere on GPU and has a pointer to it called frame, on CPU. How do I make this `frame` get into my OpenGL program? I believe this is what you meant by `Bind the NV12 texture to the texture unit you'll use in your shader`" CreationDate="2019-07-28T06:27:26.567" UserId="10964" ContentLicense="CC BY-SA 4.0" />
  <row Id="12597" PostId="9035" Score="0" Text="I've not used ffmpeg, so I don't know for sure, but if you want to avoid going from the CPU back to the GPU you'll need to get the texture ID from ffmpeg. Otherwise you can upload the texture from the CPU using `glTexImage2D()`." CreationDate="2019-07-28T14:51:26.820" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12599" PostId="9034" Score="0" Text="Do you get any warnings/errors?" CreationDate="2019-07-29T15:38:40.157" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="12600" PostId="9034" Score="0" Text="No, there are no warning and/or errors." CreationDate="2019-07-29T21:33:12.013" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="12601" PostId="9038" Score="2" Text="It’s been answered on stackoverflow already. https://stackoverflow.com/questions/15224095/gpu-pixel-and-texel-write-speed" CreationDate="2019-07-30T02:40:42.537" UserId="10980" ContentLicense="CC BY-SA 4.0" />
  <row Id="12602" PostId="9034" Score="0" Text="@ivokabel Actually as I remember correctly, there was message like &quot;expected a ')'&quot; but there were no actual problems with parentheses.&#xA;I haven't checked it with newest version. I will and I will get back with message. I guess it will be similar. However as I was checking HLSL code on online compilers it worked fine. Both with PS_5_0 as well as PS_6_0.&#xA;I am using June 2010 SDK on Windows 7 with Visual Studio 2015." CreationDate="2019-07-30T06:17:44.003" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="12603" PostId="9038" Score="1" Text="Since there's some really good answers on the SO question already, I'm going to close the question here to prevent information getting split up between the two sites. Please ignore the close reason in the banner!" CreationDate="2019-07-30T08:12:57.027" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12605" PostId="9039" Score="0" Text="Where did that come from: `the probability q should be a constant for paths of the same length, so that the Russian roulette estimator is unbiased.`?" CreationDate="2019-07-30T15:44:42.163" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12606" PostId="9039" Score="0" Text="@lightxbulb I added extra explanation for my confusion in the question" CreationDate="2019-07-31T04:47:18.577" UserId="10980" ContentLicense="CC BY-SA 4.0" />
  <row Id="12607" PostId="9039" Score="0" Text="What you wrote makes no sense. Even you `E[R]` expression is wrong. Look for better resources regarding it." CreationDate="2019-07-31T09:36:25.827" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12608" PostId="9039" Score="0" Text="@lightxbulb Can you elaborate what's wrong here? It's the same as the [one](http://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/Russian_Roulette_and_Splitting.html) on PBRT." CreationDate="2019-07-31T12:40:06.367" UserId="10980" ContentLicense="CC BY-SA 4.0" />
  <row Id="12609" PostId="9039" Score="0" Text="What they wrote is correct only if $E[F] = c$ or $c=0$. Since you mentioned neither of those assumptions - it is incorrect." CreationDate="2019-07-31T13:42:04.877" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12610" PostId="9039" Score="0" Text="You must have misunderstood something" CreationDate="2019-07-31T16:40:38.097" UserId="10980" ContentLicense="CC BY-SA 4.0" />
  <row Id="12611" PostId="9039" Score="0" Text="My bad, I seem to have made a mistake. I still don't get where your assumption that `q` should be the same for paths of the same length comes from." CreationDate="2019-07-31T17:17:06.103" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12612" PostId="5785" Score="0" Text="But.. some humans are Tetrachromats :)  https://en.wikipedia.org/wiki/Tetrachromacy" CreationDate="2019-08-02T02:20:11.820" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="12613" PostId="6267" Score="1" Text="`\frac{p_x}{d} = \frac{v_x}{v_x} \implies p_x = \frac{v_x\cdot d}{v_z} = \frac{v_x}{v_x \cdot \tan(\frac{\alpha}{2})}`. Probably you mean `\frac{v_x}{v_z}`?" CreationDate="2019-08-02T03:20:11.753" UserId="10992" ContentLicense="CC BY-SA 4.0" />
  <row Id="12614" PostId="6267" Score="0" Text="Well spotted. I fixed it" CreationDate="2019-08-02T08:17:37.810" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="12615" PostId="9043" Score="2" Text="The stratified sampler you describe generates point sets, not point sequences. There are many ways one could use such samples, but in your case it simply seems like you are looking for a sampler that generates point sequences." CreationDate="2019-08-03T14:33:35.367" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12616" PostId="9046" Score="0" Text="&quot;*The cubes in the image*&quot; Um, what cubes? I just see some seemingly arbitrary colors." CreationDate="2019-08-03T19:52:27.417" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12617" PostId="9046" Score="2" Text="Those look like colors baked into the textures." CreationDate="2019-08-04T03:05:42.683" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12618" PostId="1719" Score="2" Text="FYI if that matters, I've used your answer here https://blender.stackexchange.com/questions/146819/is-there-a-way-to-calculate-mean-curvature-of-a-triangular-mesh/147371#147371 but adding a weighting using the angle around p1. Don't know if you find that valuable? Anyway feel free to comment. Thanks." CreationDate="2019-08-05T17:26:04.553" UserId="11008" ContentLicense="CC BY-SA 4.0" />
  <row Id="12619" PostId="9039" Score="0" Text="I don't get that as well. I don't see any connection to the explanation you made with the claim that `q` should be constant for same length paths for RR to remain unbiased." CreationDate="2019-08-06T00:02:18.977" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12620" PostId="9039" Score="0" Text="From what I understand the unbiasedness doesn't depend on what probability you choose for termination. The fact that the Expected value of the RR estimator gives back the original doesnt depend on the probability used. Also it doesn't make sense either. A path of 3 length with a mirror in between should have higher amount of energy rather than a path with 3 diffuse surfaces. Why should the termination probabilty be same for both paths?" CreationDate="2019-08-06T00:45:02.067" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12621" PostId="9039" Score="0" Text="Hi, @gallickgunner. As for your example, I guess you mean we should set a higher termination probability for paths with diffuse surfaces. But that would intentionally lower the contribution of those paths and the final pixel color would be biased towards paths with specular surfaces. Actually, if q in the RR estimator is not a constant, E[R] is almost certain to be unequal to E[F]." CreationDate="2019-08-06T10:06:09.847" UserId="10980" ContentLicense="CC BY-SA 4.0" />
  <row Id="12623" PostId="9043" Score="0" Text="@lightxbulb That means they're not really meant as a continuous source of well distributed random numbers, and are only appropriate where sets make sense (e.g. sampling a pixel area)? So i guess things like sobol or halton sequences would be a better bet for an endless stream of good (quasi) random numbers then?" CreationDate="2019-08-06T17:58:46.600" UserId="10996" ContentLicense="CC BY-SA 4.0" />
  <row Id="12624" PostId="9039" Score="0" Text="Again E[R] equals E[F] because the probabilities get cancelled `E[R] = [ (1 - q)(E[F] - qc) / (1 - q) ] + qc = E[F]` Doesn't matter what probability you use here. Secondly we don't intentionally lower the contribution of the paths. If a random number doesn't fall below the termination probability it survives. If it survives we divide it by `(1- q)` The greater the value of `q` the smaller the value by which we divide. We actually accounted for that fact by increasing the radiance much more than in the specular surfaces scenario." CreationDate="2019-08-06T18:57:13.457" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12625" PostId="9043" Score="0" Text="I am not saying you cannot create a sequence from stratified samplers, you could use a technique known as padding for instance. It involves for example generating multiple 2d stratified pointsets and permuting the points in each, and then &quot;attaching&quot; them sequentially to get higher dimensional points. But yes, qmc sequences are usually better in that regard, mainly due to the better convergence under some light assumptions." CreationDate="2019-08-06T20:27:59.117" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12626" PostId="9052" Score="0" Text="It sounds like you're asking, &quot;I don't want to use a screen recording tool, I want to make my own.&quot;" CreationDate="2019-08-07T07:31:50.147" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12627" PostId="9054" Score="0" Text="&quot;*Suppose there is a frame, which is larger that the device memory*&quot; What do you mean by a &quot;frame&quot; here? In graphics terms, a &quot;frame&quot; is usually the amount of rendering and computation you need to do to display an image to the user within a given quantity of time. As such, &quot;frame&quot; doesn't really have a memory size. Are you talking about a frame *buffer*?" CreationDate="2019-08-07T13:51:28.967" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12628" PostId="9054" Score="0" Text="@NicolBolas, I think so, I just tried to sound generic. I meant the buffer, which we draw onto during the frame. I believe, its this thing https://www.khronos.org/opengl/wiki/Renderbuffer_Object in opengl. In Vulkan terminology it's several huge color attachments that do not fit into device memory." CreationDate="2019-08-07T14:38:51.070" UserId="10806" ContentLicense="CC BY-SA 4.0" />
  <row Id="12629" PostId="9052" Score="0" Text="@Dan Hulme I would like to process the frames further and keep them in-memory. Afterwards I would like to send them via TCP to another client. A screen recording tool seems unappropriate for this use case." CreationDate="2019-08-07T20:33:10.033" UserId="3377" ContentLicense="CC BY-SA 4.0" />
  <row Id="12630" PostId="9052" Score="0" Text="Processing the frames, buffering them, and sending them over the network is *exactly* what a screen capture tool like OBS does. It's open-source and has a plugin architecture, so you can use its capture architecture and write whatever code you like to consume the capture frames." CreationDate="2019-08-07T21:17:49.643" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12631" PostId="9052" Score="0" Text="@Dan Hulme Aha very interesting, I didn't know that. Would you mind writing a short answer so I can accept it? Maybe with a link to a description of OBS' plugin architecture? Thank you so much!" CreationDate="2019-08-08T16:37:10.803" UserId="3377" ContentLicense="CC BY-SA 4.0" />
  <row Id="12634" PostId="9068" Score="0" Text="This can be achieved through *instancing* and it's cheap. As in you won't be communicating with the cpu and back and forth hundred times. https://learnopengl.com/Advanced-OpenGL/Instancing" CreationDate="2019-08-13T07:14:29.650" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12635" PostId="9069" Score="0" Text="Cool!, I didn't know that the variables could be different for each vertices in the same pass. This worked for me." CreationDate="2019-08-13T10:32:55.080" UserId="11047" ContentLicense="CC BY-SA 4.0" />
  <row Id="12636" PostId="398" Score="0" Text="Depends on what you count. If you draw from both ends inwards, then Wu's algorithm does half the calculations but twice as many pixel writes. See Table 1 in Wu's paper, linked on Wikipedia. So if pixel writes are expensive, as is the case when writing to a TFT on a serial connection, then Wu's algorithm is more expensive than Bresenham's. (I must admit I do not see why Bresenham's algorithm cannot use symmetry too.)" CreationDate="2019-08-13T14:05:47.160" UserId="11049" ContentLicense="CC BY-SA 4.0" />
  <row Id="12637" PostId="9069" Score="0" Text="Can you answer one more question? Does this work will depth culling? I drew 100 spheres near and far, but the far ones can sometimes block the near ones." CreationDate="2019-08-13T14:57:27.837" UserId="11047" ContentLicense="CC BY-SA 4.0" />
  <row Id="12638" PostId="9069" Score="0" Text="you need a depth buffer to do proper per fragment depth testing." CreationDate="2019-08-13T15:46:49.400" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="12639" PostId="398" Score="1" Text="But I do agree with @Octopus, even accepting &quot;draw from one end to the other&quot;, the pseudocode is Wu's algorithm only if integer arithmetic is used throughout. Code I see online uses floating-point arithmetic, which is a significant change. In Wu's paper, the algorithm only uses integer arithmetic (or actually fixed-point arithmetic)." CreationDate="2019-08-13T15:54:21.287" UserId="11049" ContentLicense="CC BY-SA 4.0" />
  <row Id="12641" PostId="9069" Score="0" Text="Found the culprit. It's because my perspective zNear was set to 0." CreationDate="2019-08-13T18:17:13.733" UserId="11047" ContentLicense="CC BY-SA 4.0" />
  <row Id="12642" PostId="9072" Score="0" Text="Did you verify that your implementation allows you to use the swapchain image as a storage image?" CreationDate="2019-08-13T22:15:48.337" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12643" PostId="9072" Score="0" Text="Yeah, it allows. I've added push constants to the mix since I asked the question and am colouring each image of the swapchain the other colour to better see what's going on. Only image that is properly shown is the last swapchain image created, and the validation layers complain only about that image having the incorrect layout (the first barrier not working). Interestingly enough, if I change the first barrier to expect ePreinitialized layout, the program still works the same, but starts to complain about all of the swapchain images." CreationDate="2019-08-13T22:21:27.700" UserId="5215" ContentLicense="CC BY-SA 4.0" />
  <row Id="12644" PostId="9071" Score="0" Text="Try calling glDisable(GL_TEXTURE_2D) before calling gluSphere(...)" CreationDate="2019-08-14T03:10:55.630" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12646" PostId="9074" Score="0" Text="Because it is a rotation matrix. Look up the SO(3) group." CreationDate="2019-08-15T09:05:56.540" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12648" PostId="9078" Score="0" Text="I do not want to simplify the object. I just want to have a list of polygons that are not triangles to describe it. The more vertices these polygons have, the better." CreationDate="2019-08-16T09:36:10.287" UserId="11062" ContentLicense="CC BY-SA 4.0" />
  <row Id="12649" PostId="9077" Score="1" Text="Please describe what you mean by polygon, for instance are they planar?" CreationDate="2019-08-16T10:29:40.263" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="12651" PostId="9078" Score="1" Text="I'm confused. Isn't triangle a kind of polygon?" CreationDate="2019-08-16T10:59:35.247" UserId="10980" ContentLicense="CC BY-SA 4.0" />
  <row Id="12652" PostId="9078" Score="0" Text="@Looft But then you *are* simplifying your mesh since you're removing vertices from it (otherwise it's not clear how you want to create these &quot;polygons&quot;), under the requirement that you only want to simplify planar regions (which your question doesn't actually specify, but seems to be the most reasonable way to interpret it). But I agree that &quot;classic&quot; mesh simplification algorithms that work within the domain of triangular meshes might not work for your task out of the box. However, the principle of merging coplanar triangles stays the same, though." CreationDate="2019-08-16T12:28:03.290" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12653" PostId="9078" Score="0" Text="@Looft But on the bottom line, elaborating a little more in the question what exactly you're looking for, what conditions those &quot;polygons&quot; ought to satisfy and how (if at all) the polygonal object should differ from the triangular one in its geometry, might be helpful in getting more accurate answers." CreationDate="2019-08-16T12:31:57.190" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12654" PostId="9078" Score="0" Text="@ChristianRau Polygons can be triangles if necessary but as more vertices are used by a single polygon, fewer are needed to finish describing the 3D object." CreationDate="2019-08-16T12:45:20.717" UserId="11062" ContentLicense="CC BY-SA 4.0" />
  <row Id="12656" PostId="9077" Score="0" Text="Look up 3d convex hull algorithms. Note that this will work only for convex objects though. Otherwise you may start at a tri and grow planar regions, marking tris as visited along the way. The algorithm has linear complexity if you have adjacency information.&#xA;How does a &quot;non-planar polygon&quot; even work? Afaik the definition requires it to be planar." CreationDate="2019-08-16T19:02:54.550" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12658" PostId="9080" Score="0" Text="It should not matter what the starting triangle is." CreationDate="2019-08-16T20:42:21.830" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12659" PostId="9080" Score="0" Text="Maybe, but it does." CreationDate="2019-08-16T20:46:18.817" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="12660" PostId="9080" Score="0" Text="It doesn't since you grow a polygon until you can grow it no more - that procedure is finite and deterministic. You have a graph, where each node is a triangle and there's an edge in that graph between two nodes, only if the two corresponding triangles share an edge and are in the same plane. The algorithm to construct the largest polygon would make a polygon out of all connected components. Needless to say the solution is unique no matter from where you start. You iterate over all tris, and keep a list of the visited ones, the algo is O(N)." CreationDate="2019-08-16T21:44:00.660" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12661" PostId="9080" Score="0" Text="If you want to minimize the number of polygons the starting triangle is important, which the op prefers. The polygons do not have to be planar. Constructing the half-edge graph (or similar) is not O(n), so...." CreationDate="2019-08-16T21:57:27.353" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="12662" PostId="9080" Score="0" Text="I am assuming you already have an adjacency structure, so the algo when having it is O(N) - constructing one is a well studied problem either way.&#xA;Also the notion of non-planar polygons makes no sense to me, I am unaware of such a definition. So you need to define what a non-planar polygon is if you want to use such a term.&#xA;If you consider the algorithm I described you will note that &quot;the starting polygon&quot; is irrelevant, since it iterates over all triangles once either way. The solution is unique." CreationDate="2019-08-16T22:22:59.573" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12663" PostId="9077" Score="0" Text="Is there any ambiguity when it comes to polygons not on a single plane?" CreationDate="2019-08-16T22:39:48.813" UserId="11062" ContentLicense="CC BY-SA 4.0" />
  <row Id="12664" PostId="9081" Score="2" Text="At a guess I'd say that the way you're doing imageLoad in a loop with computed values is completely destroying the cache.  The perf difference between the 1080 and the 2080 may have to do with how much the per compute unit cache can hold.  A 32x32x32 16 bit image is 65k all on its own.  I'm pretty sure that's way bigger than the SM L1 cache on a 1080." CreationDate="2019-08-17T06:02:59.563" UserId="9306" ContentLicense="CC BY-SA 4.0" />
  <row Id="12665" PostId="9081" Score="0" Text="Any way I can deal with that? The goal is to have a much larger world loaded which would just make it worse..." CreationDate="2019-08-17T08:02:24.370" UserId="5215" ContentLicense="CC BY-SA 4.0" />
  <row Id="12666" PostId="9077" Score="0" Text="@Looft There's no such definition afaik. Try defining those. Otherwise there are infinitely many surfaces that have the edge of such a polygon - for instance minimal surfaces, or a surface that matches the triangulated version, etc. In general in graphics APIs non planar polygons have &quot;funny&quot; behaviour." CreationDate="2019-08-17T16:29:34.370" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12667" PostId="9071" Score="0" Text="Intuitive guess from looking at the images: Looks like you only get or show the red channel. Try to set the white color to grey and see if it turns up dark red." CreationDate="2019-08-19T08:32:36.173" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="12668" PostId="9079" Score="0" Text="You say with Euler angles you can just clear a given rotation but, AFAICS, with Euler you need to pre-choose the order the rotations are applied, e.g.let's assume X, then Y, then Z.  Each of these would have an associated angle of rotation (for ease of description, let's assume it's in degrees) and these angles are Xa, Ya, and Za.  What I don't understand is the claim that you can just 'remove' an arbitrary rotation about Y, because after Xa has been applied, the &quot;Y&quot; rotation is around a different frame of reference. If, e.g, Xa=90, then Y could now be pointing along the original Z axis." CreationDate="2019-08-19T11:09:56.097" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12669" PostId="9079" Score="1" Text="@SimonF I thought I saw some animators do this in Blender3d, remove a specific euler rotation channel for specific bone from their animation and the rotation along other axes didn't get affected by it. Unfortunately I haven't done animations with euler angles myself to confirm this. Maybe I'm mistaken, because what you are saying makes sense, it shouldn't work. I'll have to check it myself." CreationDate="2019-08-19T12:35:10.913" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="12670" PostId="2223" Score="0" Text="You might be interested in this book: http://www.pmp-book.org/" CreationDate="2019-08-19T13:58:44.780" UserId="31" ContentLicense="CC BY-SA 4.0" />
  <row Id="12671" PostId="9083" Score="0" Text="I have found this: &#xA;&#xA;https://raytracing-docs.nvidia.com/optix_6_0/whitepaper/nvidia_optix_TOG_v29_n4.pdf&#xA;&#xA;where in section 4 they mention:&#xA;&#xA;&quot; After execution of the ray tracing kernel has finished, its result data can be used by the application.  Typically, this involves reading from output buffers filled by one of the user programs or displaying such a buffer directly, e.g., via OpenGL.&quot;&#xA;&#xA;Does anyone know how OpenGL would gain access to CUDA memory?" CreationDate="2019-08-19T22:26:10.607" UserId="11035" ContentLicense="CC BY-SA 4.0" />
  <row Id="12672" PostId="9083" Score="0" Text="This could also be what I'm looking for: https://devtalk.nvidia.com/default/topic/1024404/optix/binding-an-optixtexture-to-opengl/" CreationDate="2019-08-19T22:39:31.853" UserId="11035" ContentLicense="CC BY-SA 4.0" />
  <row Id="12673" PostId="9083" Score="0" Text="I think your only choice is CUDA-OpenGL interop. If python doesn't support it start working in C++. I believe python is more suited for data science kind of stuff. If you want to learn and implement graphics algorithms, C++ has more benefits as more APIs and libraries are available for it" CreationDate="2019-08-20T08:41:32.043" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12674" PostId="9083" Score="0" Text="Will definitely port it to c++. I took it as a challenge to do this with as much python code as possible, but I have to go the interop direction." CreationDate="2019-08-20T08:47:41.770" UserId="11035" ContentLicense="CC BY-SA 4.0" />
  <row Id="12675" PostId="109" Score="0" Text="Shaders can also be separated into modules using [glslify](https://github.com/glslify/glslify), though it only works with node.js." CreationDate="2019-08-20T18:34:02.520" UserId="7553" ContentLicense="CC BY-SA 4.0" />
  <row Id="12676" PostId="9086" Score="1" Text="Doing something like Height = 1 - abs( Perlin(..) ); noise would produce bowl like shapes with ridges. Although the ridges will be sharp." CreationDate="2019-08-21T12:23:01.487" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="12677" PostId="9067" Score="0" Text="Just wanted to ask, wouldn't the shadow terminator also depend on the intensity of the light? I mean theoretically no, but practically speaking if there is a very dim light we wouldn't be able to perceive the slightly lit up surface above the terminator making it look like the terminator was way up ?" CreationDate="2019-08-22T06:00:31.150" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12678" PostId="8989" Score="0" Text="You are computing the ray direction in camera space, but the ray origin is in world space? `eye_ray.dir = normalize(vec4(x,y,z,0));&#xA;    eye_ray.dir = normalize((main_cam.view_mat * vec4(eye_ray.dir)));&#xA;    eye_ray.origin = main_cam.eye;`" CreationDate="2019-08-22T07:02:12.907" UserId="8394" ContentLicense="CC BY-SA 4.0" />
  <row Id="12679" PostId="8989" Score="0" Text="@Nadir - no actually the view mat, sorry for the poor naming, is actually the `view2world_mat` so it converts the direction from camera space to world space." CreationDate="2019-08-22T07:37:07.590" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="12680" PostId="9087" Score="0" Text="On iOS the requirement that PVRTC textures be square was  completely artificial software constraint. AFAIK all versions of the (PowerVR) HW supported *rectangular* power-of-two sizes. Some would even have handled non-power of 2. The latter is definitely exposed via PVRTC-II formats (on non IOS systems)&#xA;&#xA;As for PO2, there *may* still be some advantages for cache-alignment. I'd have to think about it, but it may also help with the lower res MIP map levels." CreationDate="2019-08-22T08:59:43.630" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12681" PostId="9091" Score="0" Text="Do you have the screen coordinate positions of the vertices + UV values,  or are you also wanting to solve/synthesise those as well?" CreationDate="2019-08-22T09:15:53.483" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12682" PostId="9086" Score="0" Text="Not really a rendering comment, but re *&quot;these variations do NOT come from the wood texture itself, but from the way it was cut&quot;*, surely the timber would originally have been planed flat at the sawmill, and this is just natural distortions happening over time due to the timber's propeties?&#xA;BTW +1 for perlin - we used it years ago to add distortions to timber grain etc" CreationDate="2019-08-22T09:22:39.330" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12683" PostId="9067" Score="1" Text="@gallickgunner That's true. You should be able to tell the difference though, because normally you only get the $L.N$ diffuse contribution approaching zero towards the terminator (or whatever BRDF you use), whereas in this case, you'd also see the $\tfrac{1}{r^2}$ fall-off of the light intensity. You could work out the expected pixel values on the back of an envelope if you could be bothered, but it's probably easier to just debug the code." CreationDate="2019-08-22T09:25:33.127" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="12684" PostId="9091" Score="0" Text="@SimonF thanks for that good question: did not know what UV means, but just discovered that there is also the concept of &quot;UV maps&quot;... if you have something that can figure out the screen coordinates + UV map itself that would be great, else it is possible to design them beforehand" CreationDate="2019-08-22T09:29:02.993" UserId="11084" ContentLicense="CC BY-SA 4.0" />
  <row Id="12685" PostId="9087" Score="0" Text="Also, PO2 textures make it easier to do Morton order, which is generally beneficial for texture mapping, although block-based/tiled storage can get much of the benefit ( See Jim Blinn's &quot;The Truth About Texture Mapping&quot; - unsure if there's a non-paywalled version)" CreationDate="2019-08-22T09:33:17.560" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12686" PostId="9091" Score="2" Text="I'm just trying to understand more precisely what the problem is that you're trying to solve. Perhaps you could add a diagram/image to your question?" CreationDate="2019-08-22T09:35:32.187" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12687" PostId="9091" Score="0" Text="Question edited to give some more background information." CreationDate="2019-08-22T09:43:24.417" UserId="11084" ContentLicense="CC BY-SA 4.0" />
  <row Id="12688" PostId="9090" Score="0" Text="Or even better, change the texture environment to `GL_REPLACE` rather than `GL_MODULATE`, if that is really the problem. (Provided entirely doing away with this legacy fixed function stuff is out of the question already.)" CreationDate="2019-08-22T11:41:41.110" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12689" PostId="9090" Score="0" Text="@ChristianRau Why is that better?" CreationDate="2019-08-23T00:13:32.683" UserId="2316" ContentLicense="CC BY-SA 4.0" />
  <row Id="12690" PostId="9090" Score="0" Text="Because that's the actual problem and a more dirtect solution. You don't *want* to combine your texture's color with a constant color in this scenario anyway. So rather than setting the constant to 1, changing the texture to actually *not* combine with the color feels like the more natural solution to the problem. (If anything, though, it might at least be good to add to the answer that *that*, the texture environment, is the actual cause of the problem, if it is.)" CreationDate="2019-08-23T00:16:36.707" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="12691" PostId="9090" Score="0" Text="@ChristianRau I'd wager that most fixed-function apps draw un-colour-modulated textures by using GL_MODULATE and a white colour. That is natural because it's the default." CreationDate="2019-08-23T00:19:22.643" UserId="2316" ContentLicense="CC BY-SA 4.0" />
  <row Id="12692" PostId="9093" Score="0" Text="Are you asking about Vulkan or D3D12? Because Vulkan doesn't have &quot;rate per instance&quot;; are you talking about instanced vertex arrays?" CreationDate="2019-08-23T13:53:06.893" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12693" PostId="9093" Score="0" Text="I meant VK_VERTEX_INPUT_RATE_INSTANCE @NicolBolas" CreationDate="2019-08-23T16:59:22.947" UserId="11089" ContentLicense="CC BY-SA 4.0" />
  <row Id="12694" PostId="9098" Score="0" Text="Thank you for the thoughtful reply. What you're saying makes sense, but it's bugging me that my other depth shaders don't have this issue. I tried copy/pasting the content of a shader that doesn't suffer this issue into the cube map depth shader (and deleting the geometry shader), and the problem persists, suggesting that it's not the shader text that's the issue. The only difference is that this shader is used with cube map textures attached. I've updated my original question to display my buffer code... Am I missing something obvious?" CreationDate="2019-08-25T18:51:28.070" UserId="11099" ContentLicense="CC BY-SA 4.0" />
  <row Id="12695" PostId="9098" Score="1" Text="@b1skit Are the other shaders also writing to a 32F texture? One thing that can cause recompiles is the precision of the output format. Some hardware needs a different instruction to export at full 32F precision versus 8- or 16-bit." CreationDate="2019-08-27T04:34:31.510" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="12697" PostId="9098" Score="0" Text="It seems like @b1skit has narrowed it down to being a cube map. Assuming that the driver conflates the vertex and geometry stages, the recompilation might depend on `gl_Layer`: it can be used to point at cube faces, as well as texture array slots. Perhaps that's it - different native ISA code for export based on whether a single texture or an array is bound?&#xA;&#xA;In any case, I don't think you're missing anything &quot;obvious&quot;. It's a black box, you'd need to ask your IHV for a concrete explanation.¯\_(ツ)_/¯" CreationDate="2019-08-27T16:38:09.997" UserId="2817" ContentLicense="CC BY-SA 4.0" />
  <row Id="12699" PostId="9101" Score="2" Text="Discrete math is not enough. I also have no idea what kind of CS degree has no linear algebra nor calculus." CreationDate="2019-08-29T06:32:43.507" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12700" PostId="5388" Score="0" Text="You should accept your answer instead : )" CreationDate="2019-08-29T08:22:49.713" UserId="11110" ContentLicense="CC BY-SA 4.0" />
  <row Id="12701" PostId="9104" Score="1" Text="The red, green, and blue channels aren't grey. They are, respectively, black to red, black to green, and black to blue.&#xA;Hold a magnifying glass up to your (LCD) monitor and you'll see the individual channels." CreationDate="2019-08-29T15:31:35.467" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12702" PostId="9104" Score="0" Text="okay, I have another doubt, maximum of each is represented as 255 then what mystery goes behind each channel that every 255 corresponds to a different color." CreationDate="2019-08-29T15:36:08.670" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="12703" PostId="9104" Score="0" Text="I don't understand your question." CreationDate="2019-08-29T15:38:11.907" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12704" PostId="9104" Score="0" Text="The value of each channel ranges from 0-255, then what makes them map to each color channels or in other words why each channel's same value is different from each other" CreationDate="2019-08-29T15:41:35.923" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="12705" PostId="9104" Score="1" Text="Do you mean, given an RGB triple, {128, 35, 199}, which is a medium bright purple, why does the 128 affect red?  Because thats how the hardware is wired up." CreationDate="2019-08-29T15:54:36.640" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12706" PostId="9104" Score="1" Text="Yes @SimonF, I meant that only." CreationDate="2019-08-29T16:12:03.933" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="12707" PostId="9103" Score="2" Text="I don't know about your research, but if you're going to bother to talk about speed benefits, it would lend you some weight to talk about *actual* speed benefits, as measured by real benchmarks, rather than hypothetical &quot;speed benefits&quot; as measured by hardware stats." CreationDate="2019-08-29T18:28:18.193" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12708" PostId="9103" Score="0" Text="I guess you are right about that. The problem is, that I have one specific differential equation and results may vary for different equations. But I also don't know for sure, because the equation come from a mathematician while I am the programmer in this project. A selfmade benchmark would be my last option, but an indicator from hardware specs would be a good overview." CreationDate="2019-08-30T16:10:45.693" UserId="11111" ContentLicense="CC BY-SA 4.0" />
  <row Id="12710" PostId="9115" Score="0" Text="*&quot;bounced off a mirror&quot;*?  IIRC there is no &quot;mirror&quot; in an CRT.   To control the direction of the beams there are charged plates at the back of the 'tube'.  A 'shadow mask' at the screen end stops electrons intended to illuminate, say, a red phosphor from iluminating the green or blue one.  see. e.g. https://image.slidesharecdn.com/rasterscandisplays-140118014413-phpapp02/95/raster-scan-and-raster-scan-displays-27-638.jpg?cb=1390009516&#xA;&#xA;Please correct your post." CreationDate="2019-09-05T09:44:39.790" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12711" PostId="9116" Score="0" Text="This is a difficult, under-determined problem. Perhaps look at minimizing&#xA;the *Willmore energy* of a closed surface that approximates the tongue. The literature on this topic might give you ideas." CreationDate="2019-09-05T12:40:36.113" UserId="4748" ContentLicense="CC BY-SA 4.0" />
  <row Id="12712" PostId="9116" Score="2" Text="2 points are not enough to accurately represent a tongue model, even if you know that they are points on some manifold with rigidity constraint. The closest thing I can think of is Sumner's paper on deformation transfer." CreationDate="2019-09-05T15:00:02.133" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12713" PostId="9117" Score="1" Text="There are infinitely many solutions to your problem, you may want to impose extra constraints." CreationDate="2019-09-05T15:01:48.870" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12715" PostId="9119" Score="1" Text="The above, together with a modified version of the YUV to RGB formula found here: https://www.fourcc.org/fccyvrgb.php (which seems to assume that Y, U and V are in the 0-255 range so I had to scale the numbers your answer produces) produced an acceptable result." CreationDate="2019-09-05T19:07:34.557" UserId="11142" ContentLicense="CC BY-SA 4.0" />
  <row Id="12716" PostId="9115" Score="0" Text="Please look up reflective vs refractive CRT and specifically the Schmidt optical system. Also Texas Instruments had a micro-mirror CRT that was released using mirrors rather than electrostatic deflection plates.&#xA;Either way, I don't feel this aspect of the discussion helps the original poster." CreationDate="2019-09-05T22:46:32.743" UserId="11136" ContentLicense="CC BY-SA 4.0" />
  <row Id="12717" PostId="9115" Score="0" Text="*&quot;Texas Instruments had a micro-mirror CRT that was released using mirrors &quot;*  Do you mean the DLP technology https://www.ti.com/dlp-chip/getting-started.html ? IIRC that wasn't a CRT - it was all optical." CreationDate="2019-09-06T13:35:36.817" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="12718" PostId="9122" Score="0" Text="&quot;*Is there any way to disable both depth clamping and clipping on older drivers*&quot; If there was a way, why would the extension need to exist?" CreationDate="2019-09-06T16:11:12.357" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="12720" PostId="9124" Score="0" Text="Could you mention which specific computer graphics book by Shirley you are quoting from? (There are several.)" CreationDate="2019-09-07T18:52:43.267" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="12721" PostId="9128" Score="0" Text="Minor nit: you don't need $g \neq f$ almost everywhere; you just need that to be true more than almost nowhere (i.e. on a set of strictly positive measure)." CreationDate="2019-09-07T22:49:00.403" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="12722" PostId="9128" Score="0" Text="@NathanReed I am not sure 'almost nowhere' is a term but I got what you mean. You are correct, since I require the functions to not agree only a measurable subset of the interval for $&gt;$ to hold. Good catch. I won't edit it, since it may become confusing me trying to explain what almost nowhere is. I hope interested readers will see your comment below." CreationDate="2019-09-07T22:55:02.060" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12723" PostId="9128" Score="0" Text="As a matter of fact, since $f$ and $g$ are required to be continuous, I think it suffices to require $g \neq f$ _somewhere_. That would automatically imply $g \neq f$ on a nonzero interval around that point." CreationDate="2019-09-07T23:01:06.430" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="12724" PostId="9128" Score="0" Text="@NathanReed Which brings us to another point - that it's $f'' \ne g''$ which is required and not $f \ne g$ (though this is sufficient). There's no continuity constraint on $g''$ on the other hand, so the &quot;almost nowhere&quot; constraint is valid for this. I wonder whether it's beneficial including that in the answer however." CreationDate="2019-09-08T00:40:45.330" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12725" PostId="9124" Score="0" Text="It is the third version of &quot;Fundamentals of computer graphics&quot; published by CRC Press." CreationDate="2019-09-08T02:58:38.843" UserId="9522" ContentLicense="CC BY-SA 4.0" />
  <row Id="12726" PostId="9127" Score="0" Text="Thanks for the answer. I really like this book and also the ray tracing in weekends series." CreationDate="2019-09-08T03:00:19.190" UserId="9522" ContentLicense="CC BY-SA 4.0" />
  <row Id="12727" PostId="9130" Score="0" Text="Try my fork of the code: https://github.com/vchizhov/InOneWeekend-1&#xA;If it works it's probably due to the rng, in which case you can use the random() from my fork in rng.hpp. We discussed this in one of the issue, but it still hasn't been implemented." CreationDate="2019-09-08T16:52:59.573" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12728" PostId="9131" Score="0" Text="&quot;other implementations can orbit around any arbitrary point in a scene and the camera stays 'still'.&quot; - please elaborate." CreationDate="2019-09-08T21:52:31.650" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12729" PostId="9130" Score="2" Text="Also shouldn't you `ofs &lt;&lt; r &lt;&lt; &quot;\t&quot; &lt;&lt; g &lt;&lt; &quot;\t&quot; &lt;&lt; b &lt;&lt; &quot;\n&quot;;`?" CreationDate="2019-09-08T21:57:14.157" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12730" PostId="9131" Score="0" Text="Imagine several objects in a scene and all those objects are placed around the origin. The camera looks at the origin (which is in the center of the image plane). Normally, the camera orbits the origin (which is also the pivot point), but by picking a different pivot point, the camera remains looking at the origin, but all the objects rotate around the pivot point." CreationDate="2019-09-09T00:07:10.663" UserId="11153" ContentLicense="CC BY-SA 4.0" />
  <row Id="12731" PostId="9122" Score="0" Text="@NicolBolas, yeah, I thought about it. Just wondered maybe there is a way to achieve this using some other transform in the pipeline or disable them both by enabling some other pipeline feature. But now as you commented with the same idea about rationale behind the extension, I see that chances are slim." CreationDate="2019-09-09T07:04:04.573" UserId="10806" ContentLicense="CC BY-SA 4.0" />
  <row Id="12732" PostId="9131" Score="0" Text="So you want a collection of objects revolving around some point in space?" CreationDate="2019-09-09T08:01:41.540" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="12733" PostId="9134" Score="0" Text="You mentioned exactly the bit I don't get, where is this &quot;embedding&quot; visible in the Steklov operator? Is it in the domain of the differential operator?" CreationDate="2019-09-09T09:03:40.807" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="12734" PostId="9134" Score="1" Text="The Steklov operator looks like it's defined on a volume in (around?) the actual mesh surface, and using the derivative along the normal to the mesh, ie the whole thing relies not just on points *on* the mesh/manifold but also the surrounding space into which it's embedded." CreationDate="2019-09-09T13:36:20.420" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="12737" PostId="9131" Score="0" Text="Amongst other things, yes. But I want to experiment with this stuff to help me understand. I guess what I'm really after is a way to determine the vector from a pixel on the image plane to any point in the scene and to know if that's even possible." CreationDate="2019-09-09T22:07:22.533" UserId="11153" ContentLicense="CC BY-SA 4.0" />
  <row Id="12738" PostId="9135" Score="0" Text="This seems like a very vague question that will attract mostly opinion-based answers. What does &quot;worth it&quot; mean here? Are you asking whether it can be done as fast as MSAA? Or with better or at least similar quality? What does &quot;worth it&quot; mean to you?" CreationDate="2019-09-10T01:45:46.637" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="12739" PostId="5846" Score="0" Text="I recommend using (Open)Simplex Noise instead of Perlin Noise, because you'll have 'integer slices' with Perlin Noise. Perlin (top) compared to OpenSimplex (bottom): https://i.stack.imgur.com/Rlib0.png" CreationDate="2019-09-10T10:00:24.460" UserId="11158" ContentLicense="CC BY-SA 4.0" />
  <row Id="12740" PostId="9135" Score="0" Text="@user1118321 in understand that it is a bit subjective. I mean in my personal testing  with even a very basic FXAA implementation (averaging 8 nearest pixels for final color) my performance was slower than 4x MSAA. And the visual quality of MSAA is far better." CreationDate="2019-09-10T11:08:55.880" UserId="11155" ContentLicense="CC BY-SA 4.0" />
  <row Id="12741" PostId="9137" Score="0" Text="Thank you for shinning light on the Specular scenario limitation with MSAA (no pun intended). I hadn't thought about that. In my testing i have found that MSAA is both faster and better visual result for my use case. &#xA;Only case when FXAA could work is on a low end phone where MSAA is just not an option, there, using a crude FXAA could be faster with some respite from jaggies." CreationDate="2019-09-10T11:22:38.790" UserId="11155" ContentLicense="CC BY-SA 4.0" />
  <row Id="13743" PostId="9139" Score="0" Text="I'm unclear on the purpose of your `screen_to_world` function. Yes, I know what it does, but I'm not clear on why you're using it. You're drawing a quad; the *only reason* you're drawing a quad is to make the fragment shader execute for each pixel. What is the point of wanting the world-space position of the *quad*? If you're doing ray-tracing, then what you need is the ray (position and direction) from the camera to that pixel. And that has nothing to do with the actual geometry you drew (though `gl_FragCoord` can be helpful in computing it)." CreationDate="2019-09-11T01:54:29.300" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13744" PostId="9131" Score="0" Text="A collection of objects rotating around another object, can be done through hierarchical transformations. In your case `T_pivot * R_pivot * T_model * pos`. Transforming screen-space coords to world coords is an entirely different problem. There are many articles on this, here's one that includes it: https://mynameismjp.wordpress.com/2010/09/05/position-from-depth-3/" CreationDate="2019-09-11T08:29:58.420" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13745" PostId="9142" Score="1" Text="Can you read the existing pixels of the image before deciding what RGB to give it? If so, you can just do the convolution yourself." CreationDate="2019-09-11T10:46:21.440" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="13746" PostId="9142" Score="1" Text="Unfortunately I cant, I was looking for an RGB combination that resembles of a blurred pixel. Right now I'm testing it with (192,192,192) which gives you a light shade of grey patch on top of the face." CreationDate="2019-09-11T10:55:59.300" UserId="11165" ContentLicense="CC BY-SA 4.0" />
  <row Id="13747" PostId="9139" Score="0" Text="`screen_to_world` is used when a raytraced object is surrounded by a hull of polygons. I render the polygons (rather than the whole screen) and then use the world-space coordinate of the fragment of the polygon to construct a ray which may or may not hit the object behind it. If I can execute a fragment shader for every fragment of the screen and obtain the world-space coordinate of that fragment without using this function, then great. I was just showing you my approach so far, and how it fails for the full-screen case." CreationDate="2019-09-11T12:27:33.697" UserId="8233" ContentLicense="CC BY-SA 4.0" />
  <row Id="13748" PostId="9139" Score="0" Text="My point is that you don't need the &quot;the world-space coordinate of the fragment of the polygon&quot; in order to &quot;construct a ray&quot;. You only need the ray *itself*, which can [easily be constructed in camera-space](https://alfonse.bitbucket.io/oldtut/Illumination/Tut13%20Correct%20Chicanery.html), without ever knowing anything about the &quot;hull of polygons&quot;. You're making everything much harder than it needs to be." CreationDate="2019-09-11T13:26:21.597" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13749" PostId="9139" Score="0" Text="But I can only construct a ray in camera-space if I am raytracing an object which is also described in camera-space, right? And even then I still need the frament coordinate to be in camera-space, I still to transform gl_FragCoord to undo the projection." CreationDate="2019-09-11T13:29:44.690" UserId="8233" ContentLicense="CC BY-SA 4.0" />
  <row Id="13750" PostId="9139" Score="0" Text="... why *wouldn't* your object/scene be in camera-space, if that's the space that's trivial to compute your ray in? Alternatively, if it's that important (and you want to disregard all the [warnings about direct usage of world-space](https://alfonse.bitbucket.io/oldtut/Positioning/Tut07%20The%20Perils%20of%20World%20Space.html)), world-space is a single matrix transform away from camera space. It's still less work than your `screen_to_world` function." CreationDate="2019-09-11T13:32:36.057" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13751" PostId="9139" Score="0" Text="Because world-space is &quot;natural&quot; to me. But yeah I can do it in camera space. But then I want to convert gl_FragCoord to camera space [which looks like a lot of work](https://www.khronos.org/opengl/wiki/Compute_eye_space_from_window_space), so I'm confused as to why you think doing in camera space will be any simpler." CreationDate="2019-09-11T13:44:27.710" UserId="8233" ContentLicense="CC BY-SA 4.0" />
  <row Id="13752" PostId="9008" Score="1" Text="There's a whole section on applications in the paper." CreationDate="2019-09-11T18:30:16.783" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13753" PostId="9008" Score="0" Text="I was talking about something more in layman terms. I wouldn't exactly count logarithmic map as something a layman can understand, sure you can compute it, but what can you do with it?" CreationDate="2019-09-12T07:53:46.500" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="13754" PostId="9008" Score="0" Text="Too many things. An excerpt from the paper: ```As observed by Schmidt et al. [2006], the logarithmic&#xA;map (referred to there as the exponential map) is useful for a large&#xA;number of tasks in geometry processing, such as interactive shape&#xA;editing [Schmidt and Singh 2010] and texture decaling (Fig. 17).```&#xA;My best advice is that you read the paper and follow the references. I am not exactly sure what you're looking for specifically. If you have a specific application in mind do mention it." CreationDate="2019-09-12T08:31:10.750" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13755" PostId="9008" Score="0" Text="Augmented Reality." CreationDate="2019-09-12T09:20:04.450" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="13756" PostId="9008" Score="0" Text="Then using the log map to virtually place decals onto actual objects should be one application. Another application has to do with the ordered intrinsic landmark section - namely keeping track of various landmarks (see Fig. 25)." CreationDate="2019-09-12T09:31:21.990" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13758" PostId="9008" Score="0" Text="Can you mention in this context why keeping track of the landmarks might be useful?" CreationDate="2019-09-12T09:58:00.610" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="13759" PostId="9008" Score="0" Text="There are too many ways in which this is useful. But for example you may want to transfer the movement of a person as an animation for some 3d model - basically animation transfer. To transfer the animation, you would need landmarks corresponding to the &quot;bones&quot; of your 3d model - for instance arms, legs etc." CreationDate="2019-09-12T10:54:56.107" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13760" PostId="9147" Score="0" Text="How does this solution compare to a quaternion-based implementation? What are the differences in performance/limitations between the two?" CreationDate="2019-09-12T14:03:57.770" UserId="7925" ContentLicense="CC BY-SA 4.0" />
  <row Id="13761" PostId="9147" Score="1" Text="@DanielMarques They are interchangeable - you can transform between the two representations depending on your needs. As far as performance goes, here is a detailed break-down: https://www.geometrictools.com/Documentation/RotationIssues.pdf" CreationDate="2019-09-12T14:10:35.980" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13762" PostId="9139" Score="0" Text="@spraff If you just want to ray trace, it is enough that you have a quad, with no projection, view, or whatever transformation. Then you do everything in your shader, like in shadertoy." CreationDate="2019-09-12T14:30:03.180" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13763" PostId="9149" Score="0" Text="Aha! That's exactly what I needed to hear. I had a feeling the -1 z plane of the NDC cube would be the screen, but I couldn't confirm it. Thanks" CreationDate="2019-09-13T05:17:23.730" UserId="11153" ContentLicense="CC BY-SA 4.0" />
  <row Id="13764" PostId="9151" Score="0" Text="In the embedded OS I am building, the command 'display' is not recognized." CreationDate="2019-09-13T08:10:49.240" UserId="11174" ContentLicense="CC BY-SA 4.0" />
  <row Id="13765" PostId="9151" Score="2" Text="Well, you need to use some program that is available in your embedded Linux distribution, or add one. I can't help you with that part." CreationDate="2019-09-13T09:01:57.893" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="13767" PostId="9152" Score="0" Text="You copied (1) wrong, it continues till $n-1$. Even then your question is unclear." CreationDate="2019-09-13T20:26:03.407" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13768" PostId="9152" Score="0" Text="@lightxbulb Yes, there was a typo in $(1)$. The question is why take the product of the reverse densities of the vertices $p_{t-1},\ldots,p_0$ on the camera path and not their forward product? If I understand it correctly, the forward density of $p_i$ should be the density which was used to sample $p_i$ at $p_{i-1}$." CreationDate="2019-09-14T04:24:56.620" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="13770" PostId="9152" Score="0" Text="They have previously defined:&#xA;$$p_s(\bar{x}) = \vec{p}(q_0)\cdots\vec{p}(q_{s-1})\overleftarrow{p}(p_{t-1})\cdots\overleftarrow{p}(p_{0})$$&#xA;The only difference when taking a different length of subpaths is:&#xA;$$p_i(\bar{x}) = \vec{p}(x_0)\cdots\vec{p}(x_{i-1})\overleftarrow{p}(p_i)\cdots\overleftarrow{p}(x_{n-1})$$&#xA;That is you just take the probabilities up to $i$ as forward." CreationDate="2019-09-14T06:30:38.550" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13771" PostId="9152" Score="0" Text="@lightxbulb My point is that this formula for $p_s(x)$ doesn't make sense (and they don't *define* it in that way, but claim that it holds). $p_s(x)$ should be the product of the density for obtaining $q_0,\ldots,q_{s-1}$ and the density for obtaining $p_0,\ldots,p_{t-1}$. The former is clearly $\overset{{}_{\rightarrow}} p(q_0)\cdots\overset{{}_{\rightarrow}} p(q_{s-1})$ and the latter should be $\overset{{}_{\rightarrow}} p(p_0)\cdots\overset{{}_{\rightarrow}} p(p_{t-1})$." CreationDate="2019-09-14T08:18:41.557" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="13772" PostId="9152" Score="0" Text="@lightxbulb I think I've found out where the confusion comes from: They always want to understand $\overset{{}_{\rightarrow}} p(x_i)$ and $\overset{{}_{\leftarrow}} p(x_i)$ as the importance transport and radiance transport densities, respectively. They actually write this, but it's highly confusing, since they associate with each such vertex $x_i$ a density `xi.pdfFwd` and `xi.pdfRev` but their meaning is different." CreationDate="2019-09-14T08:35:12.120" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="13773" PostId="9153" Score="0" Text="&quot;*that can be marked as immutable (constant)*&quot; No, &quot;constant&quot; and &quot;immutable&quot; are different things. &quot;Immutable&quot; means that you cannot *reallocate* the memory; &quot;constant&quot; means you cannot change the values *stored in* the memory. Not the same thing at all." CreationDate="2019-09-15T13:59:40.733" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13774" PostId="9154" Score="0" Text="You might have to give a little more context here. I *think* I know what you're talking about but it's a little fuzzy. For example, are we talking about polygons in 3D-space here? Are they planar (though, it seems they are when you're concentrating on triangles)? When you say &quot;looking from the top&quot;, do you actually mean &quot;looking from the side&quot;? It *seems* you're talking about the angle between adjacent triangles of a triangular mesh in 3D space, but I'm not entirely sure and a little more clarity and accuracy in the question's wording might be useful." CreationDate="2019-09-15T16:06:28.633" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="13775" PostId="9154" Score="0" Text="I'm sorry for the unclear question.&#xA;It is in 3D space. The triangles are planar.&#xA;2 Triangles always share one common edge.&#xA;For this edge I want to know if it is bent outwards or inwards. So if i look from the top of the edge if they &quot;bend towards me&quot; they are bending inwards, if they bend away from me, they bend outwards.&#xA;Does this make sense?&#xA;edit: maybe instead looking from the top, it is better to say that I look from the outside of the two triangles (their front faces)" CreationDate="2019-09-15T16:09:28.507" UserId="8757" ContentLicense="CC BY-SA 4.0" />
  <row Id="13776" PostId="9154" Score="0" Text="Yes, that helps. I tried to extend the question a little." CreationDate="2019-09-15T16:11:50.217" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="13777" PostId="9157" Score="2" Text="seems to me like the reflection ray is self intersecting with the surface it spawned from. Add a little epsilon (0.00001 for e,g) value so that the ray starts just ahead of that surface." CreationDate="2019-09-15T18:36:46.410" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="13778" PostId="9157" Score="0" Text="It worked!! :) Never would have thought this might be the issue even though I considered it when shadowing! what it is the best action to talk when one of the values of the RGB channels overflows 1? currently I just discard the value that produced the overflow but I wonder if there is a better or a standard way to handle this ?" CreationDate="2019-09-15T19:54:50.913" UserId="10590" ContentLicense="CC BY-SA 4.0" />
  <row Id="13780" PostId="9157" Score="0" Text="Depends on what you want. It's natural for colors to flow over the range of 1. Pretty common in HDR (High Dynamic Range) images. Tonemapping is applied to convert HDR images to LDR (0-1). For simple raytracing projects we usually clamp the colors to 0-1" CreationDate="2019-09-15T20:41:07.693" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="13782" PostId="9159" Score="1" Text="What method are you using to draw the dots? Are they point sprites, points, triangle meshes, or something else?" CreationDate="2019-09-16T13:14:06.910" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="13783" PostId="9159" Score="1" Text="They are points, I used GL.GL_POINTS" CreationDate="2019-09-16T13:27:36.533" UserId="11191" ContentLicense="CC BY-SA 4.0" />
  <row Id="13784" PostId="9160" Score="0" Text="Excellent, thanks for your explanation! I understand now why the size doesn't change. However, I don't understand why it worked with the speed? The speed increased when approaching the viewer (unless it was an illusion due to my expectation)" CreationDate="2019-09-16T13:57:14.150" UserId="11191" ContentLicense="CC BY-SA 4.0" />
  <row Id="13785" PostId="9157" Score="2" Text="This is a very well known issue which everybody writing their own render encounters. It's called &quot;shadow acne&quot; if you want to read up on it." CreationDate="2019-09-16T14:31:05.670" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="13786" PostId="9158" Score="0" Text="This question is far too big to be handled in our format. Many of those terms have nothing to do with the others, and many of them are used in different ways in different contexts. Overall, your question asks far too many things, to the point where any complete answer would probably have to be a chapter or two from a graphics textbook." CreationDate="2019-09-16T14:40:36.240" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13787" PostId="9160" Score="0" Text="Well, the position is perspective-projected, regardless of what primitive (lines, points, triangles) you use. Since the speed is just the change in position over time, you should expect that to work correctly. It's like you've written code to turn on a single pixel. You use a perspective projection to determine the $(x,y)$ co-ordinates of the pixel to turn on, but you're still always turning on a single pixel." CreationDate="2019-09-16T14:42:09.533" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="13789" PostId="9160" Score="0" Text="Great, thanks for your help!" CreationDate="2019-09-16T15:09:57.717" UserId="11191" ContentLicense="CC BY-SA 4.0" />
  <row Id="13791" PostId="9158" Score="0" Text="Could you link to the relevant articles so that we could see the context? Ideally provide relevant excerpts along with the links. I should also mention that I disagree with Bolas' sentiment about the terms - filtering, sampling, and aliasing have clear formal and mathematical definitions and are related through Shannon's sampling theorem." CreationDate="2019-09-16T15:19:01.867" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13792" PostId="9162" Score="1" Text="I don’t think this approach will quite work here. Since the dot product is symmetrical around 0°, it’ll have the same value whether the normals are pointing “slightly towards” or “slightly away” from each other. The blue stuff in your diagram is relevant, but it’d be worth explaining how to get that vector." CreationDate="2019-09-16T18:06:36.103" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="13793" PostId="9162" Score="1" Text="You're right.  Sorry.  To get the blue vector you take the cross product of the normal vector and the common edge.  Then you compare the blue vector and the *other* red vector.  I'll update the answer." CreationDate="2019-09-16T18:11:30.630" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="13794" PostId="9162" Score="1" Text="@NoahWitherspoon I've fixed it.  Thanks very much for pointing out that rather glaring omission." CreationDate="2019-09-16T18:19:11.707" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="13795" PostId="9160" Score="1" Text="But you can very well change the point size per-vertex in the vertex shader. I wouldn't count that as a &quot;state change&quot; in the classical sense (and personally have made good experiences with per-vertex point size). Of course you'd have to compute the screen-space size yourself from the perspective and there's other problems like point clipping, but the statement on &quot;state changes&quot; is a bit confusing when the same paragraph talks about other shader-based approaches." CreationDate="2019-09-16T22:04:07.027" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="13796" PostId="9162" Score="1" Text="While likely clear to the inclined geometry enthusiast, it's probably worth noting that direction very much matters here (as it usually does in a cross product), otherwise nothing stops me from using the common edge vector the &quot;wrong&quot; way around (or knowing which way is the &quot;right&quot; one to begin with) and basically again sitting there without knowing if the angle is the right way around. I mean, the approach is good and works, but it can't hurt to explain what's *actually* going on here." CreationDate="2019-09-16T22:16:20.063" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="13797" PostId="9162" Score="0" Text="@ChristianRau The expected behaviour is most likely consistent with the normals used for back/front face culling, so the direction is already defined by the order in which the vertices are given." CreationDate="2019-09-17T07:04:27.353" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13798" PostId="9160" Score="0" Text="@ChristianRau That's true, I'd forgotten you can set point size in the vertex shader. That's a better approach if you don't mind writing the perspective computation there." CreationDate="2019-09-17T08:06:08.200" UserId="2041" ContentLicense="CC BY-SA 4.0" />
  <row Id="13799" PostId="9163" Score="0" Text="Numerical root finders. Analytically you cannot hope for much for anything more complex than very simple objects." CreationDate="2019-09-17T09:24:06.567" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13800" PostId="9163" Score="0" Text="thanks for the reply @lightxbulb. I was hoping for some kind of smart raymarching algorithm for finding the roots probably using some kind of gradient descent for realtime applications." CreationDate="2019-09-17T09:57:04.660" UserId="6378" ContentLicense="CC BY-SA 4.0" />
  <row Id="13801" PostId="9163" Score="1" Text="An intersection point of two surfaces described by sdfs $f$ and $g$ is simply $x : f(x) = g(x) = 0$. Provided that $f$ and $g$ are not simplistic, then the roots of that system of equations cannot be found analytically, that is why I mentioned numerical root finding. Under additional constraints you can use gradient or subgradient descent, since then you can try to minimize an energy of the form: $\alpha|f(x)|^p + \beta|g(x)|^q$, which is minimized for $f(x) = g(x) = 0$. If the surfaces do not intersect, it will still be a closest point in some sense." CreationDate="2019-09-17T10:05:06.160" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13802" PostId="9163" Score="0" Text="I want to ask one more thing. What would you say about precalculating and storing offline a set of points X1n &amp; X2n where f(X1n)=0 g(X2n)=0 respectively and then using that information and I guess some kind of acceleration structure find at least some kind of approximation of the intersection points? Would it work? Could it be done in a better more efficient way? Thanks" CreationDate="2019-09-17T10:25:52.333" UserId="6378" ContentLicense="CC BY-SA 4.0" />
  <row Id="13803" PostId="9163" Score="0" Text="A lot of things may be done. Storing points is not ideal however, since all the points you pick may turn out to not correspond to the intersection. One possibility is to discretize your sdf, and then check all voxels within some threshold, and extract voxels for which both sdfs are close to 0. Another possibility is triangulating the surface and then intersecting the triangles from both surfaces. There are many possible solutions beyond these two examples too." CreationDate="2019-09-17T10:35:30.333" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13804" PostId="9163" Score="0" Text="Thanks a lot. :)" CreationDate="2019-09-17T10:39:44.680" UserId="6378" ContentLicense="CC BY-SA 4.0" />
  <row Id="13805" PostId="9169" Score="0" Text="so all of produced color after combining each channel will be only dark, gray and white right?&#xA;the different only about the saturation." CreationDate="2019-09-19T07:18:43.167" UserId="11208" ContentLicense="CC BY-SA 4.0" />
  <row Id="13807" PostId="9142" Score="1" Text="Statistically speaking, independent of application, one might expect a 50% gray color to be the average color of all possible pictures.  A 50% gray is pretty close to 192,192,192.  So that lends some credibility to your approach." CreationDate="2019-09-19T13:40:42.460" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="13808" PostId="9169" Score="0" Text="No, they won't all be dark, gray, and white. Only the ones where the saturation is 0 (black) will they be shades of gray. The other pixels where the saturation is not zero will have color." CreationDate="2019-09-19T16:10:14.500" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13809" PostId="9169" Score="0" Text="I am still confused so based on the image that I gave, black on the saturation channel has the value of 0 is black and the value of 100 is white?&#xA;and if the saturation on that image is white they will have color, because the saturation is 100%?" CreationDate="2019-09-19T21:27:02.803" UserId="11208" ContentLicense="CC BY-SA 4.0" />
  <row Id="13810" PostId="9169" Score="0" Text="Yes, that's correct. Any saturation greater than 0 will have some color." CreationDate="2019-09-20T00:43:58.490" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13812" PostId="9169" Score="0" Text="so just want to confirm. that in my example the box B and D must be have colors.&#xA;After reading your answer, I assume that, the box B has dark red saturated color and I think that, the box D does not make any sense, because the value is 100% and saturation can not be dark.&#xA;Correct me please if I am wrong.&#xA;Thanks in advance." CreationDate="2019-09-20T22:47:01.343" UserId="11208" ContentLicense="CC BY-SA 4.0" />
  <row Id="13813" PostId="9169" Score="0" Text="It looks to me like those 2 boxes both have 0 in the saturation channel, so no, I wouldn't expect them to have colors." CreationDate="2019-09-21T00:56:46.613" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13814" PostId="9169" Score="0" Text="so which box has saturation if B and D does not have saturation?.&#xA;because the other boxes has zero saturation too  due to white color in the saturation channel." CreationDate="2019-09-21T01:51:03.360" UserId="11208" ContentLicense="CC BY-SA 4.0" />
  <row Id="13815" PostId="9169" Score="0" Text="Perhaps I'm misunderstanding your diagram? I'm taking the key to mean that black = 0 and white = 1 (or 0% and 100% if you prefer). So I'm reading it as cells A and C have 100% saturation and cells B and D have 0% saturation. If I have it backwards and white = 0% and black = 100%, then your earlier answer was correct." CreationDate="2019-09-21T01:53:47.153" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13816" PostId="9174" Score="0" Text="I see two figures consisting of rectangles, both congruent, one in grayscale and one in colors. What would you like to cluster here?" CreationDate="2019-09-19T12:13:56.233" UserDisplayName="Stephan Kolassa" ContentLicense="CC BY-SA 4.0" />
  <row Id="13817" PostId="9174" Score="0" Text="gray is an image which is I should to process somehow to get second, colored image" CreationDate="2019-09-19T12:16:35.053" UserDisplayName="Oleg" ContentLicense="CC BY-SA 4.0" />
  <row Id="13818" PostId="9174" Score="0" Text="Ah. It sounds like you actually don't want to *cluster* shapes, but to *cut* them into the smallest possible number of rectangles, correct?" CreationDate="2019-09-19T12:24:52.777" UserDisplayName="Stephan Kolassa" ContentLicense="CC BY-SA 4.0" />
  <row Id="13819" PostId="9174" Score="0" Text="yeah, split them, and get coordinates of it" CreationDate="2019-09-19T12:26:04.857" UserDisplayName="Oleg" ContentLicense="CC BY-SA 4.0" />
  <row Id="13820" PostId="9174" Score="0" Text="got it, thank you Stephan!" CreationDate="2019-09-19T13:41:08.550" UserDisplayName="Oleg" ContentLicense="CC BY-SA 4.0" />
  <row Id="13821" PostId="9174" Score="0" Text="@Oleg Does it fit over there or not?" CreationDate="2019-09-20T13:16:35.570" UserDisplayName="Peter Flom" ContentLicense="CC BY-SA 4.0" />
  <row Id="13822" PostId="9174" Score="0" Text="I'm voting to close this question as off-topic because it's not clear if it should be on the computer graphics SE or not." CreationDate="2019-09-20T13:17:12.180" UserDisplayName="Peter Flom" ContentLicense="CC BY-SA 4.0" />
  <row Id="13823" PostId="9174" Score="0" Text="Yes it should, @PeterFlom" CreationDate="2019-09-20T14:30:45.110" UserDisplayName="Oleg" ContentLicense="CC BY-SA 4.0" />
  <row Id="13824" PostId="9175" Score="0" Text="imagine bw floor plan with different wall thickness, so thickness can vary in different directions" CreationDate="2019-09-20T07:25:22.643" UserDisplayName="Oleg" ContentLicense="CC BY-SA 4.0" />
  <row Id="13825" PostId="9174" Score="0" Text="Use a sweep line algorithm." CreationDate="2019-09-21T18:03:48.523" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13826" PostId="9176" Score="1" Text="It's very hard to get unbiased estimation of the standard deviation, not so for variance, where Bessel's correction provides an unbiased estimate." CreationDate="2019-09-22T17:53:56.227" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13828" PostId="9174" Score="0" Text="@lightxbulb as far I understood that algorithm more suited for finding if any two line segments intersect or not, so it not helped me much&#xA;&#xA;even if I use Canny, and get contours, I still need a fully classified rectangles with corner points" CreationDate="2019-09-23T09:44:26.483" UserId="11226" ContentLicense="CC BY-SA 4.0" />
  <row Id="13829" PostId="9174" Score="0" Text="It has multiple uses, think of it more as an algorithm design strategy, the segment intersection algo is just one application. For instance in your case you can sweep horizontally, and check the first time `y_start` and `y_end` change, where those are the lower and upper y bounds of your points. In your example it will be `(0,1)`, then you walk along `x` and at the 7th column those change, so you combine the last 6 columns in one rectangle, etc. Note that this works only for 1 rect on over 1 `x` coord, an extension to the general case is trivial though." CreationDate="2019-09-23T10:28:58.527" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13830" PostId="9174" Score="0" Text="@lightxbulb meh... it's a little bit hard to understand, could you draw a scheme?&#xA;&#xA;&#xA;I also thinking about np.meshgrid, because my data not so perfect, so I interpolate it first" CreationDate="2019-09-23T11:44:17.630" UserId="11226" ContentLicense="CC BY-SA 4.0" />
  <row Id="13831" PostId="9174" Score="0" Text="Sorry, I will not be making a scheme. I'll try to elaborate though. Let's say you are given the grid of pixels, and the lower and upper leftmost pixels that are black, for instance `(0,0)` and `(0,1)` in your image. You can make a single step along `x` and check whether anything changes - that is check that `(1,0)` and `(1,1)` are black - if they are not, your rectangle ends. You additionally check for `(1,-1)` and `(1,2)` - if any of those are black, then a new rect starts. For your example, the first time any of these checks results in a new rect is at step 7, when the bounds are `(6,0)` and" CreationDate="2019-09-23T11:59:02.497" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13832" PostId="9174" Score="0" Text="`(6,1)`, notably we find that `(6,2)` is not white, and find the new bounds `(6,0)` and `(6,2)`. This keeps going till 6 more steps in, when the check for `(12,3)` being white fails, and the new bounds are `(12,0)` and `(12,7)`." CreationDate="2019-09-23T12:02:07.630" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13833" PostId="9174" Score="0" Text="data is bidirectional, have a look, I updated OP post" CreationDate="2019-09-23T12:08:29.877" UserId="11226" ContentLicense="CC BY-SA 4.0" />
  <row Id="13834" PostId="9174" Score="0" Text="Let us [continue this discussion in chat](https://chat.stackexchange.com/rooms/98988/discussion-between-oleg-and-lightxbulb)." CreationDate="2019-09-23T12:20:44.580" UserId="11226" ContentLicense="CC BY-SA 4.0" />
  <row Id="13835" PostId="9174" Score="0" Text="I don't have time to chat, but the idea remains the same  even with your &quot;bidirectional data&quot;. You have two points at each end facing each other, and you step through with both at the same time - a dent or protrusion (the distance between the points changing) resulting in a new rect." CreationDate="2019-09-23T12:37:23.173" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13836" PostId="9174" Score="0" Text="@lightxbulb I added another one image, that shows complexity of my problem - with 3x3 kernel - amount of data is enormous, I just can't predefine all variants (plato, corners, inner  - for all edges + diagonals) Meanwhile variant suggested by you do not provide enough data for further classification (height, width, coordinates)" CreationDate="2019-09-23T14:28:44.077" UserId="11226" ContentLicense="CC BY-SA 4.0" />
  <row Id="13837" PostId="9174" Score="0" Text="No kernels are required. You just need two points on opposite sides of the boundary (inside). Then you just step perpendicular to the line segment formed by the two points and check the cells &quot;in front&quot; of those and diagonally to them. Try thinking it through - the algorithm is not that hard." CreationDate="2019-09-23T14:40:33.513" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13838" PostId="9174" Score="0" Text="meh.. maybe tomorrow, but now my brain is dead already, sorru ~_~" CreationDate="2019-09-23T14:57:07.900" UserId="11226" ContentLicense="CC BY-SA 4.0" />
  <row Id="13840" PostId="4613" Score="0" Text="I'd argue that sending the extra data from vertex to fragment shader is not worth it. Additionally, sampling a texture is way more expensive than the arithmetic of adding 2 vectors. Have you measured performance and concluded that your proposed method is indeed faster?" CreationDate="2019-09-24T21:18:52.320" UserId="4943" ContentLicense="CC BY-SA 4.0" />
  <row Id="13841" PostId="4613" Score="0" Text="I have absolutely measured it and shown a significant performance increase. The slowdown isn't from the time it takes to do the arithmetic. Apparently the shader compiler can optimize away a bunch of stuff if there's no math, so that's where the speedup comes in. This is probably less true of modern desktop GPUs, but the last time I checked mobile GPUs (which was probably around 2017 when I wrote this answer), it was still a win." CreationDate="2019-09-25T00:59:39.953" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13842" PostId="9185" Score="1" Text="did you forget a return in the ifs of the trace function?" CreationDate="2019-09-25T14:37:53.780" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="13843" PostId="9158" Score="0" Text="I like the idea of a basic comparison answer without going into details. That can then lead to other questions which can be linked to from the answer - the full subtleties don't all have to be present in one post." CreationDate="2019-09-25T20:11:59.773" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="13844" PostId="9188" Score="0" Text="Indeed, the decreased performance does certainly make sense. However, interestingly, these benchmarks seem to contradict it: http://vintage3d.org/i740.php" CreationDate="2019-09-25T23:42:24.310" UserId="10051" ContentLicense="CC BY-SA 4.0" />
  <row Id="13846" PostId="9185" Score="0" Text="`trace` is calling itself 14,000+ times. What is `maxDepth` set to? This is probably set by whomever is calling `trace`. You haven't posted that, so we can't tell you much more." CreationDate="2019-09-26T02:51:59.687" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13847" PostId="9185" Score="0" Text="You could change the function to iterate ray steps sequentially instead of calling itself recursively to avoid this, although a 14,000 step ray sounds far too much for a path tracer, usually I cap these to around 8 or so max steps." CreationDate="2019-09-26T05:46:51.173" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="13848" PostId="2286" Score="0" Text="How do you calculate the another Fresnel term?" CreationDate="2019-09-26T09:26:54.180" UserId="8120" ContentLicense="CC BY-SA 4.0" />
  <row Id="13849" PostId="9193" Score="0" Text="Thanks for the help" CreationDate="2019-09-27T04:32:42.683" UserId="11239" ContentLicense="CC BY-SA 4.0" />
  <row Id="13850" PostId="9194" Score="0" Text="Use any edge detection algorithm." CreationDate="2019-09-27T16:36:18.720" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13851" PostId="9194" Score="0" Text="I am using hough transform but the thing is it don't capture the tiny nudge present in my image. Also, could you please tell me how to proceed further" CreationDate="2019-09-27T16:38:39.293" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="13852" PostId="9194" Score="0" Text="You detect the boundary through an edge detection algorithm. Either use the gradient magnitude, the laplacian, or something more complex (for example canny edge detector)." CreationDate="2019-09-27T16:42:01.607" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13853" PostId="9192" Score="1" Text="Those seem to be arbitrary terms, but I got what you mean. Use $view = (T_cR_xR_yT_d)^{-1}$. Where $R_x$ is a rotation matrix around the $x$ axis, and $R_y$ is a rotation matrix around the $y$ axis, $T_d$ is the offset translation of the camera from point $C$, and $T_c$ is the translation matrix produced from the position of $C$. You may notice that in general you have $TRS$, in this case we do not have scaling, and we have added an extra translation (we do hierarchical transformations). You can think of it as parenting the camera to a pivot $C$ and rotating the pivot." CreationDate="2019-09-27T16:54:25.527" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13854" PostId="9190" Score="0" Text="What accuracy do you need? Could you convert the surfaces to a set of planar polygons? That might be easier to work with. (Or it might tank performance because now you have hundreds of polygons instead of 1 surface. I'm just thinking out loud.)" CreationDate="2019-09-29T01:02:55.463" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13855" PostId="9189" Score="0" Text="What do you already know about the pieslice shape that would help you draw it? Do you want to write a function that takes the same arguments as the existing pieslice function? To help people know what you're looking for, could you edit to explain what you are starting with? Do you want to construct this shape using arcs and straight lines provided by `graphics.h` or do you want to draw it one pixel at a time?" CreationDate="2019-09-29T07:19:16.317" UserId="231" ContentLicense="CC BY-SA 4.0" />
  <row Id="13856" PostId="9189" Score="0" Text="I want to draw it using arcs and line provided by ` graphics.h ` ." CreationDate="2019-09-29T07:30:03.863" UserId="11239" ContentLicense="CC BY-SA 4.0" />
  <row Id="13857" PostId="9195" Score="0" Text="Thanks a lot. Yes, my images have a same black area. I searched alot but couldn't find how to create the static mask.Could you help me out?" CreationDate="2019-09-29T13:05:24.283" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="13858" PostId="9196" Score="0" Text="Again - edge detection. Or you can take out all perfectly black pixels and use those as an inverse to the mask (in this specific image it seems like it will work)." CreationDate="2019-09-29T13:17:50.397" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13859" PostId="9196" Score="0" Text="Could you please elaborate more on inversing black pixels?" CreationDate="2019-09-29T13:46:34.393" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="13860" PostId="9190" Score="0" Text="well i am new in this field, can you maybe give me some hints about about how to convert any arbitrary surface to  a set of polygon. coz i dont think using the control point directly would really help" CreationDate="2019-09-29T13:59:26.677" UserId="11241" ContentLicense="CC BY-SA 4.0" />
  <row Id="13861" PostId="9196" Score="0" Text="Let's assume you create a new image which is black everywhere where the greyvalue (for example `(r+g+b)/3`) is lower than some threshold `T`, and white otherwise. So essentially:&#xA;`target(x,y) = dot(source(x,y),vec3(1)) &lt; 3.0*T ? 1 : 0`&#xA;Where you loop over all pixels." CreationDate="2019-09-29T14:06:18.540" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13862" PostId="9195" Score="1" Text="You can use my suggestion above. Take one image into a photo editor such as Photoshop or Gimp, use the threshold tool to turn everything where the red channel is &lt; 0.1 to black and everything else to white. You may need to adjust the threshold a little, but you want to be conservative. You don't want to have any partially-colored/partially-black pixels showing up as white in your mask. You may need to do a little clean-up by hand." CreationDate="2019-09-29T17:06:15.207" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13863" PostId="9200" Score="0" Text="I don't see how this formula makes creased edges appear to smoothly blend into its neighbours, aka phong normal interpolation (I'm not confused with gouraud interpolation)" CreationDate="2019-09-30T08:05:59.597" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="13864" PostId="6359" Score="0" Text="What leads you to say &quot;But I seem to have failed.&quot;?" CreationDate="2019-09-30T08:34:01.273" UserId="9622" ContentLicense="CC BY-SA 4.0" />
  <row Id="13865" PostId="9200" Score="1" Text="He is talking about Phon shading, not the Phong brdf: https://en.m.wikipedia.org/wiki/Phong_shading&#xA;Consider editing your answer as it is currently misleading." CreationDate="2019-09-30T09:16:54.557" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13866" PostId="9199" Score="2" Text="Refer to: &quot;Weights for Computing Vertex Normals from Facet Normals&quot;. Generally you walk over all triangles that share a vertex and use all of the adjacent triangle  normals to compute the normal at that vertex. To achieve this you can walk over all triangles and add their normals to their 3 adjacent vertices (with a weight). More generally you can create a structure which for each vertex lets you iterate over the triangles that share it." CreationDate="2019-09-30T09:23:03.297" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13867" PostId="9195" Score="0" Text="Applied but still couldn't wrap my head around how to expand it, how to go radially inwards in an image, and even if I managed to done it, it'll expand the alpha mask not the image." CreationDate="2019-09-30T13:14:01.137" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="13868" PostId="9199" Score="0" Text="@lightxbulb exactly what I was looking for, thank you! I can't upvote comments yet but otherwise I would have." CreationDate="2019-09-30T16:07:38.807" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="13869" PostId="9201" Score="0" Text="What does the quad look like if it can't be represented geometrically? If you include why you need this, then we may be able to help you choose how best to divide the quad. For example... splitting the quad along the diagonal with the smallest/largest difference in height, splitting the quad into 4 triangles, etc." CreationDate="2019-09-30T16:11:12.290" UserId="9180" ContentLicense="CC BY-SA 4.0" />
  <row Id="13871" PostId="9201" Score="2" Text="I think the confusion here may be in the assumption that a quad has a standard surface function. A GPU will render a quad by first splitting it into 2 triangles" CreationDate="2019-09-30T16:28:09.007" UserId="9180" ContentLicense="CC BY-SA 4.0" />
  <row Id="13872" PostId="9195" Score="0" Text="I've updated my answer to provide another way to expand the ROI that should be pretty clear." CreationDate="2019-10-01T01:19:18.760" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13873" PostId="96" Score="0" Text="Here's a debugging method I suggested to a related question: https://stackoverflow.com/a/29816231/758666" CreationDate="2019-10-01T02:32:19.270" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="13874" PostId="9193" Score="0" Text="Please look into the codes i have written so far(updated my question). I know its not perfect,i am very new to coding. Specially usage of ` graphics.h `" CreationDate="2019-10-01T05:01:29.780" UserId="11239" ContentLicense="CC BY-SA 4.0" />
  <row Id="13875" PostId="9191" Score="0" Text="I don't know of any per se, but https://en.wikipedia.org/wiki/List_of_16-bit_computer_color_palettes#Amiga_OCS *suggests* that the Amiga had a 12bit encoding that could be changed, so perhaps that might be a starting point?" CreationDate="2019-10-01T08:46:10.673" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13876" PostId="9195" Score="0" Text="The issue with this approach is we need to replace the boundary pixel (outside the FOV) with mean value of its neighbourhood(eight-neighbourhood) and here we are just copying the pixels to the boundary pixels." CreationDate="2019-10-01T12:51:37.960" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="13877" PostId="9195" Score="0" Text="Here, I added a new question with my issues https://stackoverflow.com/questions/58186472/artificially-increasing-the-region-of-interest-of-an-image" CreationDate="2019-10-01T14:02:31.330" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="13878" PostId="9193" Score="0" Text="It looks almost right. I think you only want one loop that goes from `startAngle` to `endAngle`. And of course you'll need to actually draw some lines between the points you generate." CreationDate="2019-10-01T16:00:37.593" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13879" PostId="9193" Score="0" Text="Can you help me with that,please! I am clueless after this." CreationDate="2019-10-01T16:08:18.507" UserId="11239" ContentLicense="CC BY-SA 4.0" />
  <row Id="13880" PostId="9205" Score="0" Text="As noted in that wikipedia article, the term &quot;Phong shading&quot; refers *specifically* to interpolating normals across a surface and using them in lighting computations. There is no multiplying of colors in &quot;Phong shading&quot;; that would be a part of [&quot;Phong lighting.&quot;](https://en.wikipedia.org/wiki/Phong_reflection_model)" CreationDate="2019-10-02T00:35:58.350" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13881" PostId="9193" Score="0" Text="This site isn't for how to write code. I suggest you post it to the main StackOverflow site to get help with that." CreationDate="2019-10-02T01:08:52.397" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13882" PostId="9205" Score="2" Text="Multiplying the material colour by the light is an approximation of light absorption" CreationDate="2019-10-02T02:30:01.820" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="13883" PostId="9205" Score="0" Text="It is, consider the rendering equation: $$L_o = L_e + \int_{\Omega}fL_i\cos\theta\,d\omega$$&#xA;For Phong $f= k_d + k_s\frac{\cos\theta_r}{\cos\theta}$." CreationDate="2019-10-02T07:27:22.213" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13885" PostId="7809" Score="1" Text="Also https://stackoverflow.com/questions/1697842/do-graphic-cards-have-instruction-sets-of-their-own and https://stackoverflow.com/questions/7353136/is-there-an-assembly-language-for-cuda" CreationDate="2019-10-02T13:16:49.660" UserId="5388" ContentLicense="CC BY-SA 4.0" />
  <row Id="13886" PostId="9195" Score="0" Text="okay, I have got it but I have another follow-up doubt, as all the operations we are doing are on the alpha mask, how to move them from alpha mask to the original required image" CreationDate="2019-10-02T15:11:36.890" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="13887" PostId="9206" Score="0" Text="You might want to add that the problems are not a major concern as a) phong isnt really well motivated from a measurement perspective b) If you need to account for real light then RGB is an approximation anyway." CreationDate="2019-10-03T05:18:34.670" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="13888" PostId="9212" Score="0" Text="You have to perform clipping on triangles (i.e.  assembled primitives) which therefore has to be done *before* the perspective divide." CreationDate="2019-10-04T10:09:49.220" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13889" PostId="9213" Score="1" Text="*&quot; primitive assembly happens after clipping,.....The vertex positions are transformed from clip-space to window space via the Perspective Divide and the Viewport Transform.&quot;* Surely that can only work IF the triangle did not need to be clipped.  The dot products VS clip plane calculations (needed as a prerequisite to actual clipping) can be done on individual vertices, but you need actual edges (i.e. pairs of vertices and hence the triangle) to do clipping." CreationDate="2019-10-04T10:15:56.923" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13890" PostId="9213" Score="1" Text="How could it be that primitive assembly happens after clipping? Surely clipping cannot be performed on vertices without first knowing their primitives?" CreationDate="2019-10-04T10:17:18.823" UserId="9420" ContentLicense="CC BY-SA 4.0" />
  <row Id="13891" PostId="9212" Score="0" Text="Why can’t perspective divide happen before clipping? Isn’t clipping much easier in NDC space?" CreationDate="2019-10-04T10:18:54.077" UserId="9420" ContentLicense="CC BY-SA 4.0" />
  <row Id="13892" PostId="9213" Score="0" Text="I see your (or rather my) problem. This is how the pipeline is ordered, which is why I first assumed as suggested in my answer. Right now I cannot find one instance where it says the primitive assembly happens before clipping (even if the pipeline lists primitive assembly after clipping), but as you are correct with your comment, I will adjust my answer accordingly." CreationDate="2019-10-04T10:48:03.533" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="13893" PostId="9212" Score="0" Text="Because once you divide through by Z, data can be destroyed.  EG if Z = 0, the resulting X'=X/Z and Y'=Y/Z are undefined.  Similarly Z&lt;0 will produce incorrect X' &amp; Y' Vals (in the sense that you can't just interpolate from one vertex to another).  You must clip in, say,  homogeneous space, i.e JUST prior to the division." CreationDate="2019-10-04T14:02:48.243" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13894" PostId="9211" Score="0" Text="It's hard to grasp the method from what you've said so that I can simply try it out." CreationDate="2019-10-04T14:24:55.960" UserId="9562" ContentLicense="CC BY-SA 4.0" />
  <row Id="13895" PostId="9211" Score="0" Text="OK.  If I get a chance next week, I'll try making some diagrams." CreationDate="2019-10-04T15:10:46.670" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13896" PostId="9211" Score="0" Text="Some code would be the best help. Maybe, using scipy (python) or eigen (c++), just so you don't have to write a 1000 lines." CreationDate="2019-10-04T15:29:17.993" UserId="9562" ContentLicense="CC BY-SA 4.0" />
  <row Id="13897" PostId="9214" Score="0" Text="Short version: just forget everything you read in that article. It's terrible and contains misleading or incorrect information." CreationDate="2019-10-04T23:00:18.857" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13898" PostId="9219" Score="0" Text="&quot;*mesh with a texture shader*&quot; What is a &quot;texture shader&quot;?" CreationDate="2019-10-05T20:10:42.863" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13899" PostId="9219" Score="0" Text="&quot;mesh with a texture shader&quot; what I meant with that was a shader that takes in a sampler2D texture and samples the coordinates and outputs the frag color." CreationDate="2019-10-05T20:35:40.360" UserId="11119" ContentLicense="CC BY-SA 4.0" />
  <row Id="13900" PostId="9220" Score="3" Text="The spherical coordinate angles defining the direction towards the light, and towards the eye." CreationDate="2019-10-05T21:35:05.137" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13901" PostId="9219" Score="0" Text="You can use the stencil buffer to create a mask. Then use that mask to apply your selection shader only on the part of the screen that the mask covers." CreationDate="2019-10-05T21:38:52.653" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13902" PostId="9220" Score="0" Text="@lightxbulb Ah, that makes sense, thanks!" CreationDate="2019-10-06T03:50:42.073" UserId="7950" ContentLicense="CC BY-SA 4.0" />
  <row Id="13903" PostId="5868" Score="0" Text="Tried CodeXL on Windows with NVIDIA gpu. Always errors on opencl were displayed. I believe it's only for AMD CPU/GPUs." CreationDate="2019-10-06T11:02:13.143" UserId="5496" ContentLicense="CC BY-SA 4.0" />
  <row Id="13904" PostId="9222" Score="0" Text="How did we all miss that error?! I think I'll see if I can edit the text." CreationDate="2019-10-07T07:51:06.203" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13905" PostId="9211" Score="0" Text="As requested, have added some C code and some example results. Also fixed a mistake in the 'update'  equation." CreationDate="2019-10-08T13:19:03.743" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13906" PostId="9211" Score="1" Text="This looks really cool. Thanks! Essentially a $(log_2 n) - 1$ iterations need to be performed to get 4 points from this." CreationDate="2019-10-08T15:38:28.780" UserId="9562" ContentLicense="CC BY-SA 4.0" />
  <row Id="13907" PostId="9211" Score="0" Text="Given that the initial end points aren't really being handled correctly in the above, I'm not sure just 4 points would be terribly useful.  The plot with just 4 (and also 8 points) looks like the outline of a nose! :-/.   I might need to fix the example code." CreationDate="2019-10-08T16:21:56.537" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13914" PostId="9219" Score="0" Text="Just because you think X is one thing does not mean a lower level system considees this to be true. Nothing stops the game engine from drawing the thing you call a mesh as 2 separate meshes, or lump what you call 2 shaders into one." CreationDate="2019-10-10T14:17:01.297" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="13916" PostId="9229" Score="0" Text="I'm voting to close this question as off-topic because it was cross posted on another site and answered there already." CreationDate="2019-10-11T15:28:37.487" UserId="7714" ContentLicense="CC BY-SA 4.0" />
  <row Id="13917" PostId="9232" Score="0" Text="Could you please suggest a good &quot;curvature based method&quot;?" CreationDate="2019-10-11T17:17:44.130" UserId="5828" ContentLicense="CC BY-SA 4.0" />
  <row Id="13918" PostId="9232" Score="0" Text="You can start by removing points with the smallest curvature (as an approximation, you can use the angle that they form)." CreationDate="2019-10-11T17:41:06.550" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13919" PostId="9229" Score="0" Text="So, everything works now as expected and the results seem to be correct. I took the average of all 3D-Point belonging to one physical point. Therefore this question should be considered as answered.&#xA;&#xA;Just a point for further investigation: Taking the average may not be a really robust way. It would be useful to implement something like an outlier control. But this was not part of the question at core.&#xA;&#xA;Please consider this as the answer. I can't mark it as an answer anymore..." CreationDate="2019-10-12T22:01:27.337" UserId="11290" ContentLicense="CC BY-SA 4.0" />
  <row Id="13920" PostId="8567" Score="0" Text="Thanks for the answer, but are you sure you haven't confused even yourself with so much symbolism (because I certainly have)? I did understand though. Thanks." CreationDate="2019-10-13T10:23:23.147" UserId="5496" ContentLicense="CC BY-SA 4.0" />
  <row Id="13921" PostId="9241" Score="1" Text="&quot;*I know what a lightning model does, eg Phong, Flat, Gouraud.*&quot; And yet, those are not lighting models. They're ways of interpolating the normal across a surface. There is such a thing as the Phong lighting model, but that's distinct from Phong shading. &quot;*As far as I know both Ray Tracing and Global Illumination are just lighting models*&quot; These too are not lighting models." CreationDate="2019-10-13T13:29:08.153" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13922" PostId="9241" Score="0" Text="@NicolBolas That's why I asked the question; to disambiguate the terminology." CreationDate="2019-10-13T13:54:33.137" UserId="5496" ContentLicense="CC BY-SA 4.0" />
  <row Id="13923" PostId="9241" Score="0" Text="@NicolBolas The question is literally &quot;Difference betwen Rendering Equation, Lighting model, Ray Tracing, Global Illumination and Shadows?&quot;, I don't know why it got downvoted. Please reconsider upvoting it back up if you downvoted it. It's a good question, and one often not explained in many references and articles." CreationDate="2019-10-13T14:42:10.470" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13925" PostId="9233" Score="1" Text="The &quot;frame&quot; will just refer to the image data while the &quot;buffer&quot; will be the memory used to store it," CreationDate="2019-10-14T08:05:00.283" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13926" PostId="9248" Score="1" Text="The usual way to get rid of moire banding is to add a small amount of noise." CreationDate="2019-10-14T18:18:46.463" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13928" PostId="9240" Score="0" Text="I understand that it depends on whether you use column or row major matrix notation, which reverse the multiplication order. However, what I've seen often is that column major format is used (this appears to be the most common from the resources I see online, literature, and in gaming engines), but it's still called MVP and TRS within the same column major matrix context. &#xA;&#xA;For instance in Unity: &#xA;https://docs.unity3d.com/ScriptReference/Matrix4x4.TRS.html&#xA;https://docs.unity3d.com/Manual/SL-UnityShaderVariables.html" CreationDate="2019-10-14T19:21:33.143" UserId="11304" ContentLicense="CC BY-SA 4.0" />
  <row Id="13929" PostId="9240" Score="0" Text="@Thomas I don't understand the issue? In the link you provided they are doing $TRSv$ which is $TRS$. Column and row-major conventions are unrelated: https://en.wikipedia.org/wiki/Row-_and_column-major_order&#xA;Their $MVP$ naming scheme seems to also be consistent with the description they have of it: &quot;Current model * view * projection matrix.&quot;" CreationDate="2019-10-14T19:37:35.587" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13930" PostId="9240" Score="0" Text="That's what the documentation says, but when I create 2 test shaders it appears to be the other way around, unless I am wholly confused about how mul() maps to MVP, which is absolutely possible. MVP: `float4x4 mvp = mul(unity_ObjectToWorld, mul(UNITY_MATRIX_V, UNITY_MATRIX_P));&#xA;                o.vertex = mul(mvp, v.vertex);`, and this is the other way around: `float4x4 pvm = mul(UNITY_MATRIX_P, mul(UNITY_MATRIX_V, unity_ObjectToWorld));&#xA;                o.vertex = mul(pvm, v.vertex);` If you run it in Unity, only the latter works as expected." CreationDate="2019-10-14T20:03:37.060" UserId="11304" ContentLicense="CC BY-SA 4.0" />
  <row Id="13931" PostId="9240" Score="0" Text="You would obviously get wrong results if you do `mvp * vector`, as you did in the first case. You should have done: `vector * mvp`." CreationDate="2019-10-14T20:17:12.400" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13932" PostId="9240" Score="0" Text="Tried this now: `float4x4 mvp = mul(unity_ObjectToWorld, mul(UNITY_MATRIX_V, UNITY_MATRIX_P));&#xA;                o.vertex = mul(v.vertex, mvp);` that still doesn't work though: https://imgur.com/a/t7vZpHy The green is with PVMv, the red is vMVP.&#xA;&#xA;Am I still doing something wrong?" CreationDate="2019-10-14T20:45:06.320" UserId="11304" ContentLicense="CC BY-SA 4.0" />
  <row Id="13933" PostId="9240" Score="0" Text="Also, it's `mul(UNITY_MATRIX_MVP, v.vertex)`, not the other way around. I believe that it's because Unity inserts the matrices as column major, and multiplying a row oriented vector with a column major matrix would not yield accurate results?" CreationDate="2019-10-14T20:56:07.360" UserId="11304" ContentLicense="CC BY-SA 4.0" />
  <row Id="13934" PostId="9240" Score="0" Text="If `pvm * vec` works, then `vec * transpose(pvm)` should also work. In that case you should have used the transposed matrices: `vec * tr(m) * tr(v) * tr(p)`. The column/row-majority should not affect multiplication - only indexing." CreationDate="2019-10-14T21:24:08.713" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13935" PostId="9251" Score="0" Text="Looks like your mesh is undersampled, in that it only has 6 vertices so its an approximate cone.. Can you increase the resolution some how ?" CreationDate="2019-10-15T02:01:07.257" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="13936" PostId="9248" Score="2" Text="Those circles are usually a symptom of numerical problems, these tend to &#xA;come from the intersection distance. Sometimes the starting point for a reflected ray starts slightly below the surface because of rounding errors. Try subtracting a small epsilon from the intersection distance to see if it improves things." CreationDate="2019-10-15T02:07:52.663" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="13937" PostId="9251" Score="0" Text="@PaulHK each of xx, yy, zz is a 21 by 21 matrix, even if I increase them to 210 by 210, nothing changes" CreationDate="2019-10-15T02:13:05.180" UserId="11312" ContentLicense="CC BY-SA 4.0" />
  <row Id="13938" PostId="9251" Score="0" Text="What is `np.sqrt(xx**2, yy**2)`? I would expect something like `np.sqrt(xx**2 + yy**2)`. Otherwise you're getting back an array of square roots, right?" CreationDate="2019-10-15T02:21:00.027" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13939" PostId="9251" Score="0" Text="@user1118321  Thanks a lot! Please move your comments to answer, I'll accept it." CreationDate="2019-10-15T02:26:16.907" UserId="11312" ContentLicense="CC BY-SA 4.0" />
  <row Id="13941" PostId="9239" Score="0" Text="MVP is actually not even consistent with itself - it encodes the transformation from model -&gt; world -&gt; view -&gt; clip(projection) space. So to be consistent, it would be better to name it World-View-Projection." CreationDate="2019-10-15T09:54:10.577" UserId="1937" ContentLicense="CC BY-SA 4.0" />
  <row Id="13942" PostId="9256" Score="0" Text="The double reflection might be from internal reflections. Maybe you're getting one reflection from the light above and one reflection of the bright spot from the bottom of the sphere?" CreationDate="2019-10-15T18:37:02.740" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13943" PostId="8359" Score="0" Text="Volumetric lighting can often be done through analytics, which can remove the need for any temporal correction. What kind of light are you using?  Any homogeneous lighting, linear/fixed falloff lighting is able to be represented with out such ghosting artifacts." CreationDate="2019-10-15T21:01:19.063" UserId="6530" ContentLicense="CC BY-SA 4.0" />
  <row Id="13944" PostId="9256" Score="2" Text="This photo of a real-world Cornell box model does show multiple images of the light source in the glass ball, FWIW: https://commons.wikimedia.org/wiki/File:Cornell_Box_with_3_balls_of_different_materials.jpg" CreationDate="2019-10-15T21:34:58.343" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="13946" PostId="9257" Score="0" Text="Thank you! You are so helpful! Does &quot;horizontal resolution&quot; mean the interval granularity between -5 and 5 along **x-axis**?" CreationDate="2019-10-16T01:54:04.657" UserId="11312" ContentLicense="CC BY-SA 4.0" />
  <row Id="13947" PostId="9257" Score="0" Text="Yes, mainly I was thinking of the x axis." CreationDate="2019-10-16T02:24:22.857" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="13948" PostId="9258" Score="1" Text="Just as an aside, sometimes, if you have the memory to spare, triple buffering is used. This allows more flexibility in the rendering rate. Imagine you are targeting 60Hz but occasionally the renderer takes &gt; 1/60th of a second. With double buffering, this would cause a dip to 30Hz. Triple buffering allows the update rate to remain more stable but at the price of more latency." CreationDate="2019-10-16T09:22:47.193" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13949" PostId="9260" Score="1" Text="Let $n(\pmb{p})$ give you the noise value at point $\pmb{p}$, then its squared &quot;magnitude&quot; is: $n^2(\pmb{p})$. If the noise produces not a scalar, but a vector quantiry, then the squared magnitude is given by the dot product: $\pmb{n}(\pmb{p}) \cdot \pmb{n}(\pmb{p})$." CreationDate="2019-10-16T15:20:08.617" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13951" PostId="9243" Score="0" Text="Excellent! This has some math that are above my current understanding, so I was more expecting a more practical/programming perspective to it. I will keep coming back to the answer though and read it again in order to better understand it. I wish you could talk a bit about Ray Tracing and Global Illumination too; like how do these fit into the big picture. Thank you!" CreationDate="2019-10-16T17:09:35.483" UserId="5496" ContentLicense="CC BY-SA 4.0" />
  <row Id="13952" PostId="9243" Score="0" Text="@Nikos I assume you understand the interpolation strategies in terms of how it works in practice? In practice (rasterized graphics) you do not get shadows, because you simply do not check whether there is an object between the point that you are shading and the light source (occlusion). In ray tracing you do check that, this corresponds to my comments about the visibility function $V(x,y)$ and how it is usually not present in rasterized graphics. Ray tracing and GI try to solve the equation without throwing away $V(x,y)$ and cutting the number of bounces to 1 (also more than point lights)." CreationDate="2019-10-16T18:19:31.910" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13953" PostId="9243" Score="0" Text="Yes! Yes, I understood these of course and most of what you said, except the Liouville-Neumann series and those integrals. Now, If rasterized graphics don't produce shadows (don't account for the $V(x,y)$ factor) then how can we get shadows in most 3d games for almost 2 decades now? Do these games use ray-tracing and/or GI (I thought it was a newer technique)? You could add all that to your answer. Thank you!!" CreationDate="2019-10-16T18:52:02.173" UserId="5496" ContentLicense="CC BY-SA 4.0" />
  <row Id="13954" PostId="9243" Score="2" Text="@Nikos It is in the answer: &quot;Approximations, that do not produce shadows, have simply thrown away or modified that visibility function (one such modification is replacing V with the ambient occlusion or with information from a $\textbf{shadow map}$).&quot;. Shadow mapping is a technique which renders the depth as perceived by a light source in a texture, and then this is used to approximate $V(x,y)$. Note that this only works for point lights, and cannot be truly omnidirectional (it just renders the 6 faces of a cube). It is also limited by the resolution of the texture map." CreationDate="2019-10-16T19:26:04.503" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13955" PostId="9256" Score="0" Text="A good test would be to compare your result against an image generated by a well tested path tracer fed with the exactly same scene and materials. I usually use Mitsuba as my ground truth (or even smallpt in some cases)." CreationDate="2019-10-16T19:39:32.330" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="13957" PostId="9261" Score="2" Text="`GL_TEXTURE0 + TILE_TEXTURE_UNIT_INDEX` You don't set sampler uniforms to OpenGL enumerators. You set them to the actual texture unit index. This isn't your actual problem, but it's non-functional never-the-less." CreationDate="2019-10-17T02:07:26.873" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13959" PostId="9256" Score="0" Text="One useful debugging technique is to display contribution of path of certain length(s): length equal to 1 is just directly visible emission of materials, 2 is just direct illumination, &gt;2 is whole indirect illumination, 2 is single-bounce indirect illumination, &gt;3 multi-bounce indirect illumination, etc. This way you can separate certain groups of paths to better understand what's going on under the hood." CreationDate="2019-10-17T15:33:57.267" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="13960" PostId="9261" Score="0" Text="I take it the shader compiled successfully at both stages ? (compile &amp; link)" CreationDate="2019-10-18T02:17:11.190" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="13961" PostId="9261" Score="0" Text="Yup.  (the first glok() not throwing an exception indicates this, but I also checked GL_LINK_STATUS  with glProgram...)" CreationDate="2019-10-18T03:11:19.757" UserId="11322" ContentLicense="CC BY-SA 4.0" />
  <row Id="13962" PostId="9260" Score="0" Text="@lightxbulb You should post that as an answer." CreationDate="2019-10-18T03:49:58.890" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13963" PostId="9260" Score="0" Text="@user1118321 Feel free to. I still don't understand what this question was about." CreationDate="2019-10-18T08:24:11.693" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13964" PostId="9260" Score="0" Text="@lightbulb I too am not quite sure what the OP is asking, but I often used squared magnitude (or square of distance) in, say, vector quantisation when mapping vectors to the code book since one is usually only interested in finding the closest code." CreationDate="2019-10-18T09:21:50.140" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13965" PostId="9260" Score="0" Text="@lightxbulb thanks for your feedback, I am discovering computer graphic currently and wondering how a square magnitude would be defined in the context of a Perlin's noise more specifically, procedural noise more broadly if it is worthy to precise it" CreationDate="2019-10-18T15:23:26.343" UserId="11315" ContentLicense="CC BY-SA 4.0" />
  <row Id="13966" PostId="9260" Score="0" Text="@Webwoman As described above. Squared magnitude is a mathematical notion, not something specific to noise - so you can just take your noise and square it. If your noise is made of vectors - then take the squared length of each vector." CreationDate="2019-10-18T15:43:57.243" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13967" PostId="9266" Score="0" Text="Thanks for your help. Where can I find the algorithms about ray tracing? I think Peter Shirley's books are vague." CreationDate="2019-10-18T21:05:25.787" UserId="11316" ContentLicense="CC BY-SA 4.0" />
  <row Id="13968" PostId="9266" Score="0" Text="@bitWise There's not a single one ray-tracing algorithm. You'll have to be more specific." CreationDate="2019-10-18T21:49:21.600" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13969" PostId="9249" Score="0" Text="Yes, you can render directly your content onto your special warped mesh. If you could upload somewhere a sample of code that reproduces the same setup, it would be easier to help you." CreationDate="2019-10-18T22:12:33.017" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="13970" PostId="9268" Score="0" Text="&quot;*there is no &quot;table 2.16&quot; in the specification document (where the language also appears)*&quot; What &quot;specification document&quot; are you looking at? Because that language does not appear in the GL 4.3 specification; it cites a different table." CreationDate="2019-10-19T04:54:05.003" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="13971" PostId="9269" Score="1" Text="Perfect, the wiki has got it.   Looks like it also appears on the &quot;resource&quot; page for `glGetActiveUniform`.  But the 4.2 core specification only has 2.1 through 2.15, oddly.  Maybe we're looking at different versions of the document?   Weird." CreationDate="2019-10-20T04:54:41.093" UserId="11322" ContentLicense="CC BY-SA 4.0" />
  <row Id="13972" PostId="9268" Score="0" Text="https://www.khronos.org/registry/OpenGL/specs/gl/glspec43.core.pdf&#xA;&#xA;4.2 Core also doesn't seem to have it (though Nicol evidently saw some version of it that does...)" CreationDate="2019-10-20T04:56:06.283" UserId="11322" ContentLicense="CC BY-SA 4.0" />
  <row Id="13973" PostId="9271" Score="1" Text="Check this out: http://www.songho.ca/opengl/gl_projectionmatrix.html" CreationDate="2019-10-20T09:14:35.123" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13975" PostId="9249" Score="0" Text="@wip, I don't have any code, I'm just thinking theoretically.  A lot of times in computer graphics the algorithm is used 1) setup framebuffer, 2) render content to framebuffer, 3) apply framebuffer as texture on a mesh 4) render the mesh to the back buffer.&#xA;&#xA;I'm just wondering if there is a theoretical way to achieve the same results without using framebuffer." CreationDate="2019-10-21T19:35:23.307" UserId="3332" ContentLicense="CC BY-SA 4.0" />
  <row Id="13976" PostId="9275" Score="1" Text="The division by $\pi$ in the brdf's constant is for energy preservation (then `material.reflectance` can be anything in `[0,1]`). This is unrelated to the division by $\frac{1}{2\pi}$ which is due to the pdf. The pdf is $\frac{1}{2\pi}$, since the probability of uniformly generating any point on the upper hemisphere is $\frac{\sin\theta}{2\pi}$. The $\sin\theta$ gets canceled with a term from the rendering equation, and $2\pi$ is the surface area of the unit hemisphere (the whole sphere has area $4\pi r^2$, and $r=1$ for the unit sphere)." CreationDate="2019-10-22T00:14:11.417" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13977" PostId="9275" Score="0" Text="Thanks for this, so it's a chance that 2 factor gets left there. Do you have a link to the full equation you mentioned there?" CreationDate="2019-10-22T06:07:52.277" UserId="11346" ContentLicense="CC BY-SA 4.0" />
  <row Id="13978" PostId="9275" Score="0" Text="The rendering equation? Search for rendering equation in google/wikipedia." CreationDate="2019-10-22T09:44:38.277" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13979" PostId="4429" Score="0" Text="What is with all that mathematics that nobody understands" CreationDate="2019-10-22T21:19:47.710" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="13980" PostId="9271" Score="0" Text="In general, questions requesting resources are off-topic here. You could salvage this question by removing the request for off-site resources and just ask for an explanation here." CreationDate="2019-10-23T03:19:55.420" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="13990" PostId="9249" Score="0" Text="Are you using OpenGL? Desktop? Use an image rather than a sampler. It prevents you from having to setup an fbo for the texture and mess with all that. Not sure if it would be better for you though." CreationDate="2019-10-24T12:21:14.137" UserId="113" ContentLicense="CC BY-SA 4.0" />
  <row Id="13991" PostId="9249" Score="0" Text="Using opengl es 2.0.  What do you mean by &quot;image&quot;?" CreationDate="2019-10-24T15:22:29.193" UserId="3332" ContentLicense="CC BY-SA 4.0" />
  <row Id="13992" PostId="9174" Score="0" Text="Is this still open?" CreationDate="2019-10-25T18:49:59.917" UserId="7955" ContentLicense="CC BY-SA 4.0" />
  <row Id="13993" PostId="9281" Score="0" Text="Nice solution mate! but as for me — I needed a 100% robust &amp; 0 maintenance way, so I ended up with plain simple template matching, categorized by wall thickness. But thank you anyway! Hope it would be helpful for someone." CreationDate="2019-10-26T15:35:11.100" UserId="11226" ContentLicense="CC BY-SA 4.0" />
  <row Id="13994" PostId="9283" Score="0" Text="http://jcgt.org/published/0006/01/01/" CreationDate="2019-10-27T18:31:52.447" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13995" PostId="2449" Score="1" Text="I was having those artifacts and this rand() function fixed it. Problem is after I walked like 2km in my terrain, artifacts like the OPs started to show up again. It was using the hash function here: http://amindforeverprogramming.blogspot.com/2013/07/random-floats-in-glsl-330.html that caused the artifacts to go away (Except at distances of 100km, bc of imprecision, but thats okay I just had to split into chunks and got that to work by hashing both values, which will let the perlin noise run nearly indefinitely). So, I'll leave this here to maybe help anyone who has the same issue." CreationDate="2019-10-27T20:01:07.667" UserId="11374" ContentLicense="CC BY-SA 4.0" />
  <row Id="13996" PostId="9284" Score="0" Text="`sin` most likely accepts radians. Also you can cancel out the `+center-center`: `dir = x * sx + y * sy + f * forward`. I am unsure why you calculate the focal length like this either." CreationDate="2019-10-27T21:58:14.227" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="13997" PostId="9284" Score="0" Text="Oh yes in my code i have pi/2 not 90, but what do you mean with the focal length?" CreationDate="2019-10-27T22:16:09.890" UserId="11375" ContentLicense="CC BY-SA 4.0" />
  <row Id="13998" PostId="9282" Score="0" Text="Can I ask some questions for clarification?So you have a function, f(x,y), that you have sampled on a regular grid of (x,y) locations, to create a set of *bilinear* patches?  Are those corner points, therefore, of the form (xi, yi, f(xi, yi))? That is Z is pointing up the screen?" CreationDate="2019-10-28T08:47:30.970" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="13999" PostId="9282" Score="0" Text="Exactly as you said" CreationDate="2019-10-28T13:36:43.940" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="14000" PostId="9282" Score="1" Text="(Sigh. I wish the comments supported equations.)  I don't understand why you are trying to manufacture a &quot;df/dz&quot;.  You have a surface, which to reduce confusion we'll call G(x,y) = (x, y, f(x,y)), and you need to compute on that surface the two partial derivatives (vectors),  dG/dx and dG/dy. These are (1, 0, df/dx) and (0, 1, df/dy) respectively,   where, unless I'm mistaken, df/dx is -2*x*f(x,y) etc  If you just want your piecewise surface to look smooth say using Gouraud shading, evaluate the tangents at the sample points, create your normals (via cross products) and shade." CreationDate="2019-10-28T14:08:33.923" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14001" PostId="9282" Score="0" Text="Thanks Simon. What you said is correct. But you don't always have the analytic formula and all you have is sampled data. The second formula above(taken from the book) I assume is supposed to calculate partial deliveries for with respect to each x,y,z. It does but not for z as expected to be 1" CreationDate="2019-10-28T18:52:56.707" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="14002" PostId="9282" Score="0" Text="Ok. Would it be ok to edit your original question to make that clearer?  As for constructing derivatives etc, assuming your surface is meant to be at least C1 continuous, if it were up to me, I'd use a different underlying piecewise interpolation." CreationDate="2019-10-29T09:00:51.280" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14005" PostId="9285" Score="0" Text="hm I am still having trouble recreating the last reference image provided for this task. this is how I am doing my ray now, but that doesnt seem to work for the last image ( probably missing something ): `sx=norm(cross(forward, up)) * tan(horizontal/2)`, `sy=norm(cross(forward, sx)) * tan(vertical/2)` ( up may not be orthogonal to forward ), `r(t)=origin + t * norm(x*sx + y*sy + forward)`. Am I missing something?" CreationDate="2019-10-30T14:53:26.653" UserId="11375" ContentLicense="CC BY-SA 4.0" />
  <row Id="14006" PostId="9282" Score="1" Text="@SimonF comments do support tex equations. For example: $\frac{\alpha_1}{\gamma}$" CreationDate="2019-10-30T15:56:44.480" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14007" PostId="9285" Score="0" Text="@Yamahari Looks correct. Maybe your fov us given in half angles? Then remove the `/2`. Also is forward normalized?" CreationDate="2019-10-30T16:35:18.623" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14008" PostId="9285" Score="0" Text="i doubt the angle is only half, for the last image the given vertical/horizontal angles are pi * 0.9. Yes the forward vector is normalized" CreationDate="2019-10-30T17:14:54.270" UserId="11375" ContentLicense="CC BY-SA 4.0" />
  <row Id="14009" PostId="9285" Score="0" Text="@Yamahari Then all should be fine, there is probably a different reason you do not get the reference." CreationDate="2019-10-30T17:24:31.753" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14010" PostId="9285" Score="0" Text="oh man I just found out what happened, I was too stupid to use c++ correctly xD thx" CreationDate="2019-10-30T17:42:52.943" UserId="11375" ContentLicense="CC BY-SA 4.0" />
  <row Id="14011" PostId="9261" Score="0" Text="I notice you're using a sampler2DArray, which isn't part of the GLSL spec (google tells me its an nvidia extension). But your glUniform1i example looks like it assuming the variable is a &quot;sampler2D&quot; - is this intentional ?  This is in addition to Nicol Bolas's point about the GL_TEXTURE0 issue." CreationDate="2019-10-31T09:40:37.027" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14012" PostId="9189" Score="0" Text="FYI: `graphics.h` is not a part of C++. It's some external library." CreationDate="2019-10-31T15:41:17.850" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14014" PostId="9291" Score="0" Text="Yes, because we know how to generate uniformly distributed samples efficiently, and we can transform those to other desired distributions." CreationDate="2019-11-02T19:10:13.523" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14015" PostId="9291" Score="0" Text="@lightxbulb Thank you!" CreationDate="2019-11-02T19:12:13.490" UserId="10191" ContentLicense="CC BY-SA 4.0" />
  <row Id="14016" PostId="9292" Score="1" Text="&quot;*it's a grid with cells of varying sizes*&quot; Then fix *that*. You only get guarantees of seamless rendering if the triangles have binary-identical shared vertex positions. The way you're building your meshes, it is very possible that you can get gaps in your rendering, small dots between the &quot;neighboring&quot; triangles that show the background." CreationDate="2019-11-02T22:30:28.203" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14017" PostId="9292" Score="0" Text="@NicolBolas the varying cell sizes are only at a certain distance from the camera, so I don't worry much about those gaps" CreationDate="2019-11-02T22:36:11.950" UserId="11409" ContentLicense="CC BY-SA 4.0" />
  <row Id="14018" PostId="9283" Score="0" Text="FWIW: I have some additional variants here: http://marc-b-reynolds.github.io/quaternions/2016/07/06/Orthonormal.html . Specifically you have a few knobs to turn:  1) lose some ops for slightly worse error bound, 2) ditch insured right-handed set 3) well define for whole sphere vs. not." CreationDate="2019-11-03T09:54:27.350" UserId="2831" ContentLicense="CC BY-SA 4.0" />
  <row Id="14019" PostId="9295" Score="1" Text="His normals are already outwards facing. The issue is that he took the wrong light direction." CreationDate="2019-11-03T20:33:45.360" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14020" PostId="9296" Score="0" Text="thanks, very much! :)" CreationDate="2019-11-03T20:37:06.133" UserId="11375" ContentLicense="CC BY-SA 4.0" />
  <row Id="14021" PostId="9295" Score="0" Text="I stand corrected." CreationDate="2019-11-03T20:37:11.220" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="14022" PostId="9292" Score="1" Text="These gaps can occur at any distance. Even if know the edges are parallel because the interpolation from both edges have different start and end points we cannot guarantee the edges along the same scanline are exactly at the same pixel." CreationDate="2019-11-04T03:18:35.347" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14023" PostId="8318" Score="0" Text="Cool stuff!.  @SimonF you just add the extension string to the array of your validation layers and it just works ))" CreationDate="2019-11-04T10:21:08.907" UserId="213" ContentLicense="CC BY-SA 4.0" />
  <row Id="14024" PostId="9299" Score="1" Text="_&quot;Those two properties make raytracing not very atractive for real-time rendering without some dedicated acceleration&quot;_  One could probably say the same for rasterisation.  ;-)" CreationDate="2019-11-05T09:25:14.920" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14025" PostId="9300" Score="0" Text="Using the z-buffer should enable you to make an approximate of the DOF effect. I don't see how it limits refraction however. Lets suppose you don't actually render the triangles using rasterization, you just tell the raytracer which portions of which triangles to shade given the z buffer (now you have your t-parameter which gives you a hitpoint ), you should be able to just refract that." CreationDate="2019-11-05T10:19:36.723" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="14026" PostId="9300" Score="0" Text="@AnnoyinC This &quot;approximation will be arbitrarily bad. In general dof depends on the aperture of your camera, which you cannot really model very well in rasterization. The issue with refraction, is that if you put a glass panel in front of your camera, your rasterization will effectively just &quot;intersect&quot; the glass panel, which may be made from 2 triangles, so you don't get the usual high gain compared to ray tracing." CreationDate="2019-11-05T10:29:28.167" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14028" PostId="9292" Score="0" Text="Just to add to @PaulHK 's comment:  there is a description on the problems of T-Junctions in this thread https://computergraphics.stackexchange.com/a/1464/209" CreationDate="2019-11-05T11:26:27.370" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14029" PostId="9282" Score="0" Text="@SimonF, thanks. You mentioned: &quot; I'd use a different underlying piecewise interpolation&quot;. So would you mind explaining what interpolation you use in this case." CreationDate="2019-11-05T11:30:41.127" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="14032" PostId="9282" Score="1" Text="I'll try to write something tomorrow." CreationDate="2019-11-05T14:15:53.900" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14034" PostId="9304" Score="0" Text="Somewhere down the line this is still mapped to whatever you monitor supports." CreationDate="2019-11-05T20:33:31.257" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14035" PostId="9305" Score="0" Text="I'm curious about buffer[(nu&lt;&lt;2) + nv*pitch]  &gt;&gt; I'm assuming you are using (nu, nv) to sample a 2d array of R,G,B bytes? if that's the case then the offset calculation should be (nu + nv * pitch) * sizeof(Pixel)  - where Pixel is defined as { uint8char r,g,b } - I'm assuming pitch is in pixel units, not byte units. It may be better to encode your pixels as 4 bytes with a padding byte so you can keep your faster *4 offset calculation (you can also copy pixels via a single uint32_t type)" CreationDate="2019-11-06T02:09:03.530" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14036" PostId="9306" Score="0" Text="superb, I never realised that data type was changing while converting and yes, when converting to same data-type they are giving the expected result." CreationDate="2019-11-06T05:05:38.330" UserId="11114" ContentLicense="CC BY-SA 4.0" />
  <row Id="14037" PostId="9305" Score="0" Text="@PaulHK the point was to get a 9x9 grid of the previous render centered on the current pixel. I figured that if I reused the equation for &quot;index&quot; but offset the vertical pixel or horizontal pixel by -1 or 1 or 0 I would get there.&#xA;These aren't really uv coordinates in [0,1], they are coordinates in x [0, width] and y [0, height]. Since I haven't had trouble with the `index` equation in my main render loop I had no reason to suspect it was wrong. I will try your suggestion!" CreationDate="2019-11-06T07:56:23.210" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="14038" PostId="9305" Score="0" Text="Ahh, premature optimisation, the root of all evil :)" CreationDate="2019-11-06T08:02:12.560" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14039" PostId="9305" Score="0" Text="While substituting your equation I made a mistake which should have made the output monocolor. This didn't happen, so now I know the 9x9 grid isn't being used correctly." CreationDate="2019-11-06T08:07:45.280" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="14040" PostId="9308" Score="0" Text="1. Yeah, if I'd started implementing this I'd probably notice that, thanks for mentioning it anyway  2. I'm not sure yet whether I want to do spins and masse shots, I'll think about that. 3. I'm already simulating friction by multiplying velocity at each step by a &lt;1 factor. Anyway, all in all do you think that the entire idea is legit?" CreationDate="2019-11-06T08:56:46.060" UserId="11414" ContentLicense="CC BY-SA 4.0" />
  <row Id="14041" PostId="9307" Score="0" Text="I haven't looked into this beyond what you posted, but this seems like the probability with which you sampled your new ray. Since it mentions proportional to Q, I am assuming you do some inverse transform sampling, so you need to take it into account." CreationDate="2019-11-06T15:58:48.370" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14043" PostId="9310" Score="0" Text="The whole point of this method is that patches wouldn't have the same value, that's where you gain something." CreationDate="2019-11-06T18:22:27.437" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14044" PostId="9311" Score="3" Text="Looks like a buggy implementation." CreationDate="2019-11-06T20:35:22.593" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14045" PostId="9311" Score="0" Text="The learning rate was too large. It should be around 0.001. Now everything looks great ! But I still don't know why this happens when the learning rate is large." CreationDate="2019-11-06T20:53:19.283" UserId="11316" ContentLicense="CC BY-SA 4.0" />
  <row Id="14046" PostId="9309" Score="0" Text="Thanks Simon. You mentioned that both partial derivatives are vectors(in i,j directions). could you write each vector components plz." CreationDate="2019-11-06T23:41:44.917" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="14047" PostId="9311" Score="0" Text="These artefacts look like a bug on the gpu, you sometimes see this if for example you use uninitialised variables or have data races in a compute or pixel shader." CreationDate="2019-11-07T02:15:14.203" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14048" PostId="9311" Score="0" Text="I don't use gpu or c++. I only use julia programming language and this is just part of my master's thesis." CreationDate="2019-11-07T10:13:03.970" UserId="11316" ContentLicense="CC BY-SA 4.0" />
  <row Id="14050" PostId="9300" Score="0" Text="@lightxbuld in that example, a pure raytracer would be even slower. It woul raytrace the first intersection, and then the refraction/reflection, so it has to do 2/3 passes for a render. Offloading the first pass to a rasterizer would still provide gains." CreationDate="2019-11-07T18:01:34.567" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="14051" PostId="9300" Score="0" Text="@AnnoyinC I wouldn't be so sure. In rt you may hit the bounding box of this glass quad and be done, in a rasterizer you may have to render millions of tris hidden behind it." CreationDate="2019-11-07T18:30:55.903" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14052" PostId="9300" Score="0" Text="Oh, I see what you mean now. Yeah, I was thinking along the lines of &quot;get a very simple rasterizer to simply get the nearest triangle for each pixel, then use a raytracer as a shader.&quot; Then the raytracing shader can concern itself with material properties like transparency." CreationDate="2019-11-07T18:43:15.600" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="14053" PostId="9300" Score="0" Text="@AnnoyinC The issue is how you find the &quot;nearest&quot; triangle. And if two tris intersect, then some parts of both are nearest - that's where z-buffer comes in, but you may have to render you whole scene for that. Some tricks may be used however to achieve something similar like acceleration structures in ray tracing." CreationDate="2019-11-07T21:11:57.277" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14055" PostId="9309" Score="1" Text="Thanks for the update. My math is not very good. I thought derivative is the rate of change of &quot;function value&quot; with respect to one of its arguments(unless it is a vector-valued function and you are assuming that the point S(x,y,z) is the function value); the gradient is a vector but not the partial derivatives, is that correct?" CreationDate="2019-11-07T22:43:07.147" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="14056" PostId="9316" Score="0" Text="You discretize it, then you render it. As far as I am aware there is no `GL_SPHERE` or anything of the sort." CreationDate="2019-11-07T23:58:01.217" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14057" PostId="9318" Score="0" Text="Use it on the bones of the model." CreationDate="2019-11-08T07:49:53.090" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14059" PostId="4438" Score="3" Text="finally someone saying that gimbal lock is not due to euler angles. Saying that quaternions aren't affected by gimbal lock for example is silly, since when you express a rotation sequence with them (or with rotation matrices for that matter) you get the same exact problem (because both representations are expressed in terms of euler angles). Is the ability to do spherical interpolation from one orientation to another that is correct." CreationDate="2019-11-08T07:58:42.330" UserId="11438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14061" PostId="9309" Score="0" Text="*&quot;I thought derivative is the rate of change of &quot;function value&quot; with respect to one of its arguments&quot;*   That is indeed correct.  But for a surface, _if_ a given first (partial) derivative is non-zero (which is guaranteed in your case because of the way you construct the surface), then it also is locally tangential to the surface. Further, we know *your* two partial derivates _aren't_ parallel, so the cross product will construct a valid normal. (FWIW For some arrangements of control points, in more general use cases, you can get so-called 'degenerate' surfaces, but you're safe here)" CreationDate="2019-11-08T10:56:07.927" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14064" PostId="9309" Score="0" Text="&quot;but for a surface, if a given first (partial) derivative is non-zero then it also is locally tangential...&quot;. I am not making a fuss :) but your argument already assumes partial derivative to be a vector(otherwise it won't be tangential) whereas I think it is scalar." CreationDate="2019-11-08T14:24:22.910" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="14065" PostId="9319" Score="1" Text="Are you sure this is the right place to ask about &quot;AI&quot; but then i suspect that you need no &quot;AI&quot;" CreationDate="2019-11-08T15:55:10.900" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14066" PostId="9321" Score="0" Text="First of all, thank you very much for your answer, Reynolds!&#xA;&#xA;About what you said that many people opt to evaluate and then interpret as standar polynomial Bézier triangle, isn't that exactly what I am doing? The equations I put as images are from Bézier-Bernstein triangle according to this [article](https://www.gamasutra.com/view/feature/131389/b%C3%A9zier_triangles_and_npatches.php?print=1)&#xA;&#xA; Which singularities do you mean? I only know the zero denominator singularity. By the way, how exactly can they be removed?" CreationDate="2019-11-08T16:35:51.420" UserId="11436" ContentLicense="CC BY-SA 4.0" />
  <row Id="14067" PostId="9321" Score="0" Text="Are your images and equations correct? Why isn't there P040 and why P310 is repeated? Why should I use the differences instead of only &quot;downgrading&quot; it to the Bézier triangles and finding control points between control points? Can you please explain to me in more detail?&#xA;&#xA;I used the two partial derivatives equations you wrote. Unfortunately, the results aren't good yet.&#xA;&#xA;Again, thank you very much for your help." CreationDate="2019-11-08T16:42:38.323" UserId="11436" ContentLicense="CC BY-SA 4.0" />
  <row Id="14068" PostId="9303" Score="0" Text="This strategy is called discrete event simulation." CreationDate="2019-11-08T16:46:20.583" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14070" PostId="9321" Score="0" Text="Sure, no problem! I really appreciate your help." CreationDate="2019-11-08T17:00:18.123" UserId="11436" ContentLicense="CC BY-SA 4.0" />
  <row Id="14071" PostId="9323" Score="2" Text="looks like you should be able to extract features by area. So remove any continious area smaller than x pixels" CreationDate="2019-11-08T17:10:20.360" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14072" PostId="9309" Score="0" Text="But you are building a surface in 3D.  I'll add another note" CreationDate="2019-11-08T17:22:00.090" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14073" PostId="9318" Score="0" Text="How can I do that? Samples/tools/free API." CreationDate="2019-11-08T17:43:51.760" UserId="4678" ContentLicense="CC BY-SA 4.0" />
  <row Id="14074" PostId="9318" Score="0" Text="I don't know about APIs, I usually code such things manually. But there certainly should be animation packages out there." CreationDate="2019-11-08T17:50:31.243" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14075" PostId="9318" Score="0" Text="Wow is it that easy to do via code?! If you got anything, please share it." CreationDate="2019-11-08T17:51:46.953" UserId="4678" ContentLicense="CC BY-SA 4.0" />
  <row Id="14076" PostId="9318" Score="1" Text="You can find tutorials on it: https://veeenu.github.io/2014/05/09/implementing-skeletal-animation.html" CreationDate="2019-11-08T17:58:03.170" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14077" PostId="9323" Score="1" Text="+1 to joojaa's comment, but you really shouldn't have got here in the first place -- any adaptive thresholding method should provide a bias parameter that will help you avoid amplifying low-amplitude noise in nearly uniform regions." CreationDate="2019-11-08T19:13:02.940" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14078" PostId="9309" Score="1" Text="Perfect. It's all clear now. I had missed the capital F function." CreationDate="2019-11-08T19:45:41.470" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="14079" PostId="9325" Score="1" Text="`#pragma omp parallel for`, you'll have to provide separate prngs. You can refer to my modification of inoneweekend: https://github.com/vchizhov/InOneWeekend-1/blob/master/src/main.cc" CreationDate="2019-11-08T21:03:01.680" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14080" PostId="9319" Score="0" Text="In another post I was recommended this group, apologies if this is the wrong place I'm not after &quot;AI&quot; just machine learning / deep learning direction, I don't want to reinvent the wheel." CreationDate="2019-11-08T22:08:34.197" UserId="11439" ContentLicense="CC BY-SA 4.0" />
  <row Id="14081" PostId="9323" Score="0" Text="@Rahul I see, thank you. Adjusting the &quot;Constant subtracted from the mean&quot; does help." CreationDate="2019-11-09T06:18:38.493" UserId="11443" ContentLicense="CC BY-SA 4.0" />
  <row Id="14082" PostId="9323" Score="0" Text="@joojaa Thank you! I found out how to find, filter, and draw contours. This is promising." CreationDate="2019-11-09T06:19:19.830" UserId="11443" ContentLicense="CC BY-SA 4.0" />
  <row Id="14083" PostId="9319" Score="1" Text="Thats fine, however you may consider explsining the nature of the data better." CreationDate="2019-11-09T06:21:55.197" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14084" PostId="9331" Score="0" Text="You find the intersection of your line, with the edge of your pixel array, and draw only the visible part." CreationDate="2019-11-09T09:04:56.013" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14086" PostId="9331" Score="0" Text="This solution works for the line example, but does not scale up well. For example, I would have to have a separate approach for every type of shape and its interaction with the edge of the pixel array. If I were to apply a transformation, such as a rotation to the array, shapes that are slightly off screen may not appear correctly. There needs to be some kind of buffer." CreationDate="2019-11-09T14:41:13.727" UserId="11446" ContentLicense="CC BY-SA 4.0" />
  <row Id="14087" PostId="9331" Score="0" Text="You will have to find intersections fir any shape, yes. The other option is to have a condition whether to draw a pixel - so that if it is outside the array, it does not do anything. Rotating that array is equivalent to applying the inverse rotation to the shapes too." CreationDate="2019-11-09T14:44:53.943" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14088" PostId="9331" Score="2" Text="&quot;I would have to have a separate approach for every type of shape and its interaction with the edge of the pixel array.&quot; If all your shapes are polygons, you only need to implement a single [polygon clipping algorithm](https://en.wikipedia.org/wiki/Sutherland%E2%80%93Hodgman_algorithm)." CreationDate="2019-11-09T16:03:48.753" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14089" PostId="9334" Score="3" Text="Can you provide images? With highlights for the relevant parts?" CreationDate="2019-11-10T12:54:42.557" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14090" PostId="9319" Score="0" Text="@joojaa does that help? It is a fairly logical problem, we know the date and time of each point cloud, the latter the date/time the deeper the data, or inversely the lower the point cloud the later it was captured. each point cloud is accurately georeferenced +/- up to 3cm (dgps) they are all within a 10x10x3 volume. So classifying the points as A) part of the volume, B) overlapping points, C) outlying data points." CreationDate="2019-11-10T20:54:22.073" UserId="11439" ContentLicense="CC BY-SA 4.0" />
  <row Id="14091" PostId="9319" Score="1" Text="sounds like a XY problem. But your not describing yourself very well. In either case if you persist in asking deep learning solutions then you are in the wrong forum. Dont specify the solution just describe the problem, youll get better answers. Why would there be overlapping points? I can guess but . Volume classifying should be trivially just the distance to a datum of some kind or just each pointcloud. Outlying datapoints should be handled by the poissob disk surface reconstruction. Anyway i am afraid you have consumed any interest I have in your problem." CreationDate="2019-11-11T06:15:58.967" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14092" PostId="9331" Score="1" Text="+1 to @Rahul 's answer - you want to look up up _clipping_.  (**BTW**  did you really mean &quot;**1** dimensional array?&quot;)" CreationDate="2019-11-11T09:17:52.903" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14096" PostId="9331" Score="0" Text="@SimonF Yes. I meant 1 dimensional array. The first element in the array is the upper left hand corner, and the last element is the lower right hand corner. I use x + width * y to place pixels in the correct element." CreationDate="2019-11-11T16:26:05.500" UserId="11446" ContentLicense="CC BY-SA 4.0" />
  <row Id="14097" PostId="9336" Score="0" Text="Could you be more specific? What exactly are you asking?" CreationDate="2019-11-11T18:27:55.020" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="14098" PostId="9336" Score="0" Text="Simply stating I want to know how to interpret values outside 0-255 range? How do i map them to 0-255" CreationDate="2019-11-11T18:41:56.867" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14099" PostId="9336" Score="1" Text="What are you using the 0-255 values for? Density?" CreationDate="2019-11-11T21:30:51.303" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14100" PostId="9319" Score="1" Text="Is there any specific reason you need to do this via deep learning other than it being da next big thing or you being academically pressured into doing *something* with deep learning?" CreationDate="2019-11-12T10:25:59.510" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="14102" PostId="9336" Score="1" Text="Yes, I think. Are there other ways to use those values? I don't really have much knowledge about it since there is so little on the internet regarding volume rendering especially one related to scientific visualization. The sites from where I get these datasets don't tell what unit are the values supposed to be in or how to interpret them. Updated the post with the sites." CreationDate="2019-11-12T11:21:22.107" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14103" PostId="9334" Score="0" Text="I divided the planes using regular grids and I added noise to the points, but the problem hasn't been solved." CreationDate="2019-11-12T14:42:05.510" UserId="11316" ContentLicense="CC BY-SA 4.0" />
  <row Id="14104" PostId="9321" Score="0" Text="Reynolds, unfortunately, it didn't work yet. I've updated the question with the new results. Any ideas? Thanks again for your help!" CreationDate="2019-11-12T15:02:47.670" UserId="11436" ContentLicense="CC BY-SA 4.0" />
  <row Id="14105" PostId="9338" Score="0" Text="I think you are misunderstanding the situation probably. Firstly although it's a `uint16` dataset, the values are in the range `0-4095` which I guess are in Hounsfield units. For a simple remapping you should be dividing by 4095 instead of $2^{16}$. Again this seems more like just refitting the data range which I thought of initially but posted the question since I was thinking of a specific mapping operation which depends on what those values represent initially. Again my speculation can be wrong and if you really are sure about it then I'll wait for other answers" CreationDate="2019-11-12T15:55:32.517" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14106" PostId="9321" Score="0" Text="Unfortunately I got the direction of the derivatives wrong, again. These should be the right derivatives." CreationDate="2019-11-12T16:09:27.470" UserId="7724" ContentLicense="CC BY-SA 4.0" />
  <row Id="14107" PostId="9338" Score="0" Text="Yes, in this case you should divide it by 4095. I don't see why you shouldn't use this mapping. Perhaps, you would need to do a previous one. I don't know this Hounsfield units, but suppose you would like to work with cm instead of meters. The dataset is uint16 with range 0-4095 in meters. Then, first you would have to multiply $oldValue$ by 100 (this is just an example). Although, as you said, I might be misunderstanding the situation." CreationDate="2019-11-12T16:10:40.927" UserId="11436" ContentLicense="CC BY-SA 4.0" />
  <row Id="14108" PostId="9334" Score="0" Text="You simply have a bug in your sampling somewhere. These are not moire patterns." CreationDate="2019-11-12T16:11:42.743" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14109" PostId="9321" Score="0" Text="At the second derivative, should it be $B_{300}(P_{310} - P_{301})$? You repeated the control point." CreationDate="2019-11-12T16:15:29.100" UserId="11436" ContentLicense="CC BY-SA 4.0" />
  <row Id="14110" PostId="9336" Score="1" Text="I would suggest remapping both the `uint8` and `uint16` version to [0,1]. Then you can multiply by some factor if necessary. More precisely: `v= v / max_v`." CreationDate="2019-11-12T16:15:33.770" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14111" PostId="9321" Score="0" Text="Yes you are right. Please also consult the figures if you encounter more errors." CreationDate="2019-11-12T16:17:48.987" UserId="7724" ContentLicense="CC BY-SA 4.0" />
  <row Id="14112" PostId="9321" Score="0" Text="Ok! I tried it out. Unfortunately, the results are the same as the last ones with normal flipping (the last pictures on the right). Even so, thanks for your help!" CreationDate="2019-11-12T16:24:30.020" UserId="11436" ContentLicense="CC BY-SA 4.0" />
  <row Id="14113" PostId="9334" Score="0" Text="No, it is moire effect. Just scale down the picture you even see some concentric semicircles.  I have generated some photos with different parameters and the semicircles are obvious." CreationDate="2019-11-12T16:39:41.743" UserId="11316" ContentLicense="CC BY-SA 4.0" />
  <row Id="14114" PostId="9334" Score="0" Text="I do not think it is. There's also nothing in your scene that should cause it, so I assume it's just broken sampling." CreationDate="2019-11-12T16:55:09.813" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14115" PostId="9334" Score="1" Text="Sorry, I was wrong. I used stratified sampling, but nothing changed. So it's not moire effect. It is a bug. I should rewrite the code." CreationDate="2019-11-12T18:05:11.283" UserId="11316" ContentLicense="CC BY-SA 4.0" />
  <row Id="14116" PostId="9319" Score="0" Text="@ChrissaysReinstateMonica I'm actually fairly late coming to ML/DL, I am hype critical regarding new technologies, particularly those which have the wow-factor. However in this case I can see a sensible application. I have achieved the cleaning of 3D point clouds to allows volume generation (mainly in python) while quicker than a purely manual approach it is still laborious. Since there is a basic logic behind the problem (an earlier ground surface is the top of a later ground surface - base) DL should (once trained) be able to solve the problem? But where to begin?" CreationDate="2019-11-12T21:41:31.363" UserId="11439" ContentLicense="CC BY-SA 4.0" />
  <row Id="14118" PostId="9341" Score="0" Text="This is speculation on my part, but is this a laptop or a desktop system? Laptops sometimes have two GPUs ( e.g. https://superuser.com/questions/908824/why-does-my-laptop-have-two-graphics-cards ).  Second idle thought is could it be using triple buffering? That might account for a later frame being displayed sooner than expected?" CreationDate="2019-11-13T14:01:10.767" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14119" PostId="4994" Score="0" Text="Where does this `radius` variable is coming from in the `LambertBRDF::Sample` function?" CreationDate="2019-11-13T19:56:45.297" UserId="11452" ContentLicense="CC BY-SA 4.0" />
  <row Id="14120" PostId="4994" Score="0" Text="That's an error. Removing" CreationDate="2019-11-13T20:00:12.933" UserId="310" ContentLicense="CC BY-SA 4.0" />
  <row Id="14121" PostId="9345" Score="0" Text="Ahhhh. Thanks. I missed that" CreationDate="2019-11-13T22:32:42.233" UserId="310" ContentLicense="CC BY-SA 4.0" />
  <row Id="14122" PostId="9341" Score="0" Text="This could also occur if your thread has delays waking up from its wait-for-vsync.The next frames wait for vsync then starts earlier than expected, even though the 2 frames end up at the display evenly paced." CreationDate="2019-11-14T14:09:19.313" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14123" PostId="9329" Score="0" Text="say you don't have access to modify the shader but just have to fudge it by putting textures in existing shader texture slots?" CreationDate="2019-11-15T02:58:58.860" UserId="11445" ContentLicense="CC BY-SA 4.0" />
  <row Id="14124" PostId="9329" Score="0" Text="also may i ask why the B value is 1?" CreationDate="2019-11-15T02:59:37.740" UserId="11445" ContentLicense="CC BY-SA 4.0" />
  <row Id="14125" PostId="9350" Score="0" Text="Probably better to add 0 and only have on 1 on the row." CreationDate="2019-11-16T14:47:46.087" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14126" PostId="9350" Score="0" Text="Also note that the  multiplication makes the matrix square." CreationDate="2019-11-16T16:19:07.693" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14127" PostId="9350" Score="0" Text="You should drop the last entry of $B$ and the last row of $X$, since you only have three equations per point pair. Also, I assume you are going to be vertically concatenating the $B$'s and $X$'s obtained from all the point pairs." CreationDate="2019-11-16T18:18:02.940" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14128" PostId="9350" Score="0" Text="P.S. I'm fairly sure the optimal solution is given simply by the translation which aligns the two point sets' centroids, and the scaling which equates the RMS distance from the points to the centroid. See [ordinary Procrustes analysis](https://en.wikipedia.org/wiki/Procrustes_analysis#Ordinary_Procrustes_analysis)." CreationDate="2019-11-16T18:20:47.223" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14129" PostId="9350" Score="0" Text="@Rahul , I am not sure if I can drop the last entry of B and X, since my T matrix is still a 4X1 matrix.  My first doubt is, does it even make sense to estimate scale and translation from just 1 point to point correspondence between 2 objects? Ordinary Procrustes analysis seems promising, and I am actually trying to implement a skeleton fitting algorithm. I'll try that method and get back with the results." CreationDate="2019-11-17T05:13:55.577" UserId="9528" ContentLicense="CC BY-SA 4.0" />
  <row Id="14130" PostId="9350" Score="0" Text="Oh, I missed the fact that you have only *one* point-to-point correspondence. Yes, in that case you cannot possibly find both scale and translation. Notice that you can scale one mesh however much you like and still find a translation that exactly aligns the points." CreationDate="2019-11-17T06:03:17.317" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14131" PostId="9350" Score="0" Text="yes, that makes sense to me. I got this question while reading this paper - https://www.cs.utah.edu/~ladislav/saito15computational/saito15computational.pdf. In section 3.4, they say they estimate uniform scale and translation for both ends of the bone, with just one point-to-point correspondence at each end. I was wondering if this was done using a least squares optimiser? Thanks for all the help!" CreationDate="2019-11-17T10:39:48.630" UserId="9528" ContentLicense="CC BY-SA 4.0" />
  <row Id="14132" PostId="9352" Score="0" Text="Where are you getting this $\cos^2(\theta_i')$ from? There is only one $\cos(\theta_i')$ and $\cos(\theta_o)$ per path segment. If the BSDF is defined with respect to solid angle then the extra cosine is bundled with the geometry term, which seems to be the case for PBRT." CreationDate="2019-11-17T17:37:00.723" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14133" PostId="9352" Score="0" Text="The PBRT's implementation indeed contains only one consine term. However, in Veach's paper, the BSDF is defined with respect to projected solid angle (http://graphics.stanford.edu/papers/veach_thesis/chapter3.ps equation 3.12), and the geometry term in chapter 8 contains one extra consine term." CreationDate="2019-11-17T17:47:46.140" UserId="11474" ContentLicense="CC BY-SA 4.0" />
  <row Id="14135" PostId="9356" Score="1" Text="Probably, there is a fft, wavelet or laplacian. Many of them are extremely noise resistant." CreationDate="2019-11-17T20:23:20.773" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14136" PostId="9356" Score="0" Text="Is there something that those techniques are weak against? I wonder if tone mapping or white balance type things would thwart them?" CreationDate="2019-11-17T20:45:42.853" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="14137" PostId="9356" Score="0" Text="Well usually a mirror image or gafting two pictures into one" CreationDate="2019-11-17T21:01:50.437" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14138" PostId="9356" Score="0" Text="Ah neat, so maybe mirror image and add some noise to cover your bases?" CreationDate="2019-11-17T21:02:49.113" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="14139" PostId="9356" Score="1" Text="@joojaa The laplacian is the opposite of noise resistant, it blows up noise disproportionately. That's why you usually apply Gaussian convolution before that. But facebook certainly uses something more robust, so it should deal with noise quite easily. Honestly, without knowing the algo, or extensive trial and error, it is very hard to know how to modify the image." CreationDate="2019-11-17T21:49:22.353" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14140" PostId="9357" Score="1" Text="Yes, everything after the first equation is wrong. The cosine in the denominator is not the same as the cosine in the numerator. As a matter of fact, if your light is a point light, the radiance for it is not technically defined, since dA makes no sense (it has no area)." CreationDate="2019-11-18T00:00:04.553" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14141" PostId="9357" Score="0" Text="Thanks for the reply. The light has an area of _LightRadius_ (see last line).&#xA;In fact equations 2 to 4 are from the [photon mapping algorithm](https://graphics.stanford.edu/courses/cs348b-00/course8.pdf)(see page 26).&#xA;Please could you clarify the difference between the cosines?" CreationDate="2019-11-18T02:09:52.583" UserId="11479" ContentLicense="CC BY-SA 4.0" />
  <row Id="14143" PostId="9357" Score="0" Text="If you have a light with radius you need stochastic sampling, which I doubt is what you are going for. The cosine in the denominator should be with respect to the normal at the point on the surface of the light from which the ray emanates, while the cosine in the rendering equation is with respect to the normal at the point being shaded. Please link an article describing the &quot;omni light&quot; you are going for." CreationDate="2019-11-18T07:36:19.000" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14144" PostId="9352" Score="0" Text="Cos in renderequation cancels it out?" CreationDate="2019-11-18T09:54:38.167" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14145" PostId="5867" Score="0" Text="http://nishitalab.org/user/nis/cdrom/sig93_nis.pdf" CreationDate="2019-11-18T11:43:23.390" UserId="8314" ContentLicense="CC BY-SA 4.0" />
  <row Id="14146" PostId="5867" Score="0" Text="https://cgg.mff.cuni.cz/projects/SkylightModelling/HosekWilkie_SkylightModel_SIGGRAPH2012_Preprint.pdf" CreationDate="2019-11-18T11:44:02.727" UserId="8314" ContentLicense="CC BY-SA 4.0" />
  <row Id="14147" PostId="5867" Score="0" Text="https://www.cs.utah.edu/~shirley/papers/sunsky/sunsky.pdf" CreationDate="2019-11-18T11:47:12.913" UserId="8314" ContentLicense="CC BY-SA 4.0" />
  <row Id="14148" PostId="9357" Score="0" Text="If i understand well the photon mapping simplification is wrong? Lights are simple isotropic spherical lights." CreationDate="2019-11-18T15:31:18.910" UserId="11479" ContentLicense="CC BY-SA 4.0" />
  <row Id="14149" PostId="9357" Score="0" Text="Photon mapping in general is not &quot;wrong&quot;. What you wrote however, seems wrong. If your lights are spherical, then you need to sample their surface, but this is not what omnidirectional lights are in realtime graphics. So my belief is that you are just confusing different terms and notions." CreationDate="2019-11-18T17:48:40.380" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14150" PostId="9357" Score="0" Text="Lines 2 to 4 are from the photon mapping algorithm.  The specialization to omnilights only appears at the last line. Then the first lines does not rely to any specific type of lights. If it is wrong it should be for all algorithm. I am not confused about the type of light. It is possible to cast rays in realtime with Direct x 12." CreationDate="2019-11-18T18:19:38.697" UserId="11479" ContentLicense="CC BY-SA 4.0" />
  <row Id="14151" PostId="9357" Score="0" Text="Not in a form I am familiar with, nor does it match the form its creator introduces in his book, so idk about that. There's no &quot;specialization to omnilights&quot; as far as I am aware. I am not even sure what omni-lights should have to do with the photon mapping formulation. It is possible to cast rays in realtime with dx12, however you cast those to spherical lights, which are usually not called omnilights, and then you don't have the funny simplifications you are trying to derive. Since it seems like you have multiple questions, I'd recommend you hop onto the CG discord, so I could clarify all." CreationDate="2019-11-18T18:50:12.240" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14152" PostId="9357" Score="0" Text="Thanks again for your help. I updated the question to make it clear it is about spherical lights. What I meant was the first 5 lines should be valid/invalid for any kind of light. What is the CG discord?" CreationDate="2019-11-18T19:36:52.307" UserId="11479" ContentLicense="CC BY-SA 4.0" />
  <row Id="14153" PostId="9361" Score="0" Text="The formuls of speed is the same in both cases" CreationDate="2019-11-18T20:07:30.237" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14154" PostId="9361" Score="0" Text="Can you please elaborate?" CreationDate="2019-11-18T21:27:12.323" UserId="11191" ContentLicense="CC BY-SA 4.0" />
  <row Id="14155" PostId="9361" Score="0" Text="Formula for (discrete) speed in abitrary dimension is the length or the vector from one frame to next. Length of a vector is just pythagoras theorem." CreationDate="2019-11-18T21:29:56.247" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14156" PostId="9362" Score="0" Text="Marvelous thanks! This is what I was looking for. What is the meaning of the lower exponent of ∥xx−yyk∥?&#xA;Also is it possible to have information about where the procedure is from?" CreationDate="2019-11-19T01:08:32.330" UserId="11479" ContentLicense="CC BY-SA 4.0" />
  <row Id="14157" PostId="9361" Score="0" Text="Thanks. How is Pythagoras theorem applied in this situation? what is the speed exactly? Could you help me understand the application to my specific case? thanks in advance" CreationDate="2019-11-19T10:22:08.200" UserId="11191" ContentLicense="CC BY-SA 4.0" />
  <row Id="14158" PostId="9362" Score="0" Text="@BLee It's indicating that this is the 2-norm/Euclidean norm. I don't think the procedure is from somewhere specifically - it's just using an estimator for the area formulation of the rendering equation. If you are asking where the area formulation was introduced - see Kajiya's paper: &quot;The rendering equation&quot;." CreationDate="2019-11-19T14:57:21.947" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14159" PostId="9321" Score="0" Text="Hey there, Reynolds! Would you mind elaborating on how should I treat the vertices and its normals considering the zero singularity, please? Perhaps this is where I am doing wrong." CreationDate="2019-11-19T15:12:17.160" UserId="11436" ContentLicense="CC BY-SA 4.0" />
  <row Id="14160" PostId="9361" Score="0" Text="Seems like your code was deleted. Do you need velocity in screenspace, or world space? Velocity is simply rate of change of position. You could calculate the distance between two 3D positions and get a scalar speed value." CreationDate="2019-11-19T19:58:09.830" UserId="9505" ContentLicense="CC BY-SA 4.0" />
  <row Id="14161" PostId="9364" Score="0" Text="They mean the unit sphere in four dimensions, which is what you're calling the hypersphere. In modern mathematical terminology it is rarely useful to restrict the term &quot;sphere&quot; to refer only to the one in three-dimensional Euclidean space." CreationDate="2019-11-20T08:06:33.783" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14163" PostId="9364" Score="0" Text="Okay it might help to think that the quaternionion is a weirdly encoded vector representation where we encode the system so that it makes sense only if the vector length is one. Offcourse the encoding is weird indeed and has properties that the vector representation does not have." CreationDate="2019-11-20T16:14:28.353" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14164" PostId="9354" Score="1" Text="do you mean copy from RAM to RAM vs. copy from VRAM to VRAM?" CreationDate="2019-11-20T20:06:43.910" UserId="9180" ContentLicense="CC BY-SA 4.0" />
  <row Id="14165" PostId="9365" Score="0" Text="Thank you for your help. But I still don't get why we can plot the unit quaternion as a point on a sphere. I have summarized my question as the second question, I wonder if you may have some suggestions?" CreationDate="2019-11-20T22:05:34.947" UserId="11492" ContentLicense="CC BY-SA 4.0" />
  <row Id="14166" PostId="9364" Score="0" Text="Thank you all.  I wonder if you could recommend some resources in why we can view a unit quaternion $q = (a+b\mathbf{i}+c\mathbf{j}+d\mathbf{k})$ as a point on a unit sphere $\mathbb{S}^2$?" CreationDate="2019-11-20T22:06:45.393" UserId="11492" ContentLicense="CC BY-SA 4.0" />
  <row Id="14167" PostId="9367" Score="0" Text="If you are smart you dont ise it!" CreationDate="2019-11-21T05:40:11.197" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14168" PostId="9361" Score="0" Text="@zeno The random spheres code is https://www.codepile.net/pile/Nj25dX36, and the optic flow code is https://www.codepile.net/pile/45NdxByK. I am very new to computer graphics, and I projected the world space on the screen, I would assume I have to calculate the speed on the screen? It is for a Neuroscience experiment, and I'd like to have the same speed perception between the two animations." CreationDate="2019-11-21T09:45:15.073" UserId="11191" ContentLicense="CC BY-SA 4.0" />
  <row Id="14169" PostId="9367" Score="1" Text="The algorithm works in the eight octants. Drawing a line from (0,0) to (let's say) (10,0) should quite straightforward. Start with that and it'll be easy to see why it doesn't step as expected." CreationDate="2019-11-21T09:46:46.177" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="14170" PostId="9363" Score="0" Text="If I understand you well:&#xA;- In the Random animation: the spheres are moving randomly in a 3D space. The formula of the speed for each sphere is the formula of the euclidean distance divided by the framerate?&#xA;- In the optic flow animation: the spheres are static, but the &quot;camera&quot; is moving towards them from the near to the far clipping plane. &quot;so all spheres have a relative velocity equal to the camera's velocity.&quot; Does this mean that each sphere's speed is equal to the camera speed? and is the formula of the camera speed is the euclidean distance divided by the framerate?" CreationDate="2019-11-21T09:55:38.030" UserId="11191" ContentLicense="CC BY-SA 4.0" />
  <row Id="14171" PostId="9369" Score="0" Text="I am not exactly sure, what your problem is. The UV set does not make sense, but it is a legal UV set nontheless. Perhaps you should specify why you find this to not make sense." CreationDate="2019-11-21T14:23:32.007" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="14172" PostId="9369" Score="0" Text="all point should be recalculated" CreationDate="2019-11-21T14:50:58.137" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="14173" PostId="9369" Score="0" Text="@Tare: See edit" CreationDate="2019-11-21T15:50:03.517" UserId="6346" ContentLicense="CC BY-SA 4.0" />
  <row Id="14174" PostId="9369" Score="1" Text="`And all other points [...] are interpolated[...]?` Actually that is up to you and your application. You can set up to have a linear filtering, in which case, yes, everything would be interpolated (although I think the 2.4 and 5.6 values you calculated might be wrong). You can also set up to always read out the nearest value, in which case your point A would read only the value 2 and see, it is closest, therefore just take it. Then you wouldn't have any interpolation. E.g: Take a look at this tutorial, showing the difference: https://learnopengl.com/Getting-started/Textures" CreationDate="2019-11-21T15:58:35.243" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="14175" PostId="9369" Score="0" Text="@Tare: Thanks for the link! (I wonder why the heck all my google search on UV mapping didn't bring this up). But how would you calculate the texture values in my example?" CreationDate="2019-11-21T16:09:18.667" UserId="6346" ContentLicense="CC BY-SA 4.0" />
  <row Id="14176" PostId="5903" Score="0" Text="The improved sampling parallelism you describe is intriguing - I have a fluid sim which is already implemented with compute shaders with a lot of instances of multiple samples per pixel.. Using groupshared memory to do single sampling with a memory barrier as you describe seems great, but I'm hung up on one bit - how do I access neighboring pixels when they would fall in a different work group? eg, if I have a 64x64 simulation domain, spread over a dispatch(2,2,1) of numthreads(16,16,1), how would the pixel with id.xy == [15,15] get its neighboring pixels?" CreationDate="2019-11-21T21:43:33.870" UserId="11510" ContentLicense="CC BY-SA 4.0" />
  <row Id="14177" PostId="9369" Score="1" Text="*Let's define coordinate 0, 0 for the left pixel and 1, 0 for the right one.*&#xA;Not quite.  If you imagine a 3x1 texel texture as a rectangle, then (0.0, 0.0) is the top left corner, and (1.0 , 1.0)  is the bottom right. (Err... I might have this upside down but bear with me).   Location  (1.0/6.0, 0.5) is the centre of the leftmost texel, (0.5, 0.5), the centre of the middle texel, and (5.0/6.0, 0.5), the rightmost.  These coordinates are then important when applying (e.g) bilinear filtering" CreationDate="2019-11-22T14:18:37.857" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14178" PostId="9352" Score="0" Text="@joojaa Can you elaborate?" CreationDate="2019-11-22T16:01:20.273" UserId="11474" ContentLicense="CC BY-SA 4.0" />
  <row Id="14179" PostId="9370" Score="2" Text="Technically, you should apply it after. However, applying it before averaging has an anti-aliasing effect (if you have samples that are too bright compared to the rest). The gamma correction part is only used for more optimal quantization and storage with respect to the human visual system." CreationDate="2019-11-23T15:27:33.433" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14180" PostId="5903" Score="0" Text="In that case, I see 2 main choices. 1) increase the group size over 64 and only write results for the 64x64 pixels. 2) first sample 64+nX64+n divided somehow in your 64x64 work group and then use that larger &quot;input&quot; grid for the computations. The best solution depends on your specific conditions of course and I suggest you write-up another question for more info since comments are poorly suited for this." CreationDate="2019-11-23T17:46:37.310" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="14181" PostId="9361" Score="0" Text="@Kathia then you have a problem. The parallax effects necessitates that the spheres move at different speeds." CreationDate="2019-11-23T21:44:51.060" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14182" PostId="9362" Score="0" Text="Thank you! One last question please. As you explained the cosine simplification I initially made was wrong. Since Photon Mapping is using it, isn't Photon Mapping wrong too?" CreationDate="2019-11-24T00:42:54.190" UserId="11479" ContentLicense="CC BY-SA 4.0" />
  <row Id="14183" PostId="9362" Score="0" Text="@BLee The photon mapping algorithm is not &quot;wrong&quot;, it's the image you pasted and your comments on it that are wrong. The cosine &quot;simplification&quot; in photon mapping is just rewriting the radiance as a double derivative of the flux, whereas you wrote intensity and you are talking about spherical lights as opposed to photons. Notably this is wrong: &quot;The simplifications show that the cosine term should be removed for spherical lights since power is a Flux.&quot;, it follows not from the eq. even if you were to write them correctly. Please make a new question for future discussions." CreationDate="2019-11-24T08:04:29.637" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14184" PostId="9374" Score="1" Text="&quot;*This is done in the fragment shader*&quot; That's silly. OpenGL has [dedicated `Proj` texture accessing functions that do the divide for you](https://www.khronos.org/opengl/wiki/Sampler_(GLSL)#Projective_texture_access). I'm surprised that the tutorial doesn't bother to use them, since letting you know they exist is half the point of such a tutorial in the first place." CreationDate="2019-11-24T14:46:35.863" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14185" PostId="9362" Score="0" Text="I never talked about intensity... &quot;I&quot; is the &quot;flux&quot; as mentioned at line 2. As I said equations 1 to 5 should be either valid, either wrong for any kind of light. The emitter could be photons, spherical, or any light with a random power distribution." CreationDate="2019-11-24T17:10:46.147" UserId="11479" ContentLicense="CC BY-SA 4.0" />
  <row Id="14187" PostId="9362" Score="0" Text="@BLee I is standard notation for intensity while $\Phi$ is standard notation for flux. The $(I \mapsto flux)$ is actually notation for function mapping, so it made even less sense (e.g. $f(x) = x^2 \iff x \mapsto x^2$). If that is flux, then 1,2,3 are correct (note that $*$ is generally used for convolution and not mult). 4 and 5 you have to formalize if you want them to mean something. I am not sure what 6 is about or where you took it from. Then again, are you doing photon mapping or do you just want to sample lights? I recommend formulating that in a new question, with more details." CreationDate="2019-11-24T17:57:16.833" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14188" PostId="3892" Score="0" Text="A couple of *very minor* points: 1) The monitor/display system is probably using sRGB which is a slightly different curve to the pow(colour, gamma_val)   (BUT you might be able to avoid that with https://stackoverflow.com/q/56204242/626644 )   2) if you are concerned about the performance, you *might* be able to get away with the approximation of using &quot;2&quot; for the gamma power. You can then use squaring and square roots which are relatively cheap." CreationDate="2019-11-25T09:42:57.417" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14189" PostId="9261" Score="0" Text="sampler2Darray has been core GL since 3.0 / GLSL330.   It was an extension before that but it certainly, definitely, absolutely supported (wokring now, core profile, not nvidia hardware, no extensions, etc)" CreationDate="2019-11-25T16:37:57.190" UserId="11322" ContentLicense="CC BY-SA 4.0" />
  <row Id="14190" PostId="9261" Score="0" Text="Nicol - That, in addition to the driver apparently returning an incorrect error code - was absolutely the problem.   Silly oversight, but didn't focus on it b/c of the code.   If you turn your comment into an answer I'll accept it." CreationDate="2019-11-25T16:38:45.737" UserId="11322" ContentLicense="CC BY-SA 4.0" />
  <row Id="14191" PostId="9376" Score="0" Text="Adaptive sampling, more samples, gradient domain path tracing." CreationDate="2019-11-25T17:16:54.190" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14192" PostId="9371" Score="0" Text="Because [the virtual image is inside the sphere](https://en.wikipedia.org/wiki/File:Convexmirror_raydiagram.svg), not far behind it like for a plane mirror." CreationDate="2019-11-25T17:29:00.470" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14193" PostId="9376" Score="0" Text="Ideally you wouldnt average but reconstruct the signal with a slightly higher order filter than a box filter. Yeah box blurs a lot Lanczos filter not so much." CreationDate="2019-11-25T18:33:07.517" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14194" PostId="9371" Score="0" Text="@Rahul   That's the one, for me.. you should make that an answer?" CreationDate="2019-11-25T19:22:40.737" UserId="8432" ContentLicense="CC BY-SA 4.0" />
  <row Id="14195" PostId="9375" Score="0" Text="Thanks Nicol,&#xA;&#xA;Before you answered I actually moved the lighting calculations over to the FS to see how they would perform... and gained a performance boost!!&#xA;&#xA;It's great to have your confirmation so I know this is the right track.&#xA;&#xA;128 vertex components &quot;ought to be enough for anybody&quot;.  :)" CreationDate="2019-11-25T21:51:33.500" UserId="11518" ContentLicense="CC BY-SA 4.0" />
  <row Id="14196" PostId="9376" Score="0" Text="@joojaa Thanks! So as I understand it, there's no way around other than trying better filters for the final averaging I guess?" CreationDate="2019-11-26T04:25:53.330" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="14197" PostId="9376" Score="0" Text="Well, i am just pointing out that if you use a strong blur as your reconstruction filter you shouldnt be supprised if you get blur as a result." CreationDate="2019-11-26T06:31:08.243" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14198" PostId="9376" Score="0" Text="@joojaa IIRC the problem with the box filter is that it blurs too much in some frequencies (particularly lower frequencies that you normally want to keep) and then fails to filter sufficiently in others!" CreationDate="2019-11-27T09:29:25.780" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14199" PostId="9376" Score="1" Text="@SimonF well if you think about it a bit its somewhat clear. Squares and sine waves dont work out very well. This is why something like a windowed sinc (aka Lanczos) works better. Hell even just switching to a triangle filter is better than box." CreationDate="2019-11-27T16:04:34.927" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14200" PostId="9378" Score="0" Text="Thanks! That pretty much confirms what I was thinking. In the case of my question above, we are not only reconstructing a texture image and resampling it with more samples per pixel (same thing we do when magnifying it), then we are also shrinking it in the final step to make it fit the final resolution. This final step introduces additional blur I guess." CreationDate="2019-11-28T06:06:19.773" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="14201" PostId="9378" Score="0" Text="&quot;Plus, our eyes have their own anti-aliasing filters that impart some noise to what we see, so we're somewhat used to it.&quot; - source?" CreationDate="2019-11-28T08:17:39.143" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14202" PostId="9378" Score="1" Text="@lightxbulb I'm not sure how Nicol's statement ties in, but with regards to just looking at a real-world scene,  I think there was something in Andrew Glasner's &quot;Principles of Digital Image Synthesis&quot; but someone has walked off with my copy of Volume 1....    However IIRC 1) The lens acts as a low-pass filter and then, in the centre regions of the eye, the density of cones is higher than the Nyquist limit  and 2) outside the central region, the cells are randomly distributed so the aliasing is remaoped into HF noise." CreationDate="2019-11-28T14:51:24.273" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14203" PostId="9378" Score="0" Text="@lightxbulb: I found something about it in &quot;An introduction to Ray Tracing&quot;, in the chapter &quot;Stochastic Sampling and Distributed Ray Tracing&quot;. In the section on Poisson Disk Sampling, it talks about how rod and cone cells are distributed in the retina. In the middle of the retina, they're packed in a honeycomb structure, which is better than a regular grid at handling aliasing. Towards the edges of the retina, the distribution mimics a poisson disk distribution." CreationDate="2019-11-28T15:01:53.523" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14204" PostId="9378" Score="0" Text="@Nicol Bolas This doesn't imply that it imparts noise, no? The PSF acts as a low pass filter, I can agree on that. I am not aware of noise being introduced by the human visual system however. At least not for people without defects of the visual system. Am I missing something? The distribution of the cones and rods is just your sample locations for sampling a 2d signal - so you should never get the noise inherent to Monte Carlo." CreationDate="2019-11-28T17:05:24.663" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14205" PostId="9378" Score="0" Text="Well technically a aa filter does not have to blur. It can also ring :)" CreationDate="2019-11-28T17:30:35.990" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14206" PostId="9378" Score="0" Text="@joojaa I do not believe that the HSV PSF introduces ringing artifacts. It is usually approximated by a Gaussian multiplied by a function that makes it anisotropic (diagonal directions are less important)." CreationDate="2019-11-28T17:37:49.200" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14207" PostId="9378" Score="0" Text="@lightxbulb: &quot;*This doesn't imply that it imparts noise, no?*&quot; Anti-aliasing isn't magic. If you digitally sample an analog signal at a sample rate lower than the Nyquist frequency for that signal, you will not reproduce the original. You will either get aliasing or noise. Anti-aliasing transforms what would have been aliasing into noise. The distribution of photoreceptors in the eye acts like poisson disk sampling of a signal, which produces a certain kind of noise instead of aliasing." CreationDate="2019-11-28T18:03:47.987" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14208" PostId="9378" Score="0" Text="Yeah the human vision is quite noisybut it should be easy to source a source on that." CreationDate="2019-11-28T18:54:24.013" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14209" PostId="9378" Score="0" Text="@NicolBolas In Monte Carlo rendering, the &quot;noise&quot; is actually your error, since at each pixel you have an inaccurate estimate. Due to the samples correlation of the seeds (or lack thereof) in screen space, this is perceived as noise. In the human visual system, at each &quot;pixel&quot; (sensor) you have an exact estimate - thus no error. There is no more noise in the human visual system than there is in sampling a 2d image. Blue noise distribution of receptors does not automatically equal noise. Unless you want to argue that the filling in is noisy (which you should know is not)." CreationDate="2019-11-28T19:10:23.943" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14210" PostId="9378" Score="0" Text="Please refer to &quot;Formation and Sampling of the Retinal Image&quot; by Thibos for more details. Notably for foveal vision you are in general limited by the PSF, not by your receptors density: &quot;The  reason  sampling limited  performance  is  not  normally  achieved  in  foveal  vision  is  because  the  extremely high  packing  density  of  adult  cone  photoreceptors  and  ganglion  cells  causes  the Nyquist  frequency  to  be  higher  than  the  optical  cutoff  of  the  eye.&quot;. In peripheral vision we still talk about aliasing and not noise:" CreationDate="2019-11-28T19:41:48.373" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14211" PostId="9378" Score="0" Text="&quot;Despite  these  favorable  conditions  for  undersampling, perceptual  aliasing  in  the  periphery  was  reported  for  the  first  time  only  relatively recently&quot;. Considering these results, you might want to correct the part about the HSV imparting noise." CreationDate="2019-11-28T19:43:13.570" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14213" PostId="9379" Score="1" Text="It makes your image unbiased. Which means you can average it with other unbiased images of the same scene to get better results. In general it increases variance, but also efficiency." CreationDate="2019-11-28T20:49:50.143" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14214" PostId="9379" Score="0" Text="Also see [this question](https://computergraphics.stackexchange.com/questions/2316/is-russian-roulette-really-the-answer)" CreationDate="2019-11-28T22:15:40.290" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14218" PostId="9382" Score="0" Text="Nice Idea. In your calculations on every iteration you calculate FULL precision - in floating point calculation calculations are never done in full precision (multiplication of 2 numbers usually create 2x more digits and half of this digits are cut off) - may be there is a way to introduce such 'cut off' in your idea - probably such cut off should be dynamic (and smaller in big zoom to not lost precision...)" CreationDate="2019-11-29T13:24:23.223" UserId="9881" ContentLicense="CC BY-SA 4.0" />
  <row Id="14219" PostId="9383" Score="0" Text="Wow.....you could be right... is this **true**?  If you look at the [example photo](https://imgur.com/Cj0GBZD), the direct background seems a comparable size to the reflected background.. but it's hardly a definitive test! I'll try to persuade myself of  it by your  method before accepting." CreationDate="2019-11-30T06:53:31.047" UserId="8432" ContentLicense="CC BY-SA 4.0" />
  <row Id="14221" PostId="9383" Score="0" Text="@RobinBetts it isn't the same size. Top: one balcony is about 1/3 the frame width. In the ball: you see a whole building (~6 balconies) in about the same width. Clearly not the same effective field of view. I think I could make a convincing example with a dSLR too. I just need to find a nice shiny ball." CreationDate="2019-11-30T13:01:20.560" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14222" PostId="9383" Score="0" Text="..and I've just confirmed your result" CreationDate="2019-11-30T15:20:51.090" UserId="8432" ContentLicense="CC BY-SA 4.0" />
  <row Id="14224" PostId="9388" Score="0" Text="This is true, and worth putting the answer up. The unspoken (sorry, I'll edit) goal is to find something not iterative." CreationDate="2019-12-02T03:21:28.800" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="14226" PostId="9389" Score="1" Text="Surely that will (or may) depend on what blend modes are being used? As long as they are &quot;associative&quot; (and you keep a destination alpha) it should be fine.  For example, if you use premultiplied alpha, you can go in either direction.&#xA;&#xA;Please read Jim Blinn's &quot;Compositing, Part 1: Theory&quot;" CreationDate="2019-12-02T09:38:07.227" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14228" PostId="9319" Score="0" Text="@SpatialDigger This sounds like a very interesting problem. Are you able to share any of the data, or pictures of the data, so we can get a sense of the challenge? If it requires supervised learning, you're in for spending a lot of money/time on labels. If you can solve it using &quot;classical&quot; approaches, like some clever meshing, then you'll be happy. Seeing the data would make it easier to guess what might work." CreationDate="2019-12-02T20:40:53.693" UserId="1988" ContentLicense="CC BY-SA 4.0" />
  <row Id="14229" PostId="9319" Score="0" Text="In fact, I'd love to help you over the phone, but I'm not sure if I'm allowed to offer that in comments." CreationDate="2019-12-02T20:48:57.207" UserId="1988" ContentLicense="CC BY-SA 4.0" />
  <row Id="14230" PostId="9388" Score="0" Text="oh. well in that case it might be worth looking into this thread about some strange connection between the projected cubic lattice, complex numbers, and polynomials https://twitter.com/stevejtrettel/status/1192827919447547905" CreationDate="2019-12-03T13:32:53.537" UserId="11547" ContentLicense="CC BY-SA 4.0" />
  <row Id="14231" PostId="9388" Score="0" Text="Very interesting!" CreationDate="2019-12-03T14:38:50.013" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="14235" PostId="9395" Score="0" Text="When you say, convert RGB to R'G'B', by the latter are you meaning sRGB encoded values?" CreationDate="2019-12-03T16:30:48.697" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14236" PostId="9395" Score="0" Text="Good question. It could theoretically be any non-linear color space. I have to rely on the meta data. My test device lists a BT.601 'transfer function' (same as sRGB as I understand) but then a BT.709-2 Y'CbCr matrix, and P3 D65 color primaries. I was hoping someone might shed some light on how these, on the surface conflicting standards, might coexist in the context of the above." CreationDate="2019-12-03T16:56:26.030" UserId="11553" ContentLicense="CC BY-SA 4.0" />
  <row Id="14238" PostId="9319" Score="0" Text="@JacobPanikulam it's fairly simple, imagine recording the ground using pointclouds, then you dig a hole and record again, then dig the hole deeper and again record. You get 3 georeferenced pointclouds which represent two volumes of soil. you just need to clean the pointclouds. I currently do this using cdist from scipy and selecting the minimum distance. `cdist(XA, XB, metric='euclidean').min()` the simplest question is getting the computer to choose which pointclouds relate to which volume, then selecting which points in those pointclouds define the volume, then it's just a meshing to a solid" CreationDate="2019-12-03T21:06:46.530" UserId="11439" ContentLicense="CC BY-SA 4.0" />
  <row Id="14239" PostId="9319" Score="0" Text="@SpatialDigger To repeat back: A reasonable example output would be two *closed* meshes, one containing the region excavated between dig 1 and dig 2, and one containing the region excavated between dig 2 and dig 3? Then the hope is to do some analysis on that? And to satisfy personal curiosity, do you have any papers from the archaeological community we could read about similar work?" CreationDate="2019-12-03T23:13:00.133" UserId="1988" ContentLicense="CC BY-SA 4.0" />
  <row Id="14241" PostId="9395" Score="0" Text="I (possibly incorrectly) have just assumed the steps are  YCbCr =&gt; Apply Matrix=&gt; r'g'b' (==sRGB) =&gt; apply gamma to decompress to linear RGB." CreationDate="2019-12-04T09:24:37.420" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14242" PostId="9398" Score="0" Text="Can you not just store some extra values on each vertex (e.g one or two displacement values) that you scale/sum to offset the vertex accordingly?  You don't need to alter the vertex data each frame, just the shader constants." CreationDate="2019-12-04T11:33:44.100" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14243" PostId="9395" Score="0" Text="I think you're right, although I'm not sure how the color space comes into play. This article from MS goes through the steps: https://docs.microsoft.com/en-us/windows/win32/medfound/extended-color-information#color-space-conversion I'm assuming after step 4 you can calculate luminance using the primaries from the meta data?" CreationDate="2019-12-04T11:53:37.937" UserId="11553" ContentLicense="CC BY-SA 4.0" />
  <row Id="14244" PostId="9395" Score="1" Text="Chapter 3 of &quot;Video Demystified&quot; might be of use to you https://sites.cs.ucsb.edu/~mturk/Courses/CS281B-2011/Misc/VideoDe.pdf" CreationDate="2019-12-04T14:49:58.507" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14245" PostId="9398" Score="0" Text="Why not use the vertex shader, polar coordinates and noise?" CreationDate="2019-12-04T22:45:13.667" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="14246" PostId="9245" Score="0" Text="The medial axis is easier to calculate then the straight skeleton, the edge cases of the straight skeleton are a nightmare!" CreationDate="2019-12-04T23:04:55.963" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="14248" PostId="5295" Score="0" Text="@JulienGuertault In Idea A, say #1 is done, how do you render the mesh into depth buffer?" CreationDate="2019-12-06T13:39:10.627" UserId="11569" ContentLicense="CC BY-SA 4.0" />
  <row Id="14249" PostId="9403" Score="1" Text="It looks like the top image is rendered to a lower resolution render target (or texture)." CreationDate="2019-12-07T12:26:02.113" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14250" PostId="9403" Score="1" Text="@PaulH It was the first which I was suspecting. For testing I’ve generated the gradient coloring in Webgl and the result was perfect. &#xA;I suspect that the reason behind the problem is that by the ES GLSL 2.0 standard (which is used in WebGL). float should contain at least 16 bit. They give a freedom in the standard to implement more precise floats, but don’t require that. At the same time the float in OpenGL is 32 bit" CreationDate="2019-12-07T12:56:16.100" UserId="10139" ContentLicense="CC BY-SA 4.0" />
  <row Id="14252" PostId="9403" Score="2" Text="@David - you are correct about that. highp in GLES is 16bit where as in normal opengl floats are 32 bit. I think that's where the problem may lie. You can try to use 16 bit floats in normal OpenGL and see if it produces the same result as WebGL one" CreationDate="2019-12-08T07:53:31.630" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14253" PostId="9405" Score="1" Text="What API are you looking to use? In general you just need image and text rendering libraries." CreationDate="2019-12-08T12:46:27.613" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14254" PostId="9405" Score="0" Text="@lightxbulb I'm not sure if I was planning on using an API to handle this. I've never really worked with image and text rendering libraries before, especially one that can do both the rendering and the exporting. Any recommendations for something web-based?" CreationDate="2019-12-08T20:16:13.503" UserId="11577" ContentLicense="CC BY-SA 4.0" />
  <row Id="14255" PostId="9405" Score="0" Text="WebGL can be used for the 3d stacking effect (load the texture and apply it on several quads). There are many libraries providing text rendering for it (for example Three.js)." CreationDate="2019-12-09T00:53:19.307" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14256" PostId="9405" Score="0" Text="@DerekMei you do undrerstand that it is very rare for people not to use an API of somekind. But i guess a person bootstrapping a new processor technically need not be using an api." CreationDate="2019-12-09T07:52:26.727" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14257" PostId="9329" Score="1" Text="Because its normalised to fit in RGB range (0..1), when expanded to a normalised vector range (-1 to 1) by performing N=(colour-0.5)*2 then that RGB becomes (0,0,1) which is the default normal" CreationDate="2019-12-09T08:24:21.973" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14258" PostId="9409" Score="0" Text="Why do you think it should  always decrease?" CreationDate="2019-12-09T09:06:39.410" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14259" PostId="9409" Score="0" Text="I don't know, it was just intuition. I don't think I really understand what it is suppose to represent" CreationDate="2019-12-09T09:08:35.177" UserId="11452" ContentLicense="CC BY-SA 4.0" />
  <row Id="14260" PostId="9413" Score="1" Text="To answer the title question: it is not." CreationDate="2019-12-09T21:15:10.840" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14261" PostId="9417" Score="0" Text="Isnt this a standard pole vector problem? Z is vector from center of circle. X is Z cross up (vector from center to north pole) , Y is Z cross X and the translate is R" CreationDate="2019-12-11T18:46:25.460" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14262" PostId="9417" Score="0" Text="I am not sure I am understanding your question, this isn;t a kinematic issue" CreationDate="2019-12-11T18:50:55.997" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14263" PostId="9417" Score="0" Text="You understand affine matrices?" CreationDate="2019-12-11T18:54:43.617" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14264" PostId="9417" Score="0" Text="I want to say yes, it's a transformation that remains the same regardless of the position of the input. e.g trnaslation is affine, scaling isn;t" CreationDate="2019-12-11T18:56:23.203" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14265" PostId="9417" Score="0" Text="Scaling is affine, so is skew, rotate, mirror and translate. Anyway all you need is to construct the tranformation matrix as described and stick yoir camera under that. (This works well eveywhere except on the singularities the pole)" CreationDate="2019-12-11T18:58:55.053" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14266" PostId="4183" Score="0" Text="Couldnt you cheat a bit though. You could raycast on a cube and solve which parta of cube edge hits which sphere." CreationDate="2019-12-11T19:09:56.200" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14267" PostId="9417" Score="0" Text="while joojaa is right I think you are more concerned with how to calculate the fraction of the total lenght of the vector `d` for each mouse wheel click (as the camera gradually zooms) if I am correct? Why don'y you just take a small fraction of it say `0.1*d`" CreationDate="2019-12-11T20:13:40.483" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14268" PostId="9417" Score="0" Text="The camera needs to move towards the cursor as the user clicks the wheel, that's the main thing." CreationDate="2019-12-11T20:22:33.070" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14270" PostId="9417" Score="0" Text="Oh okay so you are asking the procedure on how to move the spherical camera in the same way as google map does (but it isn't a spherical camera)" CreationDate="2019-12-11T20:24:10.130" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14271" PostId="9417" Score="0" Text="So, if I understand correctly, you are saying to move both the camera and the center along the vector cursor - camera&#xA;&#xA;Correct?" CreationDate="2019-12-11T20:24:46.513" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14272" PostId="9417" Score="0" Text="That'd be my initial guess yes just nudge the camera basically the whole trackball system as it is a little in the direction of the translation then zoom" CreationDate="2019-12-11T20:25:16.350" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14273" PostId="9417" Score="0" Text="@gallickgunner Yes I know google maps isn;t a spherical camera, I am nonetheless trying to create the same effect." CreationDate="2019-12-11T20:25:30.907" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14274" PostId="5295" Score="0" Text="@JuneWang You can render a mesh with depth writing enabled but without writing to the color buffer. See for example something called **&quot;Z pre-pass&quot;** or **&quot;depth pre-pass&quot;**." CreationDate="2019-12-12T21:54:51.820" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="14275" PostId="9419" Score="0" Text="Is the bspline aspect relevant to your question? I don't see how and it looks like you're actually asking about polygon quadrangulation." CreationDate="2019-12-14T01:34:21.667" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14279" PostId="9422" Score="0" Text="`I'm writing this from memory but I think you need to divide by the PDF. Which means that your weights will be close to 1. But not quite 1 as the sampling distributions don't include the shadowing terms.`  &lt;br&gt; So this probably sounds like beginner question, but how exactly do I divide by a PDF. Do you know some good sources where I might read up on that?" CreationDate="2019-12-14T15:37:50.037" UserId="11598" ContentLicense="CC BY-SA 4.0" />
  <row Id="14280" PostId="9422" Score="0" Text="Is it just dividing the BTDF by D(m)|m * n|?" CreationDate="2019-12-14T17:49:50.107" UserId="11598" ContentLicense="CC BY-SA 4.0" />
  <row Id="14281" PostId="9422" Score="1" Text="@Clutterhead Yes, that's your sampling distribution and it is normalized (integrates to 1) so it is a PDF. You can see (sampling with D(m) + division by PDF) as an alternative to (uniform sampling everywhere). The former yielding lower noise but otherwise the same result." CreationDate="2019-12-15T16:10:13.803" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14282" PostId="9423" Score="0" Text="this worked but I had to change Float4 to Float3 and had to use a jpg instead of png. Float4 format still gives me an error saying that the image format is unsupported." CreationDate="2019-12-15T17:10:37.500" UserId="8962" ContentLicense="CC BY-SA 4.0" />
  <row Id="14284" PostId="9423" Score="0" Text="I feel it's not supported as per official doc: &#xA;&#xA;https://openimagedenoise.github.io/documentation.html" CreationDate="2019-12-16T05:47:13.490" UserId="5066" ContentLicense="CC BY-SA 4.0" />
  <row Id="14286" PostId="9403" Score="0" Text="@PaulHK You were completely right. I used css styles to setup the width and height of the canvas up to 100%. And it was just stretching the canvas, but the actual size was of viewports was 300x300 pixels" CreationDate="2019-12-16T10:56:52.803" UserId="10136" ContentLicense="CC BY-SA 4.0" />
  <row Id="14287" PostId="9422" Score="0" Text="I tried blending the reflection sample and the refraction sample with the corresponding Fresnel values and got the following image, with which I'm quite satisfied:&#xA;![MyImage](https://imgur.com/a/pSDrhfK).&#xA;&#xA;But still I didn't use any weight or the BxDF terms, so I feel this isn't done properly. &#xA;Every time I tried to combine a sample with a weight it just ended in a value too big or too small.&#xA;I tried dividing the BTDF by the PDF, or by the weight (given in 5.3).&#xA;Or just dividing the sample by them &#xA;Or is there something else I'm supposed to do with the BxDF value?" CreationDate="2019-12-16T11:08:26.660" UserId="11598" ContentLicense="CC BY-SA 4.0" />
  <row Id="14288" PostId="9424" Score="0" Text="Sure that should work" CreationDate="2019-12-16T22:28:24.343" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14289" PostId="9424" Score="0" Text="@joojaa Thanks! In addition, i also wanna scale the model by a factor $f$. Do i have to apply the scaling matrix before $T$ (as in $T  \cdot S$), or after it (as in $S\cdot T$)? I never worked with base change matrices before." CreationDate="2019-12-16T22:58:34.237" UserId="11605" ContentLicense="CC BY-SA 4.0" />
  <row Id="14290" PostId="9424" Score="0" Text="Depends on about what you want to scale. OTOH if you do a $T^{-1} S T$ it does not matter much." CreationDate="2019-12-17T07:19:26.910" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14291" PostId="9424" Score="0" Text="@joojaa I basically want to scale the model I am placing, so I would like to basically scale it, and then move it into the new coordinate system. That would be $T * S$ then, right?" CreationDate="2019-12-17T14:37:33.287" UserId="6997" ContentLicense="CC BY-SA 4.0" />
  <row Id="14292" PostId="9424" Score="0" Text="But scale about what point" CreationDate="2019-12-17T14:53:19.807" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14293" PostId="9424" Score="0" Text="@joojaa the local model origin" CreationDate="2019-12-17T16:49:41.897" UserId="6997" ContentLicense="CC BY-SA 4.0" />
  <row Id="14294" PostId="9426" Score="0" Text="rayd.x = dir.x + plane.x * cameraX;&#xA;      rayd.y = dir.y + plane.y * cameraX;  -- Shouldn't that second rayd.y assignment be taking a cameraY variable instead ?" CreationDate="2019-12-18T05:47:48.560" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14295" PostId="9426" Score="0" Text="CameraX is -1 to 1 and corresponds with the X axis on screen, so no" CreationDate="2019-12-18T09:24:22.620" UserId="11609" ContentLicense="CC BY-SA 4.0" />
  <row Id="14300" PostId="9364" Score="0" Text="You will likely get much better answers for this sort of question here: https://math.stackexchange.com/ While quaternions are used in computer graphics, most graphics engineers don't put a significant amount of effort into understanding the theoretical mathematics any more deeply than what you've already stated in your question." CreationDate="2019-12-21T18:43:20.420" UserId="6145" ContentLicense="CC BY-SA 4.0" />
  <row Id="14302" PostId="9417" Score="2" Text="The goal seems a bit strange since the entire point of an orbiting camera is to keep the target point at the center. I would recommend doing what Blender does, and having the target point change only when the user pans. Doing what google maps does in 3D doesn't make much sense because there are infinitely many positions along Z that match the mouse coordinates, and even zooming changes which of those points are visible (assuming perspective projection)." CreationDate="2019-12-21T19:44:04.980" UserId="6145" ContentLicense="CC BY-SA 4.0" />
  <row Id="14303" PostId="9435" Score="0" Text="Wait, does pincushion distortion actually crush inward? I always got the idea it just pulls the edges outward :P" CreationDate="2019-12-21T19:51:12.460" UserId="11623" ContentLicense="CC BY-SA 4.0" />
  <row Id="14305" PostId="9435" Score="0" Text="Please disregard my previous deleted comments, I was being a dumbass. Your solution does 100% what I need. I'll be creating a gist tonight which you may include in your answer if you'd like." CreationDate="2019-12-21T22:04:32.610" UserId="11623" ContentLicense="CC BY-SA 4.0" />
  <row Id="14306" PostId="9435" Score="0" Text="Here is the gist for the final solution: https://gist.github.com/aggregate1166877/a889083801d67917c26c12a98e7f57a7 . I've added a preview to my question for the sake of completion." CreationDate="2019-12-22T07:13:10.567" UserId="11623" ContentLicense="CC BY-SA 4.0" />
  <row Id="14307" PostId="9435" Score="0" Text="@aggregate1166877 Nice work! Glad my answer was useful. Thanks for sharing your final solution." CreationDate="2019-12-23T00:36:36.477" UserId="6145" ContentLicense="CC BY-SA 4.0" />
  <row Id="14308" PostId="9432" Score="0" Text="Thanks for going all the way &amp; editing the image to make it obvious :) &#xA;&#xA;However, this still doesn't clear from where (Pi/4) factor came in the first quarter..." CreationDate="2019-12-23T04:48:51.793" UserId="5066" ContentLicense="CC BY-SA 4.0" />
  <row Id="14309" PostId="9436" Score="0" Text="Make point in middle of cylinder cap" CreationDate="2019-12-23T06:39:59.593" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14310" PostId="9438" Score="0" Text="&quot;*The bresenham's version just look terrible and feels laggy while the floating point version is much smoother.*&quot; What about the massive amount of dot crawl in your &quot;naive&quot; algorithm? I'd call that pretty rough." CreationDate="2019-12-23T14:26:53.800" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14311" PostId="9438" Score="0" Text="There is no point on a modern platform, integer algorithms were intended for machines from the 70-80s.." CreationDate="2019-12-23T14:26:59.450" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14313" PostId="9439" Score="0" Text="Projective, homogeneous; homogeneous, projective. I'll include both names in the question." CreationDate="2019-12-23T15:40:16.123" UserId="11630" ContentLicense="CC BY-SA 4.0" />
  <row Id="14314" PostId="9439" Score="0" Text="I'm confused by your vector magnitude example. If you are using homogenous coordinates then  surely all vectors of the form ${W.V_x, W.V_y, W.V_z, W}$ are &quot;equivalent&quot; and so wouldn't the magnitude thus be $\frac{\sqrt{V_x^2+V_y^2+V_z^2} }{W}$" CreationDate="2019-12-23T15:45:12.897" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14315" PostId="9439" Score="0" Text="@SimonF: That's why I specified that I'm assuming vectors are in canonical form, with $w=1$." CreationDate="2019-12-23T16:04:13.313" UserId="11630" ContentLicense="CC BY-SA 4.0" />
  <row Id="14316" PostId="9439" Score="0" Text="Oops. Sorry, missed that!  FWIW you do get use cases, however, where $w\neq1$ in graphics e.g. extruding shadow volumes to infinity or doing rational  splines." CreationDate="2019-12-23T16:11:25.933" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14317" PostId="9440" Score="0" Text="Ok but regarding the visual result, which of the 2 versions is preferrable? The first version feels laggy but produces perfect circles whereas the second version is much smoother but if we pause the animation, the circles are not very round" CreationDate="2019-12-23T17:02:42.157" UserId="10928" ContentLicense="CC BY-SA 4.0" />
  <row Id="14318" PostId="9440" Score="0" Text="You're asking for an 'artistic/ aesthetic’ evaluation?  That's getting into trading-off temporal vs spatial sampling/aiasing/noise." CreationDate="2019-12-23T17:31:21.573" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14319" PostId="9440" Score="0" Text="Actually, having taken a snapshot of the &quot;floating point&quot; version, it seems to me that the 'line width' is inconsistent." CreationDate="2019-12-23T17:35:14.167" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14320" PostId="9440" Score="3" Text="@Jojolatino The bresenham version is not &quot;laggy&quot; because of misprediction - it simply allows the circle to move only by a single pixel at a minimum. The floating point version allows subpixel precision in some sense,  which causes not all points of the circles to move by the same amount creating an illusion of smoothness. What you are seeing is a combination of aliasing and temporal discontinuity, the floating point version handles it better because it does not enforce the exact same sampling, which gives more of a leeway for temporal continuity." CreationDate="2019-12-23T19:38:47.640" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14321" PostId="9439" Score="0" Text="Yes, when your 4D vector is not just a projective extension of your 3D one. Some examples off the top of my head: spatio-temporal vector, 4d rendering, quadric surfaces, quaternions." CreationDate="2019-12-23T19:42:57.117" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14322" PostId="9439" Score="0" Text="@lightxbulb: My understanding was that quaternions are treated separately, as they come with their own set of operations; but maybe those operations are identical in calculation (if not in concept) to the vector operations? As for 4D rendering, I am not familiar with this and would like to know more..." CreationDate="2019-12-24T02:24:24.127" UserId="11630" ContentLicense="CC BY-SA 4.0" />
  <row Id="14323" PostId="9439" Score="0" Text="You can write quaternions as 4D vectors as long as you  define the operations correctly. Then the magnitude is your usual 4d vector magnitude (for rotation quaternions it is always 1). 4D rendering is just what it implies - rendering a 4D scene. This is usually achieved by projecting the 4D world to a 3D volume and then projecting that to a 2D plane." CreationDate="2019-12-24T10:43:06.873" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14324" PostId="9444" Score="0" Text="It can have a brdf, there's no &quot;no scattering is present&quot; requirement." CreationDate="2019-12-26T07:20:52.663" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14325" PostId="9444" Score="0" Text="@lightxbulb But isn't it the case that we usually stop tracing after we hit a light source (when we start a random walk on the camera)?" CreationDate="2019-12-26T07:35:42.930" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14326" PostId="9444" Score="0" Text="No, we usually do not, unless the brdf is perfectly absorbing. If you did this for light sources with a brdf that is not perfectly absorbing you will get a wrong solution of the rendering equation. Whether you decide to model all your light sources as perfectly absorbing or not in practice is a separate question that does not affect the theory." CreationDate="2019-12-26T10:36:44.143" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14329" PostId="9445" Score="0" Text="Thank you for your answer. Well, I guess what's confusing me is that in the context of the path integral formulation of the light transport equation it seems like that we only consider the emitted radiance from the first vertex. To be precise, if we consider a path $(x_0,\ldots,x_k)$ of length $k$, where $x_k$ is on the camera, there's only the emitted radiance $L_e(x_k\to x_{k-1})$ occurring in the expansion of the recursive area form of the LTE. Or am I missing something?" CreationDate="2019-12-27T13:58:30.777" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14330" PostId="9444" Score="0" Text="@lightxbulb Please take note of my comment below [httpdigest's answer](https://computergraphics.stackexchange.com/a/9445/9254)." CreationDate="2019-12-27T13:59:03.060" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14331" PostId="9445" Score="0" Text="If you are referring to the path integral formulation as done in Veach's thesis on chapter 8, then a path starting from the eye and ending at a light source is not meant to say that we always stop there. If the light source is a surface reflecting or transmitting light, there will still be other paths along the same vertices as the first path from the eye to the light source but we do not consider the surface a light source then, but keep on going further along more path vertices until we hit another light source. And the final result is the accumulations of all such paths." CreationDate="2019-12-27T15:14:09.100" UserId="8592" ContentLicense="CC BY-SA 4.0" />
  <row Id="14332" PostId="9445" Score="0" Text="Yes, I know. I was actually talking about the measurement contribution function as defined on page 223 of Veach's thesis." CreationDate="2019-12-27T15:34:41.437" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14333" PostId="9445" Score="0" Text="I am not sure I still understand what your question is now. Want exactly are you asking?" CreationDate="2019-12-27T15:47:39.367" UserId="8592" ContentLicense="CC BY-SA 4.0" />
  <row Id="14334" PostId="9445" Score="0" Text="@0xbadf00d The earlier $L_e$ are simply absorbed by the earlier terms of the Neumann expansion. When you do recursive bounces, and are not actually restarting the path for each path length you are really estimating the first $M$ terms of the Neumann expansion, where $M$ is the number of bounces you do. That is you actually evaluate $M$ estimators. Reusing the throughput you have computed on the path is just an optimization, one could technically regenerate paths from scratch for each path length." CreationDate="2019-12-27T16:46:12.587" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14335" PostId="8321" Score="0" Text="You can technically feed any pointset in the inverse transform mapping. As a special case this pointset may be produced by jittering the points if a regular grid, so that each sample is jittered within its cell. What they have done there is just mix up the two. I'd suggest keeping it modular." CreationDate="2019-12-27T16:54:59.673" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14336" PostId="9448" Score="6" Text="A square can be made of two triangles. A cube can be made out of 12 triangles because it has 6 square faces, and you can decompose each into 2 triangles. Triangles are used in graphics because they are simple, have nice properties, and define a plane uniquely if not degenerate (all vertices lie on a line/in a single point)." CreationDate="2019-12-28T09:35:27.827" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14337" PostId="9449" Score="0" Text="Sure, you can use gl_VertexID / 3 (for triangles). however this only works for non-indexed rendering." CreationDate="2019-12-28T17:28:31.590" UserId="8592" ContentLicense="CC BY-SA 4.0" />
  <row Id="14338" PostId="9447" Score="0" Text="You might find the discussion, code and links at [svgo's issue #65](https://github.com/svg/svgo/issues/65) relevant." CreationDate="2019-12-29T09:06:45.543" UserId="30" ContentLicense="CC BY-SA 4.0" />
  <row Id="14339" PostId="9447" Score="0" Text="This is in fact a bit error prone. I do some amount of  cnc work and due to the controller of some machines it would be great to identify circles and circle segments. But the thing is to identify circles well the software needs to be quite agressive at detecting them. Which is bad because it kills the carefully constructed C2 continious fillets of some users. The problem being that a bezier is never a circle so there is allways some guesswork involved especially since there are many ways to approximate a circle." CreationDate="2019-12-29T14:39:43.440" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14340" PostId="9449" Score="1" Text="@httpdigest: It also only works for drawing triangle lists." CreationDate="2019-12-29T16:48:37.340" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14341" PostId="9452" Score="0" Text="The first is not light tracing, nor is the second path tracing starting from the camera. in fact the second follows from the first only if a Neumann expansion is valid: &#xA;$$(I-F)L = L_e$$&#xA;$$L= (I-F)^{-1}L_e = \sum_{k=0}^{\infty}F^kL_e$$&#xA;The equations just represent the steady state, they are unrelated to where you start from (you have no sensor sensitivity function anyways in this formulation)." CreationDate="2019-12-30T07:40:19.173" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14342" PostId="9452" Score="0" Text="That is to say, $L$ gives you the radiance at every point and in every direction of the scene, not just at the camera or light." CreationDate="2019-12-30T07:54:29.770" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14343" PostId="9452" Score="0" Text="@lightxbulb Thanks for the explainations, would you mind expanding the comments into an answer?" CreationDate="2019-12-30T12:18:16.563" UserId="6470" ContentLicense="CC BY-SA 4.0" />
  <row Id="14344" PostId="9450" Score="0" Text="It doesn't sound like your BSP tree is correct for CSG operations. BSP tree's for CSG should have nodes which contain at least half space information (position and normal) or ideally a triangle (to check for coplanar cases), either original triangle of the BSP'ed mesh, or the split triangle as a result of spliting when an original mesh triangle straddles the plane/halfspace of another node. Inside/Outside is deduced by following the nodes inside/outside children, or splitting when straddling and putting back through the tree. No inside child node means triangle is inside and vice versa." CreationDate="2019-12-30T14:03:57.483" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14345" PostId="9450" Score="0" Text="If you do this, you will have a new mesh within the tree (each node has a triangle) and this is easy to visualise. See here for an example of BSP CSG in C++ https://github.com/spiroyster/qdcsg/blob/master/qdcsg.hpp" CreationDate="2019-12-30T14:04:16.663" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14346" PostId="8663" Score="0" Text="Going through metal and trying to understand this bit. I am interested in seeing complete sample code" CreationDate="2019-12-31T04:03:34.430" UserId="11664" ContentLicense="CC BY-SA 4.0" />
  <row Id="14347" PostId="9455" Score="2" Text="In general you cannot store $N$ bytes into $\frac{N}{3}$ bytes without some form of lossy compression. Depending on the different needs, assumptions, and constraints, entirely different compression methods may be adequate." CreationDate="2019-12-31T12:07:51.577" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14348" PostId="8663" Score="1" Text="It has been a few months since I looked at this code. I'll review it and get back to you in a few days." CreationDate="2020-01-01T04:02:10.550" UserId="10117" ContentLicense="CC BY-SA 4.0" />
  <row Id="14350" PostId="9455" Score="0" Text="Unit directions can always be represented in two floats (longitude &amp; latitude), and you could store them quantized as a pair of half-floats. Does one of your directions point toward the other position? If that's true, you could store one position as 3 half-floats, and the other position as a half-float distance. That would be a total of 8 half floats, or 1 float4." CreationDate="2020-01-01T20:11:24.903" UserId="11494" ContentLicense="CC BY-SA 4.0" />
  <row Id="14351" PostId="9457" Score="0" Text="Admittedly the meanings of the variable names aren't entirely clear to me, (eg does camera refer to the camera *or* a vertex transformed into camera space...I'll assume the latter) but that does *not* look like the correct thing to do.  You _can't_ do the division by Z before doing clipping. You must clip each line/primitive first before the division." CreationDate="2020-01-02T11:41:08.360" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14352" PostId="9455" Score="0" Text="Are you sure you cannot add more attributes? you should have up to 16 vec4s worth of attribute space for the vertex shader input data." CreationDate="2020-01-03T10:55:37.793" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="14353" PostId="9461" Score="0" Text="Looks like a blue noise point set. You can try Lloyd relaxation." CreationDate="2020-01-03T14:26:32.593" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14354" PostId="9461" Score="0" Text="Thanks for your comment. As I said, the generation of the sampling parts is more or less clear to me. I am more having problems with the threshold values. Does Lloyd relaxation help with that? I guess it at least gives a more explicit notion of which points are neighbors (thanks to the voronoi graph)." CreationDate="2020-01-04T09:16:30.177" UserId="11679" ContentLicense="CC BY-SA 4.0" />
  <row Id="14355" PostId="9461" Score="0" Text="I think that maximizing the distance to the neighbours is with respect to the sample locations. The values that the samples hold (threshold values) seem to be required to be evenly distributed. The problem is that this can mean too many things, and just from the slides it's not obvious which meaning exactly they have in mind. I would go with uniform random thresholds." CreationDate="2020-01-04T09:42:11.640" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14356" PostId="9461" Score="0" Text="Sorry maybe I failed to describe this in much detail. A couple of slides earlier they speak about ordered dithering and that there the threshold values are picked such that they maximize the distance to neighboring values. In the speaker notes for this presentation they then say that they apply the same to their own sampling pattern." CreationDate="2020-01-04T10:48:16.653" UserId="11679" ContentLicense="CC BY-SA 4.0" />
  <row Id="14357" PostId="9461" Score="0" Text="Ordered dither is usually defined on a rectilinear grid however. My best guess would be blue noise on a blue noise distributed pointset: $\sum_k v_k\delta(x - v_k)$ having a blue noise spectrum, but I am nkt aware how to optimize this efficiently on the GPU. Some pseudo-Jacobi method for example. All in all, the slides simply do not provide enough details, look for papers." CreationDate="2020-01-04T11:11:41.527" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14358" PostId="9463" Score="3" Text="Yes. Find the diameter of the mesh, and use it for a bounding sphere (or a bounding box containing that sphere). That way the box will be loose, but you won't need to recompute it. The other option is to work with a lower LoD version of the mesh (that bounds it) and recompute the AABB for it." CreationDate="2020-01-04T14:06:41.290" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14359" PostId="9450" Score="0" Text="@lfgtm Thanks for the comment and sorry about a late answer, I was sick.&#xA;&#xA;I don't understand what you mean with the triangle for a coplanar case. My nodes will just contain a point and normal to specify the plane. I also don't understand why you mention inside/outside deduction. Like I wrote in OP, I get how to do the merge - I'm not sure how to actually render the resulting tree. This doesn't really have anything to do with CSG operations - I was just hoping there might be an easier way to render the resulting tree, but I guess I'll just implement a mesh-slicing function in the end." CreationDate="2020-01-04T20:09:02.580" UserId="11656" ContentLicense="CC BY-SA 4.0" />
  <row Id="14360" PostId="9463" Score="0" Text="@lightxbulb Thanks for your hint =) specially a version of mesh with lower LoD looks like a good idea :)" CreationDate="2020-01-05T05:39:40.433" UserId="9169" ContentLicense="CC BY-SA 4.0" />
  <row Id="14361" PostId="9450" Score="0" Text="If not CSG (https://en.wikipedia.org/wiki/Constructive_solid_geometry), What do you mean by boolean operations on solids?" CreationDate="2020-01-05T21:50:03.617" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14362" PostId="9463" Score="0" Text="@user3405291 be careful that your lower LoD version actually enclose the object. eg. The typical polygonal approximation to a circle can, if oriented just right, yield a box which leaves out parts of the circle." CreationDate="2020-01-06T02:44:08.707" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14363" PostId="9463" Score="0" Text="@Olivier I didn't quite grasp what you meant. Can you provide a link to an example? Thanks :)" CreationDate="2020-01-06T05:36:58.030" UserId="9169" ContentLicense="CC BY-SA 4.0" />
  <row Id="14365" PostId="9450" Score="0" Text="I mentioned CSG for context, because it's basically the topic. For example however, I'm not really worried about starting with 2 meshes - I'm completely ignoring the fact of converting a mesh to a BSP. I'm starting with two BSP trees, I merge them, and I display the result.&#xA;&#xA;In essence when doing CSG with BSP trees the problem can be split into:&#xA;1, Convert meshes to BSP trees.&#xA;2, Merge trees with the desired operation.&#xA;3, Convert resulting BSP tree back to a mesh.&#xA;&#xA;What I'm essentially trying to do, is 3, in easiest possible way, or even skip it and just &quot;render&quot; the resulting tree." CreationDate="2020-01-06T18:06:40.863" UserId="11656" ContentLicense="CC BY-SA 4.0" />
  <row Id="14366" PostId="9450" Score="0" Text="Just out of curiosity, would you mind expanding on why you use the triangle for the plane representation and what you mean by the co-planar case? I don't understand how using a triangle helps compared to just using a plane. Numerical robustness?" CreationDate="2020-01-06T18:12:53.000" UserId="11656" ContentLicense="CC BY-SA 4.0" />
  <row Id="14374" PostId="9464" Score="0" Text="If its isometric you can use the area of the sides and their centroids for starters?" CreationDate="2020-01-07T11:46:31.520" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="14375" PostId="9463" Score="1" Text="@user3405291 Draw a circle (high detail model). Draw an axis aligned square with its four vertices on the circle (very low detail model). The bounding box of the square is the square itself. But it leaves out parts of the circle. The same can happen with a more general and 3D model, causing subtle errors with the bounds. Those may not matter to your specific application though." CreationDate="2020-01-07T12:54:31.440" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14376" PostId="9463" Score="0" Text="@Olivier Now I got it =)" CreationDate="2020-01-07T13:26:28.827" UserId="9169" ContentLicense="CC BY-SA 4.0" />
  <row Id="14377" PostId="9464" Score="0" Text="I don't have the cube shape (area, edges, surface, etc). The image is just 3D cube that tagged with yaw, pitch &amp; roll and I need to know what are the new yaw, pitch &amp; roll after camera rotation." CreationDate="2020-01-07T13:58:27.573" UserId="3305" ContentLicense="CC BY-SA 4.0" />
  <row Id="14378" PostId="9476" Score="0" Text="Thank you for your answer! &quot;If your ray [...] hits a light source prematurely, you would use fewer samples.&quot;: Consider the $(s,t)$-strategy so that we generate a camera subpath with *exactly* $t$ vertices. If we hit a light source prematurely, then the `GenerateCameraSubpath` routine returns a vertex count less than $t$ and the PBR implementation returns $0$ radiance. So, &quot;In these cases, you'd keep the subpart of the path that is relevant to avoid discarding the full state.&quot;, seems to be wrong. Am I missing something?" CreationDate="2020-01-07T16:12:03.533" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14379" PostId="9476" Score="0" Text="You're right, I've edited my answer. There _might_ be a way to actually recycle but you'd have to adjust the MH ratio accordingly and I'm not sure how. In my experience, this recycling is probably not worth doing." CreationDate="2020-01-07T16:30:27.167" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14381" PostId="9476" Score="0" Text="I think that I still don't understand the citation: Say the current primary sample space state is $u\in[0,1)^\mathbb N$ and we generated a path $x$ of length $k=s+t-1$ with $s$ and $t$ vertices on the light and camera subpath, respectively, from it. Say the camera subpath was generated using $u_1,\ldots,u_{d_1}$ and the light subpath using $u_{d_1+1},\ldots,u_{d_2}$. In the next iteration $u$ is mutated to $v\in[0,1)^\mathbb N$ (i.e. all dimensions are mutated). If I got you right, then - no matter whether we are still using the $(s,t)$-strategy in the next iteration or any other strategy -" CreationDate="2020-01-07T19:31:04.230" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14382" PostId="9476" Score="0" Text="it might be the case that a subset of $v_1,\ldots,v_{d_1}$ is now used for the light subpath and a subset of $v_{d_1+1},\ldots,v_{d_2}$ is now used for the camera subpath (and any dimension $&gt;d_2$ might be used for the light subpath as well). Is this correct? (a) If so, how does the index scheme described in the paragraph solve this problem? (b) You've mentioned the inversion described in RJ-MLT: How does this solve the problem? (c) Is there an implementation of the inversion in PBR available? See also my other question for that: https://computergraphics.stackexchange.com/q/9443/9254." CreationDate="2020-01-07T19:31:08.903" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14383" PostId="9476" Score="0" Text="Or let me put question (a) otherwise: If I understand correctly what they are doing, they actually have three separate vectors for each &quot;stream&quot; so that never a component of the camera stream vector is used for sampling the light subpath (and vice versa). If that's the case, I don't understand why they use the (somehow complicated) indexing scheme of a single vector." CreationDate="2020-01-07T19:38:56.790" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14384" PostId="9464" Score="0" Text="I see that I misunderstoof you partly. First, find the rotation of the image (this should be fairly easy). Second, convert from YPR to a rotation matrix, multiply this with the rotation, convert back to YPR (well covered, google it :) )." CreationDate="2020-01-08T11:47:41.780" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="14385" PostId="9464" Score="0" Text="I tough about it. But `multiply this with the rotation` is the part that I think I have troubles with. `YPR to a rotation matrix` - fair, so I have 3x3 matrix. But how can I rotate it like camera rotation?" CreationDate="2020-01-08T14:16:01.037" UserId="3305" ContentLicense="CC BY-SA 4.0" />
  <row Id="14386" PostId="9464" Score="0" Text="Rotating a 2D image would be a rotation about only one axis." CreationDate="2020-01-08T15:06:04.593" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="14387" PostId="9478" Score="0" Text="Why? Why break up a square into two triangles?" CreationDate="2020-01-08T19:09:24.627" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="14388" PostId="9478" Score="1" Text="Because triangles are the 2D simplex. it;s the simplest possible shape in 2D. Having a universal shape that you use to do all your calculations allows for some very powerful assumptions that simplify your logic (and by extension GPU manufacturing)" CreationDate="2020-01-08T19:24:15.757" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14389" PostId="9478" Score="0" Text="I understand that they can create everything, but why not use squares on square faces instead of two triangles? Why break it into triangles, and instead use the appropriate polygon?" CreationDate="2020-01-08T19:38:27.417" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="14390" PostId="9478" Score="0" Text="I am explaining to you, because having a STANDARD shape simplifies computations. If you always have to deal with the same shape, you can simplify and accelerate the logic of projection, rasterization, coloring... which makes for really fast hardware / algorithms" CreationDate="2020-01-08T20:52:16.640" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14391" PostId="9478" Score="0" Text="But then a &quot;Triangle count&quot; would be higher than a &quot;polygon count&quot; (by a lot). For examples, a cube could be 6 polygons, but 12 triangles." CreationDate="2020-01-08T21:14:08.427" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="14392" PostId="9478" Score="0" Text="It has nothing to do on the polygon count, 6 vertices are 6 vertices. Here, let's say you want to make an algorithm for rasterizing. If your algorithm is designed to always rasterize triangles, then you get your data, you know what to do. But what if you have to support squares as well? Now you need some kind of check, to determine if your data represents a triangle or a square. If you support 16 different shapes then you have to do 16 checks. If you support any aribtrary polygon now you have to deal with an infinite amount of problems that triangles don;t have." CreationDate="2020-01-08T21:20:57.660" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14393" PostId="9478" Score="0" Text="Is the polygon concave or convex? Does the polygon fit in memory? Is the polygon self intersecting? What is the topology? Is it a doughnut polygon? ..." CreationDate="2020-01-08T21:21:02.027" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14394" PostId="9478" Score="0" Text="There are no 0 cost abstractions, a general polygon rasterization algorithm would be inherently slower than a triangle rasterization triangle, for no other reason than having to to deal with a broader problem space." CreationDate="2020-01-08T21:23:57.780" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14395" PostId="9475" Score="0" Text="That sounds logical, have you tried taking the pixel position in screenspace and converting that to world space.  You would use that in place of the values in const vec4 camPos    = cam.viewInverse * vec4(d.x, d.y, 0, 1);.  I believe that is the correct value." CreationDate="2020-01-08T21:44:52.780" UserId="11291" ContentLicense="CC BY-SA 4.0" />
  <row Id="14396" PostId="9478" Score="0" Text="What do you mean by does the pollygon fit in memory?" CreationDate="2020-01-08T22:26:15.040" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="14397" PostId="9478" Score="0" Text="Imagine a polygon taking a quintillion vertices. That's a perfectly possible case scneario you need to take into account for handling any arbitrary polygon." CreationDate="2020-01-08T22:52:06.117" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14398" PostId="9478" Score="0" Text="Just like a polygon with a quintillion vertices is impossible to represent in the computer, but there is no point in triangles, as it would be impossible to represent them in triangles too." CreationDate="2020-01-09T01:27:51.053" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="14399" PostId="9478" Score="0" Text="You are wrong, a quintillion triangles doesn;t fit in ram at once, but it fits on disk, so you can render one triangle at a time until you get the final composition." CreationDate="2020-01-09T01:45:40.170" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14400" PostId="9478" Score="0" Text="If you render one at a time, it will still be stored in ram. I am talking about game graphics" CreationDate="2020-01-09T05:06:00.483" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="14401" PostId="9478" Score="0" Text="One a t a time uses ram, but doesn;t use all of it at once. GPUS do more than render real time graphics, they are also used to render off line graphics.&#xA;&#xA;Regardless do you understand that picking a consistent data representation accelerates the speed of your program? This is generally true for any system." CreationDate="2020-01-09T06:05:17.230" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14402" PostId="9475" Score="0" Text="This is called from a raygen program, I don't think I have access to the pixel position in screenspace :(" CreationDate="2020-01-09T09:22:31.483" UserId="11699" ContentLicense="CC BY-SA 4.0" />
  <row Id="14403" PostId="9464" Score="0" Text="But which one? Yaw, Pitch, Roll? Can you please write an answer with more details of how to do that mathematically, and I'll accept it?" CreationDate="2020-01-09T09:58:41.150" UserId="3305" ContentLicense="CC BY-SA 4.0" />
  <row Id="14404" PostId="9464" Score="0" Text="One axis in the rotation matrix" CreationDate="2020-01-09T10:24:17.963" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="14405" PostId="9478" Score="1" Text="@Makogan the answer is essentially right. The problem with a any random polygon is that they have a ill defined shape if they are not flat. Since the object is manipulated by vertices there is no guarantee what the actual shape of a quadrangle is. So in practuce devices split quads to triangles on render. They strictly dont have to but do (there are exceptions). However, for the author its easier to show a compound polygon because he does not need to care if he does not want to" CreationDate="2020-01-09T12:36:45.510" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14407" PostId="9478" Score="0" Text="I mean if you send info to a GPU you HAVE to split it into triangles, modern GPUS ca only handle triangles, lines and points." CreationDate="2020-01-09T16:20:52.690" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14408" PostId="9464" Score="0" Text="Yaw, pitch or roll? Can you please post an answer with more details? I will really appreciate that" CreationDate="2020-01-09T20:02:02.487" UserId="3305" ContentLicense="CC BY-SA 4.0" />
  <row Id="14409" PostId="9464" Score="0" Text="What is this for? It sounds like a CS assignment." CreationDate="2020-01-09T20:08:39.170" UserId="9194" ContentLicense="CC BY-SA 4.0" />
  <row Id="14412" PostId="9464" Score="0" Text="Does it matter? Actually it is not a CS assignment, I'm building a personal project (CV project)... I'm trying to get cube pose from an image with RaspberryPi. I have a predictor that can read the cube pose from the image, but the problem is that the camera is rotated. So I'm getting the pose relatively to the camera which is rotated. I try to transfer the predicted pose to the real pose by rotating the image in the opposite direction. The problem is, I didn't succeed to calculate the new pose after the rotation" CreationDate="2020-01-09T21:16:44.057" UserId="3305" ContentLicense="CC BY-SA 4.0" />
  <row Id="14413" PostId="9464" Score="0" Text="I tired to multiply the rotation matrix with another rotation matrix of one axis: yaw/pitch/roll. but it perform 3D rotation and not 2D rotation (camera rotation)" CreationDate="2020-01-09T21:18:05.287" UserId="3305" ContentLicense="CC BY-SA 4.0" />
  <row Id="14415" PostId="9478" Score="0" Text="Well, you dont have to. You can implement the ngon rendering with shader code. So strictly speaking you dont have to, you just usually do. But if you for example  you implement a raytacer then why not." CreationDate="2020-01-10T06:23:08.643" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14416" PostId="9448" Score="0" Text="Seeing how hard it is to get this answered, it seems to me that its not so trivial to deduce the reasoning.  Which means the qiestion is worth asking thus worth upvoting." CreationDate="2020-01-10T06:26:51.963" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14421" PostId="9448" Score="0" Text="@joojaa Thank you" CreationDate="2020-01-10T19:37:46.077" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="14423" PostId="9478" Score="0" Text="@joojaa Ok, how are you calculating your ray / ngong intersection? if you use triangles then it;s just barycentric coordinates, but if you want to support arbitrary ngong ray intersection, I wish you good luck with that, it;s not going to be easy to implement." CreationDate="2020-01-13T00:39:35.843" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14424" PostId="9478" Score="0" Text="@Makogan i didn say its easy. Just that it has been done. Most notably pixars renderman used to work with microdiced quads." CreationDate="2020-01-13T04:55:07.007" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14427" PostId="9492" Score="0" Text="*&quot;The **projection matrix is then essentially...&quot;.*   AFAICS That's not a projection matrix.. it's not really a &quot;2D&quot; rotation + translation matrix either.         Are you familiar with homogeneous coordinates? You really need to understand those first." CreationDate="2020-01-14T08:40:13.337" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14428" PostId="9492" Score="1" Text="@SimonF this matrix can be easily broken into 2D Scaling &quot;+&quot; 2D translation. Also, this is essentially the simple perspective projection matrix. (multiply by it and then homogenise the co-ordinates.)" CreationDate="2020-01-14T17:28:23.823" UserId="11735" ContentLicense="CC BY-SA 4.0" />
  <row Id="14429" PostId="9492" Score="1" Text="@SimonF I now understand that the projection matrix (with the viewing frustum mapped to a cube) is used as it contains depth information which is lost in the simple perspective projection matrix I had written earlier." CreationDate="2020-01-14T17:30:47.073" UserId="11735" ContentLicense="CC BY-SA 4.0" />
  <row Id="14431" PostId="9493" Score="0" Text="Full edge epresentation where the edge has a list of connections." CreationDate="2020-01-15T05:17:23.897" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14432" PostId="9492" Score="0" Text="Oh good. :-)  FWIW my first comment was that you'd written [R|T] which seemed to me to imply **Rotation** +Scaling, but the matrix requires another term." CreationDate="2020-01-15T09:42:39.033" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14433" PostId="9493" Score="0" Text="Are you mainly (only?) interested in 3D geometry or is 2D also of interest?" CreationDate="2020-01-15T09:46:39.627" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14434" PostId="9494" Score="0" Text="Do you need to actually save a depth texture?  Can you not just use the Z-buffer and send the models twice within the same render pass?  (Further, you can probably then just leave out the first step if ever you are using TBDR hardware)" CreationDate="2020-01-15T15:45:33.893" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14435" PostId="9493" Score="1" Text="2D is acceptable, but I am definitely more interested in 3D" CreationDate="2020-01-15T16:24:14.227" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14436" PostId="9493" Score="1" Text="My favorite 3D data structure is combinatorial map (cf. for example one implementation in CGAL here https://doc.cgal.org/latest/Combinatorial_map/)." CreationDate="2020-01-16T07:39:15.783" UserId="11747" ContentLicense="CC BY-SA 4.0" />
  <row Id="14437" PostId="9493" Score="0" Text="You could look at [Data structures for simplicial complexes: an analysis and a comparison](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9969&amp;rep=rep1&amp;type=pdf) by de Floriani and Hui; it covers structures in 2D and 3D, but if you want to represent something other than triangular / tetrahedral meshes there are other things out there." CreationDate="2020-01-16T09:21:53.040" UserId="7647" ContentLicense="CC BY-SA 4.0" />
  <row Id="14438" PostId="9497" Score="0" Text="Can we describe that in terms of matrices?" CreationDate="2020-01-16T14:55:01.057" UserId="11748" ContentLicense="CC BY-SA 4.0" />
  <row Id="14439" PostId="9497" Score="0" Text="Yes, a diagonal matrix with non-equal elements: $a_{ii} \ne a_{jj}, i \ne j$." CreationDate="2020-01-16T15:19:30.433" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14440" PostId="9497" Score="0" Text="@I tried that but it dodn't seem to work. it looks like the eigenvalues are the stretching factors and the eigenvectors are (1, 0), (0, 1) by using diagonalization of a matrix we have $P^{-1} D P$" CreationDate="2020-01-16T15:23:53.557" UserId="11748" ContentLicense="CC BY-SA 4.0" />
  <row Id="14441" PostId="9497" Score="0" Text="You tried what? The normalized eigenvectors of a 2x2 diagonal matrix are indeed $(1,0), (0,1)$, what's the issue there? And the scaling factors are the eigenvalues. I don't get what your issue with that is, it's a non-uniform matrix as I said. The initial image didn't have its angles changed, because the rectangle's edges are parallel to the eigenvectors. If you had a circle initially, you would have gotten an ellipse stretched along the $Y$ axis." CreationDate="2020-01-16T15:31:36.363" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14442" PostId="9497" Score="0" Text="I mean I tried to multiply the rotated shape with multiple scaling matrices but that didn't seems to work at all. What do you mean exactly the matrix is not uniform?" CreationDate="2020-01-16T15:40:40.487" UserId="11748" ContentLicense="CC BY-SA 4.0" />
  <row Id="14443" PostId="9497" Score="0" Text="Non-uniform scale matrix $a_{ii} \ne a_{jj}, i \ne j$, zero everywhere else. Assume that your image was stretched along $Y$ by a factor of $2$, then you have to rescale your rectangle by a factor of $0.5$ along $Y$ if you want it to keep its size, but this will change to which $Y$ it corresponds to on the measure bar on the left. Example matrix:&#xA;$$M(1,0; 0, 0.5)$$&#xA;Note, you have to know what the image was rescaled with, in order to put the correct value in $a_{22}$." CreationDate="2020-01-16T15:59:47.540" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14444" PostId="9497" Score="0" Text="Could we find the stretching factors from the image sizes? I mean by cropping the image to the outer black rectangle of the figure." CreationDate="2020-01-16T16:31:34.483" UserId="11748" ContentLicense="CC BY-SA 4.0" />
  <row Id="14445" PostId="9497" Score="0" Text="@Navaro Not necessarily - the margins and scaling may be different. If this was generated by matplotlib then there's probably a way to retrieve that information from it, or even make it work like you want through some option. I cannot help you further though, since I am not that well-versed in matplotlib anyways." CreationDate="2020-01-16T16:48:04.067" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14446" PostId="9497" Score="0" Text="Are you familiar with Matlab?" CreationDate="2020-01-16T18:39:47.513" UserId="11748" ContentLicense="CC BY-SA 4.0" />
  <row Id="14447" PostId="9497" Score="0" Text="@Navaro Not enough to help you with that." CreationDate="2020-01-16T19:24:38.660" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14448" PostId="9497" Score="0" Text="Thank you. I have asked this question several times but didn't get an answer." CreationDate="2020-01-16T19:55:10.257" UserId="11748" ContentLicense="CC BY-SA 4.0" />
  <row Id="14450" PostId="9498" Score="0" Text="I noticed that you have been posting a lot in recent days. While I think it is great that you are interested in light transport, the mathematical level you are using to approach your problems is far too high for the majority of this community. Your questions are mostly research-oriented and, as such, I'm not sure how much this forum can help you. It is clear that you are coming from a pure math background like Veach. Realistically though, the intersection of people who fully understand his thesis and also lurk here is probably nil. I hope someone can prove me wrong..." CreationDate="2020-01-17T00:27:15.417" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14451" PostId="9497" Score="0" Text="@Navaro A simple solution is rather than stretch your image, allow different bounds for your axes." CreationDate="2020-01-17T04:07:11.460" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14452" PostId="9498" Score="0" Text="@Hubble Thank you for your comment." CreationDate="2020-01-17T05:05:24.370" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14453" PostId="9498" Score="0" Text="Is this what you're looking for? https://agraphicsguy.wordpress.com/2016/02/04/the-missing-primary-ray-pdf-in-path-tracing/" CreationDate="2020-01-17T21:20:47.337" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14454" PostId="9501" Score="0" Text="Could you rewrite this formally as a mathematical problem? It seems you have problems with the math, in which case the rest is irrelevant and honestly (at least for me) confusing." CreationDate="2020-01-18T22:18:30.723" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14456" PostId="9501" Score="0" Text="Sure, I actually wrote a Maple worksheet describing the problem in an effort to find a way to solve for the control point. But the math for quadratic beziers gets very complicated very quickly and so I'm looking first and foremost for a conceptual approach. In other words, I don't think the right solution is mathematically perfect, that may be impossible, but instead just a &quot;well behaved&quot; solution that looks good." CreationDate="2020-01-18T23:39:20.247" UserId="11754" ContentLicense="CC BY-SA 4.0" />
  <row Id="14457" PostId="9501" Score="0" Text="In any case let me look into a better way to describe this. But basically if you understand the case of &quot;stroking&quot; a bezier curve as just mapping a long rectangle of the appropriate length onto a source curve, this is just a generalization of that to any shape that you can make out of bezier curves and might want to map onto another curve path. Let me know if that makes any sense." CreationDate="2020-01-18T23:54:53.733" UserId="11754" ContentLicense="CC BY-SA 4.0" />
  <row Id="14458" PostId="9501" Score="0" Text="&quot;mapping a long rectangle of the appropriate length onto a source curve&quot; - there are infinitely many ways to do this, that is why I believe a formalization will help." CreationDate="2020-01-19T07:09:48.803" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14459" PostId="9502" Score="1" Text="OpenGL is just a rendering API; it doesn't know anything about saving files or file formats or COLLADA or anything of the kind. So basically, your question is that you have generated some vertex data of some kind, and you want to know how to save that in a COLLADA file. That's going to require a lot of work, involving understanding the COLLADA format, XML writing, and so forth." CreationDate="2020-01-19T18:22:26.860" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14460" PostId="9501" Score="0" Text="Have you read the pomax bezier info?" CreationDate="2020-01-19T19:43:22.610" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14463" PostId="9501" Score="0" Text="Here is a link to an attempt at formalizing the problem. Let me know if this link is good: https://medium.com/@ianbloom/-6e88ea8e8888" CreationDate="2020-01-19T19:50:14.483" UserId="11754" ContentLicense="CC BY-SA 4.0" />
  <row Id="14464" PostId="9501" Score="0" Text="I have read most of the pomax info, I think this relates closest to section 40, graduated curve offsetting, but it's not exactly the same problem since we would need to assume that the control point in the curve we are trying to transform is collinear." CreationDate="2020-01-19T19:58:15.923" UserId="11754" ContentLicense="CC BY-SA 4.0" />
  <row Id="14465" PostId="9501" Score="1" Text="Why dont you formulate the exact curve and then fit to it. Its not like this has a analytical solution in arbitrary cases." CreationDate="2020-01-20T19:04:05.203" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14466" PostId="9501" Score="0" Text="Oh and btw you should add @username then people get a ping when you answer otherwise the might not notice yoir talking." CreationDate="2020-01-20T19:07:13.077" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14467" PostId="9501" Score="0" Text="How do you consider your current result wrong? Is it because your straight diagonal becomes a curve? Or because it's not the correct curve? Meaning the one you'd get by applying your transform to every one of its points." CreationDate="2020-01-21T02:33:01.647" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14468" PostId="9502" Score="0" Text="Oh, that looks like my brain will require huge memory storage..hmm. Thanks for answering. " CreationDate="2020-01-21T03:30:32.153" UserId="11757" ContentLicense="CC BY-SA 4.0" />
  <row Id="14469" PostId="9501" Score="0" Text="@Olivier, in the current implementation it's possible to get a control point that is outside the horizontal range of the original curve, so I think my tangent-intersection solution was just wrong." CreationDate="2020-01-21T14:05:23.283" UserId="11754" ContentLicense="CC BY-SA 4.0" />
  <row Id="14470" PostId="9501" Score="0" Text="@Joojaa, I'm looking for a solution that can be computed very quickly. The end goal for this library is to handle images that have millions of arbitrary curves (it does that now but without the projection operation.)" CreationDate="2020-01-21T14:08:20.363" UserId="11754" ContentLicense="CC BY-SA 4.0" />
  <row Id="14471" PostId="9501" Score="0" Text="@lightxbulb here is a link to an attempt at formalizing the problem: medium.com/@ianbloom/-6e88ea8e8888" CreationDate="2020-01-21T14:09:16.210" UserId="11754" ContentLicense="CC BY-SA 4.0" />
  <row Id="14472" PostId="9501" Score="0" Text="Tangent intersection is about as heavy operation. Just produces bad fit because it cant split the curve if curvature needs it. Anyway this is a embarassingly parallel job so consider doing it on a gpu." CreationDate="2020-01-21T15:36:20.257" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14473" PostId="9505" Score="0" Text="is this a raytracer or something? If so can you try to disable shadows and see if it renders everything then? It happened with me once due to incorrect intersection testing and it was thinking that the lit surface was in shadow" CreationDate="2020-01-21T19:33:36.923" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14474" PostId="9505" Score="0" Text="@gallickgunner No, it's just a regular forward renderer. It loads a scene from an .obj -file and renders it using the shader above. There are no shadows yet. Thanks though." CreationDate="2020-01-21T19:50:17.347" UserId="11771" ContentLicense="CC BY-SA 4.0" />
  <row Id="14475" PostId="9505" Score="0" Text="Quick thought, it may be because of the diffuseFactor term or the att term going to zero. Did you try inverting the normals or the light direction and see how does it look then? You can also try to just set the material color and add terms one by one so as to figure out which factor is causing the problem" CreationDate="2020-01-21T22:31:44.293" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14476" PostId="9505" Score="0" Text="Wrong normals perhaps? Test with M == identity to see if that's the problem. You should transform the normal with the transpose of the inverse matrix. And then normalize it again." CreationDate="2020-01-22T01:56:45.503" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14477" PostId="9501" Score="0" Text="@Joojaa can you explain a little more exactly how the fitting operation could work." CreationDate="2020-01-22T02:04:53.243" UserId="11754" ContentLicense="CC BY-SA 4.0" />
  <row Id="14478" PostId="9506" Score="0" Text="I see. So, for example, if a ray hit the left side of the box, passed through, and then exited from the right side, the final tmin should equal tminx, and the final tmax should equal tmaxx. And if it hits an edge on entry, then tmin should equal *two* of (tminx, tminy, tminz), right?" CreationDate="2020-01-24T02:44:36.760" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14479" PostId="9509" Score="0" Text="Although I agree that that shape does indeed match the description, under which criteria are you saying that's what the document is talking about? I didn't see the mention of the spherical component in the thesis" CreationDate="2020-01-24T04:01:39.467" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14480" PostId="9510" Score="2" Text="Space partitioning would be one way to describe it, your example is similar to an oct-tree partition." CreationDate="2020-01-24T05:33:18.050" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14481" PostId="9510" Score="3" Text="How about Exploded view?" CreationDate="2020-01-24T05:33:26.327" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14482" PostId="9506" Score="0" Text="@TylerShellberg that's it exactly. And if you're lucky enough to hit the near corner, it will equal all three." CreationDate="2020-01-24T12:11:12.260" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14483" PostId="9509" Score="1" Text="What the document’s referring to is an approximation to the actual shape—there isn’t a good word for “the shape formed by sweeping a sphere of linearly increasing radius along a line”, but that shape *looks* basically like a cone and has similar characteristics, so “cone” is close enough." CreationDate="2020-01-24T19:51:45.990" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="14484" PostId="9509" Score="1" Text="Well, I get the idea and I will accept this answer, however, after thinking about it a bit, it's impossible for a radius test to describe the shape you drew. since the sphere you are testing by occupies a sections of space that are outside of the cone's shape. However, you do get a sorta rough approximation of that shape using discrete steps.&#xA;TL:DR, It is correct that the document is mathematically wrong, but from a computation perspective, it;s close enough." CreationDate="2020-01-24T20:19:55.503" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14485" PostId="9509" Score="0" Text="Granted—two outer lines should be tangent to the circle rather than attached to its diameter. Harder to draw that, though. :)" CreationDate="2020-01-25T00:43:56.760" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="14488" PostId="9514" Score="0" Text="Before equation 15, they have several constraints. I guess they extended those a bit further in order to get a higher continuity at $F_{\lambda}^{(k)}(1) = 0$." CreationDate="2020-01-26T13:42:52.973" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14489" PostId="9514" Score="2" Text="@lightxbulb Thanks. An exponent of $3$ or $4$ would meet the stated conditions, but I suspect $5$ was obtained either with an approximation of (13) or empirical trial &amp; error." CreationDate="2020-01-26T14:16:51.483" UserId="11797" ContentLicense="CC BY-SA 4.0" />
  <row Id="14490" PostId="9514" Score="0" Text="Just plug in a larger $k$ in what I wrote, and you'll get the higher degree." CreationDate="2020-01-26T14:48:18.480" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14491" PostId="9514" Score="2" Text="@lightxbulb I understand the calculus, but such conditions have no motive apparent in the paper." CreationDate="2020-01-26T14:50:23.907" UserId="11797" ContentLicense="CC BY-SA 4.0" />
  <row Id="14492" PostId="9515" Score="0" Text="What you have above is a polar coordinate mapping, you can code it yourself." CreationDate="2020-01-26T17:35:46.063" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14493" PostId="9515" Score="0" Text="@lightxbulb not entirely trivial though." CreationDate="2020-01-27T05:49:19.167" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14494" PostId="9515" Score="0" Text="Yeah, I can code it but I can't find any libraries to provide basic tools to code with. I looked into pycairo but it only supports affine transformations. Looked into lib2geom, couldn't understand it's different representations of vector components. The documentation did not seem descriptive enough." CreationDate="2020-01-27T15:32:25.370" UserId="11799" ContentLicense="CC BY-SA 4.0" />
  <row Id="14495" PostId="8573" Score="1" Text="Also &quot;OpenGL 4.X-class&quot; doesn't mean &quot;opengl 4.0&quot;. It's just a simple and handy shortand because for as much as all the desktop hardware (I am aware of) that supports shader5 should be able to go all the way up to 4.5 at least, some was left rotting on 4.2, others to 4.4 and so on. [AFAIK](https://computergraphics.stackexchange.com/questions/5059/what-prevents-older-gpus-from-supporting-vulkan) compute shaders are the actual magic feature that make vk possible." CreationDate="2020-01-27T21:24:58.717" UserId="9500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14496" PostId="9520" Score="0" Text="Looks well written, will definitely check it out thanks." CreationDate="2020-01-28T23:29:13.333" UserId="10079" ContentLicense="CC BY-SA 4.0" />
  <row Id="14497" PostId="9520" Score="0" Text="If you catch any mistakes or things are not clear let me know, I'll correct them and clarify anything that isn't clear." CreationDate="2020-01-28T23:30:21.703" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14498" PostId="9517" Score="0" Text="What exactly do you mean by &quot;topology information&quot; here? Do you mean which components are inside/outside other components?" CreationDate="2020-01-29T05:01:08.500" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14499" PostId="9517" Score="0" Text="Yes, that's exactly what I wanted. Sorry for the confusion on my question." CreationDate="2020-01-29T07:52:08.913" UserId="11786" ContentLicense="CC BY-SA 4.0" />
  <row Id="14500" PostId="9511" Score="0" Text="Read this. I am still not convinced though since I haven't found any concrete mathematical proof.&#xA;https://en.wikipedia.org/wiki/Davenport_chained_rotations#Conversion_between_intrinsic_and_extrinsic_rotations" CreationDate="2020-01-29T10:29:52.947" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14501" PostId="9526" Score="0" Text="I saw your question in gamedev. I was going to post an answer, but the question was already deleted. Took me a while to find the question moved here." CreationDate="2020-01-29T15:18:48.137" UserId="6284" ContentLicense="CC BY-SA 4.0" />
  <row Id="14502" PostId="9527" Score="0" Text="which is in sRGB clamped to 0-255; i want it unclamped in linear space; floating point color can have values above 1.0." CreationDate="2020-01-29T15:53:21.713" UserId="11813" ContentLicense="CC BY-SA 4.0" />
  <row Id="14503" PostId="9525" Score="0" Text="Are you accounting for the probability of choosing a light?" CreationDate="2020-01-29T20:08:02.670" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14506" PostId="9525" Score="0" Text="I'm not 'choosing' a light (in the above code), I accumulate the contribution of all lights. Also, this is reproable if I use just one light only." CreationDate="2020-01-30T13:04:19.827" UserId="11811" ContentLicense="CC BY-SA 4.0" />
  <row Id="14509" PostId="9525" Score="0" Text="I saw your code, you are not dividing by the `bsdf_pdf` and missing the `ndotl` factor while computing your bsdf sample. Also post the code for both the calculations here. Not everyone's gonna download the whole project." CreationDate="2020-02-01T06:44:35.740" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14510" PostId="9525" Score="0" Text="because the denominator of the Cook-Torrance BRDF cancels it out. I found some other errors in the code tho, I update the post with my findings." CreationDate="2020-02-01T10:32:48.423" UserId="11811" ContentLicense="CC BY-SA 4.0" />
  <row Id="14511" PostId="9533" Score="0" Text="Nuts, I have this bug in some of my code... I think, in this case, you want 0 * inf to be 0, since it happens at the origin of the Ray. That should result in a ray lying on one face of the box to be a hit, the opposite face a miss. But you need to check for the zero." CreationDate="2020-02-01T15:27:53.633" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14512" PostId="9533" Score="1" Text="I think the pragmatic fix for BVH use would be to inflate the boxes by 1 ulps. I suspect that would beat any fix I can think of as all of them involve several extra instructions. I will keep thinking about it though, it is an interesting problem." CreationDate="2020-02-02T00:30:22.097" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14514" PostId="7809" Score="0" Text="Also see https://gpuopen.com/compute-product/amd-rdna-1-0-instruction-set-architecture/" CreationDate="2020-02-02T15:07:51.150" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14515" PostId="9533" Score="0" Text="Making the boxes physically larger in my case would make the results totally worthless. I can check if the result is NAN but that doesn't guarantee a hit, only that the origin started on one of those dimensions.Also, making them larger would just mean it would be the same problem, just at a different point in space, no? Or do you mean, if I detect a nan, extend the box and then try again?" CreationDate="2020-02-03T02:50:16.907" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14516" PostId="9538" Score="1" Text="Can you expand on the math behind it? Why does the sign of determinant tell you which face the triangle is facing?" CreationDate="2020-02-03T14:21:59.703" UserId="10461" ContentLicense="CC BY-SA 4.0" />
  <row Id="14517" PostId="9540" Score="1" Text="actually I'm using about the face normal for the dot product, not the normals that go along with the vertices. But perhaps I'm misunderstanding your point? What do you mean by edge-on?" CreationDate="2020-02-03T18:57:50.773" UserId="10461" ContentLicense="CC BY-SA 4.0" />
  <row Id="14518" PostId="9540" Score="0" Text="@HarryKane: &quot;*What do you mean by edge-on?*&quot; Hold your hand in front of your face, with the palm facing you. Now rotate your wrist 90 degrees. That's edge-on." CreationDate="2020-02-03T20:36:35.293" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14519" PostId="9543" Score="0" Text="You can specialize the intersection function based on the zero components (with 7 different versions), and avoid the runtime cost of the checks as you traverse for a given ray." CreationDate="2020-02-03T21:00:03.003" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14520" PostId="9543" Score="0" Text="@DanielMGessel Could you clarify what you mean by that?" CreationDate="2020-02-03T21:09:26.303" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14521" PostId="9543" Score="0" Text="Yeah, guess that was a bit sloppy. If you’re traversing a BVH for a single ray (on a scalar processor, not a gpu) consider having a version of the traversal code for the general case where no component of the ray direction is zero (most rays), eliminating your added checks, and a version with the checks. Select traversal code based on ray direction, Might speed things up on a cpu." CreationDate="2020-02-04T00:25:10.347" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14522" PostId="9543" Score="0" Text="Just grokked the pbr book’s solution. I’m slow today. Check it out: http://www.pbr-book.org/3ed-2018/Shapes/Basic_Shape_Interface.html" CreationDate="2020-02-04T00:44:59.317" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14523" PostId="9541" Score="0" Text="Changing this value shouldn't really affect the behavior of your algorithm though. I think you should look into how you handle PDFs of zero. In fact, changing the epsilon in Mitsuba doesn't break MIS so your problem probably lies somewhere else." CreationDate="2020-02-04T00:50:55.270" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14524" PostId="9533" Score="0" Text="@TylerShellberg I meant always make them larger. This will give a tiny % of false positives, which for a BVH is of no importance. If you require an exact intersection test, you might want to edit your question to specify that and also if you care about fixing the problem on both the min/max side of each axis or only on one side." CreationDate="2020-02-04T01:13:51.653" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14525" PostId="9538" Score="0" Text="The determinant of that matrix is proportional to the *signed* area of the triangle (can't recall the exact relationship and am a bit busy to plug in the values at present - sorry). In the meantime,  I will try to give some thought as to an explanation." CreationDate="2020-02-04T10:48:51.400" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14526" PostId="9541" Score="0" Text="simply put, I don't (have to) handle them :) Since I'm using the power heuristic, the MIS weight could be Inf or NaN only when both PDFs are zero (which is unlikely). Also I don't use Dirac deltas (yet)." CreationDate="2020-02-04T11:43:37.907" UserId="11811" ContentLicense="CC BY-SA 4.0" />
  <row Id="14527" PostId="9543" Score="0" Text="@DanielMGessel How is checking what type of ray, then choosing a function, rather than the function determining it any faster? Either way, the exact same checks need to happen on the ray. It doesn't seem like less work is being done." CreationDate="2020-02-04T15:48:31.670" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14528" PostId="9546" Score="0" Text="Well, question, have you considering adapting the marching cubes algorithm to your specific use case?" CreationDate="2020-02-04T16:44:39.367" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14529" PostId="9543" Score="0" Text="I’ve been assuming this is part of the traversal of a BVH, where you could do the checks once before starting traversal. My mistake: this is “one off” box hit testing. Apologies for my confusion," CreationDate="2020-02-04T16:53:36.647" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14530" PostId="9543" Score="0" Text="@DanielMGessel No, this is not part of a traversal for a BVH. Thank you in any case." CreationDate="2020-02-04T18:24:31.780" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14531" PostId="9533" Score="0" Text="@Oliver I am not using this for a BVH." CreationDate="2020-02-04T18:25:33.103" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14532" PostId="9547" Score="2" Text="Not radiosity. You can look into photon mapping, vertex connection and merging, and MLT." CreationDate="2020-02-04T18:41:00.920" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14533" PostId="1551" Score="1" Text="The website hosting the old blog is dead. This is the latest archived version: https://web.archive.org/web/20160322090245/http://web.onetel.net.uk/~simonnihal/texcom/texcompcomp.html" CreationDate="2020-02-05T01:38:36.773" UserId="1652" ContentLicense="CC BY-SA 4.0" />
  <row Id="14534" PostId="9546" Score="0" Text="You can look into the literature on [surface reconstruction from slices](https://scholar.google.com/scholar?hl=en&amp;q=surface+reconstruction+from+slices)." CreationDate="2020-02-05T07:43:21.583" UserDisplayName="user106" ContentLicense="CC BY-SA 4.0" />
  <row Id="14535" PostId="9546" Score="0" Text="Tanks for the keywords, I will look into that." CreationDate="2020-02-05T07:55:38.327" UserId="9309" ContentLicense="CC BY-SA 4.0" />
  <row Id="14536" PostId="9547" Score="0" Text="Hey nice to see a fellow countrymen lurking around a Graphics site :) As lightxbulb said you'd be better off looking towards photon mapping/MLT. While the core rendering algorithm does matter, the equally or maybe more important part is simulating the sea effectively using proper light attenuation, absorption and scattering models, etc. So do check up on that as well." CreationDate="2020-02-05T07:55:49.357" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14537" PostId="9542" Score="0" Text="I have already discovered this folder, thank you though." CreationDate="2020-02-05T08:54:48.060" UserId="10468" ContentLicense="CC BY-SA 4.0" />
  <row Id="14538" PostId="9533" Score="0" Text="This post may help you: https://tavianator.com/fast-branchless-raybounding-box-intersections-part-2-nans/. It presents this specific issue and a workaround for it." CreationDate="2020-02-05T20:42:34.040" UserId="9321" ContentLicense="CC BY-SA 4.0" />
  <row Id="14539" PostId="9533" Score="0" Text="@Razakhel Sadly, that post details a solution where plane-hits like I describe are *ignored*, rather than treated as hits. I actually need them in my case. They specify this, with: *&quot;Since this is an edge case, you might wonder why we don't choose to return true instead of false for rays on the boundary. **It turns out to be much harder to get this behavior with efficient code.**&quot;*" CreationDate="2020-02-05T21:39:30.547" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14540" PostId="5945" Score="0" Text="shouldn't that read `while ( err = glGetError() ) )`" CreationDate="2020-02-06T13:54:58.117" UserId="11854" ContentLicense="CC BY-SA 4.0" />
  <row Id="14541" PostId="9551" Score="0" Text="It might be helpful to use a much higher resolution input image, at 6x6 it's not east to see what's happening across a range of brightnesses" CreationDate="2020-02-07T07:57:09.760" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14542" PostId="9551" Score="0" Text="[here](https://i.stack.imgur.com/qEKEG.png) is a snap for 90x90, seems abnormal.. that's why asking ... can you verify the code..&#xA;`for y in range (0,n):&#xA;    for x in range (0,n):&#xA;        i = x % dn&#xA;        j = y % dn&#xA;        res[x][y] = 255 if (real[x][y] &gt; dither[i][j]) else 0`" CreationDate="2020-02-07T09:04:41.890" UserId="11853" ContentLicense="CC BY-SA 4.0" />
  <row Id="14543" PostId="9549" Score="3" Text="I'm assuming by &quot;planes&quot; you mean aeroplanes (as I originally interpreted 'planes' in the mathematical/graphics sense :-) ). Seriously though, when you say *&quot;lets say I have been given 3D models of several enclosed spaces.&quot;* do you mean they are a set of objects the aircraft must avoid (e..g like the stereotypical &quot;spaceship in an asteroid field&quot; ) or do you mean, given an arbitrary one of these enclosed volumes, allow the aircraft to fly around inside it (and not crash)?" CreationDate="2020-02-07T09:22:12.030" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14544" PostId="9554" Score="0" Text="I've not looked at the code, but visually it looks to me that you're supplying it with, say, vertices in the order &quot;ABCD&quot; when it perhaps expects &quot;ABDC&quot; (or vice versa)." CreationDate="2020-02-07T09:26:18.300" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14545" PostId="9549" Score="0" Text="@Simon F Hmm, I am interested in learning both, but for this question I'll restrict it to enclosed volumes if the methods are fundamentally different." CreationDate="2020-02-07T14:45:46.550" UserId="11850" ContentLicense="CC BY-SA 4.0" />
  <row Id="14546" PostId="9554" Score="1" Text="Oh yes @SimonF it does seem to get fixed on reordering the vertices in domain shader. However, an ordering only works with one model type on Unity for me. For instance if ABCD works for Planes, then maybe a different ordering like ADBC works for Cubes. Is there any way to have a consistent ordering across objects?" CreationDate="2020-02-08T04:30:02.600" UserId="11858" ContentLicense="CC BY-SA 4.0" />
  <row Id="14547" PostId="9556" Score="0" Text="Wouldn't it make more sense to just switch to a game engine that isn't horribly limiting your ability to do what you need to do?" CreationDate="2020-02-08T06:22:41.280" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14548" PostId="9551" Score="0" Text="Your algorithm looks sound, but I'm not a python programmer, so I can't say if there is a logic problem in your code, I'm not sure about your use of range(0,n) in both the X/Y for loops? Maybe a better test image would be a vertical gradient (say 256x256) so you can see how it behaves across an entire range of inputs." CreationDate="2020-02-08T10:34:23.410" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14549" PostId="9528" Score="0" Text="I believe this is the same one used in blender; it produces incorrect results below 2000C if i remember right?  at least when i tried to use blender's black body node to color lava the results were very incorrect.  but thank you though!" CreationDate="2020-02-08T17:12:25.677" UserId="11813" ContentLicense="CC BY-SA 4.0" />
  <row Id="14550" PostId="9556" Score="0" Text="Sadly this is out of question. Besides godot gives me a lot of useful stuff." CreationDate="2020-02-08T18:10:37.490" UserId="1865" ContentLicense="CC BY-SA 4.0" />
  <row Id="14551" PostId="9556" Score="0" Text="&quot;*Besides godot gives me a lot of useful stuff.*&quot; If it's not even allowing you to use basic aspects of a shader like `gl_VertexID`, then I would seriously contest that statement. I'm fairly sure that other engines can provide whatever &quot;lot of useful stuff&quot; that Godot does without Godot's pointless limitations. For example, my suggestion would be to use a buffer texture or an SSBO for such &quot;attributes&quot;, but given that it doesn't even allow you to use `gl_VertexID`, I would be surprised if it let you use those features." CreationDate="2020-02-08T18:28:36.140" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14552" PostId="9556" Score="0" Text="Godot allows the use of texture uniforms (thus the question...) and i can abuse UV and other existing attributes to smuggle the vertexID in. It isn't nice, but it works. BUT this question is not if godot is a good engine to use, but if there are technical concerns regarding that approach and what the downsides and caveats to this technique would be." CreationDate="2020-02-08T18:40:04.087" UserId="1865" ContentLicense="CC BY-SA 4.0" />
  <row Id="14553" PostId="9556" Score="0" Text="The problem is that I don't know what Godot will *allow* you to do, so I can't advise you. For example, I know how to make the texture-based idea work in a way that offers most if not all of the features of a vertex attribute. But I don't know if Godot will actually let you do those things needed to make it work. Again, if it's not letting you touch basic OpenGL functionality like `gl_VertexID`, who knows? Does it allow you to use floating-point image formats? What about integer textures? What about the GLSL functions for doing integer normalization? What about `texelFetch`? Etc." CreationDate="2020-02-08T18:47:52.930" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14554" PostId="4183" Score="0" Text="@joojaa if you were talking about cubemap stored infintie spheres that could work, but I devised an iterative process that works like light traveling through &quot;portals&quot; (mirrors except change position instead of direction) which gets the exact same image but using the classic recursive raytracing algorithm with imaginary objects." CreationDate="2020-02-09T04:52:04.533" UserId="11547" ContentLicense="CC BY-SA 4.0" />
  <row Id="14555" PostId="4183" Score="0" Text="As for the tracing onto a plane, using that as coordinates for infinite spheres, here is a little project I am proud of I have been plotting for months and is nearly complete which serves as an excellent proof of concept https://www.shadertoy.com/view/3lXXR8" CreationDate="2020-02-09T04:53:19.037" UserId="11547" ContentLicense="CC BY-SA 4.0" />
  <row Id="14556" PostId="4183" Score="0" Text="that project basicallt traces onto a plane, and figures out which sphere has a chunk of itself in the checkerboard cell using floor(x), floor(y). Then it checks the 8 surrounding spheres (like conwats game of life) in case any of them has a chunk in that cell. Good luck figuring out exactly how I did it. Important thing is it runs 60FPS fullscreen, but maintains 100% accuracy for this fractal-ish shape" CreationDate="2020-02-09T04:56:11.327" UserId="11547" ContentLicense="CC BY-SA 4.0" />
  <row Id="14557" PostId="4183" Score="0" Text="I saw the +4 views. It is unlisted. If you guys are interested in exactly the technique I used to accurately trace hundreds of spheres in realtime, or at least appreciate my work of art, feel free to contact me in comments somewhere or leave a like respectively! I know some of you have shadertoy.com accounts." CreationDate="2020-02-09T05:26:09.703" UserId="11547" ContentLicense="CC BY-SA 4.0" />
  <row Id="14558" PostId="9555" Score="0" Text="One cheap fix for this would be to use 2 sided triangles, so if rays do escape inside a closed mesh they will be trapped there instead of leaking parts of your skybox. A more correct fix is to use epsilons to fix the numerical precision issues that are (usually) the cause for those leaks. Epilons should be adjusted so they are X units from the surface rather than X units along the ray, that should avoid angle dependent precision loss." CreationDate="2020-02-09T07:19:47.833" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14559" PostId="9524" Score="0" Text="What's OLD_HANDLE ?" CreationDate="2020-02-09T07:28:09.053" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14560" PostId="9524" Score="0" Text="Sorry it should be OLE_COLOR*. I edited." CreationDate="2020-02-09T08:44:49.940" UserId="9971" ContentLicense="CC BY-SA 4.0" />
  <row Id="14561" PostId="9558" Score="0" Text="Short answer: math." CreationDate="2020-02-09T09:56:58.350" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14562" PostId="9555" Score="0" Text="Thank you @PaulHK, I'll play around with epsilons" CreationDate="2020-02-09T10:12:44.093" UserId="11862" ContentLicense="CC BY-SA 4.0" />
  <row Id="14563" PostId="9559" Score="0" Text="what are you doing with `real = (real[:,:,0] + real[:,:,1] + real[:,:,2]) / 3.0`" CreationDate="2020-02-09T16:34:03.540" UserId="11853" ContentLicense="CC BY-SA 4.0" />
  <row Id="14564" PostId="9528" Score="0" Text="@PatrickJeeves I don't know about blender but this one looks &quot;ok&quot; even at low temperatures. It fades to orange, then red. The dynamic range is extreme but I think that is correct; picture a glowing piece of steel vs an arc welder flash." CreationDate="2020-02-09T18:45:20.497" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="14565" PostId="9478" Score="0" Text="Wow that has a lot of comments" CreationDate="2020-02-09T21:49:34.323" UserDisplayName="user11352" ContentLicense="CC BY-SA 4.0" />
  <row Id="14566" PostId="9559" Score="1" Text="It is a RGB to gray color coversion (here average the three color channel). Indeed, you need to convert the image before applying dithering. If you have only scalar values (as your original example), you do not need to do this conversion." CreationDate="2020-02-09T22:50:18.483" UserId="11702" ContentLicense="CC BY-SA 4.0" />
  <row Id="14567" PostId="9524" Score="0" Text="That should be fine. You need to take care of memory alignment. This should be fine for pointers allocated via 'new' or other memory allocators, but be careful passing subscripts (e.g.  the 2nd element of an array, that may not be aligned to 16 bytes like the [0]th element)" CreationDate="2020-02-10T02:31:06.117" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14568" PostId="9562" Score="0" Text="What book is that? Directional and point lights are described through Dirac deltas." CreationDate="2020-02-10T06:22:35.193" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14569" PostId="9478" Score="0" Text="One issue with Quads is that they can be &quot;bent&quot; if all 4 vertices don't lie on the same plane. So there are a large range of invalid configurations for that primitive. Triangles/Facets have the property that they cannot be further divided and that they are always flat. Also triangles are simpler to interpolate values across." CreationDate="2020-02-10T06:47:30.763" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14570" PostId="9560" Score="0" Text="It's been a while since I did something similar, but I think I used N-sided polygons." CreationDate="2020-02-10T10:15:21.287" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14571" PostId="9554" Score="0" Text="I don't know unity, but I'd be surprised that vertex ordering would change between a flat quad and a cube... but perhaps there is an inconsistency in the system. I know of some Unity people on twitter, but I don't know if they also have a presence here." CreationDate="2020-02-11T09:12:07.113" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14572" PostId="9564" Score="3" Text="A corner has measure zero, so you don't have to care, since technically you cannot intersect it in practice due to numerics - just return the normal of one of the planes." CreationDate="2020-02-11T15:45:40.357" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14573" PostId="9146" Score="0" Text="The problem isn’t lookat (though you could just use the inverse of your camera transform) but that your up is fixed to the y axis. When the pitch crosses +/-90, the fixed up direction causes the camera to rotate 180. You could rotate your up vector, first by pitch then by yaw, just as you transform the relative position of the camera, if I’m understanding your code correctly." CreationDate="2020-02-11T19:00:09.963" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14574" PostId="9562" Score="0" Text="@lightxbulb Real-Time Rendering 3rd edition. Chapter 7 I think, in the BRDF section." CreationDate="2020-02-11T21:51:43.437" UserId="11871" ContentLicense="CC BY-SA 4.0" />
  <row Id="14576" PostId="9478" Score="0" Text="In terms of 3D modeling, polygons are often supported, sometimes with user interfaces that constrain them to be coplanar and have nicely defined texture mapping. Mathematically we often talk about vertexes, edges and faces (not triangles) of polyhedra. But in terms of GPU rendering performance, as @Makogan has alluded to, GPUs have dedicated rasterization setup engines which (generally) handle triangles but no other polygons. Thus the number of triangles needed has been a good performance/complexity metric." CreationDate="2020-02-12T01:33:13.863" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14577" PostId="9562" Score="0" Text="Can you quote the relevant part?" CreationDate="2020-02-12T05:01:53.390" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14579" PostId="9566" Score="0" Text="Iterate in both directions - from the current clockwide, and anti-clockwise." CreationDate="2020-02-13T04:58:12.477" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14580" PostId="9566" Score="0" Text="Right, but that's essentially the second loop I mentioned. Which I am hoping to avoid if can be" CreationDate="2020-02-13T05:01:40.743" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14581" PostId="9566" Score="0" Text="You visit all elements exactly once - what is the issue? Otherwise you would have to assign to your vertices, the leftmost/rightmost edges - then you would start at one end and finish at the other." CreationDate="2020-02-13T06:34:18.343" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14583" PostId="9549" Score="0" Text="Look into winding numbers" CreationDate="2020-02-14T20:31:57.480" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14586" PostId="9571" Score="1" Text="Same way a computer knows how to do anything. There is a API that has calls that the graphics card driver handles." CreationDate="2020-02-15T19:17:19.037" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14588" PostId="9571" Score="2" Text="Most, if not all, approaches to GPU programming require the developer to manually identify code that will be executed on the GPU. The code for the GPU is compiled separately from CPU code, so it is clear which binary code is for which processor type. Moving the GPU code to VRAM is often done with DMA, but may be moved to GPU accessible memory using other techniques such as CPU writes across PCIe, or direct CPU memory access by the GPU." CreationDate="2020-02-15T20:38:09.717" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14589" PostId="9571" Score="0" Text="Thanks for clear explanation.I know that something like a driver lives in kernel space and some library like CUDA needs to use driver API to communicate with GPU.How can a userspace code use the driver API at runtime ?Is it like a system call?" CreationDate="2020-02-16T07:02:35.310" UserId="11895" ContentLicense="CC BY-SA 4.0" />
  <row Id="14590" PostId="9571" Score="0" Text="Exactly! The userspace part of the driver communicates with the kernel part of the driver via system calls. As much as possible is put in userspace to protect the integrity of the system. The kernel writes to the hardware managing it as a shared resource. Command buffer construction and the GPU compiler will usually reside in userspace." CreationDate="2020-02-16T11:16:43.303" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14591" PostId="9571" Score="0" Text="Is command buffer like an event queue?And which kind of syscalls do it need?" CreationDate="2020-02-16T11:38:35.720" UserId="11895" ContentLicense="CC BY-SA 4.0" />
  <row Id="14592" PostId="9571" Score="0" Text="A command buffer is a sequence of binary instructions to do things like initiate draws, dma or set shader code addresses. The syscalls vary depending on the OS and may even be specialized to the driver. You’d need to dig into documentation and perhaps source code for the OS and driver." CreationDate="2020-02-16T12:07:54.227" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14593" PostId="9571" Score="0" Text="Thanks @Daniel.I've been looked for an answer for a long time.Please post these as an answer since comments may get deleted." CreationDate="2020-02-16T12:19:16.363" UserId="11895" ContentLicense="CC BY-SA 4.0" />
  <row Id="14594" PostId="9572" Score="0" Text="Could it be due to numerical issues for ray-triangle intersection?" CreationDate="2020-02-16T19:51:10.580" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14595" PostId="9572" Score="0" Text="@Hubble Well, hard to say, but I guess the error indeed depends on the surface the ray hits." CreationDate="2020-02-16T20:09:23.357" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14596" PostId="9572" Score="0" Text="I wouldn't be surprised if the visibility tester is inconsistent if you test on a scene with complex meshes with high-frequency details. Maybe try with two triangles facing each others, or reduce the robustness epsilon, and see if the problem is still there." CreationDate="2020-02-16T20:13:29.353" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14597" PostId="9572" Score="0" Text="@Hubble I don't know if you're familiar with it, but I've tested it with the &quot;sample.pbrt&quot; scene in PBRT which is as simple as possible: https://i.stack.imgur.com/txaN0.png." CreationDate="2020-02-16T20:25:28.730" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14598" PostId="9576" Score="2" Text="What's stopping it is the current computational resources of the average consumer. On the other hand, photorealism is very much here even in real-time:&#xA;&#xA; https://youtu.be/BpT6MkCeP7Y&#xA;&#xA;https://youtu.be/vpi-3q98uH4&#xA;&#xA;The resolution matters very much. At 320x240 the above will easily run on most current systems, and on the original systems you could have a lot more details. Photorealism is a solved problem, photorealism at &quot;decent&quot; resolution, with high polygon counts, difficult light effects, and for a reasonable computation time on current hardware - not so much." CreationDate="2020-02-17T04:18:50.180" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14599" PostId="9579" Score="0" Text="Your virtual camera film becomes smaller, but your screen remains the same size, meaning that a smaller region of the world is projected on the same area of the screen - that is a zoomed in region." CreationDate="2020-02-17T11:26:33.243" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14600" PostId="9579" Score="0" Text="@lightxbulb , does focal distance and focal length refer to the same thing in CG? It seems to be different in photography , because in my textbook it is mentioned that when you change the focal distance you also change the frustum, but that doesn't change if you change the plane of projection. Do you instead change the center of projection to change the focal distance(or is it focal length), could you please explain a bit about these?" CreationDate="2020-02-17T11:41:19.367" UserId="11832" ContentLicense="CC BY-SA 4.0" />
  <row Id="14601" PostId="9579" Score="0" Text="In this model, you can imagine the virtual film changing size on a fixed plane of projection (which changes the view frustum) or the focal length changing with a fixed virtual film size (also changing the view frustum). When you do the math to project a point on to the film, as a fraction of the size of the film, it works out to be the same, e.g. halving the size of the virtual film or doubling the focal length." CreationDate="2020-02-17T16:50:34.207" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14602" PostId="9578" Score="0" Text="Is it easy to try with f32 and see if things get worse?" CreationDate="2020-02-17T17:12:07.040" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14603" PostId="9578" Score="0" Text="They don't it looks the same with floats" CreationDate="2020-02-17T18:07:25.120" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14604" PostId="9572" Score="0" Text="Huddle is certainly right: It might be due to numerical issues for the ray-triangle intersection. PBRT offset the spawned ray (https://github.com/mmp/pbrt-v3/blob/9f717d847a807793fa966cf0eaa366852efef167/src/core/interaction.h#L73) to avoid such issue. However, there is no silver bullet here." CreationDate="2020-02-17T18:07:45.883" UserId="11702" ContentLicense="CC BY-SA 4.0" />
  <row Id="14605" PostId="9576" Score="0" Text="I’m curious which modern games you’re referring to, because by my (also fairly high) standards, there’ve been several recent ones that crossed the threshold of photorealism. Horizon: Zero Dawn, Far Cry 5, and Assassin’s Creed Odyssey all have terrific outdoor environment detail; Control, with its raytracing settings turned up, has incredible lighting and reflection effects. Can you go into what details you’ve found to be missing in the games you’ve looked at?" CreationDate="2020-02-17T21:28:00.533" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="14606" PostId="9579" Score="0" Text="@DanielMGessel , I think the mistake I was making in understand the entire textbook was that I was assuming the plane of projection is the one that is changed, but to increase or decrease d, also the center of projection can be changed, which one is usually changed?" CreationDate="2020-02-18T02:53:24.487" UserId="11832" ContentLicense="CC BY-SA 4.0" />
  <row Id="14607" PostId="9579" Score="0" Text="i think of the center of projection as the location of the camera, so to zoom, I imagine moving the plane of projection. However, once you turn it all into matrices, zooming becomes a scale of the projected coordinates and that can be thought of as changing the angles of the frustum! I had to work through the math a few times before it started to sink in." CreationDate="2020-02-18T04:51:33.597" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14608" PostId="9579" Score="0" Text="@DanielMGessel , I also did that, but the problem is that the frustum and FOV would remain the same if you change the plane of projection" CreationDate="2020-02-18T05:45:09.243" UserId="11832" ContentLicense="CC BY-SA 4.0" />
  <row Id="14609" PostId="9579" Score="0" Text="@DanielMGessel , and once we have -ze/n as the value for `w` the homogeneous coordinate, why and how can it be set to -ze? Even the math is making no sense to me" CreationDate="2020-02-18T05:48:23.080" UserId="11832" ContentLicense="CC BY-SA 4.0" />
  <row Id="14610" PostId="9584" Score="0" Text="Thanks for answering but I meant to understand the equation that mapped the frustum to the standard NDC cube (in the link given in my question)" CreationDate="2020-02-18T12:22:46.750" UserId="11832" ContentLicense="CC BY-SA 4.0" />
  <row Id="14611" PostId="9587" Score="0" Text="Unfortunately, I've not been able to find the presentation slides online, but I'm pretty sure at SIGGRAPH 2016, Luke Peterson of Imagination Technologies described (or at least hinted at) using the FP rounding modes to make the ray box test conservative." CreationDate="2020-02-19T09:57:09.943" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14612" PostId="9590" Score="0" Text="To support this, the driver may have to disable features like depth/stencil compression (or do a decompression pass) but I bet it could be made to work on most HW. It’d be interesting if there’s a way to make this work from GL or any other API." CreationDate="2020-02-19T16:35:07.040" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14613" PostId="9578" Score="0" Text="You're not accidentally updating a source vertex position before it's been referenced by other vertex calculations are you?" CreationDate="2020-02-19T18:06:41.677" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14614" PostId="9578" Score="1" Text="@SimonF I don't think I am, I can go and check again, but the algorithm first calculates all the new positions, stores them in a container, and only once the calculation loop is finished do I start the loop to update the positions.&#xA;&#xA;Also these artifacts only start to pop after the 4th ish iteration, the first 2-3 don't seem to exhibit any artifacts." CreationDate="2020-02-19T18:27:45.153" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14615" PostId="9593" Score="0" Text="An easy way to find put the exact answer is to tale the Fourier transform which will turn the convolution into a multiplication. Furthermore you can find the expression for the Gaussian Fourier transform. Then perform the multiplication and backtransform." CreationDate="2020-02-20T05:04:24.243" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14616" PostId="9591" Score="0" Text="Technically you can pick it to be whatever you like. In practice it is a low-pass filter and the first requirement is obviously desirable. You cannot guarantee the second one for arbitrary $f$ and $h$. How did you come up with (b)?" CreationDate="2020-02-20T05:21:53.123" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14617" PostId="9591" Score="0" Text="(b) is closely related to the asymptotic variance of estimates of $I_j$. I've missed a constant on the right-hand side. Does it change your comment?" CreationDate="2020-02-20T05:34:27.913" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14618" PostId="9591" Score="0" Text="With the $\alpha$ it's trivial if you can guarantee that $\exists \alpha, \forall x, \, |h_j(x)f(x) - I_j|^2 \leq \alpha |f(x) - \int f|^2$. Note that this may prove problematic only at points where the right hand-side is zero. So I guess you need some extra assumptions on $h$." CreationDate="2020-02-20T05:48:48.597" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14619" PostId="9591" Score="0" Text="@lightxbulb Yes, that' clear. But, again, I'm lacking information on $h$. Can we say more if we assume that $h$ is a &quot;Box filter&quot;?" CreationDate="2020-02-20T05:57:38.483" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14620" PostId="9591" Score="1" Text="If you can rewrite the reconstruction as convolution with a low pass filter, I believe you can use Young's theorem to get a bound. Sketch: $\|e * h\|^2_2 \leq \|e\|^2_2\|h\|^2_1$, if the 1-norm of $h$ is less or equal to 1 (should probably hold for your low pass filter), then you can bound the lhs by the 2-norm of $e$." CreationDate="2020-02-20T06:05:55.590" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14621" PostId="9591" Score="0" Text="@lightxbulb What are $e$ and $h$ in your equation?" CreationDate="2020-02-20T06:10:53.890" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14622" PostId="9591" Score="0" Text="Functions in $L^2$ and $L^1$. It's just an idea, I haven't looked into the details of applying this to your problem. But it solves a similar problem where $e = f - \int f$. I believe it should be applicable to your problem with a few more details taken care of." CreationDate="2020-02-20T06:17:45.313" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14623" PostId="9591" Score="0" Text="@lightxbulb I'll think about that. Thanks. What's worrying me more at the moment is the scaling problem described here: https://computergraphics.stackexchange.com/q/9589/9254. I'm sure you know the answer to it. Would be great if you could take a look." CreationDate="2020-02-20T10:04:49.553" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14625" PostId="9593" Score="0" Text="Wasn't this answered here? https://computergraphics.stackexchange.com/a/258/209" CreationDate="2020-02-20T11:10:16.053" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14628" PostId="9591" Score="0" Text="@lightxbulb Wait a second. When $I_j$ is estimated using Metropolis Light Transport, is the filter being applied at all? From the pbrt implementation (see http://www.pbr-book.org/3ed-2018/Light_Transport_III_Bidirectional_Methods/Metropolis_Light_Transport.html#fragment-Splatbothcurrentandproposedsamplestomonofilm-0) this doesn't seem to be the case, since the samples are directly splatted to the single pixel corresponding to the raster position." CreationDate="2020-02-21T09:50:31.370" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14629" PostId="9591" Score="0" Text="@lightxbulb So, it seems like $h_j$ is simply $1$ if the ray from the camera to the first vertex has an intersection with the image plane corresponding to pixel $j$ and $0$ otherwise. Am I missing something?" CreationDate="2020-02-21T09:50:37.833" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14630" PostId="9591" Score="0" Text="I haven't looked into PBRT in a while, let alone waddled through their implementation details, so I cannot answer your question. The questions you have, however, illustrates something notable about the PBRT book: it doesn't go into enough theoretical details - it's more of a book about how to implement the PBRT code, and not so much on the fundamentals of light transport. I would recommend reading the papers on MLT, rather than trying to reverse-engineer the theory from their code." CreationDate="2020-02-21T11:38:03.547" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14631" PostId="9596" Score="1" Text="The $B_j$ are not necessarily disjoint - consider having an aperture with some area. Furthermore, a reconstruction filter $h$ in theory (I am considering what PBRT does here), can have an effect over the whole screen if desired. Consider a Laplace or Poisson reconstruction problem from sparse samples (the Dirichlet data). The latter is even the case if you consider gradient domain path tracing." CreationDate="2020-02-21T12:25:16.040" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14632" PostId="9596" Score="0" Text="@lightxbulb You're right. Meanwhile, I've figured the source of my understandings out: https://computergraphics.stackexchange.com/a/9598/9254." CreationDate="2020-02-21T14:28:29.257" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14634" PostId="5945" Score="0" Text="It returns `GL_NO_ERROR` which I assumed to have value 0, when there is no error, that's why I had `!=` there. In any case   it should be `while((err = glGetError()) != GL_NO_ERROR)`." CreationDate="2020-02-21T17:42:15.887" UserId="7814" ContentLicense="CC BY-SA 4.0" />
  <row Id="14635" PostId="9596" Score="0" Text="On second thought - the $B_j$ are disjoint. I was thinking in terms of directions: (x_1 - x_0), in which case they would not have been. But as a tuple, they are, since one of the points (depending on your convention) is on the film, and those sets (of the points corresponding to a specific pixel) are disjoint. Basically pixels do not overlap. Now I am curious what the result will be of having overlapping pixels." CreationDate="2020-02-22T08:34:43.493" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14636" PostId="9596" Score="0" Text="@lightxbulb I need to think about your comment, but let me ask you a simple question. Say each $h_j$ is a box filter with radius $\frac12$ centered at the pixel center. Won't this yield that $h_j$ (considered as a function on the raster space) is simply the indicator function $1_{S_j}$ of the region $S_j$ of the image plane (in raster space) occupied by the $j$th pixel?." CreationDate="2020-02-22T13:26:16.420" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14638" PostId="9596" Score="0" Text="That is correct if you assume your pixels are of size 1. In the PBRT implementation this disjointedness probably also holds - you'll have to look into the internals to see if this is the case. They have a convolution of the normalized filter $\frac{f}{\sum_i f_i}$ with $wL$. If they make sure that the filter cutoff radii cover only a single pixel then the sets are disjoint. One can however pick a global filter - then the above simply does not hold. Ultimately, one wants to compute the integral within each pixel - so you do not even need to consider a filter in the continuous case." CreationDate="2020-02-22T14:11:06.100" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14639" PostId="9596" Score="0" Text="In the discrete case the filter becomes useful as a means to make up for the lack of infinitely many estimates over the image. I believe the two things should be considered separately - think of the filtering as a numerical integration rule and you won't be that far off the mark." CreationDate="2020-02-22T14:13:23.107" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14640" PostId="9596" Score="0" Text="@lightxbulb In raster space, the &quot;pixels&quot; are always of size $1$. My point is that I think that PBRT is using the Box filter with radius $\frac12$ implicitly in their implementation of (M)MLT without stating this at any point. Another point which is worrying me in the path tracing implemention: If we use a primary state space sampler which doesn't produce uniformly distributed samples (like the Halton sampler, which is the default one used by PBRT), doesn't this introduce bias in the final estimate?" CreationDate="2020-02-22T14:24:07.357" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14641" PostId="9596" Score="1" Text="Depends on your convention/implementation - pixels can have any size you would like, as long as you account for this properly (I am not sure whether pbrt uses size 1). You don't have to think too hard - just check PBRT's film functions. As for Halton - it's deterministic - then taking the expectation of the error obviously does nothing and you get that it's biased, sure. However, low-discrepancy samplers have a different property that guarantees bounds on the error and the equivalent of consistency also. Look into Koksma-Hlawka's inequality." CreationDate="2020-02-22T14:38:47.293" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14644" PostId="9600" Score="1" Text="I think 4D Toys is slicing the 4D object with a 3D volume to construct a 3D polyhedron, analogously to slicing a 3D polyhedron with a plane to construct a 2D polygon." CreationDate="2020-02-22T17:41:35.770" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14645" PostId="9600" Score="0" Text="@DanielMGessel yeah, that is correct. I actually got this part working too - it works by tetrahedralizing the 4D object, then intersecting each tetrahedra with an adjustable hyperplane (normal, offset). But you still need to generate the 4D object topology before you can do any of this...unless I'm missing something?" CreationDate="2020-02-22T17:44:44.603" UserId="7656" ContentLicense="CC BY-SA 4.0" />
  <row Id="14646" PostId="9600" Score="1" Text="One possibility is a 4D version of marching cubes. Somehow, each hypercube on a boundary defines some set of tetrahedral “faces” that connect with neighbors. You could define objects using hyper-CSG or anything that gives you a density function in 4D that you can take a level set of." CreationDate="2020-02-22T17:51:37.817" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14647" PostId="9600" Score="0" Text="@DanielMGessel that is very interesting! After a quick search, I found a couple of interesting references: the paper &quot;Isosurfacing in Higher Dimensions&quot; as well as &quot;Direct Construction of a Four-Dimensional Mesh Model from a Three-Dimensional Object with Continuous Rigid Body Movement,&quot; which I had actually come across before. I'll look into it, but this seems promising!" CreationDate="2020-02-22T18:50:53.907" UserId="7656" ContentLicense="CC BY-SA 4.0" />
  <row Id="14648" PostId="9596" Score="1" Text="@lightxbulb I'm sorry to bother you again (it's the last time on this, I promise), but the whole scaling thing is continuing to confuse me. Can you tell me what they mean by the &quot;image radiance function&quot; $L$ here http://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/Bias.html#?" CreationDate="2020-02-23T16:01:13.210" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14649" PostId="9596" Score="0" Text="@lightxbulb I really got problems to understand how precisely the image function $\Phi$ on the (continuous) raster space $S$ is defined, since the radiance arriving at an point on $S$ (which is infinitely small) should be $0$. There is clearly a map $\psi$ from the path space $E$ to $S$ (such that $\psi(x)$ only depends on $x_0,x_1$) and we should clearly have $\int_B\Phi=\int_{\psi^{-1}(B)}f\:{\rm d}\mu$, where $f$ is the measurement contribution function and the first integral is integrated wrt the Lebesgue measure on $S$." CreationDate="2020-02-23T16:01:27.350" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14650" PostId="9596" Score="0" Text="That's the radiance function assuming a pinhole camera, then $L(x,y) = L((c_x,c_y) \leftarrow (x,y))$, where $c$ is the point of the aperture. Then there is a unique correspondence between a ray direction and a point on the film. At the end of the day, you have discrete pixels, so you want to integrate the radiance coming from all directions at a pixel point, and integrate over the area of the pixel:&#xA;$$I_{i,j} = \int_{x_i}^{x_{i+1}}\int_{y_j}^{y_{j+1}}\int_{\Omega}W(x,y,\omega)L(x,y,\omega)\,d\omega\,dx\,dy$$&#xA;As you will notice there is no reconstruction filter - only sensor function $W$." CreationDate="2020-02-23T17:24:45.447" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14651" PostId="9596" Score="0" Text="@lightxbulb But the reconstruction filter is &quot;built into&quot; your $W$, isn't it? So $W$ depends on $i,j$ or am I missing something?" CreationDate="2020-02-23T17:29:16.600" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14652" PostId="9596" Score="0" Text="It is not built into it, and has no reason to be. I suggest you check out the CG discord so we could actually chat in real time." CreationDate="2020-02-23T17:30:02.127" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14653" PostId="9596" Score="0" Text="@lightxbulb Everything I'm asking is related to my scaling problem in my other question nhttps://computergraphics.stackexchange.com/q/9589/9254. There's obviously a crucial thing I'm missing. My understanding (which seems to be wrong) is that the measurement of the $j$th pixel is $\Phi_j=\int(h_j\circ\psi)f\:{\rm d}\lambda$, $h_j:S\to[0,\infty)$ being the filter for pixel $j$, $\psi$ the map from path space $E$ to raster space $S$, $f:E\to[0,\infty)^3$ being the measurement contribution function, $\lambda$ the infinite-area measure." CreationDate="2020-02-23T17:41:43.073" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14654" PostId="9596" Score="0" Text="@lightxbulb Let's only consider paths of length $k$ (so $E$ and $\lambda$ replaced accordingly) and say we generate samples $X_n\sim q\lambda$ ($q$ being any bidirectional path tracing strategy; for simplicity, assume path tracing if you like). Then the estimate for $\Phi_j$ should simply be $\frac1n\sum_{i=1}^n\frac{(h_j\circ\psi)f}q(X_i)$. Now PBRT is implicitly assuming a box filter with radius $r=1/2$. So, $h_j$ is simply the indicator function of the $j$th pixel region $S_j$ in raster space. That's fine." CreationDate="2020-02-23T17:41:59.770" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14655" PostId="9596" Score="0" Text="@lightxbulb Now PBRT is summing $\sum_{j\in J}\sum_{i=1}^n1_{S_j}(\psi\circ X_i)\frac fq(X_i)$ (so each summand contributes to precisely one pixel, since the $S_j$ are disjoint). Now we should be left with multiplying each pixel value by $\frac1n$, but PBRT multiplies by $\frac{|J|}n$ ($|J|$ being the number of pixels which is equal to the area of the raster space). It cost me a while, but since PBRT takes $n$ to be of the form $n=\text{samplesPerPixel}\cdot|J|$, $\frac{|J|}n$ happens to be equal to $\text{samplesPerPixel}$." CreationDate="2020-02-23T17:42:10.100" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14656" PostId="9596" Score="0" Text="@lightxbulb So, where does the need for the multiplication by $|J|$ came from? Sure, $h_j$ needs to be normalized, but since the measure of $S_j$ is $1$, there is nothing to normalize here ... (Sorry for extreme long comment. I'll delete them later)." CreationDate="2020-02-23T17:42:15.447" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14657" PostId="9596" Score="0" Text="This seems to be purely PBRT specific, and as I mentioned I haven't looked into the details of its code in a while. However you mentioned that $n = spp \cdot |J|$, and that the final result is multiplied by $\frac{|J|}{n} = \frac{1}{spp}$. Then everything should be fine, right? What is the issue? Did I maybe misunderstand some part? I highly recommend joining the computer graphics discord so it will be easier to clarify things, rather than producing many comments that are somewhat unrelated to the question." CreationDate="2020-02-23T18:11:38.683" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14658" PostId="9596" Score="0" Text="@lightxbulb I will take a look at the discord thing. No, it's not PBRT specific. When I divide by $n$ then the produced image is black (since obviously $n$ is the wrong factor to divide by). But when I multiply by $|J|/n$ (as PBRT does), the output is correct. I simply don't get why we need to multiply by $|J|/n$ (and not by $1/n$). So, I guess my formula for $\Phi_j$ or $h_j$ is wrong." CreationDate="2020-02-23T18:49:00.567" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14659" PostId="9596" Score="0" Text="But you just said that $n = spp \cdot |J|$, and multiplying by $\frac{|J|}{n}$ is the same as multiplying by $\frac{1}{spp}$. Which is exactly what you want." CreationDate="2020-02-23T18:53:46.533" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14660" PostId="9596" Score="0" Text="@lightxbulb I said that I figured out (from testing) that multiplying by $|J|/n$ is the correct thing to do, but I have no theoretical justification where the factor $|J|$ comes from. The theoretical Monte Carlo estimate of $\Phi_j=\int(h_j\circ\psi)f\:{\rm d}\lambda$ is $\frac1n\sum_{i=1}^n\frac{(h_j\circ\psi)f}q(X_i)$. So, *it seems like* either my formula for $\Phi_j$ is wrong or I need to &quot;normalize&quot; $h_j$ by multiplying it with $\frac1{|J|}$ (but since $h_j$ should be equal to $1_{S_j}$ and the region $S_j$ of the $j$th pixel has measure $1$, that doesn't make sense to me)." CreationDate="2020-02-23T19:00:23.117" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14661" PostId="9596" Score="0" Text="But your formula has $n$ be $spp$, and in the implementation $n$ was $spp \cdot |J|$, no?" CreationDate="2020-02-23T19:17:09.620" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14662" PostId="9596" Score="0" Text="@lightxbulb My formula works for every $n$. So, in the implementation I *theoretically* need to divide by $n=spp\cdot|J|$. Either my $h_j$ or my $\Phi_j$ is wrong." CreationDate="2020-02-23T19:20:53.727" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14663" PostId="9596" Score="0" Text="The $n$ in your formula needs to be the number of samples, it cannot be arbitrary. That is, it is exactly $spp$. And since in pbrt$ n= spp \cdot |J|$ it obviously requires a multiplication by $|J|$ in the numerator." CreationDate="2020-02-23T19:26:09.820" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14664" PostId="9596" Score="0" Text="@lightxbulb This is the essential part of the code I'm working with: https://coliru.stacked-crooked.com/view?id=7f55a67c5e1dab10. The code uses the suggested number of samples $n=spp\cdot|J|$ and divides by $spp$ at the end of the `render` function. It works, but I don't understand why. From my understanding it should be `m_camera-&gt;film-&gt;WriteImage(pbrt::Float{ 1 } / m_n);` instead." CreationDate="2020-02-23T19:26:12.437" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14665" PostId="9596" Score="0" Text="@lightxbulb Sure, $n$ is the number of samples. What I meant was that my $n$ can be any number of samples; it doesn't need to be of the specific form $n=spp\cdot|J|$." CreationDate="2020-02-23T19:27:11.750" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14666" PostId="9596" Score="0" Text="It seems we're misunderstanding each other. PBRT can also use any number of samples, it's just that in your formula your $n$ variable is not their $n$ variable, it's their $spp$ variable. Why do you believe you should divide by the film area?" CreationDate="2020-02-23T19:30:18.903" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14667" PostId="9596" Score="0" Text="@lightxbulb No, that's not correct. When we talk about the number of generated samples, we mean the number of generated paths $X_i$ and they generate $n = spp\cdot|J|$ many; not only $spp$. Please take a look at the code." CreationDate="2020-02-23T19:32:21.953" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14668" PostId="9596" Score="0" Text="But this is over the whole screen, since for every pixel you get $spp$ samples. So the weight per pixel is $\frac{1}{spp}$." CreationDate="2020-02-23T19:34:48.453" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14669" PostId="9596" Score="0" Text="@lightxbulb What you are doing now is arguing with intuition. And I agree to you; that makes sense. However, the reason why every pixel is getting $spp$ samples is the multiplication by the filter $(h_j\circ\psi)(X_i)$. But, once again, the correct Monte Carlo estimate is $\frac1n\sum_{i=1}^n\frac{(h_j\circ\psi)f}q(X_i)$. From my understanding, $h_j=1_{S_j}$, but it seems like it should be $|J|1_{S_j}$ instead." CreationDate="2020-02-23T19:39:56.630" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14670" PostId="9596" Score="0" Text="I am not arguing from intuition. I am arguing based on the math that I wrote several replies back. You reconstruct each pixel separately - you do not have an estimator over the whole screen - that would make no sense unless you have a single pixel. So if you want to argue that $|J|$ has to be in there you need to motivate that. This is precisely the reason why I told you you should separate the filter from the estimate. In the original problem there is no filter - a filter is required only after you discretise the problem to have finitely many samples." CreationDate="2020-02-23T19:45:26.457" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14671" PostId="9596" Score="0" Text="@lightxbulb Okay, let me continue to denote the region of the $j$th pixel in raster space by $S_j$. Assume a pinhole camera with eye $x_0$. You agree to me that the measurement $\Phi_j$ of the $j$th pixel is $\Phi_j=\int_{D_j}\sigma^\perp(x_0,{\rm d}\omega)W_e(x_0,\omega)L_i(x_0,\omega)$, where $\sigma_M^\perp$ is the projected solid angle kernel on the scene surfaces $M$ and $D_j$ the set of directions corresponding to the $j$th pixel, right? (My $\Phi_j$ is your $I_{x,y}$ for $j=(x,y)$ and my $S_j$ is your $[x_i,x_{i+1}]\times[y_i,y_{i+1}]$.)" CreationDate="2020-02-23T19:57:45.230" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14673" PostId="9596" Score="0" Text="@lightxbulb Using the map $\psi:E\to S$, this can be written as $\Phi_j=\int\sigma_M({\rm d}x_1)(1_{S_j}\circ\psi)(x)g(x_0\leftrightarrow x_1)W_e(x_1\to x_0)L_o(x_1\to x_0)=\int\lambda({\rm d}x)(1_{S_j}\circ\psi)(x)f(x)$. The estimator for this is $\frac1n\sum_{i=1}^n\frac{(1_{S_j}\circ\psi)f}q(X_i)$ which is the same as before, since we assumed the filter $h_j$ is equal to $1_{S_j}$." CreationDate="2020-02-23T20:03:20.207" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14674" PostId="9596" Score="0" Text="Here $n$ is $spp$ in PBRT, that is the only thing you keep believing is not the case for some reason. I'll make it simple for you: here's the problem you want to solve: $I_{i,j} = \int_{x_i}^{x_{i+1}}\int_{y_j}^{y_{j+1}}\int_{\Omega}W(x,y,\omega)L(x,y,\omega)\,d\omega\,dx\,dy$&#xA;An estimator for it is:&#xA;$$\frac{1}{spp}\sum_{k}^{spp}W(X_k, Y_k, Z_k) L(X_k, Y_k, Z^k)$$" CreationDate="2020-02-23T20:05:35.963" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14675" PostId="9596" Score="0" Text="@lightxbulb Let me stress that $U_i:=\psi(X_i)$ are the projections of the sample paths $X_i$ to the raster space and the sample sequence may vary over the whole screen. What you write corresponds to what one is usually doing ordinary path tracing (more generally, in what's been done by the `pbrt::SampleIntegrator`). In path tracing we generate a sample sequence $(X_n)_{n\in\mathbb N}$ for each pixel separately. It seems like you're confusing these approaches." CreationDate="2020-02-23T20:06:21.857" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14677" PostId="9596" Score="0" Text="@lightxbulb Your estimator is only correct if you fix $j$ and use it for a sequence $(X^{(j)}_n)_{n\in\mathbb N}$ of paths such that $\psi(X^{(j)}_n)\in S_j$ for all $n$. This is what `pbrt::SampleIntegrator` is doing. But in MLT only *one* path sequence $(X_n)_{n\in\mathbb N}$ is generated and $\psi(X_n)$ varies over $S=\biguplus_jS_j$." CreationDate="2020-02-23T20:11:20.563" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14678" PostId="9596" Score="0" Text="Your reconstruction is still performed per pixel. I believe your whole confusion arises from the fact that you have merged the reconstruction process with the actual problem you are trying to solve - you're basically modelling the problem wrong, and confusing yourself further with PBRT's implementation. The reconstruction process is a pure 2D problem. It's signal reconstruction from sparse data. In pbrt they have decided to perform the reconstruction per pixel. Example - signal reconstruction through radial basis functions, interpolation, approximation, PDEs etc. Anyways I have to go, try to" CreationDate="2020-02-23T20:12:03.133" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14679" PostId="9596" Score="0" Text="figure out what I mean. If you still haven't figured it out I can try to make some time tomorrow to chat with you." CreationDate="2020-02-23T20:12:57.080" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14680" PostId="9596" Score="0" Text="@lightxbulb Thanks for trying to help. Ignore the reconstruction. If you agree that $\Phi_j=\int\sigma_M({\rm d}x_1)(1_{S_j}\circ\psi)(x)g(x_0\leftrightarrow x_1)W_e(x_1\to x_0)L_o(x_1\to x_0)=\int\lambda({\rm d}x)(1_{S_j}\circ\psi)(x)f(x)$, then the correct estimator for this is $\frac1n\sum_{i=1}^n\frac{(1_{S_j}\circ\psi)f}q(X_i)$. I'm sure you're confusing this with sampling per pixel as it is done by the `SamplerIntegrator`: http://www.pbr-book.org/3ed-2018/Introduction/pbrt_System_Overview.html#fragment-SamplerIntegratorMethodDefinitions-0." CreationDate="2020-02-23T20:14:34.460" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14681" PostId="9596" Score="0" Text="You can't ignore the reconstruction when you've literally plugged it into your formula. What you just wrote is a box filter that was multiplied with $\frac{1}{|J|}$ for unknown reasons. Anyways, drop me a line on the CG discord tomorrow and I'll try to clarify it." CreationDate="2020-02-23T20:17:06.830" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14682" PostId="9596" Score="0" Text="@lightxbulb What do you mean? Which thing is a box filter multiplied by $|J|^{-1}$? There is no reconstruction in my latter formula for $\Phi_j$. It should be the same quantity as you denoted by $I_{x,y}$ (for $j=(x,y)$). Are you really sure you don't confuse this with generating path samples per pixel individually (as done by the `SamplerIntegrator`)?" CreationDate="2020-02-23T20:18:57.193" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14683" PostId="9603" Score="2" Text="No flickering and artifacts over multiple frames." CreationDate="2020-02-24T01:10:40.863" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14684" PostId="9596" Score="0" Text="@lightxbulb Might the missing $|J|$ come from the Jacobian of the transformation $\psi$ from path to raster space?" CreationDate="2020-02-24T06:13:56.600" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14686" PostId="9607" Score="0" Text="Barycentric interpolation is linear, but it works for triangles only: $c = (1 - \xi - \eta)c_1 + \xi c_2 + \eta c_3$. For quadrilaterals it us not unique (there is redundancy)." CreationDate="2020-02-24T12:41:01.897" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14687" PostId="9607" Score="0" Text="Thank you very much, are you aware of something for squares as well?" CreationDate="2020-02-24T12:48:21.890" UserId="11939" ContentLicense="CC BY-SA 4.0" />
  <row Id="14688" PostId="9607" Score="0" Text="There are generalized barycentric coordinates, but those are not exactly linear. A linear interpolation on a 2d texture would be either the interpolation in a column or a row - so actually 1D interpolation. Honestly that question in your slides is just ill-posed and stupid - it's equivalent to your lecturer saying &quot;guess what I am thinking&quot;." CreationDate="2020-02-24T13:06:11.070" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14689" PostId="9607" Score="0" Text="Couldn’t you use independent barycentric interpolation on the lower left and upper right triangles of each texel, effectively  turning the texture map into a triangular grid?" CreationDate="2020-02-24T13:18:39.680" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14690" PostId="9607" Score="0" Text="You could make 2 triangles from the quad - sure. You could do a number of other things though, hence why it is ill-posed." CreationDate="2020-02-24T13:34:07.867" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14691" PostId="9607" Score="0" Text="Another thing to note us that such a decomposition will make the problem piecewise-linear, which is not the same as linear." CreationDate="2020-02-24T13:39:56.990" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14692" PostId="9607" Score="0" Text="Thank you very much for your replies, I guess this is one of the cases where I would have had to have been in the lecture to be not be tripped up by it :)" CreationDate="2020-02-24T13:58:22.587" UserId="11939" ContentLicense="CC BY-SA 4.0" />
  <row Id="14693" PostId="9607" Score="0" Text="The question is stupid as I mentioned, I may be jumping to conclusions, but if such questions are commonplace in the slides I would have skipped too and read from an actual resource." CreationDate="2020-02-24T14:26:38.710" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14694" PostId="9596" Score="0" Text="Your multiplication with the characteristic function of $S_j$ and the subsequent averaging over $n$ is exactly that - box filtering. If $n$ is not $spp$, but rather $spp \cdot |J|$, then it's also mixed with brightness decrease based on the number of pixels in the image. Additionally you keep mentioning MLT, but the code you linked is not for MLT. MLT has a brightness correction factor $b$ anyways, since it works over the whole sample space, and relies on the Metropolis algorithm. I highly suggest separating this from the implementation and figuring what is the question that you have." CreationDate="2020-02-24T15:14:38.050" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14695" PostId="9596" Score="0" Text="On the other hand, if you're not considering MLT, and instead want to apply your formula for vanilla PT (by considering the sampling sets for ALL pixels), you get something akin to rejection sampling though the characteristic function of $S_j$ that you introduced. Then as you are aware - you need to account for this probability (which happens to be exactly $\frac{1}{|J|}$). Can't say I understand why you would do that (considering samples that have obviously 0 contribution to the pixel) - but you are free to do so, sure - you just have to account for the probability introduced by this." CreationDate="2020-02-24T15:19:58.687" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14696" PostId="9596" Score="0" Text="@lightxbulb The probability introduced by this is already accounted for by the division by $q(X_i)$ in $\frac1n\sum_{i=1}^n\frac{(1_{S_j}\circ\psi)f}q(X_i)$. I'll try to clarify things in a separate thread, once I've figured out what's going on. Trust me, the basic problem is the transformation $\psi$. Let's simplify things: Assume a pinhole camera model. In camera space, there is a natural projection $\pi$ of the hemisphere $H^2:=\{\omega\in S^2:\omega_3&gt;0\}$ onto $\mathbb R^2\times\{1\}$. The directions $\omega$ in $H^2$ correspond to the &quot;forward-facing&quot; directions and" CreationDate="2020-02-24T16:04:50.917" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14697" PostId="9596" Score="0" Text="@lightxbulb $\pi(\omega)$ is the mapped raster position (it lies on the infinite extension of the image plane). Now, our filter functions $h_j$ are defined on the raster space $S\subseteq\mathbb R^2$. If $\Phi:S\to[0,\infty)^3$ is the image function, then the value of the $j$th pixel is defined to be $\Phi_j:=\int_Sh_j\Phi\:{\rm d}\lambda^2$ ($\lambda^2$ denoting the Lebesgue measure on $\mathbb R^2$)." CreationDate="2020-02-24T16:05:04.637" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14698" PostId="9596" Score="0" Text="@lightxbulb If we want to express this quantity in terms of the measurement contribution function $f$ and as an integral over the product area measure $\mu$, we need to transform from path space to $S$ by a transformation $\psi$ and account for that transformation in the integral. My guess is that $\Phi_j=\int_Sh_j\Phi\:{\rm d}\lambda^2=\frac1{\lambda^2(S)}\int(h_j\circ\psi)f\:{\rm d}\mu$, noting that $\lambda^2(S)=|J|$. So, the remaining question is: How precisely is $\psi$ defined." CreationDate="2020-02-24T16:05:13.493" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14699" PostId="9596" Score="0" Text="I can not know how $\psi$ is defined when this is something you came up with. But I think I finally got what you're trying to do (or at least I hope I did). You want to map $x_0, x_1$ to $[0,1]^2$ I guess? But in your notation they are in $[x_0, x_N] \times [y_0, y_M]$. Then you apply the change of variables where your Jacobian pops out from. Assuming that each pixel is of size $1$, then $(x_N-x_0)(y_M-y_0) = N\cdot M = |J|$. Note that this is equivalent to what I said about rejection sampling, but without requiring the change of variables. I am unsure what the point of this is though." CreationDate="2020-02-24T16:37:20.203" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14700" PostId="9596" Score="0" Text="@lightxbulb No, I want to map $x_0,x_1$ to the raster space $S=[0,a)\times[0,b)$, where $a,b\in\mathbb N$ correspond to the horizontal and vertical image resoulution (as described here http://www.pbr-book.org/3ed-2018/Camera_Models/Projective_Camera_Models.html#). Assuming a pinhole camera, $x_0$ is fixed though." CreationDate="2020-02-24T16:41:26.640" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14701" PostId="9596" Score="0" Text="Nevermind, your $x_0, x_1$ are something different from what I have here. It's probably just best if you rewrite what exactly it is that you want, because it is currently unclear. I suggest rewriting your integral/estimator so that all $X_i$ are in $[0,1]^D$." CreationDate="2020-02-24T16:43:04.287" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14702" PostId="9596" Score="0" Text="@lightxbulb $x_0$ is the camera position and $x_1$ is a surface point. As long as the direction from $x_0$ to $x_1$ is inside the viewing frustum, there is a map mapping $x_1$ to its raster space coordinates." CreationDate="2020-02-24T16:44:55.607" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14703" PostId="9596" Score="0" Text="Something you might find interesting btw: https://agraphicsguy.wordpress.com/2016/02/04/the-missing-primary-ray-pdf-in-path-tracing/" CreationDate="2020-02-24T19:25:21.623" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14704" PostId="9607" Score="0" Text="I don't understand why we are assuming there is a problem. Once you accept that the linear mapping is fixed by defining 3 (non colinear) XY locations, with associated UV values, it's done." CreationDate="2020-02-25T08:56:22.180" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14706" PostId="9609" Score="0" Text="I suggest explicitly writing out your convention for path space. For example: $X \in [0,1]^D$ is a nice convention (which allows all the integrals in the Neumann expansion to be in $[0,1]^{D_k}$). Your first vertex (the one on the film) is then simply $RSx_0 + o$, where $R$ and $S$ are respectively a rotation and scale matrix, and $o$ is one of the corners of the film. The remaining vertices are generated as usual." CreationDate="2020-02-25T12:05:09.513" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14707" PostId="9609" Score="0" Text="@lightxbulb Please take note of my edit." CreationDate="2020-02-25T13:52:23.203" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14708" PostId="9609" Score="0" Text="You do not have to overcomplicate the problem - it does not help. Having raster coordinates $(\xi, \eta)$, a filter $h : [-r_x, r_x] \times [-r_y, r_y] \rightarrow \mathbb{R}_{\geq 0}$, the film $\vec{S}(\xi,\eta) = \vec{o} + \xi \vec{e}_1 + \eta \vec{e}_2$, and pixel coordinates $(\xi_{i,j}, \eta_{i,j})$ we want to express $h$ in terms of the world coordinates of $(\xi, \eta)$: $(x, y, z) = \vec{S}(\xi, \eta)$. Then $h(\xi - \xi_{i,j}, \eta - \eta_{i,j})$ $= h(S^{-1}(x,y,z) - (\xi_{i,j}, \eta_{i,j}))$. Note that $S^{-1}$ is just a matrix multiply and a translation." CreationDate="2020-02-25T15:35:21.403" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14709" PostId="9610" Score="0" Text="The key observation from the mathematics stack exchange link is that, if the two touch, either the center of the (infinite) cylinder goes through the box or an edge of the box intersects the cylinder. Does that help?" CreationDate="2020-02-25T17:19:57.043" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14710" PostId="9610" Score="0" Text="@DanielMGessel Well the center is easy, I already have a ray-box intersection test. The edge of the box I'm not sure. I could manually calculate all the startpoint/endpoints of the box's edges, but then I guess I would be looking for a line-segement and cylidner intersection test?" CreationDate="2020-02-25T17:24:15.350" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14711" PostId="9610" Score="0" Text="Yes, I was thinking a line segment vs cylinder intersection test too. When you do the math, you might find each of the 4 parallel lines along one axis, with the same extent, share/simplify computation. On the other hand, I also think a bounded cylinder is a little more challenging." CreationDate="2020-02-25T17:33:06.373" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14712" PostId="9610" Score="0" Text="To be honest, the fact that the cylinder is bounded is not a requirement. Infinite works too! I should specify that in the question. I thought bounded would be easier." CreationDate="2020-02-25T17:35:37.137" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14714" PostId="9610" Score="0" Text="@DanielMGessel What about the case where the box is much smaller than the radius of the cylinder and is inside it. It could not intersect with the lines of the box, while the center would miss the box as well." CreationDate="2020-02-25T20:45:28.153" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14715" PostId="9610" Score="0" Text="@TylerShelberg Think of clipping the box edges to the volume of the cylinder then checking to see if anything is left, instead of seeing if the edges intersect the surface of the cylinder." CreationDate="2020-02-25T21:36:07.803" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14716" PostId="9610" Score="0" Text="What do you mean by &quot;clipping the box edges to the volume of the cylinder&quot;? I don't follow." CreationDate="2020-02-25T21:39:06.453" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14719" PostId="9606" Score="2" Text="Deterministic choices are taken with a probability 1, hence pdf = 1, unless I am missing something." CreationDate="2020-02-26T09:45:44.480" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14720" PostId="5564" Score="1" Text="Which apparently prompted https://github.com/KhronosGroup/OpenGL-API/issues/14 !" CreationDate="2020-02-26T13:00:57.040" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="14721" PostId="5564" Score="0" Text="I referenced your issue in my research for https://stackoverflow.com/questions/56340333/glsl-about-coherent-qualifier/60434348#60434348 Since you wrote the wiki article in question, I'd be very interested to have you take on my findings." CreationDate="2020-02-27T13:29:44.137" UserId="7644" ContentLicense="CC BY-SA 4.0" />
  <row Id="14722" PostId="9541" Score="0" Text="Epsilon values are a fact of life under a floating-point regime, but I think any geometric problem should be able to *justify* a value based on the propagation of some measurement uncertainty in a system. Why 1e-9? Even something based on FLT_EPSILON (2^-23), or even HALF_EPSILON (2^-10), as a kind of intrinsic 'uncertainty', should yield something physically consistent when measurement error is propagated through a physical system." CreationDate="2020-02-27T13:45:34.340" UserId="5574" ContentLicense="CC BY-SA 4.0" />
  <row Id="14723" PostId="9541" Score="0" Text="I know that may come across as incredibly tedious, but I've been going through a lot of floating-point code and trying to quantify errors and failures in the domain of graphics. I mean, even the typical implementation of the inner product is subject to catastrophic cancellation... Otherwise, +1 for the care and feeding of your physically-based renderer!" CreationDate="2020-02-27T13:50:34.940" UserId="5574" ContentLicense="CC BY-SA 4.0" />
  <row Id="14724" PostId="9601" Score="0" Text="Part of your summary: &quot;straight lines... are straight in World space but may be curved in NDC space,&quot; is totally incorrect. This might be *technically* true for a real camera, with real lenses - but for mathematical models being discussed - straight lines in WCS will *always* be straight lines after projection." CreationDate="2020-02-27T14:12:20.960" UserId="5574" ContentLicense="CC BY-SA 4.0" />
  <row Id="14725" PostId="9601" Score="0" Text="@BrettHale, You're partly right but also largely misunderstanding.  You're thinking that the projected image of a 3D straight line into 2D space will always be straight.  That's true -- **in x &amp; y only**.  But NDC space is a 3D space and the depth component is nonlinear because it has been divided by W.  I thought my picture showed it pretty well especially since the sidewalk is clearly curved when you visualize the depth component (looking at it &quot;from the side&quot;, like I did in the diagram). Looks like it didn't explain it well enough to you." CreationDate="2020-02-27T14:23:15.237" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="14726" PostId="9601" Score="0" Text="I see where you're coming from. It's why you can linearly interpolate vertex attributes correctly (or use barycentric interpolation on triangles) in homogeneous coordinates, but not *after* projection, for the reasons you describe." CreationDate="2020-02-27T14:45:37.363" UserId="5574" ContentLicense="CC BY-SA 4.0" />
  <row Id="14727" PostId="9601" Score="0" Text="@BrettHale, yes!  That's exactly right.  Your comment about interpolating attributes highlights the beauty of working in a homogeneous coordinate system / projective space." CreationDate="2020-02-27T14:50:49.817" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="14728" PostId="9520" Score="1" Text="@IntegrateThis I assume the document was of help?" CreationDate="2020-02-27T17:47:59.433" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14729" PostId="9617" Score="0" Text="The fact that it has an ASCII header for the binary format is a little frustrating, but I think I can work around that. Do you know how properties are handled for software that reads in PLY files? The header specifies where the properties are, what they apply to, and their size, but the names and types are arbitrary. How does any software deal with the fact that the colors could be called &quot;red&quot; &quot;blue&quot; and &quot;green&quot;, or &quot;rd&quot; &quot;bl&quot; &quot;gr&quot; (or anything else), and they could be anything from uchar to float? How is that possible?" CreationDate="2020-02-27T18:56:46.997" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14730" PostId="9617" Score="0" Text="As a sidenote, can colors even be applied to faces? Or is the only option to define them per-vertex, or do some kind of user-defined elements? This is all a little confusing." CreationDate="2020-02-27T19:02:29.490" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14731" PostId="9617" Score="0" Text="@TylerShellberg Unfortunately, you might have to rely on common practice for that, in the case of colours likely `uchar red`... That's the disadvantage of an entirely felxible file format like that." CreationDate="2020-02-27T19:03:48.277" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="14732" PostId="9436" Score="0" Text="@joojaa That wouldn't really change much about the problem of emulating a triangle fan with a strip that the question is facing. In fact it would make answer by Simon inapplicable even." CreationDate="2020-02-27T19:09:01.767" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="14735" PostId="9611" Score="0" Text="Sorry for the late reply.&#xA;How does this boil down to a line-parallelogram intersection, again? Is the parallelogram the profile of the box from the perspective of the cylinder?&#xA;Does this mean the steps boil down to:&#xA;1: Rotate cylinder to the Z axis (somehow)&#xA;2: Apply same rotation to the box (somehow)&#xA;3: Use the X and Y positions of the box after rotation to get the line segments that make it up.&#xA;4: Convert them into parametric form?&#xA;5: Plug into the equation, solve for T&#xA;6: T &gt;= 0, intersection. Otherwise, false.&#xA;Does that sound about right? Never worked with parametrics before." CreationDate="2020-02-27T20:46:48.750" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14736" PostId="9611" Score="0" Text="If the cylinder is a circular one, a and b become the radius, right?" CreationDate="2020-02-27T21:01:33.360" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14737" PostId="9611" Score="0" Text="I think I must be wrong about some of those steps. Read over what you wrote again. I don't follow a few things. What rotation are we getting the inverse of? The rotation of the ray? So we rotate the box first...and then the cylinder after? Why the inverse of the rotation? Is there some way to boil this down to a line segment and circle intersection test or is there a reason that doesn't work? I feel very lost. And I think any of this rotation stuff will require quaternions and converting to-from them as well." CreationDate="2020-02-27T21:34:53.427" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14738" PostId="9611" Score="0" Text="The parallelogram was for testing the central axis of the cylinder vs box faces, but you could also rotate the coord system and just do ray-AABB intersection in that case. As for the radii for circular cylinder, $a=b=r$, yes.&#xA;If the cylinder is not aligned to the Z axis, it can be transformed to such through a rotation matrix $R$. Then apply this to your rays before intersecting with the Z aligned cylinder." CreationDate="2020-02-27T21:56:59.767" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14739" PostId="9611" Score="0" Text="Why rotate the coordinate system to test for intersection between the center of the cylinder and the box? Wouldn't just doing a normal ray-box intersection work for that?" CreationDate="2020-02-27T21:58:42.980" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14740" PostId="9611" Score="0" Text="And, do the steps I outlined seem right? Is there something I am missing there? It seems like solving that equation for T is not trivial, not sure how to even approach that." CreationDate="2020-02-27T22:00:29.733" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14741" PostId="9611" Score="0" Text="I used Wolfram to solve it, that's a monstrosity. I could maybe put it into code but I'm not even sure if that's really how I should be going about this." CreationDate="2020-02-27T22:33:14.473" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14742" PostId="9611" Score="0" Text="Some pseudocode, or even just some of the steps and sub-steps in an ordered list would be an enormous help." CreationDate="2020-02-27T22:35:50.410" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14743" PostId="9199" Score="0" Text="Also worth pointing out the interpolated normals need renormalising per pixel." CreationDate="2020-02-28T04:44:56.270" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14744" PostId="9611" Score="1" Text="@TylerShellberg I edited the post to include the segment-cylinder intersection details. Note that if the cylinder is not aligned with the $Z$ axis, you need to find the rotation $R$ that transforms the $Z$ aligned cylinder to yours. Then use that rotation on the  edges of your AABB to get the line segments ($\vec{o} = R\vec{o}'$, $\vec{d}=R\vec{d}'$)." CreationDate="2020-02-28T19:36:22.330" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14745" PostId="9611" Score="0" Text="I asked this from another commenter, but what happens if the box is entirely within the volume of the cylinder, but doesn't touch the center of it either? In that case, in 2D, the parallelogram would be inside the circle, but there would be no interesections. Does this method handle that? Also, if the cylinder is infinite and is along the Z axis, how is there a &quot;center&quot; point? Do we just call it (0,0,0)?" CreationDate="2020-02-28T20:28:39.023" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14746" PostId="9611" Score="0" Text="I think I have found a much simpler solution, which I have added to the OP. I will let you know how that progresses." CreationDate="2020-02-28T20:52:53.540" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14747" PostId="9611" Score="0" Text="This checks for intersections between the surfaces, and not the volumes, so no, it will not work. Also your solution has a number of fail cases." CreationDate="2020-02-28T21:37:21.820" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14748" PostId="9611" Score="0" Text="I am looking for intersection between volumes, not surfaces. I guess I should have made that more clear in the OP, though I did specify that the box being completely inside the cylinder should count. In what cases would it fail?" CreationDate="2020-02-28T21:43:39.247" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14749" PostId="9611" Score="0" Text="For what it's worth, I'm sorry that you ended up spending a lot of time on a solution that wasn't what I was looking for. Regardless, I appreciate your time and thoroughness. Thank you." CreationDate="2020-02-28T23:27:10.217" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14751" PostId="9591" Score="0" Text="@lightxbulb I don't think that we can show (b) in its previous form, but I've managed to replace the desired inequality by an inequality in terms of the average over the filter support. Do you think we can show this now?" CreationDate="2020-03-02T11:28:43.193" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14752" PostId="9622" Score="0" Text="Thank you for your answer. It would be enough for me to show (b) for the box filter with radius 1/2." CreationDate="2020-03-02T12:30:54.737" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14753" PostId="9622" Score="0" Text="Regarding your &quot;whole term equation&quot;: Note that it simplifies to $$\mu|h_jf|^2-\frac{|\mu(h_jf)|^2}{\mu(B_j)}\le\alpha\left[\mu|f|^2-\frac{|\mu f|^2}{\mu(B)}\right],$$ where $B:=\{f\ne0\}$, $B_j:=B\cap\{h_j&gt;0\}$ and $\mu g:=\int g\:{\rm d}\mu$. And it might be useful to note that, if each $h_j$ is a box filter and hence takes only the values $0$ or $1$, we've got $|h_j|^2=h_j$." CreationDate="2020-03-02T12:47:35.317" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14754" PostId="9622" Score="0" Text="And let me note that the aspect of (b) I'm looking for is that we can bound the left-hand side in terms of the variance of $f$.  So, the factor $\frac1{\mu(B)}$ before the integral in the definition of $f_0$ is not important for me." CreationDate="2020-03-02T12:57:45.077" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14755" PostId="9622" Score="0" Text="If you set $a=b=1$, then $h_j \leq \sqrt{\alpha}$ is your requirement. But I believe you allow for $\alpha$ to be any, thus you just require for $h_j$ to never go to infinity (which holds for filters). Since it's also an integral, I would say that it's even ok for it to explode on sets of measure zero." CreationDate="2020-03-02T13:00:45.130" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14756" PostId="9622" Score="0" Text="How do I need to read your equation lines? Are they meant to be equivalent to each other or are they meant to be implications (eq. 1 implies eq.2 and so on)? I begin to struggle what you did from (and including) the third equation on. Do you agree that your first equation is  equivalent to the one in my second comment?" CreationDate="2020-03-02T13:04:48.773" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14757" PostId="9622" Score="0" Text="And note that your claim &quot;$\left(\int u\right)^2 \leq \int u^2$&quot; is not correct, unless you normalize the measure over which you're taking the integrals." CreationDate="2020-03-02T13:06:03.150" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14758" PostId="9622" Score="0" Text="The equations in the second edit are equivalent - it's just algebraic transformations. I have a meeting in a few minutes, so I cannot exactly go in details, but try writing it down ($a,b$ are just substitute variables)." CreationDate="2020-03-02T13:06:11.877" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14759" PostId="9622" Score="0" Text="Substitute variables for what?" CreationDate="2020-03-02T13:06:35.687" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14760" PostId="9622" Score="0" Text="The ugly fractions." CreationDate="2020-03-02T13:07:14.857" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14761" PostId="9622" Score="0" Text="You've made a mistake in your third equation. The $\mu^2$ in the denominator should be a $\mu$. I guess you've pulled the corresponding term out of the integral and forgot to multiply it with the measure over which the integral is taken." CreationDate="2020-03-02T13:11:44.497" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14762" PostId="9622" Score="0" Text="You mentioned that $(\int u)^2 \leq \int u^2$ doesn't have to hold. Do you have a reference for this? As far as I know Holder's inequality holds regardless of the normalization of the measure. I am not sure what the issue with $\mu^2$ is - or at least I cannot find the mistake. I haven't pulled anything out of the integral - I just took the common denominator so I could merge the terms." CreationDate="2020-03-02T15:04:51.283" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14763" PostId="9622" Score="0" Text="Sure. You're using [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality#Measure-theoretic_and_probabilistic_form). Hölder's inequality is the same in this particular case. You cannot use it cause you would need to apply it for the second function to be the constant function $1$. Now you only need to note that the $p$-norm of the constant $1$ is equal to the measure of the entire space. So you we need to divide this factor out (hence normalize the measure) again." CreationDate="2020-03-02T15:07:50.520" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14764" PostId="9622" Score="0" Text="Ok, got what you meant. Honestly, I would keep my measure normalized to avoid this, but in the alternate case I guess you would have to pop-out a constant out of the integrals. I don't think it should change much with respect to the conclusion though. Actually it should probably make the equation more symmetric." CreationDate="2020-03-02T15:10:10.120" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14765" PostId="9622" Score="0" Text="Regarding the second part of your comment: Simply note that $$\int_{B_j}\left(\frac{\int h_jf}{\mu(B_j)} \right)^2\:{\rm d}\mu=\mu(B_j)\cdot\left(\frac{\int h_jf\:{\rm d}\mu}{\mu(B_j)} \right)^2.$$ Hence the square is canceled out. Doing this with the other term as well yields the equation in my second comment." CreationDate="2020-03-02T15:12:47.117" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14766" PostId="9622" Score="0" Text="Finally, regarding your latest comment: Keeping the measure normalized might be problematic. Note that $\mu$ is *not* a finite measure (it is a $\sigma$-finite measure though); hence it cannot be normalized. However, it would be sufficient for the purpose of the question that the trace of $\mu$ on $\{f\ne0\}$ is finite (i.e. that $\mu(B)&lt;\infty$). But I have no clue if we can assume this without loss of generality." CreationDate="2020-03-02T15:16:06.997" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14767" PostId="9622" Score="0" Text="In practice your (a) and (b) should always hold (even without all of those arguments), provided that you assume $f &lt; \infty$, and $h_j$ is min-max preserving. I am still unsure why you are considering any dimensions beyond the film ones though. Ideally I would just work with a function $g$ defined only on the film - then you do not even need to care about any of the above, and you can just work with assumptions on $g$." CreationDate="2020-03-02T15:24:27.437" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14768" PostId="9622" Score="0" Text="But $f$ is not defined on the film. We can clearly use that $I_j=\int\sigma_M^{\otimes\{0,\:1\}}({\rm d}(x_0,x_1))h_j(x_0,x_1)\int\sigma_M^{\otimes\{2,\:\ldots\:,\:k\}}({\rm d}(x_2,\ldots,x_k))f(x_0,\ldots,f_k)$ (considering paths of length $k$ only), but I don't see how this simplifies the problem." CreationDate="2020-03-02T15:29:25.627" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14769" PostId="9622" Score="0" Text="Consider the image function $g(x,y) = \int_{\Omega}W(x,y, \omega)L_i(x,y,\omega)\,d\omega$. This basically gives you the intensity at every point of the film. It's a lot nicer to work with - and as far as I got it, you only care about the vertex on the film (at least the filter does). Your problem reduces to an image processing problem through that simplification. Then you simply have $I_j = \int\int h_j(x,y)g(x,y)\,dx\,dy$." CreationDate="2020-03-02T15:32:03.737" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14770" PostId="9622" Score="0" Text="The reason why I'm considering the path space is that my actual concern is slightly more complicated. First of all: Note that (and I missed that before) the inequality (b) in the question simply follows from the fact that if $X$ is a Hilbert space (say $H$) valued random variable on a probability space $(\Omega,\mathcal A,\operatorname P)$, then the function $H\ni x\mapsto\operatorname E\left[\left\|X-x\right\|_H^2\right]$ attains its minimum at $\operatorname E[X]$. Assuming $\lambda(B)\in(0,\infty)$, we only need to apply this fact to $X=f$ and $\operatorname P:=\frac{\lambda(1_Bf)}{λ(B)}$." CreationDate="2020-03-02T20:00:45.527" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14772" PostId="9622" Score="0" Text="I lost you. I have no idea what you are trying to do anymore, or why. Random new details keep popping up in your formulation without rhyme or reason (at least from my perspective). What are you trying to do? Why do you need to consider the whole path space as opposed to just the image function? I believe these two questions are related, but seeing as I do not get what your end goal is, it's hard to help even if I want to." CreationDate="2020-03-02T20:37:26.753" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14773" PostId="9624" Score="1" Text="Will still fail in the case where a box edge intersects the cylinder, none of the box vertices are inside the cylinder, and the cylinder's axis doesn't intersect any fact of the box." CreationDate="2020-03-03T07:33:02.690" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14774" PostId="9623" Score="0" Text="can you control what is on the shipping labels? Cause if so you can add barcodes or QR-like codes" CreationDate="2020-03-03T09:36:43.427" UserId="137" ContentLicense="CC BY-SA 4.0" />
  <row Id="14775" PostId="9624" Score="0" Text="Shoot. That's a good point! I guess I will need to come up with some workarounds for those cases if I can detect them." CreationDate="2020-03-03T15:47:08.687" UserId="11282" ContentLicense="CC BY-SA 4.0" />
  <row Id="14776" PostId="6350" Score="0" Text="This is basically right for light going in a single direction like a laser beam; if you model a fixed number of focused directions it’s like a bunch of crisscrossing laser beams. Somehow, you need to incorporate the notion of divergence or spread so some of the light goes to neighboring cells. In the limit of representing all directions and infinitely small cells, one goal might be to have your model converge on the right result. This is an interesting challenge..." CreationDate="2020-03-03T15:56:14.847" UserId="2500" ContentLicense="CC BY-SA 4.0" />
  <row Id="14777" PostId="9624" Score="0" Text="Well, the surface intersection should work in that case. The only possibility where the volumes intersect but the boundaries (surfaces) do not, is when one is inside the other. So just use 3 checks: 1) any box vertex is closer to the cylinder axis than the cylinder radius, 2) cylinder axis-box intersection, 3) box edges-cylinder intersection." CreationDate="2020-03-03T16:05:00.533" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14778" PostId="5949" Score="0" Text="I don't understand `while( !(err = glGetError())`  Why the `!` ?  Shouldn't it be: `while(err = glGetError())` so that it loops and outputs error messages?  Otherwise, isn't it an infinite loop that repeatedly displays `0` on the first successful call?" CreationDate="2020-03-04T05:49:07.690" UserId="8009" ContentLicense="CC BY-SA 4.0" />
  <row Id="14779" PostId="8247" Score="1" Text="@user1118321 since this is a simple problem, I might just write the solution here. Are there any other sites we can refer the user to that does share more code? If so perhaps I'll answer it there..." CreationDate="2020-03-05T05:50:58.550" UserId="9669" ContentLicense="CC BY-SA 4.0" />
  <row Id="14781" PostId="9596" Score="0" Text="@lightxbulb Okay, I think I'm close to figuring out what the source of my scaling confusion is. I've completely rewritten the question: https://computergraphics.stackexchange.com/q/9589/9254. Maybe you can take a look." CreationDate="2020-03-05T07:56:51.510" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14782" PostId="9589" Score="0" Text="Are you certain they have what you refer to as $g$ in their code/formulation? It could be that they have $\frac{hf}{q'}$ where $q'$ is the probability without the first vertex (for which it is $\frac{1}{|J|}$). If that is the case then what you wrote and what they have would be equivalent, since what you have is really: $\frac{1}{spp}\sum \frac{h(X_i)f(X_i)}{q'(X_i)}$. $q'$ would be the probability of picking the remaining dimensions of the sample (without the factor coming from picking a uniformly a sample on the film) - that is $q(X_i) = \frac{q'(X_i)}{|J|}$." CreationDate="2020-03-05T10:26:37.107" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14783" PostId="9629" Score="1" Text="Wow, thanks for a full solution!" CreationDate="2020-03-05T10:29:45.807" UserId="9668" ContentLicense="CC BY-SA 4.0" />
  <row Id="14784" PostId="9589" Score="0" Text="With regards to your new edit - how does `sample_path` compute the probability? If it doesn't include the $\frac{1}{|J|}$ probability multiplication (the contribution being divided by it), then what happens is exactly what I outlined above." CreationDate="2020-03-05T10:47:02.367" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14785" PostId="9589" Score="0" Text="@lightxbulb That's a good point. Well, they don't compute $f$ and $q$ separately, but only the fraction $\frac fq$ (which is what I called `contribution` and they refer to a $\beta$ in the book). The relevant $β_1⋯β_k$ here would be the one for the second vertex $z_1$ on the camera subpath. This $\beta$ is always equal to $1$. Formally, it should be $$\beta_1:=\frac{W_e(z_1\to z_0)}{p_0(z_0,\omega_{z_0\to z_1})},$$ where $p_0(z_0,\omega_{z_0\to z_1})$ is the density of choosing the first ray $r_1=(z_0,\omega_{z_0\to z_1})$ on the camera subpath with respect to the path throughput measure." CreationDate="2020-03-05T10:48:52.467" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14786" PostId="9589" Score="0" Text="@lightxbulb Now, if I understood them correctly, they defined $W_e(z_0\to z_1)$ (for a perspective camera) to be equal to this density; see equation 16.3 here: http://www.pbr-book.org/3ed-2018/Light_Transport_III_Bidirectional_Methods/The_Path-Space_Measurement_Equation.html#SamplingCameras). Maybe the normalization of $W_e(z_0\to z_1)$ is the source of the error. What do you think?" CreationDate="2020-03-05T10:49:03.240" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14787" PostId="9589" Score="0" Text="Then they simply do not have this $\frac{1}{|J|}$ in the probability, since they have taken it outside. This means that their contribution is divided by $q'$, instead of $q$, which explains the $\frac{1}{spp}$. They basically simply took out the raster probability out of the sum, and canceled it out. Note that this makes sense, especially since this makes it consistent with the other pixel estimators. In general if you had pixels with varying area, you would instead have $\frac{A_j}{\sum_k A_k}$ for example. Simply put - you're missing the $\frac{1}{|J|}$ in your probability in your code." CreationDate="2020-03-05T10:53:28.363" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14797" PostId="9589" Score="0" Text="Any progress on that - did you find whether they do not have this probability in their code? Basically, was my assumption correct, or does this arise from somewhere else?" CreationDate="2020-03-05T12:51:40.713" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14799" PostId="9541" Score="0" Text="I can't choose an arbitrarly small epsilon. For example the Smith-GGX function above tends to return zero for example at the silhouette of a sphere, which then messes up everything. Same thing happens with very small epsilon values. 1e-9 is &quot;so far so good&quot;." CreationDate="2020-03-05T14:17:20.673" UserId="11811" ContentLicense="CC BY-SA 4.0" />
  <row Id="14809" PostId="9451" Score="0" Text="The idea of using the VS in combination with the provoking vertex is attractive but in most cases it is not possible. The reason is that even for the most simple type of regular triangular mesh, asymptotically, there are twice as many triangles as there are vertices. &#xA;Mathematically, you would need to build a mesh that has at most as many triangles as there are vertices. To fill a 2D flat space, since every vertex would  have 3 triangles connected to it on average, the average angle of each triangle summit is 120°. That's not possible since the sum of angles in a triangle is 180°." CreationDate="2020-03-06T19:28:56.973" UserId="12000" ContentLicense="CC BY-SA 4.0" />
  <row Id="14810" PostId="9449" Score="0" Text="How do you see that SPIR-V specs say that geometry capability must be supported to have PrimitiveID? The specs say that PrimitiveID is enable by either of Geometry or Tesselation. So it seems that turning on geometry is over the requirements. I am running on MoltenVK which does not support geometry but supports tesselation. Use of gl_PrimitiveID triggers a warning in the validation layers about geometry being flagged yet not available. However gl_PrimitiveID does work. This lead me to this discussion." CreationDate="2020-03-07T17:27:40.523" UserId="12000" ContentLicense="CC BY-SA 4.0" />
  <row Id="14811" PostId="5949" Score="0" Text="@Wyck Yes, you are correct. I've updated it to make it more explicit by comparing it to the proper error value." CreationDate="2020-03-07T21:07:03.590" UserId="3003" ContentLicense="CC BY-SA 4.0" />
  <row Id="14812" PostId="9629" Score="0" Text="I’m glad this fits your needs!" CreationDate="2020-03-09T08:29:11.970" UserId="9669" ContentLicense="CC BY-SA 4.0" />
  <row Id="14813" PostId="9639" Score="1" Text="You would have to know the pixels for the text." CreationDate="2020-03-10T18:44:49.023" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14814" PostId="9639" Score="0" Text="I hoped there would be some heuristics (based on stuff like edge detection on different color channels) to find pixels likely corresponding to text, although I couldn't find any." CreationDate="2020-03-10T23:23:51.733" UserId="12022" ContentLicense="CC BY-SA 4.0" />
  <row Id="14815" PostId="9639" Score="0" Text="Possibly a neural network can help. Any simpler heuristic will probably also include other elements than the text." CreationDate="2020-03-11T04:47:17.623" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14816" PostId="9639" Score="0" Text="If you can (a) assume a certain RGB layout for the display (and I imagine you could quickly determine this based on the first vertical edge) and (b) assume fixed background and foreground colours (hopefully white and black) *then* you should be able to replace your source image with a 'monochrome' image which is 3x the resolution in X." CreationDate="2020-03-13T08:48:29.117" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14818" PostId="9640" Score="0" Text="&quot;*What I would like is a single output for each work group being the y of the work item with the maximum x.*&quot; What should happen if two work items produce the same `x`?" CreationDate="2020-03-13T14:41:40.370" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14819" PostId="9640" Score="0" Text="It's not exactly clear what the problem here is. If you can determine which work item gave the maximum `x`, then can you not just fetch the `y` value it generated?" CreationDate="2020-03-13T14:43:06.920" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14820" PostId="9640" Score="0" Text="I guess my current thinking is I would not like to store each y in order to decide after all the work items have finished and would like to just have it swap out the y as the work items are run. This may not be the correct way to do it and I'm open to that. Regarding two work items producing the same x, in my application, that should be inconsequential which is decided to be the new maximum." CreationDate="2020-03-13T15:54:55.567" UserId="12033" ContentLicense="CC BY-SA 4.0" />
  <row Id="14821" PostId="9639" Score="0" Text="@SimonF In my case, I would have actual screen captures (text with various colors mixed with GUI elements and images). I know the subpixel layout (queried from the display). I guess a GAN could be trained to do screenshot superresolution, but I am more interested in real-time (think Window Magnifier) solutions. I was hoping that, if my eyes can easily see sub-pixel text as a single color, there is a simple signal processing operation that could do the same..." CreationDate="2020-03-14T12:10:24.107" UserId="12022" ContentLicense="CC BY-SA 4.0" />
  <row Id="14822" PostId="9589" Score="0" Text="@lightxbulb Sorry, I couldn't return to this problem for a while. The source of the error was indeed the normalization of the importance function. So, multiplying the importance function by the area solves the problem." CreationDate="2020-03-18T16:04:50.720" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14823" PostId="9644" Score="0" Text="A non-zero path can always be reinterpreted as being generated by another strategy (provided that both have the same support). Say you have path 1: $x_1, ..., x_N$, and you know that $x_1$ is generated with probability $p_1(x_1)$, $x_2$ with $p_2(x_2)$ etc. Then the probability of the path is $\Pi p_i(x_i)$. Now introduce a different strategy with probabilities $q_1(x_1), q_2(x_2), ...$, then the probability for the path being generated by the new strategy is $\Pi q_i(x_i)$. All you have to do is compute the new probabilities to reinterpret it." CreationDate="2020-03-18T19:02:56.507" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14824" PostId="9644" Score="0" Text="More precisely, the two primary estimators are: $\frac{f(x_1,...,x_N)}{\Pi_i p_i(x_i)}$ and $\frac{f(x_1,...,x_N)}{\Pi_i q_i(x_i)}$. In general you would prefer the one that yields a lower variance (that is, where the probability density better matches $f$)." CreationDate="2020-03-18T19:05:40.780" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14825" PostId="9644" Score="0" Text="@lightxbulb Thank you for your comment. Everything you wrote is clear to me; see my edit. I think that the problem is that `.si.bsdf-&gt;Pdf` is converting the pased directions to local coordinates and then determines whether they lie on the same hemisphere via `pbrt::SameHemisphere`. In my example, the $z$-components have different signs and hence `SameHemisphere` returns false. On the other hand, in `f` the computation of `bool reflect = Dot(wiW, ng) * Dot(woW, ng) &gt; 0;` involves the normal and this product is positive." CreationDate="2020-03-18T19:35:38.407" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14826" PostId="9644" Score="0" Text="@lightxbulb In my example `c` is not zero, but `p` is zero. And I'm unsure whether there is an error in my computations, since I'd actually expect that the density `p` of sampling the direction from $y_{s-1}$ to $z_{t-1}$ should be positive (assuming that the contribution under the strategy under which the path was constructed is nonvanishing)." CreationDate="2020-03-18T19:38:17.160" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14827" PostId="9644" Score="0" Text="@lightxbulb Actually, I don't understand why the computation in `SameHemisphere` doesn't involve the surface normal (`ng`). Maybe it does implicitly, since the directions are transformed via `WorldToLocal`." CreationDate="2020-03-18T20:15:26.417" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14828" PostId="9644" Score="0" Text="@lightxbulb In any case, if you take a look at http://www.pbr-book.org/3ed-2018/Light_Transport_I_Surface_Reflection/Sampling_Reflection_Functions.html#BxDF::Pdf, they explicitly state *Samples from this distribution will give correct results for any BRDF that isn’t described by a delta distribution, since there is some probability of sampling all directions where the BRDF’s value is non-0* (see the fourth paragraph). So, it seems like there is something wrong with my computations." CreationDate="2020-03-18T20:15:31.573" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14829" PostId="9644" Score="0" Text="@lightxbulb I guess I've figured out where the problem comes from. It seems like `pdf` is considering the hemisphere wrt the &quot;shading normal&quot; but `f` is considering the hemisphere wrt the &quot;geometry normal&quot; ... I'm not sure what I should conclude from that ..." CreationDate="2020-03-19T07:14:09.227" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14830" PostId="9644" Score="0" Text="Seems to be a question specific to the PBRT implementation. They most likely work in a space where the geometric normal is aligned with one of the axes, and then the transformation aligns it with the actual normal, I haven't looked into the details of PBRT so take that with a grain of salt. If you are considering shading normals however, you must account for the lack of symmetry - see page 150 in Veach's thesis and figure 5.6." CreationDate="2020-03-19T11:22:15.373" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14834" PostId="9644" Score="0" Text="@lightxbulb (a) It seems like that their local coordinate system is such that the positive $z$-axis points in the direction of the *shading* normal. So, `BxDF::Sample_f` samples directions over the hemisphere induced by the shading normal. On the other hand, `y[s - 1].f(z[t - 1], pbrt::TransportMode::Importance)` is evaluating reflection if the direction from `y[s - 1]` to `z[t - 1]` lies in hemisphere wrt the geometric normal (`bool reflect = Dot(wiW, ng) * Dot(woW, ng) &gt; 0;`)." CreationDate="2020-03-19T15:14:51.083" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14835" PostId="9644" Score="0" Text="@lightxbulb So, when the subpaths are connected, `y[s - 1].f(z[t - 1], pbrt::TransportMode::Importance)` might be nonzero even when `BxDF::Sample_f` would never have sampled a direction from `y[s - 1]` to `z[t - 1]`. Does this make sense? This would imply (and that's the important thing for me) that we cannot conclude that a path with a nonzero luminance wrt a $(s,t)$-strategy has a nonzero luminance wrt all other strategies. (b) Are we considering `z[t - 1]` and `y[s - 1]` as mutually visible if the direction to `z[t - 1]` lies in the same hemisphere over the shading or the geometric normal?" CreationDate="2020-03-19T15:15:30.467" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14836" PostId="9644" Score="0" Text="`This would imply (and that's the important thing for me) that we cannot conclude that a path with a nonzero luminance wrt a (s,t)-strategy has a nonzero luminance wrt all other strategies.` - this should be impossible for a correct implementation. A path always carries the same radiance, only the probability with which the vertices are sampled may change. b) See Figure 5.6 in Veach's thesis - it needs to be in the same hemisphere as both the geometric and shading normal." CreationDate="2020-03-19T16:39:14.623" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14837" PostId="9589" Score="0" Text="I would suggest you formulate an answer to your question yourself, since you're more familiar with the PBRT details you were referring to. It may be useful for someone in the future." CreationDate="2020-03-19T17:02:10.737" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14838" PostId="9644" Score="0" Text="@lightxbulb Sorry, I've phrased (a) in the wrong way. The problem is that I've constructed a path with nonzero luminance using the $(2,3)$-strategy observed that the density of sampling this path using the $(3,2)$-strategy is zero. See my edit 3." CreationDate="2020-03-19T17:11:31.747" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14839" PostId="9644" Score="0" Text="Then either you have a bug, or your estimator is both biased and inconsistent. It will never converge to the actual solution, if it's missing paths that are otherwise viable. Your densities should be non-zero for parts of the domain where your function is non-zero, otherwise you get inconsistent estimators, that will never yield the correct solution regardless of the number of samples. Also, concerning your last edit, please see the Figure in Veach's thesis that I mentioned." CreationDate="2020-03-19T17:54:09.030" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14840" PostId="9644" Score="0" Text="@lightxbulb As I've written above, the problem is that `Sample_f` would never sample the direction from $y_1$ to $z_2$, but the connection has still a nonzero contribution `c`." CreationDate="2020-03-19T18:04:58.577" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14841" PostId="9644" Score="0" Text="Then there are 3 options: 1) that's a bug, 2) the estimator is inconsistent, 3) the contribution is actually zero (may be evaluated to zero at some other place in the code)." CreationDate="2020-03-19T18:07:05.233" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14842" PostId="9644" Score="0" Text="@lightxbulb I can guarantee that neither (2) nor (3) is the case. The problem really is that `wi` is not on the hemisphere wrt ´ns` (which is obvious), but `BxDF::Sample_f` samples only directions on that hemisphere (via `CosineSampleHemisphere`). On the other hand, `y[s - 1].f(z[t - 1], pbrt::TransportMode::Importance)` evaluates the BSDF `f(wo, w_i)` (which is nonzero, since the BSDF is Lambertian) as long as `bool reflect = Dot(*wiWorld, ng) * Dot(woWorld, ng) &gt; 0;` is true (which is the case)." CreationDate="2020-03-19T18:38:18.770" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14844" PostId="9644" Score="0" Text="@lightxbulb So, to me it seems like the connection step (which is shown here http://www.pbr-book.org/3ed-2018/Light_Transport_III_Bidirectional_Methods/Bidirectional_Path_Tracing.html#fragment-Handleallotherbidirectionalconnectioncases-0) should not evaluate `f` in this way (in the link it is `qs.f(pt)`). The only step where it should be detected is the evaluation of `G(scene, sampler, qs, pt);`. But it returns a nonzero value. That's why I've asked you (b)." CreationDate="2020-03-19T18:45:02.170" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14845" PostId="9644" Score="0" Text="@lightxbulb As you can see in the implementation of `BSDF::f` in the question, they ony check whether `wi` and `wo` are on the hemisphere wrt `ng`, but they don't check the hemisphere wrt `ns`. I think this is in accordance what they write here http://www.pbr-book.org/3ed-2018/Materials/BSDFs.html#BSDF::f: *In evaluating the scattering equation, however, the dot product of the normal and the incident direction is still taken with the shading normal rather than the geometric normal.*" CreationDate="2020-03-19T18:45:14.160" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14846" PostId="9644" Score="0" Text="```they ony check whether wi and wo are on the hemisphere wrt ng, but they don't check the hemisphere wrt ns``` - then their bsdf is not actually a bsdf since it is not symmetric. Note, this is only the case if you're reading the code correctly (as I mentioned I haven't gone into such details in their code). The reason this implies a non-symmetric bsdf can be found in Veach's thesis as already mentioned. May also be what Veach refers to as &quot;light leaks&quot;." CreationDate="2020-03-19T18:47:53.737" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14847" PostId="9644" Score="0" Text="@lightxbulb The relevant function should be [CorrectShadingNormal](http://www.pbr-book.org/3ed-2018/Light_Transport_III_Bidirectional_Methods/The_Path-Space_Measurement_Equation.html#CorrectShadingNormal). But therein they take the absolute value of the inner product: `AbsDot(wo, isect.shading.n) * AbsDot(wi, isect.n);`." CreationDate="2020-03-19T18:50:52.560" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14848" PostId="9644" Score="0" Text="@lightxbulb Regarding the light leak. They write: *Now it should be clear why pbrt requires BxDFs to evaluate their values without regard to whether and are in the same or different hemispheres. This convention means that light leaks are avoided, since we will only evaluate the BTDFs for the situation in Figure 9.2(a), giving no reflection for a purely reflective surface.* (http://www.pbr-book.org/3ed-2018/Materials/BSDFs.html#BSDF::f)" CreationDate="2020-03-19T18:53:51.380" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14849" PostId="9644" Score="0" Text="I guess this means that they take care of this somewhere (meaning that the contribution is actually correctly zeroed out when required). Anyways, as mentioned, the opposite would imply either an unintentional bug or an inconsistent estimator. Considering that PBRT's been thoroughly tested, I guess it makes sense that it's neither." CreationDate="2020-03-19T18:57:57.490" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14850" PostId="9644" Score="0" Text="@lightxbulb You're right. I've find the place where they state this: http://www.pbr-book.org/3ed-2018/Reflection_Models.html (the last bullet point at the very end of the page). However, at least in the context of `ConnectBDPT`, I don't see where *Higher level code in pbrt [..] ensure[s] that only reflective or transmissive scattering routines are evaluated as appropriate.*." CreationDate="2020-03-19T19:26:13.030" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14851" PostId="9650" Score="0" Text="I won't be able to render the scene to mimick reflections more effectively (see the edit). As to SSRT I am not sure you can get a reflective effect if you don;t have the depth information of the image." CreationDate="2020-03-21T16:17:39.683" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14852" PostId="9650" Score="0" Text="@Makogan: See my edits." CreationDate="2020-03-22T01:27:02.433" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14853" PostId="9650" Score="0" Text="yes I understand that no matter what I do the image won;t be accurate, my intent is merely to try to change the &quot;zoom&quot; level just a little bit such that objects in the reflection are somewhat less distorted.&#xA;&#xA;Essentially if we say that currently it takes 10 seconds to figure out what;s wrong in the image, I am trying to push it so that it;s at least 30 seconds, maybe a minute if lucky." CreationDate="2020-03-22T03:14:16.777" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14855" PostId="9651" Score="0" Text="The rendering equation." CreationDate="2020-03-22T16:30:18.143" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14856" PostId="9643" Score="0" Text="I always got the impression divergence is a bottleneck here. How are you handling bounces ? I found it was quicker to update a ray-state structure instead of updating the accumulation buffer (which requires a fully resolved path) so each PT pass updates one step of each per pixel ray. This way all threads are doing more even workloads. This complicates image output as pixels can only be output when a ray has completed, but it does mean a lot of resources are not idly waiting on that 1 path that needed a full 8 or whatever bounces. This is not a solution for your problem,more a talking point" CreationDate="2020-03-23T07:20:04.023" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14857" PostId="9653" Score="0" Text="You can draw a circle in 2D without representing it through triangles - see Bresenham's algorithm for example." CreationDate="2020-03-23T09:55:56.003" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14859" PostId="9648" Score="0" Text="Thanks for replying. But I still wonder why the gradients point along the axis and diagonals causing clump." CreationDate="2020-03-24T14:27:51.413" UserId="12046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14862" PostId="9643" Score="0" Text="Well since it's a megakernel, single pass approach, Each work item is assigned to one pixel and the path terminates through Russian Roulette. I realize this can increase  the divergence but I have no other choice. I guess there really aren't that many ways to improve mega-kernel performance." CreationDate="2020-03-25T06:16:31.343" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14863" PostId="9643" Score="0" Text="How about occupancy ? What happens to neighbouring paths when 1 path is at worst case maximum bounces ?" CreationDate="2020-03-25T09:41:49.537" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14864" PostId="9643" Score="0" Text="That's the thing, since all the work items in a workgroup are operating on separate pixels, the whole workgroup will get delayed till whatever work-item has completed the maximum bounces" CreationDate="2020-03-25T15:01:35.743" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="14865" PostId="4254" Score="0" Text="As soon as I saw your name, I knew you'd include something on blue noise :)" CreationDate="2020-03-25T19:39:00.840" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14866" PostId="4253" Score="0" Text="You say &quot;4 samples MSAA would be:&quot;  Presumably you meant SSAA.  MSAA, as used in rasterisation as a cost/quality tradeoff, first determines which object (read triangle) each of the samples hits. If all of them *hit the same object*, only one *shading* operation is performed.  If 2 different objects are found, then 2 shading operations are performed and the results are *weighted* and summed by the number of samples hitting each object, and so on for 3 etc objects.  What you are describing is standard super sampling AA, i,e, SSAA." CreationDate="2020-03-26T08:14:05.730" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14868" PostId="4254" Score="0" Text="Since this popped up as active again: To explain why this is the case - blue noise sampling patterns excel at integrating functions with most of the energy concentrated in the low frequencies. So by using a blue noise pattern you effectively assume that your signal is mostly smooth/dominated by low frequencies. For high frequency signals, this may actually be counterproductive. Fortunately, natural images have their energy concentrated in the low frequencies. For more details see: https://sampling.mpi-inf.mpg.de/2019-singh-fourier.html" CreationDate="2020-03-26T15:39:01.990" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14870" PostId="9653" Score="0" Text="Wouldn't that require you to rasterise it pixel-by-pixel, in HTML ?  (Sorry if that sounds ignorant, I don't know HTML)" CreationDate="2020-03-27T07:23:36.530" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14871" PostId="9661" Score="0" Text="Is this for the case where parts of the reflected lobe intersect the surface, usually seen at glancing angles? I think for that case you need to flip the vector using the surface normal." CreationDate="2020-03-27T07:26:40.357" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14872" PostId="9651" Score="0" Text="This isnt such a bad question but needs some more focus. Just saying mathematics in computer graphics catches mostly all of the math i was taught in university, and school. CG is mathematical modeling after all." CreationDate="2020-03-27T07:42:30.253" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="14873" PostId="9661" Score="0" Text="I solved it by just resampling the normal if the reflection direction is bad. This might be rather computationally expensive but I was able to reduce the overhead so it is barely noticeable." CreationDate="2020-03-27T14:06:43.857" UserId="12069" ContentLicense="CC BY-SA 4.0" />
  <row Id="14874" PostId="9661" Score="0" Text="just flipping results in non accurate reflections" CreationDate="2020-03-27T14:07:40.597" UserId="12069" ContentLicense="CC BY-SA 4.0" />
  <row Id="14876" PostId="9670" Score="1" Text="This is not computer graphics." CreationDate="2020-03-28T11:02:08.720" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14877" PostId="9669" Score="0" Text="I'm fairly certain that is the default mode, so will not change anything in my picture. Also, I'm more interested in, for instance, what something like that does (which is just interpolate colors between vertices), rather than just a formula to fix the problem." CreationDate="2020-03-28T16:18:57.267" UserId="12073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14878" PostId="4253" Score="1" Text="@SimonF Thank you for pointing it. Fixed." CreationDate="2020-03-28T17:53:03.557" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="14880" PostId="9656" Score="0" Text="Hello and welcome to CGSE. Your question is way too broad at the moment; this website is not intended for &quot;how do I even start&quot; kinds of questions. Please be more specific about implementation or design problems you are encountering." CreationDate="2020-03-28T18:05:47.550" UserId="182" ContentLicense="CC BY-SA 4.0" />
  <row Id="14881" PostId="9651" Score="0" Text="Not sure if this fits here, but Straight Skeletons are a thing I'd like to research on." CreationDate="2020-03-28T20:50:17.743" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="14882" PostId="9673" Score="0" Text="It's not clear what you're asking about here. A fragment shader *is* a &quot;user-defined function to determine the colour of each fragment&quot;. Determining the fragment's color is the *entire point* of a fragment shader. It's just code; you can do what you want with it." CreationDate="2020-03-29T02:15:48.843" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14883" PostId="9673" Score="0" Text="@NicolBolas From the wiki: _&quot;The user-defined inputs received by this fragment shader will be interpolated according to the interpolation qualifiers declared on the input variables declared by this fragment shader&quot;_. I cannot find an example of how you would specify a different colour per fragment. All the wiki seems to mention is specifying colours per vertex which then get automatically interpolated for each fragment." CreationDate="2020-03-29T12:53:25.090" UserId="13085" ContentLicense="CC BY-SA 4.0" />
  <row Id="14884" PostId="9673" Score="0" Text="@NicolBolas Nevermind, I think I am getting confused thinking that the fragment shader is the part doing the interpolation, when in the example image the interpolation would actually be done by the vertex shader, or somewhere inbetween the two of them? The fragement shader is just being passed the interpolated value? Is this correct?" CreationDate="2020-03-29T13:06:27.997" UserId="13085" ContentLicense="CC BY-SA 4.0" />
  <row Id="14885" PostId="9677" Score="0" Text="Look up Cook-Torrance." CreationDate="2020-03-29T20:20:44.113" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14886" PostId="9677" Score="0" Text="Note that how you render the image doesn't really affect whether the surface is metallic. A rasterizer is just as capable of getting a surface to appear metallic as a raytracer. It's all about the lighting model you employ." CreationDate="2020-03-30T01:33:50.583" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14887" PostId="9677" Score="0" Text="won't the way the light interacts with the surface determine whether its metallic or not?" CreationDate="2020-03-30T05:52:15.937" UserId="13087" ContentLicense="CC BY-SA 4.0" />
  <row Id="14888" PostId="9677" Score="0" Text="Could you post an image?" CreationDate="2020-03-30T06:37:23.570" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14889" PostId="9677" Score="0" Text="I'm not sure of what exactly" CreationDate="2020-03-30T06:38:34.473" UserId="13087" ContentLicense="CC BY-SA 4.0" />
  <row Id="14890" PostId="9670" Score="1" Text="I'm voting to close this question as off-topic because it's not a CG question" CreationDate="2020-03-30T06:41:08.830" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="14894" PostId="9677" Score="0" Text="@JerSci: &quot;*won't the way the light interacts with the surface determine whether its metallic or not?*&quot; Sure, but how &quot;light interacts with the surface&quot; is governed by your lighting equation. And you can implement pretty much any lighting equation in a rasterizer." CreationDate="2020-03-30T21:16:10.957" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14895" PostId="9671" Score="0" Text="Please provide more information (code if possible). How do you define your box (vertex attributes)? How do you populate the buffers? How do your shaders look like? To me, it looks as if your texture coordinates are not set correctly, but I can't tell for sure without the corresponding code." CreationDate="2020-03-31T09:50:45.157" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="14896" PostId="9677" Score="0" Text="A surface will look metallic if it is essentially perfectly reflective. So when you do the ray tracing, your object should take on the color of the background almost completely. If you add a picture of what you have, that would be very helpful." CreationDate="2020-03-31T15:32:02.410" UserId="12073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14897" PostId="9677" Score="0" Text="@jheindel the problem with that is that my sphere completely disappears into the background because it's completely reflective. Say I have a blue background and a red sphere in the scene, I wont be able to see the red sphere because its completely reflective and blends into the blue background" CreationDate="2020-03-31T15:36:23.893" UserId="13087" ContentLicense="CC BY-SA 4.0" />
  <row Id="14898" PostId="9677" Score="0" Text="@jheindel just putting this up here because I dont think the tag worked" CreationDate="2020-03-31T15:37:02.207" UserId="13087" ContentLicense="CC BY-SA 4.0" />
  <row Id="14899" PostId="9682" Score="0" Text="Because it's tedious and solved by any epsilon offset." CreationDate="2020-03-31T18:27:23.973" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14900" PostId="9682" Score="0" Text="@lightxbulb While this would be an easy fix for a lot of cases, unfortunately not all. Consider two cubes next to each other, not intersecting, with one face touching. If you were to union them and apply an offset to one of the cubes so there is no coplanar check, the resulting mesh would be two manifolds (when it should be one)." CreationDate="2020-03-31T22:25:35.813" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14901" PostId="9682" Score="0" Text="Also in the case of OP, a decision would have to be made in advanced about which mesh to offset, if the blue mesh was to be offset along it's triangle normals, this would result in an empty space (what is desired), if the blue mesh was offset the other way (normal inverse) then the result would be the orange mesh (not desired, it should be an empty space). Of course if you are well disciplined you could get away with in many cases, however there are always situations which this cannot fix. Also note offsetting would only really work if the two triangles are equal, and not just coplanar." CreationDate="2020-03-31T22:25:51.213" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14902" PostId="9671" Score="0" Text="Which version of OpenGL are you using? The second issue (skewed texture) tends to be the result of an image dimension mismatch when uploading (glTexImage2D(...)). Double check you are specifying the correct dimensions when uploading texture to OpenGL. Also maybe check you can use NPOT (non-power of two) textures which depends on the OpenGL context version you create." CreationDate="2020-03-31T22:38:56.900" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14903" PostId="9682" Score="1" Text="I never said it's foolproof. It explains why you have such failure cases in many libraries - it's a tedious edge case that requires a disproportionate amount of code to deal with robustly." CreationDate="2020-03-31T22:51:44.097" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14904" PostId="9682" Score="0" Text="@lightxbulb Yes it is certainly an extra layer of complexity ontop of what would otherwise be quite an elegant algorithm. I agree in most cases simply offsetting is fine, if I was using it for a hobby project I certainly wouldn't haven't bothered going the extra mile, however unfortunately professionalism sometimes demands that extra bit from me and in this case for CSG I had to comply. Certainly in OP's case, it seems they require this extra complexity due to the nature of required precision of the input data... but yes agree if you can get away with it, best to save oneself the extra pain ;)" CreationDate="2020-03-31T22:59:09.137" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14905" PostId="9671" Score="0" Text="@lfgtm He is using OpenGL v4.6 as mentioned at the beginning. That's why I don't think that NPOT textures are the problem. Additionally, if there is a dimension mismatch, wouldn't there be parts missing? To me, it just looks like the textures are shifted to the right with increasing y value." CreationDate="2020-04-01T07:34:30.333" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="14906" PostId="9662" Score="0" Text="What is wrong with using one integer to store ARGB? You can extract the values from the int if required using bit shifting and masking." CreationDate="2020-04-01T08:35:26.300" UserId="11022" ContentLicense="CC BY-SA 4.0" />
  <row Id="14907" PostId="9671" Score="0" Text="@wychmaster It's not texture coordinates. The skew and lack of colour is a dead giveaway that the texel &quot;stride&quot; is incorrect. If the dims and number of channels are correct, the input image is probably not aligned as GL expects it to be. `glPixelStorei` would probably be the fix, but without knowing more details about the input texture, impossible to say what arguments `glPixelStorei` should take." CreationDate="2020-04-01T13:30:37.597" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14909" PostId="9671" Score="0" Text="@wychmaster I doubt there is a shift in the coordinates because the same code was used to display the multiple roads image. Also, just to be safe, I tested the render on this (also jpg) and it works- https://imgur.com/Sm3zuMy" CreationDate="2020-04-01T15:15:11.027" UserId="13083" ContentLicense="CC BY-SA 4.0" />
  <row Id="14910" PostId="9671" Score="0" Text="@lfgtm I'm looking up what npot textures are. Never came across them on my tutorials. &#xA;&#xA;Btw, I was doing a bit of reading on the .png files.. It seems they also contain other details like gamma correction. Would these be represented in a simple RGBA value? Presently, I'm trying to set-up the libpng library to see if the problem persists." CreationDate="2020-04-01T15:18:29.097" UserId="13083" ContentLicense="CC BY-SA 4.0" />
  <row Id="14911" PostId="9671" Score="0" Text="Btw, this is a render of a png with the same code- https://imgur.com/oc4fRBt" CreationDate="2020-04-01T15:25:20.453" UserId="13083" ContentLicense="CC BY-SA 4.0" />
  <row Id="14929" PostId="9691" Score="0" Text="The variance is computed per pixel - you are estimating a different integral in each pixel. It is reasonable to have a different variance for different pixels." CreationDate="2020-04-04T09:10:19.807" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14930" PostId="9691" Score="0" Text="For Q1 you could just plot graphs for the same integrand at different sample counts. That is - measure the variance at 1 pixel for different sample counts and plot it." CreationDate="2020-04-04T09:21:32.030" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14931" PostId="9691" Score="0" Text="@lightxbulb It doesn't really matter, but you've got me wrong: I've got different estimators and so I'm comparing them with a fixed sample size. But what exactly do you mean by &quot;plotting the graph&quot;? Currently I'm writing the pixel variances as an image into a file, but that doesn't really make sense. For example, if a variance value in the red-channel is $\ge1$ it will always be &quot;purely red&quot;. So, I would need to apply some scaling or something like that." CreationDate="2020-04-04T10:17:04.963" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14932" PostId="9691" Score="1" Text="Pick one pixel and use one estimator, then compute the variance at different sample counts, store the sample counts and the corresponding variance at each in a text file. Do the same for a different estimator. Make a plot of sample count vs variance for both (you can use matplotlib in Python or Matlab). This way you'll be able to compare the asymptotic behaviour of the two for the integrand at the chosen pixel. You can use a log-log scale if the difference warrants it. It's a common way of comparing low discrepancy vs random sequence performance." CreationDate="2020-04-04T10:56:47.970" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14933" PostId="9691" Score="0" Text="@lightxbulb Okay, but this would be a visualization for the change of variance of a single pixel, right? Is there any common approach to visualize the variance (for a fixed estimator; not in comparison between two of them) of the whole image? I thought it might be possible to visualize the per-pixel variance by an image itself." CreationDate="2020-04-04T11:01:35.580" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14934" PostId="9691" Score="0" Text="You can visualize the per-pixel variance of the image: just output an HDR image - it is not limited to 0-1. You can use stb_image_write." CreationDate="2020-04-04T13:10:08.253" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14938" PostId="9691" Score="0" Text="@lightxbulb Okay, I need to admit that I have no theoretical background in HDR. I've tested `stb_image_write` by `std::vector&lt;float&gt; image(3 * width * height);` `float r = 0; for (std::size_t i = 0; i &lt; width * height; ++i, r += .01) image[3 * i] = r;` and `stbi_write_hdr(&quot;test.exr&quot;, width, height, 3, image.data());`. The resulting image has a red channel ranging from $0$ to $3584$. How does this help with my problem? It seems like the scaling problem is still present (since I should obtain the same image output if I divide by 3584 before writing the image.)" CreationDate="2020-04-04T15:43:30.530" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14939" PostId="9691" Score="0" Text="@0xbadf00d you can save the variance as floats for each channel, save the full image as an EXR and do the visualization in Python using some colormap. You can use [`pyexr`](https://github.com/tvogels/pyexr) to load the image into a Numpy array–which you can rescale as you wish–and save to an LDR (e.g. PNG) to visualize the per-pixel variance." CreationDate="2020-04-04T16:06:40.713" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14940" PostId="9691" Score="0" Text="@Hubble Currently I'm opening the EXR using RenderDoc where I can rescale as well. So, I guess using Python isn't superior to that or am I missing something? I didn't get your last sentence. How would I need to rescale so that I'm able to compare two variance images? Dividing by the max component value wouldn't make sense ... (Is there any good reading on this topic?)" CreationDate="2020-04-04T17:02:25.297" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14941" PostId="9691" Score="0" Text="@Hubble Oh, and is there a useful metric which indicates that the &quot;overall variance&quot; of one estimator $E_1$ is smaller than the overall variance of another estimator $E_2$? It might clearly be the case that fixed pixels $j_1,j_2$ estimator $E_1$ has smaller variance in $j_1$, but larger variance in $j_2$. Is there a metric which helps to decide which estimator is superior for the whole image?" CreationDate="2020-04-04T17:20:17.617" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14942" PostId="9691" Score="1" Text="Try visualizing the variance through a program like tev: https://github.com/Tom94/tev&#xA;As for &quot;overall&quot; variance, I am not aware of a specific metric. The simplest thing you could do is take the average, but obviously you lose some information this way - it's perfectly reasonable that one estimator is better in one part of the image, while the other is better in another part. What's the purpose of this exercise?" CreationDate="2020-04-04T17:44:36.320" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14943" PostId="9691" Score="0" Text="@lightxbulb My estimator depends on a parameter which I want to tune for variance reduction.  The problem is that - in contrast to bidirectional path tracing where the parameters are the multiple importance sampling weights - my (asymptotic) variances depend on the individual pixel and there is no easy bound (as it is the case in BDPT yielding the balance heuristic)." CreationDate="2020-04-04T17:48:24.367" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14944" PostId="9691" Score="1" Text="@0xbadf00d typically one would compute a reference image with several thousand samples and look at an averaged, pixel-wise comparison with your algorithm as a function of time. Popular metrics are [sMAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) and relMSE but other perceptual metrics also exist. If you plot the error over time (log-log) you can then track how fast your new estimator can reduce the variance compared to a baseline. See Figure 5 of [this paper](https://arxiv.org/abs/1704.06835) for an example in MCMC." CreationDate="2020-04-04T17:48:36.203" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14945" PostId="9691" Score="1" Text="This [CGF paper](https://www.cg.tuwien.ac.at/research/publications/2019/celarek_adam-2019-qelta) from 2019 might give you some insights on what you are trying to accomplish." CreationDate="2020-04-04T17:52:25.823" UserId="6588" ContentLicense="CC BY-SA 4.0" />
  <row Id="14946" PostId="9691" Score="0" Text="@Hubble Thank you for the suggestions. Did I get this right: sMAPE would be the average of my $\sigma_j^2$?" CreationDate="2020-04-04T18:30:47.570" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14947" PostId="9691" Score="0" Text="@Hubble BTW, my parameter optimization problem would be easier to solve if I would replace $f$ by the luminosity function in the definition of $I_j$, i.e. if I would consider the per-pixel variance of the relative color luminosity instead of the RGB values. Would this still make sense?" CreationDate="2020-04-04T18:50:43.583" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14949" PostId="9694" Score="2" Text="You could make a 3d normal from a 2d normal with something like &gt; float height = 1.0; N = normalize(vec3(Normal2D, height));" CreationDate="2020-04-05T15:57:16.283" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14950" PostId="9694" Score="0" Text="@PaulHK brilliant! That works quite well. Still working on getting it tuned to where I need. It." CreationDate="2020-04-05T20:31:02.370" UserId="7072" ContentLicense="CC BY-SA 4.0" />
  <row Id="14951" PostId="9691" Score="0" Text="@lightxbulb Regarding the loss of information in the &quot;overall&quot; variance: Do you think it is a good idea to remedy this issue by subdividing the image into subimages and running a separate Markov chain on each subimage? Or is this tiling known to produce artifacts?" CreationDate="2020-04-06T04:59:42.433" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14952" PostId="9691" Score="0" Text="What's your issue with considering only per pixel variance?" CreationDate="2020-04-06T07:15:53.650" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14953" PostId="9691" Score="0" Text="@lightxbulb I want to optimize a parameter of the Markov chain for variance reduction. Usually, the Markov chain runs over the whole image or a subregion of it. Unless we run it on a single pixel, I cannot consider per-pixel variance but need some variance notion for a set of pixels." CreationDate="2020-04-06T07:52:07.520" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14954" PostId="9691" Score="0" Text="If you do tiling, I would assume that this will result in a visible difference of the error between two tiles, so I would recommend not to do it. You can still try it to see whether my assumption is correct (that the difference will be too large, and thus visible). Tiling in general introduces discontinuities." CreationDate="2020-04-06T15:55:05.900" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14955" PostId="9691" Score="0" Text="@lightxbulb I think there shouldn't be a visible difference of the error as long as each tile has the same area and the chains all have the same sample size. (And a first simple test was positive.)" CreationDate="2020-04-06T18:06:25.073" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14956" PostId="9691" Score="0" Text="Do post some screen after you do some experiments. I am interested." CreationDate="2020-04-06T21:00:52.870" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14958" PostId="9696" Score="0" Text="Usually textures need to be aligned such that the total bytes in a line is divisible by 4. With RGB format your have 3 bytes per pixel so it's easier to misalign. You can also avoid this in RGB format by making sure that image width divisible by 4." CreationDate="2020-04-07T05:47:35.727" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14959" PostId="9694" Score="0" Text="Would be interesting to see your results, happy to help" CreationDate="2020-04-07T08:33:03.587" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14960" PostId="9691" Score="0" Text="@lightxbulb I will try some things but I think we need to remember that the whole reason for using MCMC instead of convential &quot;per-pixel&quot; approaches like bidirectional path tracing is that it automatically finds regions which make a large contribution to the image and takes the majority of samples from these regions. In the degenerate case where our tiles contain only a single pixel, my problem of a suitable variance notion would disappear, but would the resulting algorithm (which runs a separate chain on each pixel) still make sense?" CreationDate="2020-04-07T09:54:26.630" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14961" PostId="9329" Score="2" Text="@ina You usually can't magically make a normal mapping lighting shader into an unlit shader just by setting some magic normal map texture. In any case, that would require some more knowledge about how that shader works exactly beyond just that it's a &quot;standard normal map shader&quot;." CreationDate="2020-04-07T14:32:51.437" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="14964" PostId="9329" Score="0" Text="Also in this case I am trying to convert the Unity Standard Shader to Unlit https://docs.unity3d.com/Manual/shader-StandardShader.html" CreationDate="2020-04-08T00:43:52.017" UserId="11445" ContentLicense="CC BY-SA 4.0" />
  <row Id="14965" PostId="9696" Score="0" Text="That makes sense, all the images that were skewed were not a multiple of 4 in width. Does this mean that the texture input by default expects RGBA values?" CreationDate="2020-04-08T02:23:31.033" UserId="13083" ContentLicense="CC BY-SA 4.0" />
  <row Id="14966" PostId="9691" Score="0" Text="@lightxbulb Since Hibble didn't respond to this question (yet): Does it make sense to consider the variances of the luminance function $p$ instead of the measurement contribution function $f$ itself ($p=\alpha_1f_1+\alpha_2f_2+\alpha_3f_3$)?" CreationDate="2020-04-08T08:52:22.840" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14967" PostId="8318" Score="0" Text="Unfortunately, it didn't work for `vulkansdk-1.2.131.2` on ubuntu 19.10 for glfw-based app, although such layer loaded properly as inspected via `VK_LOADER_DEBUG=all`. But anyway, it works with vkcube." CreationDate="2020-04-08T08:54:56.527" UserId="9837" ContentLicense="CC BY-SA 4.0" />
  <row Id="14968" PostId="9691" Score="0" Text="Make sense for what purpose? What are the alphas and $f$s here?" CreationDate="2020-04-08T09:25:32.100" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14969" PostId="9699" Score="0" Text="`ddx` and `ddy` will simply approximate a finite difference in some pixel neighbourhood. As an example, consider pixels $(i,j)$ and $(i+1,j)$. Let the world positions at $(i,j)$ and $(i+1,j)$ be $w_{i,j}$ and $w_{i+1,j}$ respectively. Then $\frac{\partial}{\partial x}w_{i,j} \approx w_{i+1,j} - w_{i,j}$, and `ddx` is an approximation of this." CreationDate="2020-04-08T10:06:51.080" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14970" PostId="9691" Score="0" Text="@lightxbulb For minimizing the image variance. $f$ is the measurement contribution function and since $p$ is the luminance function, $p=0.212671f_1+0.715160f_2+0.072169f_3$. This would make the analysis easier since the target distribution density of MLT is $p$." CreationDate="2020-04-08T10:09:19.013" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14971" PostId="9691" Score="0" Text="You can do either. There's no specific prescription unless you are referring to a specific paper. You can even apply a nonlinearity if you so wish. At the end of the day this just affects the metric that you want to minimize, if you take perception into account this metric will obviously be a lot more complex." CreationDate="2020-04-08T10:13:00.203" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="14975" PostId="9691" Score="0" Text="@lightxbulb The integral corresponding the value of a pixel is of the form $\mu g:=\int g\:{\rm d}\mu$, where $g$ is a $[0,\infty)^3$-valued function on the path space $E$ and $\mu$ is the infinite-area product measure. So, it's not clear what the &quot;variance&quot; of an estimate to $\mu g$ should be (since it is vector-valued, not scalar-valued). However, if we know that our estimator $A_n$ is such that $A_n\psi\to\mathcal N(0,\sigma^2(\psi)$ for all scalar-valued $\psi$ on $E$, then has asymptotic variance $\sigma^2(\psi)$, then" CreationDate="2020-04-08T15:27:15.297" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14977" PostId="9691" Score="0" Text="@lightxbulb $\langle a,\sqrt n(A_ng-\mu g)\rangle\to\mathcal N(0,\sigma^2(\langle a,g\rangle))$ for all $a\in\mathbb R^3$ and hence $\sqrt n(A_ng-\mu g)\to\mathcal N(0,\Sigma(g))$, where $\Sigma(g)$ is a nonnegative symmetric $3\times3$-matrix  whose trace norm is equal to $\sigma^2(|g|)$, where $|g|$ is the Euclidean norm of $g$. So, mathematically, it would make sense to try to minimize $\sigma^2(|g|)$, but I guess from a visual perception point-of-view it makes more sense to $\sigma^2(\ell(g))$, where $\ell$ is the luminance function. Do you agree on that?" CreationDate="2020-04-08T15:53:00.023" UserId="9254" ContentLicense="CC BY-SA 4.0" />
  <row Id="14979" PostId="9702" Score="0" Text="Do you normalise the length of the normal vectors before adding them together ?" CreationDate="2020-04-09T05:38:28.343" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14980" PostId="9702" Score="0" Text="Yes, the error comes from the number of normals not their magnitudes." CreationDate="2020-04-09T05:50:44.397" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14981" PostId="9702" Score="0" Text="Are the lengths of the normals 1 though ? The first pic looks like what happens when you sum up cross-products with normalising them." CreationDate="2020-04-09T06:09:16.527" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14982" PostId="9702" Score="0" Text="ETA: Given this is a simple cube we should be able to look at the normals with our eyes and know what to expect. Can you list an example of 1 vertex being computed along with all the face normals that are summed up into it ?" CreationDate="2020-04-09T09:04:53.853" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="14985" PostId="9706" Score="0" Text="That's just doing the area weighted average of the face normals, it's just going about it differently than I am" CreationDate="2020-04-09T15:49:03.983" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14986" PostId="9706" Score="0" Text="@Makogan the vertex normal has nothing to do with area. When you say *calculate normal*, what is it do you mean exacatly? Did you check out the qu I linked to." CreationDate="2020-04-09T15:53:58.853" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14987" PostId="9706" Score="0" Text="I read the question you linked, what Inigo Quilez is doing is, he's adding the cross product then nromalizing the vertex, the cross product gives you  the normal times the area of the triangle times 2.&#xA;&#xA;Then when you nromalize it, that;s equivalent to dividing by the norm, and the norm is equal to the sum of the areas of all trinagles times 2.&#xA;&#xA;i.e the formula in the linked question is the same as averaging the face normals based on the area of the adjacent triangles." CreationDate="2020-04-09T15:57:01.587" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14989" PostId="9707" Score="0" Text="Please provide the code where you verify that the queue family you are using does in fact support presentation. Also, show the code where you perform a present operation on a queue from that family." CreationDate="2020-04-09T17:28:34.323" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="14990" PostId="9706" Score="0" Text="@Makogan I see what you are saying, apologies my bad, you are looking for *face weighted* vertex normal rather than the vertex normal. This is not the correct normalisation to do for the topology you have then (i.e a cube, which has hard edges) and so not going to give you the answer you are looking for. Agree with wychmaster, you should weight the normals based on adjacent angle difference, and **not** area. The type of normalisation you wish to do depends on what you want it for (lighting, topology analysis etc), and the topology you have. What do you want the normal for?" CreationDate="2020-04-09T18:00:19.170" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="14991" PostId="9706" Score="0" Text="I need the normal to do afine scaling" CreationDate="2020-04-09T18:11:59.117" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14992" PostId="9705" Score="0" Text="Using the angle worked" CreationDate="2020-04-09T18:12:10.210" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="14993" PostId="9707" Score="0" Text="[This is the code](https://vulkan-tutorial.com/code/15_hello_triangle.cpp)." CreationDate="2020-04-09T19:17:56.160" UserId="13151" ContentLicense="CC BY-SA 4.0" />
  <row Id="14994" PostId="9707" Score="1" Text="Clearly, that's not your code." CreationDate="2020-04-10T07:28:05.913" UserId="13124" ContentLicense="CC BY-SA 4.0" />
  <row Id="14995" PostId="9204" Score="0" Text="You can't calculate the center of the face in the shader, instead you have to supply it to the shader as `attribute vec3 a_face_center;` The good news is that you can calculate the distance to the center in the vertex shader, which will then be interpolated per-fragment." CreationDate="2020-04-10T17:34:23.120" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="14996" PostId="2123" Score="1" Text="You are not alone." CreationDate="2020-04-11T12:01:28.203" UserId="13165" ContentLicense="CC BY-SA 4.0" />
  <row Id="14997" PostId="9712" Score="0" Text="I don't really understand what exactly you want to achieve. Okay, you want to somehow project a rectangle (somewhere), and calculate/code the projection yourself. Usually, I would say, search for &quot;projection matrix&quot;, but since you posted some formulas (without much explanation) I thought you might be looking for something more complex and I have no clue what. So please specify your exact problem. Also, specify why exactly you need an SVD? This is a quite expensive/complicated operation and I know only a small number of use cases, where it is the right tool." CreationDate="2020-04-12T18:35:50.470" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="14998" PostId="9712" Score="0" Text="@wychmaster , I updated my question. I try to find out a method how to apply projective transform. The method explained in the video is very complex. Could you explain when we need to use SVD?" CreationDate="2020-04-12T18:48:47.940" UserId="6625" ContentLicense="CC BY-SA 4.0" />
  <row Id="14999" PostId="9711" Score="0" Text="It doesn't have to satisfy $\int_{0}^{2\pi}lightpdf(\omega)\,d\omega$." CreationDate="2020-04-12T18:55:49.190" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15000" PostId="9712" Score="0" Text="I'm not 100% sure I understood what you want, but if you just want to project a mesh onto the screen you should google &quot;perspective transformation matrix&quot;. It will work for 2d meshes as well, as long as you set a value for the third dimension." CreationDate="2020-04-12T20:23:49.000" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15001" PostId="9711" Score="0" Text="@lightxbulb Why not? Using a BRDF shouldn't it be 1 over our support? $2\pi$" CreationDate="2020-04-13T02:45:04.627" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15002" PostId="9711" Score="0" Text="1) Your pdf support is not $[0,2\pi]$, it's the area of the light. 2) The solid angle formulation doesn't result in an integral in [0,2\pi]. 3) `including the half surface we cannot see from p:` - this is taken care of by the visibility function when sampling lights." CreationDate="2020-04-13T05:52:18.757" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15003" PostId="9711" Score="0" Text="@lightxbulb Hmm I see, thanks for the clarification! However let's assume for a moment we have a weirdly shaped light which covers the entire BRDF hemisphere (and has a back and front face). In that case by randomly sampling its surface our support in solid angle terms should be $4\pi$ and the pdf $r^2/cos\theta A$ (or in area terms $A$ and $1/A$ respectively) However thanks to the visibility function, samples on the back side ($2\pi$) will be 0. Is my understanding correct?" CreationDate="2020-04-13T06:36:32.523" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15004" PostId="9711" Score="0" Text="No, the solid angle support will be still $2\pi$, since angles are considered and not surface points, those are taken care of by the visibility function. Rewrite it in the area formulation to get a better idea." CreationDate="2020-04-13T07:45:18.697" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15005" PostId="9711" Score="0" Text="@lightxbulb Ok, so if the support is still $2\pi$ in the above example, each direction actually hits two points on the light surface (front and back), but the latter is discarded by the visibility function. Did I get it correctly? In the area formulation case, samples taken on the back face are also discarded by the visibility function." CreationDate="2020-04-13T08:47:43.470" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15006" PostId="9711" Score="0" Text="The visibility is implicit in the solid angle formulation of the rendering equation in the $L_i$ term and the integration domain. In the area formulation it is explicit and the integration domain is larger. As for the pdf integrating to one - that cannot be written in the solid angle formulation in the general case without decomposing it into a sum over several integrals, simply because the &quot;back&quot; surface kf the sphere won't be visible from some points." CreationDate="2020-04-13T09:58:58.940" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15007" PostId="9711" Score="0" Text="@lightxbulb Thanks for the explanation!" CreationDate="2020-04-13T13:43:26.953" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15009" PostId="9712" Score="0" Text="I watched the video you linked. The guy is like a typical academic tutor. Certainly knows his stuff and how to use it, but he isn't really good at teaching it (in my opinion), since he does not explain well, why he does what. After some research, I think I got the basics. I have written an answer that might help you." CreationDate="2020-04-13T15:05:10.543" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15014" PostId="9720" Score="0" Text="From what I understood he draw red lines over a black background, meaning that the anti-aliasing was done automatically, which means that wrong color space is not the source of this effect." CreationDate="2020-04-14T00:58:19.613" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15015" PostId="9720" Score="0" Text="an interesting thought, but disabling anti-aliasing shows that this is no the source of the pattern." CreationDate="2020-04-14T01:32:29.307" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15016" PostId="9713" Score="2" Text="thanks for your answer, I found it helpful in understanding the cause of the pattern. However, since lfgtm did help me identify it, I will leave his answer as accepted." CreationDate="2020-04-14T01:43:24.307" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15017" PostId="9709" Score="2" Text="One issue here is your lines are constant width, so they don't make sense in perspective as distant lines appear thicker than closer ones. If you where to draw the grid using a texture you can use anisotropic filtering which eliminates a lot of the aliasing pattern. A texture would also be perspective correct and lines in the distance would be thinner as you would expect." CreationDate="2020-04-14T06:17:03.627" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15018" PostId="9719" Score="0" Text="If linear is not supported then you could read 4 texels and then do bilinear interpolation of the values yourself in the shader ?" CreationDate="2020-04-14T06:23:41.940" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15019" PostId="7825" Score="0" Text="Just to add, I've found I get much better results if the offset/epsilon is done along the normal vector of the surface and not along the ray vector. For rays that are very close to being parallel to a surface, when you go back along the ray by some epsilon, you are still very close to that surface." CreationDate="2020-04-14T06:25:44.327" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15020" PostId="9707" Score="0" Text="@j00hi TBH I can't see that he has asserted that was 'his' code but just that he was using that tutorial." CreationDate="2020-04-14T06:40:10.323" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15022" PostId="9709" Score="0" Text="@PaulHK, Yes in my experience, anisotropic where applicable has produced the best results when reducing this." CreationDate="2020-04-14T08:46:49.977" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="15023" PostId="9709" Score="1" Text="@Marvin, I have updated the answer with an explanation of why this is occuring in your image. Hope this helps." CreationDate="2020-04-14T08:48:06.457" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="15024" PostId="9709" Score="0" Text="Maybe off topic, but I wonder if there is a analytic solution for this exact case, like measuring how much percentage of an imaginary grid would occupy any given trapezoid area instead of doing it in a approximate way that involves dozens of samples" CreationDate="2020-04-14T09:21:44.340" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15025" PostId="9709" Score="0" Text="@PaulHK I redrew the lines as surfaces and added alpha blending to the shader with a gradient transparency across the width of the line. This improved the definition of lines at a distance but did not have an appreciable effect on the interference. I don't know how to implement anisotropic filtering, so I'll need to look into that." CreationDate="2020-04-14T11:27:28.117" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15026" PostId="9710" Score="0" Text="Thanks for expanding your answer. I have updated my question with a failed attempt to improve things by drawing the lines as surfaces with a gradient transparency across the line width. I think this failed because alpha blending occurs in the fragment shader, so the _watershed moment_ of having to pick a pixel has already happened. Are there ways to influence how the rasterizer creates fragments? i.e. can it &quot;split&quot; a colour across two fragments using the alpha channel?" CreationDate="2020-04-14T12:24:32.063" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15027" PostId="9707" Score="1" Text="@SimonF Maybe my comment was a bit misleading --- it wasn't meant to be directed towards code ownership or copyright issues. What I actually wanted to point out: The tutorial code uses the same queue for drawing and presentation. We can not see the code changes w.r.t. &quot;different queues for drawing and presenting the image&quot;. Therefore, we can not help if the code changes are not provided." CreationDate="2020-04-14T14:15:15.510" UserId="13124" ContentLicense="CC BY-SA 4.0" />
  <row Id="15028" PostId="9719" Score="0" Text="have you tried enabling the extensions `OES_texture_half_float_linear` and `OES_texture_float_linear`?" CreationDate="2020-04-14T14:18:27.010" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15029" PostId="9709" Score="0" Text="If you're OK rendering textures then it's only a couple of state changes to enable AF" CreationDate="2020-04-14T14:30:03.783" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15030" PostId="9709" Score="0" Text="@PaulHK I've not done texture rendering yet, so I'll need to work on that before I can try it." CreationDate="2020-04-14T14:37:06.440" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15031" PostId="9710" Score="1" Text="@Marvin you can certainly perform your own blending in the fragment shader (i.e perform your own form of AA), and perhaps offset/nudge in a geometry shader... but afaik the vertex shader will rasterize the vertex in the pixel prior to the fragment shader being executed (like you say). So without changing the geometry itself, no way to decide (if using vertex/fragment shaders and no geometry shaders). tbh it's just kicking the can down the road. The problem will never go away, simply manifest under different circumstances." CreationDate="2020-04-14T14:48:32.820" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="15032" PostId="9709" Score="1" Text="@Marvin Yes thats the problem with AF, it's only performed on textures. Changing the resolution (not always feisable obviously) will change it's manifestation. You could also try MSAA which yeilds good results at removing moire, but then cannot do stuff like deffered rendering etc... essentially there are pro's and cons to each method which aims to reduce it. Like I said, it will never go away, and even if you fix from one angle, chances are other camera angles will produce it. Certinaly it sounds like you have already spent more time attmpeting to remove it than I ever have ;) Fair play." CreationDate="2020-04-14T14:52:00.827" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="15033" PostId="9709" Score="0" Text="Just re-read your original post and I didn't realise you are doing this in a shader. In that case are you generating rays to intersect a plane ?" CreationDate="2020-04-14T14:53:48.790" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15034" PostId="9709" Score="1" Text="@Marvin the other thing to mention... I'm not too sure how you go about detecting it (although there probably are ways). If you could detect it you could perhaps adapt your plan of attack to removing it... but since it depends on many things like resolution, perspective projection etc... it would be difficult to know what to do to fix it for any particular frame." CreationDate="2020-04-14T14:54:18.570" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="15035" PostId="9720" Score="1" Text="@zoran404 just because it is done automatically does not mean it is done correctly." CreationDate="2020-04-14T17:30:02.173" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15036" PostId="9719" Score="0" Text="@Marvin I have, nothing changed, even after I switched to `DEPTH_COMPONENT32F` it still only works with `NEAREST`. But from your tone it sounds like it should have worked. All examples I managed to find used `NEAREST`, but maybe you know some examples that don't?" CreationDate="2020-04-14T17:33:44.220" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15037" PostId="9719" Score="0" Text="@PaulHK Are you suggesting that 4 reads per fragment would be faster than drawing the depth texture onto another l texture that has a linear filter?" CreationDate="2020-04-14T17:36:51.953" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15038" PostId="9720" Score="1" Text="@Marvin It's a little more than a thought, I've spent weeks staring at poorly filtered grids as part of my day job. And written better filters to improve them. I will add an image so you can see for yourself." CreationDate="2020-04-14T17:59:53.713" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15039" PostId="9709" Score="0" Text="@PaulHK I generated lines on a plane that form a grid; they are not true rays." CreationDate="2020-04-14T18:22:11.413" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15040" PostId="9720" Score="0" Text="If I understand this correctly, you are saying that although the interference is created before any anti-aliasing is applied, a better calibrated anti-alias would filter out the interference quite effectively? and that to improve the anti-aliasing, I need to fix the colour space it is working in?" CreationDate="2020-04-14T18:26:24.057" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15041" PostId="9719" Score="0" Text="@zoran404 not really, I just came across [this question](https://stackoverflow.com/questions/46262432/linear-filtering-of-floating-point-textures-in-webgl2) and thought it was worth a quick check before I looked any further (I've not really worked with textures yet, so this is a learning exercise for me too)." CreationDate="2020-04-14T18:29:36.680" UserId="13167" ContentLicense="CC BY-SA 4.0" />
  <row Id="15042" PostId="9720" Score="1" Text="@Marvin that's a reasonable summary, yes. Just like digital cameras usually have a physical lowpass filter (ie. a piece of glass to blur the image) to avoid moiré at the sensor. Except for the color space issue, it appears to be doing a reasonable job for hardware rendering. I unfortunately don't know enough about OpenGL to be of much help on how to fix it. Filtering your texture (the grid pattern) is another possible solution. With offline rendering, we usually do both." CreationDate="2020-04-14T18:36:33.447" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15045" PostId="9721" Score="2" Text="1) The versions without the sine are with respect to the solid angle measure, the sine pops out when you go to spherical coordinates from the Jacobian. Either way, the sine gets canceled with the sine from the rendering equation always. 2) You're missing the sine since you plugged in the pdfs with respect to the solid angle measure into an integral over spherical coordinates. 3) The sine comes from the parametrisation. It gets canceled in the estimator by the pdf." CreationDate="2020-04-14T19:32:42.257" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15046" PostId="9542" Score="0" Text="Could you include the names and authors of a few papers you feel are most relevant to the question, as part of your answer? Or even the whole list if you have it in a useful format. As it is, this falls under &quot;link only answer&quot; with the issue that it will become useless should you one day delete the folder, or google shuts down their service, etc. I won't flag it as such because it looks like a very useful collection though." CreationDate="2020-04-14T19:50:46.593" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15047" PostId="9721" Score="0" Text="Thanks for your answer @lightxbulb. If I understand correctly : whatever change of variable we do : from directions (solid angle) to spherical coordinates and from the hemisphere (spherical coordinates) to the unit square (to uniformly pick random numbers), because what we do is selecting a new direction (for reflection), the pdf stay in the &quot;directions/solid angle&quot; coordinate system ?" CreationDate="2020-04-14T20:01:56.793" UserId="13179" ContentLicense="CC BY-SA 4.0" />
  <row Id="15049" PostId="9721" Score="1" Text="You always go to spherical coordinates at some point if you use the solid angle formulation. The issue you had comes from the fact that you used a pdf with respect to the solid angle measure as a pdf with respect to spherical coordinates. Note: $d\omega = \sin\theta \,d\theta d\phi$. The sine of the pdf always gets canceled with the sine in the rendering equation in the estimator." CreationDate="2020-04-14T20:18:54.223" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15051" PostId="9721" Score="0" Text="What is this unit square estimator, why is there a $\pi$, and why is $\cos\theta$ missing from the numerator?" CreationDate="2020-04-15T18:20:04.037" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15052" PostId="9721" Score="0" Text="$\pi$ appeared and $\cos{\theta}\sin{\theta}$ disappeared because of the Jacobian of the mapping from the square to the spherical coordinates. See Lafortune : Using the Modified Phong Reflectance Model for Physically Based Rendering, page 9)." CreationDate="2020-04-15T18:31:22.160" UserId="13179" ContentLicense="CC BY-SA 4.0" />
  <row Id="15054" PostId="9721" Score="0" Text="I looked at the reference you cited. The integral there is for the hemispherical reflectivity and no estimators are presented. What is shown on page 9 is simply different parametrizations. The one you seem to be confused about $(\xi_1, \xi_2)$ involves a cosine weighted mapping from the unit square coordinates to spherical coordinates (or rather the inverse). Let's ignore for a second that the integral is different. The analogue for the rendering equation estimator would be exactly the spherical one, because you used a cosine weight there." CreationDate="2020-04-15T20:28:05.273" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15055" PostId="9730" Score="0" Text="A texture where structures have a specific directionality (e.g. Fig. 7 stripes). If you compute the power spectrum of such a texture, you will see that the energy is concentrated along specific directions." CreationDate="2020-04-16T05:24:22.907" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15056" PostId="9730" Score="0" Text="@lightxbulb what is a texture spectrum?" CreationDate="2020-04-16T05:31:40.013" UserId="10079" ContentLicense="CC BY-SA 4.0" />
  <row Id="15057" PostId="9730" Score="1" Text="These textures where used before anisotropic filtering appeared as a hardware feature, it's basically a hack to be make bilinear filtering more anisotropic." CreationDate="2020-04-16T05:38:00.667" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15058" PostId="9724" Score="0" Text="AI_MATKEY_SHADING_MODEL is going to return the shading model type (e.g. flat shading/smooth shading). I don't think there is a &quot;Illum&quot; defined in AssImps material definitions, but there is AI_MATKEY_COLOR_EMISSIVE. You should check if that key is filled, maybe the MTL Illum data is loaded into the emmisive colour, at a guess.." CreationDate="2020-04-16T05:41:54.950" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15060" PostId="9730" Score="1" Text="https://en.m.wikipedia.org/wiki/Spectral_density&#xA;An anisotropic texture has structures that are &quot;elongated&quot; along some direction(s)." CreationDate="2020-04-16T06:56:15.047" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15062" PostId="9725" Score="0" Text="Would it be possible to include a link to the particular pages on the LearnOpenGL website that you are asking about. It'd probably make it easier for someone to provide help." CreationDate="2020-04-16T10:22:38.703" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15063" PostId="9731" Score="0" Text="Ah yes I am an idiot, the vertical band can be fixed by dividing by `v_resolution` instead of `v_resolution-1` To prevent the last set of vertices from being duplicated. &#xA;&#xA;I am still thinking about the polar ones" CreationDate="2020-04-16T15:47:04.687" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15064" PostId="9732" Score="0" Text="`The problem with this is that unless the albedo color is very dark, the final color quickly increases` - how is it a problem, and how does it &quot;quickly increase&quot;? `Other implementations instead...` - link those." CreationDate="2020-04-16T19:46:46.373" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15065" PostId="9731" Score="0" Text="Well, you need to treat them separately in their own loop. Additionally, try to avoid duplicating the polar points. Even without the degenerated triangles, the normals are incorrect, because each duplicate polar point calculates it's normal only from 2 adjacent triangles instead of all (`v_resolution`) triangles that are connected to the pole." CreationDate="2020-04-16T20:57:30.160" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15066" PostId="9731" Score="0" Text="But separate loops are fugly! But yes I think I'll hve to go with that" CreationDate="2020-04-16T21:09:30.830" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15068" PostId="9732" Score="0" Text="@lightxbulb I've updated the question with more details. Regarding the quickly increasing color, let's assume for simplicity you have a fully white material (1, 1, 1) and we take only one indirect sample ($N=1$) which happens to be fully red (1, 0, 0),  we end up with: $hitcolor = \frac{2\pi*indirectDiffuse}{N}*\frac{albedo}{\pi} = \frac{2 * (1, 0, 0)}{1}*(1, 1, 1) = (2, 0, 0)$ which is double of the indirect diffuse sample we took. It looked strange to me." CreationDate="2020-04-17T04:37:23.580" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15069" PostId="9732" Score="0" Text="1) it's Shirley not Sherley. 2) The $2pi$ is missing in RT in a weekend, because it's using cosine weighted sampling and the constant $\pi$ factor is assumed to cancel with an implicit factor in the albedo. 3) The $2\pi$ factor will be absent in an estimator sampling point lights because those can be sampled only through next event estimation, in which case the pdf is not the one for bsdf sampling. 4) Point lights are represented though Dirac deltas which gets rid of the integrand and $d\omega_i$." CreationDate="2020-04-17T08:02:30.490" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15070" PostId="9732" Score="0" Text="@lightxbulb `The 2pi is missing in RT in a weekend, because it's using cosine weighted sampling and the constant π factor is assumed to cancel with an implicit factor in the albedo.` - If the π term cancels out it should leave a $2$ factor, which isn't present? - `Point lights are represented though Dirac deltas which gets rid of the integrand and dωi` - Thanks for the explaination, this confirms what I was suspecting" CreationDate="2020-04-17T08:07:22.767" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15071" PostId="9732" Score="0" Text="↑corrected RT weekend author's name" CreationDate="2020-04-17T08:14:24.867" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15072" PostId="9732" Score="0" Text="@lightxbulb Looking at this question: https://computergraphics.stackexchange.com/questions/4664/does-cosine-weighted-hemisphere-sampling-still-require-ndotl-when-calculating-co I understood there's no 2 factor in the case of cosine weighted sampling. However the hitcolor being doubled in the case of uniform sampling in my comment above still leaves me perplexed. Is it correct?" CreationDate="2020-04-17T08:30:02.967" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15073" PostId="9732" Score="0" Text="There's no reason that the $2$ should bother you. It is a normalization factor to account for the fact that your domain has size $2\pi$ (the area of the upper unit hemisphere). You always have to divide by the pdf to get an unbiased estimator: $E[\frac{f}{pdf}] = \int f$." CreationDate="2020-04-17T08:36:06.340" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15074" PostId="9732" Score="0" Text="@lightxbulb I see, thanks for the clarification! So if I try to replace the cosine weighted sampling in RT weekend with an uniform one, and add a $2*NdotL$ (assuming $\pi$ cancels out with the albedo norm factor) in Shirley's implementetion (p.27) from `return attenuation*color(scattered, world, depth+1);` to `return 2*NdotL*attenuation*color(scattered, world, depth+1);` it should converge to the same exact image. Am I correct?" CreationDate="2020-04-17T08:58:38.353" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15075" PostId="9732" Score="0" Text="Yes, it should converge to the same image provided that you change the sampling to take uniform directions and not cosine weighted. In the process of converging it will have a higher variance however because uniform is worse than cosine sampling. Note that originally in RT in a weekend there was a mistake where the samples were taken inside a ball rather than on its surface resulting in a $\cos^3$ pdf, but I believe they have corrected that since." CreationDate="2020-04-17T09:01:29.800" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15076" PostId="9732" Score="0" Text="In the linked pdf it seems the samples are still taken inside a ball indeed (p.22-23)" CreationDate="2020-04-17T09:10:06.273" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15077" PostId="9734" Score="1" Text="The vertex shader usually sets ups any number of &quot;varying&quot; variables which are then passed to the fragment shader, it's these varying variables which get interpolated across the primitive." CreationDate="2020-04-17T09:52:15.843" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15078" PostId="9732" Score="0" Text="See the latest code on github." CreationDate="2020-04-17T09:57:42.450" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15079" PostId="9733" Score="0" Text="You like square roots?" CreationDate="2020-04-17T10:14:46.950" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15080" PostId="9734" Score="0" Text="See the function `triangle` for reference: https://github.com/ssloy/tinyrenderer/blob/master/our_gl.cpp" CreationDate="2020-04-17T10:17:07.283" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15081" PostId="9734" Score="0" Text="@PaulHK  so the interpolation uses variables from fragment shader? Then that means it does not happen inside rasterizer but rather in fragment shader?" CreationDate="2020-04-17T10:45:39.180" UserId="13192" ContentLicense="CC BY-SA 4.0" />
  <row Id="15082" PostId="9735" Score="0" Text="This explanation would have been so convincing and I had really hoped it was that simple, but if we take the square of the distance, we don't get this beautiful $x_{k+1}^2 + y_k^2 - r^2$ but instead a weird $x_{k+1}^2+y_k^2 - 2r\sqrt{x_{k+1}^2+y_k^2}+r^2$, and so I still can't see the link between this weird thing and the actual $f(N)$ that we use in the algorithm." CreationDate="2020-04-17T17:08:36.513" UserId="10928" ContentLicense="CC BY-SA 4.0" />
  <row Id="15083" PostId="9734" Score="0" Text="*Some* part of the interpolation is done outside of the fragment shader, be that interpolation of the actual vertex colours (or texture UVs etc). or it might just be barycentric coordinates which can be used to generate the interpolated values." CreationDate="2020-04-17T17:28:57.760" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15084" PostId="9735" Score="0" Text="You are right. I have adjusted my answer for now and will further enhance it tomorrow. Running out of time today. However, even though the algorithm is not always taking the closer pixel, it only picks the wrong one if both distances are really close to each other. So you probably can't tell by just looking at the circle, that the wrong one was choosen." CreationDate="2020-04-17T19:42:57.983" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15086" PostId="9740" Score="0" Text="Yes and no. Not the way you think." CreationDate="2020-04-18T07:19:19.357" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15087" PostId="9742" Score="0" Text="`The function sin(x) is pretty much uniformly thick` it's not uniform at all, you would notice this better if you used a higher frequency" CreationDate="2020-04-18T14:18:15.847" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15088" PostId="9735" Score="0" Text="So, I updated my answer, after I did some lengthy, insightful calculations." CreationDate="2020-04-18T14:21:54.780" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15089" PostId="9742" Score="0" Text="If I used a higher frequency, I would amplify the derivative, which is precisely the point of the question." CreationDate="2020-04-18T14:39:49.103" UserId="13204" ContentLicense="CC BY-SA 4.0" />
  <row Id="15090" PostId="9742" Score="0" Text="I was talking about `sin(x)` in your screenshot, it's not uniform. The thickness around 0 is around 25% thinner." CreationDate="2020-04-18T15:07:56.793" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15091" PostId="9735" Score="0" Text="Thank you so much for your detailed answer, now it's clear to me" CreationDate="2020-04-18T15:41:33.417" UserId="10928" ContentLicense="CC BY-SA 4.0" />
  <row Id="15095" PostId="9745" Score="1" Text="To name a few: computing triangle normals, creating a basis for normal mapping or scattering rays in path tracing, computing the lookAt matrix (usually for a camera)." CreationDate="2020-04-20T07:09:36.477" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15096" PostId="9745" Score="0" Text="Do you only want to know about higher dimensions? Because I know a few uses in 3D, starting with one or two vectors, but don't remember ever needing this for higher dimensions." CreationDate="2020-04-20T14:57:45.317" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15097" PostId="9745" Score="0" Text="@Olivier  No, 3D is fine.  Please name them." CreationDate="2020-04-21T01:17:36.547" UserId="13215" ContentLicense="CC BY-SA 4.0" />
  <row Id="15098" PostId="9745" Score="0" Text="@lightxbulb  Much appreciated!" CreationDate="2020-04-21T01:17:53.147" UserId="13215" ContentLicense="CC BY-SA 4.0" />
  <row Id="15099" PostId="9732" Score="0" Text="@lightxbulb I'm sorry to bring back the discussion, but while reading RT weekend I had another doubt about the above question. In the case of reflected rays off a rough surface, Shirley uses sampling a resized and offset sphere (p.28). Is the pdf still $\frac{cos\theta}{\pi}$ in that case?" CreationDate="2020-04-21T09:05:03.500" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15100" PostId="9732" Score="0" Text="No, it isn't. But I haven't derived the pdf for it, neither has he I believe. It's probably feasible to derive, but it would take some work, see for reference: https://github.com/vchizhov/Derivations/blob/master/Probability%20density%20functions%20of%20the%20projected%20offset%20disk%2C%20circle%2C%20ball%2C%20and%20sphere.pdf" CreationDate="2020-04-21T10:06:34.093" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15101" PostId="9744" Score="0" Text="i'm not sure what you mean by &quot;reflected light&quot;. however, ambient light is just a simple approximation for global illumination. it should be a simple value added without anny effect of reflection or viewer position taken into account. therefore, metalness does not affect the ambient light (unless your metalness affects your albedo, which i can't see int he code)" CreationDate="2020-04-21T12:23:34.063" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="15102" PostId="9732" Score="0" Text="@lightxbulb I see. Thanks as always!" CreationDate="2020-04-21T12:54:58.663" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15103" PostId="9732" Score="0" Text="Wait, by rough surface you mean not the fuzzy mirror scattering but the diffuse? If that is the case, then I misunderstood you. For a rough surface (offset ball along the normal) it is indeed $\frac{\cos\theta}{\pi}$ (provided you take points on the ball, and not in the ball - you can find more details in the pdf I linked). If you were talking about the fuzzy mirror reflection then my previous comment about the pdf not being clear holds." CreationDate="2020-04-21T18:50:56.377" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15104" PostId="9745" Score="0" Text="It has similar uses to look at in robotics an mechanism design. It has uses in optimisation." CreationDate="2020-04-21T22:46:13.107" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15105" PostId="9748" Score="0" Text="Awesome.  I'd upvote if I could.  Thank you." CreationDate="2020-04-22T01:41:16.987" UserId="13215" ContentLicense="CC BY-SA 4.0" />
  <row Id="15106" PostId="1634" Score="0" Text="A few years passed since this (excellent) answer, and one can find that this shift of hitches from the first drawcall to pipeline state creation wasn't a silver bullet either, because the game may not know all the state in advance (especially a highly varied / moddable game) to create it early enough..." CreationDate="2020-04-22T03:48:10.820" UserId="13234" ContentLicense="CC BY-SA 4.0" />
  <row Id="15107" PostId="9745" Score="0" Text="@joojaa  Cool I will look into those." CreationDate="2020-04-22T04:32:52.220" UserId="13215" ContentLicense="CC BY-SA 4.0" />
  <row Id="15108" PostId="9749" Score="0" Text="2% reflected energy can still be significant when using HDR." CreationDate="2020-04-22T05:40:08.703" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15109" PostId="9749" Score="1" Text="Just to open pauls comment. Think of the sky, its really, really bright. Even a little energy of that is still bright. Remember we dont clamp light to 0-1 range." CreationDate="2020-04-22T07:07:03.680" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15110" PostId="9749" Score="0" Text="What do you mean by clamping light to the range? I have an indoor scene similar to this for reference. Does it mean I have to make my lightbulb really really bright? http://raytracey.blogspot.com/2015/10/gpu-path-tracing-tutorial-1-drawing.html" CreationDate="2020-04-22T07:29:59.310" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15111" PostId="9732" Score="0" Text="@lightxbulb Yes, actually I meant the former (fuzzy mirror scattering, not diffuse). If I understood correctly, In the case of a non fuzzy perfect mirror, we have to yet again use a Dirac delta (in which case as with point lights, radiance along the ray equals total irradiance in the BRDF)" CreationDate="2020-04-22T07:32:01.803" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15112" PostId="9732" Score="0" Text="For mirrors you use a Dirac delta to get only the light coming from a specific direction." CreationDate="2020-04-22T07:39:31.723" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15113" PostId="9744" Score="0" Text="@Tare I should edit the question so it's clear that I'm talking about directional light. The ambient light is complementary" CreationDate="2020-04-22T15:56:32.097" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15114" PostId="9750" Score="0" Text="Thanks for this response! The RPN idea especially sounds like a good fit. I was just recently trying to think of good ways to do grouping in my current system that only does left-to-right evaluation, and that might be just the thing.  Right now I am rendering 1 quad per SDF object, rather than tiling, but I imagine there's some tipping point for &quot;too much overlapping stuff per pixel&quot; where the tiling will win." CreationDate="2020-04-22T20:40:55.943" UserId="13154" ContentLicense="CC BY-SA 4.0" />
  <row Id="15115" PostId="9751" Score="0" Text="Thanks for your answer but I still didn't get a few points. You are speaking about &quot;diffuse component&quot; but I used microfacet  BRDF which have only one component &quot;specular&quot;. It selects a microfacet normal and does a perfect reflection. Microfacet normals are distributed based on roughness. The calculation is based on 3 parts (F, G, D). F is for Fresnel effect (higher the angle between microfacet normal the more reflection), G stands for geometric shadowing (higher the roughness or angle mean a higher chance of light being blocked by another microfacet) and D is just distribution of normals." CreationDate="2020-04-22T21:54:26.760" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15116" PostId="9751" Score="0" Text="So there is no diffuse component because it is replaced by the distribution of microfacet normals. Do you mean that G should negate the effect of Fresnel?" CreationDate="2020-04-22T21:57:12.153" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15117" PostId="9751" Score="0" Text="Ohh, wait a minute. So Fresnel says how much light gets reflected directly and rest of it ( 1 - Fresnel) go through the material and because of my materials aren't transparent then it exits (for simpler model) at the same point in a random direction (diffuse)?" CreationDate="2020-04-22T22:46:03.923" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15118" PostId="9751" Score="1" Text="I don't know the answer exactly because I haven't worked with microfacet models. But your last comment is not correct. Fresnel will tell you how much light is reflected specularly, and the rest of it (1-fresnel) is absorbed by the surface (if it is an opaque material)." CreationDate="2020-04-22T23:56:01.283" UserId="13237" ContentLicense="CC BY-SA 4.0" />
  <row Id="15119" PostId="9751" Score="0" Text="I found what is the problem of my. Now I am writing the answer. Thanks a lot." CreationDate="2020-04-23T00:37:28.017" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15120" PostId="9750" Score="0" Text="@jwd if your operations are such that you can manage one quad per primitive (eg. don't need too many temporary whole screen buffers), it may not be such a bad idea either. You could certainly write your own answer with it. It seems optimal in terms of how many times each primitive is evaluated. Perhaps not at keeping all the GPU cores busy in some cases. It makes it easy to use a space partition to trim the function in a huge world. And it seems a lot simpler." CreationDate="2020-04-23T00:52:18.227" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15121" PostId="9542" Score="0" Text="@Olivier I just added a bunch of paper titles with the year of pubblications to be found easily." CreationDate="2020-04-23T20:00:53.740" UserId="6114" ContentLicense="CC BY-SA 4.0" />
  <row Id="15123" PostId="9754" Score="0" Text="Can you explain this part: &quot;Being part of the BRDF you have to accumulate the Fresnel term while evaluating the BRDF and only then eventually do 1-Facc for the diffuse part.&quot;. I am thinking about Fresnel that there is some chance that it will reflect and some that it will refract. I have to get rid of Fresnel in evaluation specular BRDF with a microfacet model. So the equation will be: f = D * G / (4 * (N * wi) * (N * wo) )" CreationDate="2020-04-23T21:37:35.037" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15124" PostId="9754" Score="1" Text="@VítGardoň your eval fnc for example will get back with the accumulated Fresnel computed during the BRDF (the one in FDG/..  divided by the number of samples.. that's the 'rough Fresnel' or 'glossy Fresnel'... for example in vray they did it wrong for years and onyl recently made it correct..  take a look at the last image in this link..  https://www.chaosgroup.com/blog/understanding-glossy-fresnel" CreationDate="2020-04-23T21:44:49.600" UserId="6114" ContentLicense="CC BY-SA 4.0" />
  <row Id="15125" PostId="9754" Score="0" Text="Ohh right, I understand now. Because the surface is made out of micro surfaces I have to first sample the normal and only after that, I can calculate Fresnel because until that I didn't know what normal to use. Is this what you meant? Let me know so that I can correct it in the explanation. At first, I didn't understand what you were talking about, but when I started coding I realized that I don't have a local normal to use in a Fresnel equation :D" CreationDate="2020-04-23T21:46:21.297" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15126" PostId="9754" Score="1" Text="@VítGardoň exactly that ! You use the normal from the micro facets which is every time different and not always the same as your macro surface normal." CreationDate="2020-04-23T21:48:56.383" UserId="6114" ContentLicense="CC BY-SA 4.0" />
  <row Id="15127" PostId="9754" Score="1" Text="@VítGardoň And for pure pathtracing it's there that you go stochastic and pick your lobe... so you don't need to accumulate." CreationDate="2020-04-23T21:49:53.977" UserId="6114" ContentLicense="CC BY-SA 4.0" />
  <row Id="15128" PostId="9754" Score="0" Text="Thank you very much. So just to be clear. I don't use the F in the equation anymore?" CreationDate="2020-04-23T21:52:38.210" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15129" PostId="9754" Score="1" Text="@VítGardoň added some code to the answer so you can inspect it a bit. Sry to late here to type it .." CreationDate="2020-04-23T22:10:30.687" UserId="6114" ContentLicense="CC BY-SA 4.0" />
  <row Id="15130" PostId="9754" Score="0" Text="I am not able to understand your code. I cannot distinguish where you calculate the diffuse component. I see the formula for specular BRDF but handling diffuse with 1 - F is nowhere. I added also my code new code based on your advice." CreationDate="2020-04-23T23:07:36.280" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15131" PostId="9695" Score="0" Text="Would this also work on non-rtx cards that nevertheless have compute capability? And AMD cards?" CreationDate="2020-04-24T09:55:51.670" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="15132" PostId="9695" Score="0" Text="I think that's one of the points of `VK_KHR_ray_tracing`: that it will be supported by a variety of cards in the future (this is why it is a `KHR`-type extension). The extension does not demand RTX cores to be available in a GPU. And in fact, there are GPUs without RTX cores that support it as of now --- however, all of them are from NVIDIA (Pascal, Volta, and Turing) if the data on [gpuinfo.org](https://vulkan.gpuinfo.org/listdevicescoverage.php?extension=VK_KHR_ray_tracing&amp;platform=windows) is up to date." CreationDate="2020-04-24T10:34:18.587" UserId="13124" ContentLicense="CC BY-SA 4.0" />
  <row Id="15133" PostId="9754" Score="1" Text="@VítGardoň the code is just about Fresnel. I don't split there because I'm accumulating Fresnel. For splitting.. take a look at Mitsuba code for example (around line 318.. https://github.com/mitsuba-renderer/mitsuba/blob/master/src/bsdfs/plastic.cpp" CreationDate="2020-04-24T17:01:33.053" UserId="6114" ContentLicense="CC BY-SA 4.0" />
  <row Id="15134" PostId="9754" Score="0" Text="Alright then. Once again thank you very more." CreationDate="2020-04-24T21:00:36.170" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15135" PostId="9364" Score="0" Text="A beautiful visual description [here](https://youtu.be/d4EgbgTm0Bg)." CreationDate="2020-04-27T15:01:33.020" UserId="5574" ContentLicense="CC BY-SA 4.0" />
  <row Id="15136" PostId="9755" Score="0" Text="I have apparently misunderstood the point of PBR and have since dumped my shader in favor of the one I found at https://learnopengl.com/PBR/Lighting But I just don't get why do you claim that &quot;PBR metallic materials do not use ambient term&quot;?" CreationDate="2020-04-28T11:04:17.840" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15137" PostId="9736" Score="0" Text="So i understand color is being passed down the graphics pipeline but how would the rasterizer identify the color variable as both vertex and color attributes are passed, especially if they are both defined as varying attributes?" CreationDate="2020-04-28T12:33:11.067" UserId="13192" ContentLicense="CC BY-SA 4.0" />
  <row Id="15138" PostId="9736" Score="0" Text="This is what the linker does for you. If you have a vertex shader with the output variables `position`, `color`, and `tex_coord`, then your fragment shader needs the exact same variables as inputs (assuming geometry and tesselation shaders are not used). Otherwise, the linker would complain when you try to link the program. Since the linker takes care of the connection between the variables, the rasterizer does not need to know about the purpose or name of a variable he is interpolating. All he needs to know is where it comes from, which type it is and where the result must be written to." CreationDate="2020-04-28T13:22:41.353" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15139" PostId="9736" Score="0" Text="Additionally, `gl_Position` is a required output of a vertex shader. From that, the rasterizer knows the position of each vertex. That is all he needs to perform the rasterization." CreationDate="2020-04-28T13:30:32.053" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15140" PostId="9736" Score="0" Text="Imagine the vertex shader passes attributes `a0` and `a1` as `varying`. The fragment shader may set the color based on `a0`, `a1`, some combination `f(a0, a1)`, or just set everything to red. The rasterizer, which executes _before_ the fragment shader has no idea of the eventual color. So how can the rasterizer decide any color? One way could be that it interpolates every attribute that passes through it. But then all that work will go to waste and need to be redone if the fragment shader sets everything to red. Any thoughts on that?" CreationDate="2020-04-28T13:46:00.783" UserId="10475" ContentLicense="CC BY-SA 4.0" />
  <row Id="15141" PostId="9736" Score="1" Text="The final color you see is determined by the fragment shader. It is whatever you write to `fragColor`(GLSL). The rasterizer just interpolates whatever you pass down the pipeline and does not know what `a0` or `a1` are supposed to be. In the fragment shader, you just get their interpolated values based on their position in the triangle. If you decide to discard those values, the interpolation was pointless, right. It's been a while since I have written my last shaders, but..." CreationDate="2020-04-28T14:14:59.227" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15142" PostId="9736" Score="1" Text="... as far as I can remember if the compiler figures out, that a variable does not contribute to the output of a shader, it is optimized away. This will lead to a program linkage error since you have an output variable in the vertex shader but no corresponding input in the fragment shader. This should automatically prevent pointless interpolations. As I said, it has been a while. So I might be wrong here." CreationDate="2020-04-28T14:18:25.193" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15143" PostId="2416" Score="0" Text="https://commons.wikimedia.org/wiki/File:Unrolled_cardioid.svg" CreationDate="2020-04-28T15:17:20.030" UserId="2289" ContentLicense="CC BY-SA 4.0" />
  <row Id="15144" PostId="9755" Score="0" Text="@zoran404 because first 'ambient' in a PBR is global illumination. Second, in a pure metallic material there isn't any diffuse part so no global illumination nor ambient term." CreationDate="2020-04-28T20:15:05.513" UserId="6114" ContentLicense="CC BY-SA 4.0" />
  <row Id="15145" PostId="9755" Score="0" Text="If ambient light is light coming from all directions, and metals reflect light, then why is it wrong to just multiply the ambient light and albedo? I don't see why this wouldn't work for both metals and non-metals." CreationDate="2020-04-28T21:04:25.407" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15146" PostId="9768" Score="0" Text="While this question is totally valid to be asked here, I would suggest to ask directly in the OpenGL forums: https://community.khronos.org/ --- Alternatively, you might open an issue on their GitHub site. You will probably get a quicker answer there. In case you do, please answer your own question with the links/information you got there. :)" CreationDate="2020-04-29T17:28:26.730" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15147" PostId="9771" Score="0" Text="It's not clear to me how this data can be &quot;static&quot; if you're unaware of how much data you're actually going to put into it. If you are modifying it, then it isn't &quot;static&quot;." CreationDate="2020-04-30T05:43:40.930" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15148" PostId="9771" Score="0" Text="@NicolBolas I think he is referring to static data that is loaded and unloaded during runtime, like the geometry of a specific section in a large computer game world, which won't change after it is loaded. However, in case I am right, I would like to know why this is a concern. It shouldn't be a regular event? So no real performance concern, or am I missing something? Apart from that, the obvious benefit of a large buffer should be that you can render more stuff with a single draw call." CreationDate="2020-04-30T06:32:04.173" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15149" PostId="9769" Score="0" Text="I'm not that familiar with HDR, but IIRC the BBC+NHK's Hybrid-Log Gamma https://www.bbc.co.uk/rd/projects/high-dynamic-range effectively has a range of [0.0,12.0].  The subrange [0,1.0] represents the usual SDR you get with normal monitors, but allows up to 12x the brightness for HDR devices" CreationDate="2020-04-30T08:44:33.627" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15150" PostId="9771" Score="0" Text="Excuse my poor choice of wording. @wychmaster is right. I render a game world where a lot of 3D mesh data can come and go because these are game entities. But the data is &quot;static&quot; (i.e. uploaded on GPU once, then read-only). By &quot;render more stuff with a single draw call&quot;, did you think about the glDraw*Indirect family of functions ? That's something I've yet to try." CreationDate="2020-04-30T13:30:24.233" UserId="13284" ContentLicense="CC BY-SA 4.0" />
  <row Id="15151" PostId="9771" Score="0" Text="@Scylardor: &quot;*But the data is &quot;static&quot; (i.e. uploaded on GPU once, then read-only).*&quot; The &quot;data&quot; may be static, but the *buffer* is not, since you're uploading multiple times. It seems to me that you're not talking about a static case; you're talking about streaming content where you're progressively adding and removing content based on the current level/camera position/etc." CreationDate="2020-04-30T14:48:07.347" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15152" PostId="9771" Score="0" Text="@Scylardor: Ultimately, there are no &quot;best practices&quot;, as any kind of buffer usage depends entirely on your uploading patterns. That is, when are you uploading data, is there any coherency to the uploads, when does data become irrelevant and thus can go away, etc. There's no one-size-fits-all solution; it has to be tailored to your specific needs, or you will be sacrificing performance." CreationDate="2020-04-30T14:52:33.283" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15153" PostId="9772" Score="1" Text="&quot;*What I mean by “state of graphics” is, how polygons, textures, lighting and any other scene elements are stored and computed for any particular frame.*&quot; That kind of thing changes within a frame. Indeed, the graphics card doesn't even know what &quot;lighting&quot; or a &quot;scene element&quot; *is* in most cases. It's just executing user-written programs that ultimately write data into blocks of memory that the GPU will eventually send to the display device. It has no idea what any of it means." CreationDate="2020-04-30T22:50:58.043" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15154" PostId="9772" Score="1" Text="GPUs do so must stuff that even if you could read that info you probably wouldn't get any meaningfull information. You're better off using a profiling tool for your game engine to view this info. Also this question seems off-topic for this stackoverflow." CreationDate="2020-04-30T22:51:57.707" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15155" PostId="9771" Score="0" Text="@Scylardor Yes. Try searching &quot;approaching zero driver overhead opengl&quot; (short AZDO). It is a collection of techniques to find the &quot;fastest paths&quot; through the OpenGL API. Here is a nice talk about it: https://www.gdcvault.com/play/1020791/Approaching-Zero-Driver-Overhead-in --- It will address the thing I mentioned (if I remember correctly)" CreationDate="2020-05-01T08:56:47.467" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15156" PostId="9773" Score="0" Text="Sounds like you are talking about point lights. If you used a deferred rendering technique you could pass all the point positions and brightness as an array of uniforms and render them on the screen in a single pass" CreationDate="2020-05-01T09:27:46.177" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15157" PostId="9772" Score="0" Text="@zoran404 After rereading the [on topic help pages](https://computergraphics.stackexchange.com/help/on-topic) I can see, why you come to the conclusion that this question might be off-topic. However, I think it is at least disputable and I would consider it okay." CreationDate="2020-05-01T09:46:45.713" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15158" PostId="9776" Score="3" Text="Does this answer your question? [Why are quads used in filmmaking and triangle in gaming?](https://computergraphics.stackexchange.com/questions/5465/why-are-quads-used-in-filmmaking-and-triangle-in-gaming)" CreationDate="2020-05-01T10:38:37.910" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15159" PostId="9776" Score="0" Text="Further information can be found [here](https://computergraphics.stackexchange.com/questions/8966/triangle-why-is-it-the-basic-building-block-for-complex-shapes/8969#8969), [here](https://gamedev.stackexchange.com/questions/66312/quads-vs-triangles), and [here](https://gamedev.stackexchange.com/questions/9511/why-do-game-engines-convert-models-to-triangles-instead-of-using-quads)" CreationDate="2020-05-01T10:42:46.657" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15160" PostId="9775" Score="1" Text="At this point it would be good to undetstand differences between humanities and sciences. Humanities deal with relative statements and sciences deal with how things work. As art is generally a humanist endeavour an artist might say that it looks good everything is hunky dory. Thats fine. However from a sientist perspective its still wrong because there are so many implied assumptions that are wrong in such a setup. You simply miss on a lot of refinements if you side step this. Everything from antialias and shading routines are slightly off. You can tweak things but would be better if not." CreationDate="2020-05-01T10:57:11.793" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15161" PostId="9775" Score="0" Text="@nicol Thanks. So do you mean that the material reflectance must first be gamma corrected (raised to the power of gamma) before doing the ray tracing ? In other words, if I am setting up my materials using any Color Picker app, are the RGB values already in non-linear space? and therefore I first need to convert it to linear space before feeding it to ray tracing?" CreationDate="2020-05-01T12:32:36.690" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15162" PostId="9779" Score="0" Text="Erm i dont think you can do a blanket statment like you do here. You triangulate when  you need to force the triangulation so you can adjust it. Yes it can bake better because it has the triangulation you needed. Obviously if you flipped the quad it would now look off. There is no difference animating triangular or nontriangular models. Just more clutter on screen." CreationDate="2020-05-02T11:47:15.783" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15163" PostId="9761" Score="0" Text="float luma(const vec3&amp; color)&#xA;{&#xA;	return dot(color, vec3(0.2126f, 0.7152f, 0.0722f));&#xA;}" CreationDate="2020-05-02T14:10:11.473" UserId="6114" ContentLicense="CC BY-SA 4.0" />
  <row Id="15164" PostId="9783" Score="0" Text="But if the model is centered around the center (0,0,0) in a unit cube, is there really a need for rescaling points back to the unit cube?  Moreover, can I use the same rotation matrix for normals as I used for points?" CreationDate="2020-05-04T01:26:52.767" UserId="13298" ContentLicense="CC BY-SA 4.0" />
  <row Id="15165" PostId="9777" Score="0" Text="For Nvidia cards, O.P can also use NSight (https://developer.nvidia.com/nsight-graphics). It provides more low-level and performance information than RenderDoc. I don't think it can be used on a released commercial application however, you probably need to have access to the application sources first." CreationDate="2020-05-04T02:12:38.267" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="15166" PostId="9785" Score="0" Text="&quot;*the positions are passing successfully but producing a GL_INVALID_OPERATION error.*&quot; How can something be &quot;successful&quot; while erroring? Also, how are &quot;uniform buffer objects&quot; involved; you don't use `glUniform*` calls to upload to UBOs." CreationDate="2020-05-04T03:01:16.090" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15168" PostId="9761" Score="1" Text="For metals, or basically mix of diffuse and specular reflection I suggest you the Ashikhmin BRDF. It properly accounts for the diffuse part , Fresnel effect. Look at  https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://www.cs.utah.edu/~michael/brdfs/jgtbrdf.pdf&amp;ved=2ahUKEwjl2uDvqZnpAhVmURUIHcylCLUQFjADegQIBBAB&amp;usg=AOvVaw2ywV--US_6Cpf4kWotzy79&amp;cshid=1588565037935" CreationDate="2020-05-04T04:06:41.827" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15169" PostId="9761" Score="1" Text="Are you asking of the probability of choosing either of the two BRDFs? If yes, then it can be any number(can be 0.5) as later you divide brdf by this probability to account for rays that you didn't trace." CreationDate="2020-05-04T04:10:06.977" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15170" PostId="9746" Score="0" Text="The integral of the microfacets distribution is NOT over the hemisphere. It is over the area of the surface. The cosine term is related to the projected areas of the microfacets. and the integral basically says that the sum of projected areas of microfacets must equal the surface area." CreationDate="2020-05-04T04:29:24.280" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15171" PostId="9746" Score="0" Text="Have a look at pbrt here somewhere in the middle of the page has the diagram:  http://www.pbr-book.org/3ed-2018/Reflection_Models/Microfacet_Models.html" CreationDate="2020-05-04T04:29:41.423" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15172" PostId="9749" Score="0" Text="The 2% is only at normal angle. That is the lowest reflection you get. As the reflection angle goes away from the normal this percentage increases rapidly" CreationDate="2020-05-04T04:42:29.860" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15173" PostId="9785" Score="0" Text="To clarify, the animated meshes appear to be animating correctly but an error is being reported. My guess at the moment is that the matrices are accessible in the shader as expected but an error is being reported. My best guess is that I am either observing a bug or undefined behavior that happened to work.&#xA;&#xA;So succeeding in the sense that the information seems to be making it to the hardware, but an error is still reported by the instruction that puts them there." CreationDate="2020-05-04T04:45:12.983" UserId="13304" ContentLicense="CC BY-SA 4.0" />
  <row Id="15174" PostId="9785" Score="0" Text="My mistake on the uniform-buffer-object tag, I just removed it." CreationDate="2020-05-04T04:56:18.903" UserId="13304" ContentLicense="CC BY-SA 4.0" />
  <row Id="15175" PostId="9761" Score="1" Text="Thank you. I already came up with solution. I decided to choose them randomly 50:50. Then I average the PDF. I just forgot about this question. I will post a answer so that the question will not be left without one. Thanks you once again." CreationDate="2020-05-04T09:00:01.460" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15176" PostId="9783" Score="0" Text="also, I don't like your approach by scaling by maximum value across all axises (x,y,z), since it degrades pointcloud objects significantly. Say you have a pencil like object, if you scale it by it's maximum value, which is surely going to be along the pencil length, and divide both it's width and height by the same value, you will get something that looks more like a small radius stick than the pencil." CreationDate="2020-05-04T11:32:30.577" UserId="13298" ContentLicense="CC BY-SA 4.0" />
  <row Id="15177" PostId="9785" Score="0" Text="Well, if the uniforms are uploaded correctly, are you sure the error is thrown by that exact function call and not some earlier or later code? It's also a bit confusing why the error message talks about `glUniform3`." CreationDate="2020-05-04T12:06:04.013" UserId="6" ContentLicense="CC BY-SA 4.0" />
  <row Id="15178" PostId="9783" Score="0" Text="Question 1: Depends on your model. But take the unit cube itself as an example. If you rotate it by any angle that isn't a multiple of 90 degrees, The corners will end up outside the unit cube. To visualize it, take two dice and put them on top of each other so that they are aligned. Now rotate the upper one by 45 degrees. See the problem? Only if all your points are initially inside the unit sphere, you can be sure that no point will end up outside the unit cube after an arbitrary rotation." CreationDate="2020-05-04T12:09:31.033" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15179" PostId="9783" Score="0" Text="Also, why scaling by some value, it's rotation and should be shape and length preserving, wouldn't shifting/translation be correct way of bringing object back to the unit cube?" CreationDate="2020-05-04T12:11:57.177" UserId="13298" ContentLicense="CC BY-SA 4.0" />
  <row Id="15180" PostId="9783" Score="0" Text="Question 2: Yes, actually you HAVE to, otherwise you get wrong normals. A quote from my answer: &quot;Normals are rotated the same way as the rest of your mesh/point cloud.&quot;" CreationDate="2020-05-04T12:13:05.083" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15181" PostId="9783" Score="0" Text="Regarding your second comment: If you use the scaling I described, it just shrinks the model. So you will just get a smaller pencil, but the ratio between thickness and length is preserved. Maybe you should read a little bit more about the basic operations that are used in computer graphics: Scaling, rotation, translation. See [this tutorial](https://learnopengl.com/Getting-started/Transformations)." CreationDate="2020-05-04T12:20:22.367" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15182" PostId="9783" Score="0" Text="Question of your 3. comment: Translation MIGHT work, but in case of the rotated unit cube (angles != 90° * N)  as a model, there is no way to translate it into the unrotated unit cube. The only way to fit it without scaling is to rotate it back." CreationDate="2020-05-04T12:25:51.250" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15183" PostId="9785" Score="0" Text="Please provide more code (shaders, uploading of the data, getting the uniform locations). As @ChristianRau mentioned, this part of the error message `glUniform3` looks suspicious. Throw an exception from your callback function if an error occurs. This way you can use your debugger to activate a breakpoint and traverse the call stack to find out which API call actually causes the error. It might be another function and not the one you expected." CreationDate="2020-05-04T12:43:24.260" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15184" PostId="9783" Score="0" Text="Is centroid (central point of the pointcloud) preserved after rotation translation? Could I by just shifting centroid to 0,0,0 bring back my object to unit cube?" CreationDate="2020-05-04T14:40:24.130" UserId="13298" ContentLicense="CC BY-SA 4.0" />
  <row Id="15185" PostId="9785" Score="0" Text="I am not certain that the error was thrown by this exact function; I'll add the exception today to verify which function is throwing the error." CreationDate="2020-05-04T15:23:17.497" UserId="13304" ContentLicense="CC BY-SA 4.0" />
  <row Id="15186" PostId="9788" Score="1" Text="It's still centered, it just does not fit in the initial box anymore." CreationDate="2020-05-04T16:42:35.753" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15187" PostId="9789" Score="1" Text="See: https://gamedev.stackexchange.com/questions/62354/method-for-interpolation-between-3-quaternions/&#xA;There is no analytic solution, you'll have to use an iterative one." CreationDate="2020-05-04T18:49:39.397" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15189" PostId="9791" Score="0" Text="The `print(x,y, color)` trace shows it is working for me. It's just the image writing code which fails. I suspect because of the negative indices." CreationDate="2020-05-05T01:28:52.587" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15190" PostId="9791" Score="0" Text="@Olivier yes, i figured that by looking at the pattern of print(x,y,color),  is having a viewport necessary as i have seen some code directly doing `for x in range(Cw)` and `for y in range(Ch)` ," CreationDate="2020-05-05T06:36:20.047" UserId="13309" ContentLicense="CC BY-SA 4.0" />
  <row Id="15192" PostId="9791" Score="0" Text="you could loop that way and redefine `canvas_to_viewport` accordingly. There's no single correct way to do this." CreationDate="2020-05-05T13:15:04.157" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15193" PostId="9791" Score="0" Text="@Olivier I changed the main render loop to avoid the negative index while generating images but still the output is blank, I have updated the original code above." CreationDate="2020-05-05T18:36:34.343" UserId="13309" ContentLicense="CC BY-SA 4.0" />
  <row Id="15194" PostId="9771" Score="0" Text="Thank you :-) I already watched that talk. Cool information about sparse bindless textures etc. TBH I'm not there yet, still trying to figure out correct buffer usage for my application first, so it's going to take some time to test things out. I'll maybe update that question or make a new one when I have more relevant info." CreationDate="2020-05-06T00:42:00.490" UserId="13284" ContentLicense="CC BY-SA 4.0" />
  <row Id="15195" PostId="9410" Score="0" Text="`BRDF in your case corresponds to material-&gt;Eval(wi, wo, normal) / dot(normal, wi)` why?" CreationDate="2020-05-07T09:37:18.160" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15196" PostId="9799" Score="0" Text="I'm voting to close as this question seems like it would be much better on https://gamedev.stackexchange.com/" CreationDate="2020-05-07T18:06:42.783" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15197" PostId="9798" Score="1" Text="Read about winding order. The left one is usually called even odd winding rule.vand the other is just any number other than 0. What you do is you calculate how many points on the scanline then add a number if curve is intersecting upwards and subtract when down (or vice versa)" CreationDate="2020-05-07T18:31:12.067" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15198" PostId="9798" Score="0" Text="@joojaa Awesome, that helps a lot! Thank you." CreationDate="2020-05-07T19:15:44.630" UserId="13318" ContentLicense="CC BY-SA 4.0" />
  <row Id="15199" PostId="9799" Score="0" Text="Yes engine can do it all, consider a webpage as a analogy. Strictly speaking a webpage needs no javascript. But you still might find it useful for some stuff. In game terms, you might want some procedural element, or unique intersection finding for yout zubaluba beam, or some shader because hey its less work than starting photoshop. And so on..." CreationDate="2020-05-07T19:21:30.583" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15200" PostId="9795" Score="0" Text="Yes, if a pixel is assumed to have a single color value, then its underconstrained. If we treat each pixel as 3 color channels, each of which has to solve the above equation with the same reconstructed alpha, it seems a bit more tractable. Even then, I fear there may be corner cases." CreationDate="2020-05-07T20:48:17.697" UserId="13260" ContentLicense="CC BY-SA 4.0" />
  <row Id="15201" PostId="9410" Score="0" Text="@lightxbulb,  that is more or less the definition of Brdf. E.g. a Diffuse brdf is a constant, non negative,  (obviously)reciprocal function. Using the functions of the OPs referenced question, that's what it would look like. People have a tendency to fold terms together to avoid repeated computation, which is confusing to beginners trying to understand path tracing from the underlying physics." CreationDate="2020-05-08T10:59:33.873" UserId="11581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15202" PostId="9410" Score="1" Text="Where did the dot pop out from?" CreationDate="2020-05-08T11:33:31.070" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15203" PostId="9410" Score="0" Text="@lightxbulb, in the definition of BRDF, there is a &quot;normalizing&quot; (normal direction) term $1 / \cos \theta_i$, which is the angle between the normal and the incomming $w_i$. When $n$ and $w_i$ have unit length, the $\cos \theta_i = \ w_i \cdot normal$, because $a \cdot b \equiv |a| \times |b| \times \cos \theta$, where $\theta$ is the angle between $a$ and $b$. Makes sence?" CreationDate="2020-05-08T12:59:50.140" UserId="11581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15204" PostId="9410" Score="0" Text="Can you link the definition of BRDF that you are citing?" CreationDate="2020-05-08T14:13:25.977" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15205" PostId="9410" Score="0" Text="@lightxbulb, hmm, depends on what you need. There is nothing wrong with the definition on [wikipedia](https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function), but if you want context too, maybe try [Ray Tracing in One Weekend](https://raytracing.github.io/) (specifically the 3rd book, I think, goes into some detail), or the popular [Physically Based Rendering:&#xA;From Theory To Implementation](http://www.pbr-book.org/), both of which are online, free, and contain source code too." CreationDate="2020-05-08T15:54:27.037" UserId="11581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15206" PostId="9410" Score="0" Text="Seems you are confusing what your function is defined as formally, and what you have in practice. You do not have the analytical form of the $dL_r$ or $L_i$ in practice, so the `eval/dot` makes no sense. Instead you define some function (which describes your material) that fulfills the 3 required properties. There is no cosine division requirement. In fact the simplest BRDF - Lambertian, is just a constant function that is in $[0,\frac{1}{\pi}]$. I would suggest just deleting your answer, as it is  mostly wrong: e.g. the BRDF thing, the BRDF reaching inifnity for mirrors." CreationDate="2020-05-08T16:14:40.220" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15208" PostId="9410" Score="0" Text="I have told you what the function is, and why the division is there (because it is folded into eval in OPs referenced question). I have also told you that a lambertian diffuse is a constant function. The fact that BRDF is infinite for perfectly specular reflections has been answered [here](https://computergraphics.stackexchange.com/questions/4767/why-the-brdf-of-specular-reflection-is-infinite-in-the-reflection-direction)." CreationDate="2020-05-08T16:41:27.797" UserId="11581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15209" PostId="9410" Score="0" Text="And I am telling you that you are wrong - for the cosine division entirely, for the infinity thing: depending on how informal you want to get. The BRDF for mirrors is not &quot;infinite&quot; - it is a Dirac delta. You're not plugging infinity into your tracer upon implementation due to that reason - instead you're getting rid of the integral due to the main property of the Dirac delta. While it may  be easier to explain it to readers without mathematical background as being &quot;infinite&quot; (which is why I assume the authors of AGI did it), that doesn't change the fact that it is a wrong explanation." CreationDate="2020-05-08T17:53:13.433" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15210" PostId="9410" Score="0" Text="In your &quot;example&quot; `material-&gt;Eval(wi, wo, normal) / dot(normal, wi)` material-&gt;Eval(wi,wo,normal) should by all means be the BRDF, and there is no cosine factor required in the numerator. In no BRDF is such a factor required - only if you want to model a material that has a behaviour $\propto \frac{1}{\cos\theta}$ will you introduce such a factor in the BRDF. I suggest reading up on measure theory if you want to understand in depth where your misunderstanding came from." CreationDate="2020-05-08T17:57:49.937" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15211" PostId="9410" Score="0" Text="@lightxbulb, there is no cosine division. Focus on the lambertian diffuse, a constant function, $f_r = \rho / \pi$, where do you see a cosine division?" CreationDate="2020-05-08T21:30:04.927" UserId="11581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15212" PostId="9410" Score="0" Text="That's exactly what I have been trying to explain to you - see what you wrote: `material-&gt;Eval(wi, wo, normal) / dot(normal, wi)`, `in the definition of BRDF, there is a &quot;normalizing&quot; (normal direction) term $1/\cos\theta_i$`.&#xA;The above simply do not hold in the general case." CreationDate="2020-05-08T21:47:15.467" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15213" PostId="418" Score="0" Text="For the PS image, is that an isomorphic projection? Did you calculate the values by hand or do you have a function to do projections?" CreationDate="2020-05-08T23:10:01.893" UserId="482" ContentLicense="CC BY-SA 4.0" />
  <row Id="15214" PostId="9410" Score="0" Text="@lightxbulb, oh I see, that's my mistake, that is very confusing. The reason for $/ \cos$ in my answer, is to cancel out $* cos$ in `eval`, because only the constant part of `eval` is the Lambertian BRDF (despite eval being called brdf by the author). The reason why it is usualy folded together is to avoid repeated computation. The place it came from is the rendering equation, and the reason why it's _there_ is to cancel out the normalizing $\cos$ from the definition of BRDF." CreationDate="2020-05-08T23:25:50.673" UserId="11581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15215" PostId="9410" Score="0" Text="You're not supposed to cancel out the cosine in the rendering equation. The only reason I can think of why you would have the division is from the probability density function." CreationDate="2020-05-09T04:30:22.713" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15216" PostId="9804" Score="0" Text="Beautiful being subjective, I will focus on the objective part of generating any plausible landscapes. I suggest looking into procedural terrain generation. I am referring mainly to using some noise (value/perlin/worley) and then layering it at different scales (fractal brownian motion). You can find many examples on shadertoy." CreationDate="2020-05-09T05:10:45.750" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15217" PostId="418" Score="0" Text="@luserdroog i have a function that takes a 3D coorinate and projects it yes. Basizally its a isometrix matrix plus a offset move for z. Why? You need something like that?" CreationDate="2020-05-09T09:28:39.713" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15218" PostId="9804" Score="0" Text="Thank you I will look on Github and HackerNews for papers/tutorials/sources examples. Do you think I can generate terrains like these ones (less or not realistic of course, with more abstract or gaming textures than these realistic ones): https://youtu.be/ceGLEhahLKQ?t=1 ?" CreationDate="2020-05-09T12:27:16.043" UserId="8709" ContentLicense="CC BY-SA 4.0" />
  <row Id="15219" PostId="9804" Score="0" Text="Generating terrains as in the video you linked may be feasible, but it's certainly not easy. Here's a representative example of something considered quite good: https://www.shadertoy.com/view/4ttSWf" CreationDate="2020-05-09T13:14:01.043" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15220" PostId="9804" Score="0" Text="OK thank you! Well, I have developed a procedural terrain simulator. When I launch it, I end with a 3D terrain with water, a &quot;plausible terrain&quot;. But it's not beautiful: there is no snow, no grass, no trees, no flowers, no... nothing, just the terrain and the water. Di you think it would be feasible to use a deep-learning model &quot;Picture2Picture&quot; in order to add these textures, these things to my 3D virgin terrain image? Are &quot;Picture2Picture&quot; networks easier and faster to train than GAN (less than 1 hour)? Do you know if there is a pre-trained Pic2Pic model that could help me with my 3Dterrain?" CreationDate="2020-05-09T13:33:58.790" UserId="8709" ContentLicense="CC BY-SA 4.0" />
  <row Id="15221" PostId="9804" Score="0" Text="Don't hesitate to write an answer so I will be able to upvote and accept your answer, giving yourself 10 points of reputation :-)" CreationDate="2020-05-09T13:34:21.413" UserId="8709" ContentLicense="CC BY-SA 4.0" />
  <row Id="15222" PostId="9410" Score="0" Text="@lightxbulb, no. The cos is the attenuation due to angle... This is ridiculous. It is not helping anyone. Lambertian diffuse being equal to `material-&gt;Eval(wi, wo, n) / dot(n, wi)` or $f_r = \rho / \pi$ (a constant function) is a fact. BRDF not being upper-bounded is a fact.&#xA;PLEASE..." CreationDate="2020-05-09T13:48:53.823" UserId="11581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15223" PostId="9804" Score="0" Text="I have no idea on how to use ML for this task. As for formulating this as an answer - I believe others may have more valuable input on this." CreationDate="2020-05-09T14:59:17.700" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15224" PostId="9410" Score="0" Text="The attenuation due to angle (Lambert's cosine law) doesn't show up in the denominator of arbitrary BRDFs, it shows up in the rendering equation as a factor not part of the BRDF. I already explained that Lambertian diffuse is constant - no cosine division required. Neither did I argue that the BRDF needs to be bounded from above - however, that does not mean that the mirror BRDF is infinity - it is a Dirac delta." CreationDate="2020-05-09T15:02:25.700" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15225" PostId="418" Score="0" Text="I was re-reading the answers here because I needed to make some diagrams and flowcharts. Nah, I guess I don't really need a function like that, I was just curious about it. Thanks" CreationDate="2020-05-09T16:00:07.470" UserId="482" ContentLicense="CC BY-SA 4.0" />
  <row Id="15226" PostId="418" Score="0" Text="@luserdroog i dont think you would have had problems doing this. Anyway you dont happen to know some simple graph layout algo for the flowcharts." CreationDate="2020-05-09T16:04:36.080" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15227" PostId="9410" Score="0" Text="I finally got why you are saying this - I apologize. Seems like in that code they stuck in the cosine into eval for some reason, which confused me. I thought you were dividing the brdf itself by cosine for some reason (but you clarified it was not in the answer before your last). Please do remove the infinity thing about the mirror though - a Dirac delta has very specific properties - it's not just an &quot;infinity&quot;. I can give you a brdf that is infinity over sets of measure zero though, and it will not matter." CreationDate="2020-05-09T16:09:51.447" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15228" PostId="418" Score="0" Text="I don't know about any algorithms, but I think there were quite a few papers from Bell Labs about `pic` that might be worthwhile. For my task I tried Inkscape but it seemed like I wasn't going to be able to get what I want quickly. I'm now using https://app.diagrams.net which seems pretty good." CreationDate="2020-05-09T16:34:14.630" UserId="482" ContentLicense="CC BY-SA 4.0" />
  <row Id="15229" PostId="418" Score="0" Text="@luserdroog yEd might e worth a try... thats what i use if i need to influence the result" CreationDate="2020-05-09T18:04:36.587" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15230" PostId="9808" Score="1" Text="Why would you have both the probabilities at 50 though. It's gonna waste a lot of samples if, let's say we have a material that is 90% specular and 10% diffuse. You'd end up sampling the diffuse and specular 50% of the times when clearly you should have sampled the specular more." CreationDate="2020-05-09T23:49:22.133" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15231" PostId="9807" Score="0" Text="There aren't many resources for BDPT without MIS. I'd suggest reading the original Lafortune's paper on BDPT where he describes one such naive method." CreationDate="2020-05-09T23:50:36.267" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15232" PostId="9807" Score="0" Text="I suggest directly reading Veach's paper on MIS." CreationDate="2020-05-10T08:15:46.043" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15233" PostId="9808" Score="1" Text="Yes, it would be better, but my specular is different for each RGB value so it wouldn't be consistent. I would have to average the reflectivity and then decide what sampling to use. That's definitely an option, but I am not so far yet with optimization. But a good point. Thank you" CreationDate="2020-05-10T08:41:09.977" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15234" PostId="9807" Score="0" Text="Lafortune paper?? Never heard of it. I will give it a try." CreationDate="2020-05-10T08:41:58.333" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15235" PostId="9807" Score="0" Text="But do I have to directly sample the light source??" CreationDate="2020-05-10T09:26:50.203" UserId="12040" ContentLicense="CC BY-SA 4.0" />
  <row Id="15236" PostId="9813" Score="3" Text="Someone has to write and maintain the engines! thats at least one use! ;)... Just like anything in the software world, there are varying levels of abstraction for a developer to use (i.e high/low level language, toolkits/engines/libs etc). This is the same with graphics programming... you could be close to the metal (Vulkan), you can go slightly higher (opneGL) or you could use something that uses one or many other API's. Everytime you go higher, you absolve yourself of a little control/reposonsibility and possibly performance (usually, but not always)... thats the trade off." CreationDate="2020-05-10T22:24:09.283" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="15237" PostId="9813" Score="0" Text="Also, mathematics (matrix/geometry theory) is pretty much a must in graphics programming. You won't get far without it." CreationDate="2020-05-10T22:25:18.540" UserId="9438" ContentLicense="CC BY-SA 4.0" />
  <row Id="15238" PostId="9813" Score="0" Text="&quot;*This environment doesn't interest me much because it certainly cannot lead to an individual business.*&quot; Why do these APIs need to serve the needs of &quot;an individual business&quot;? This question overall seems quite opinion-based." CreationDate="2020-05-11T00:17:44.160" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15239" PostId="9813" Score="0" Text="The question is the same shape is, &quot;What is the point of writing any software?  There is already lots of software that exists I could use.&quot;  Or indeed, &quot;What is the point of writing?  Things have already been written.&quot;  I can no more enumerate every use for OpenGL than I can list every novel that still needs to be written." CreationDate="2020-05-11T01:28:25.857" UserId="9097" ContentLicense="CC BY-SA 4.0" />
  <row Id="15240" PostId="9813" Score="0" Text="Since you are learning graphics, my advice is to just go for Vulkan directly and don't waste time on directx. The latter is a vendor lock-in API while the former is quite open and cross platform. Besides, dx11 is ancient technology, and dx12 is pretty much the same as Vulkan, except only limited to windows, so just go Vulkan directly. (I'm a graphics developer at Google)." CreationDate="2020-05-11T03:41:21.383" UserId="6442" ContentLicense="CC BY-SA 4.0" />
  <row Id="15241" PostId="9814" Score="1" Text="You could try Perona-Malik or Mumford-Shah" CreationDate="2020-05-11T04:59:23.663" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15242" PostId="9811" Score="1" Text="try glPixelStorei(GL_PACK_ALIGNMENT, 1); on the bound GL_TEXTURE_2D." CreationDate="2020-05-11T09:18:29.493" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15245" PostId="9811" Score="0" Text="@PaulHK Thanks, this fixed it!" CreationDate="2020-05-11T16:51:34.377" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15246" PostId="9818" Score="0" Text="Agreed, but that's not what I'm asking. I'm asking whether this sequence of terms is bounded by above by a exponentially decreasing function, but by all means, a specific deeper path can still have a greater contribution than a specific shorter one. Saying something about the asymptotic behaviour of a sequence says nothing about two specific terms." CreationDate="2020-05-11T17:22:51.603" UserId="13225" ContentLicense="CC BY-SA 4.0" />
  <row Id="15247" PostId="9809" Score="0" Text="You might be interested in reading this: http://www.reedbeta.com/blog/the-radiance-field/ which goes a bit into global convergence conditions for the whole light field in the scene; though I'm not sure it proves anything you haven't already established. More broadly, though, I suspect looking this via _backward_ paths from camera to light is simply the wrong direction—there is no guarantee their contribution will fall off over bounces. Looking at _foward_ paths from light to camera, though, it should be possible to establish with mild conditions on the scene materials." CreationDate="2020-05-11T17:24:07.670" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15249" PostId="9818" Score="0" Text="Each contribution at depth i will be multiplied by,  (fr  * cos) ^ i ? I think if you consider the recursive equation for the render equation this might help you prove this." CreationDate="2020-05-11T17:34:23.773" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15250" PostId="9809" Score="0" Text="@NathanReed An interesting read. From the blog: &quot;Is the absorption inequality a necessary condition for convergence? No; it’s possible to “break the rule” of absorption in careful ways and still have a convergent scene.&quot; This is *exactly* the thing I want to formalize. My two conditions are attempts to formulate precisely when it is okay to 'break the rule' without having a divergent radiance." CreationDate="2020-05-11T17:40:43.413" UserId="13225" ContentLicense="CC BY-SA 4.0" />
  <row Id="15251" PostId="9809" Score="0" Text="For your second point, I do not agree. I my question I did not talk about concepts like forward or backward paths. These are simply implementation details about how to compute $L_o(\vec{p}_0, \omega_0)$, but don't change it's definition. There is only one rendering equation." CreationDate="2020-05-11T17:43:54.260" UserId="13225" ContentLicense="CC BY-SA 4.0" />
  <row Id="15252" PostId="9822" Score="0" Text="Very helpful, thanks very much!" CreationDate="2020-05-12T02:29:59.087" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15254" PostId="9818" Score="0" Text="I appreciate the attempt, but still not what I'm trying to prove. As stated in the question, I want to prove that $L_o^{(d)}(\vec(p), \omega_0) = \mathcal{O}(t^d)$, and nothing else. Note that it *must* be true that $L_o^{(d)}$ converges to 0, otherwise the total radiance $L_o$ would be infinite!" CreationDate="2020-05-12T15:14:20.070" UserId="13225" ContentLicense="CC BY-SA 4.0" />
  <row Id="15255" PostId="9818" Score="0" Text="Although your post is not an answer to my question, still two remarks. (1) For what I'm trying to prove, you don't need to assume that $L_e$ is constant. See for example my proof in &quot;what I tried so far&quot;. (2) Saying that $f_r$ is energy conserving is not equivalent with the condition on $f_r$ you posted. (see for example [the wikipedia page about BRDF's](https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function#Physically_based_BRDFs)) Still, you make it right by adding the $\cos$ factor, so only a minor mistake." CreationDate="2020-05-12T15:19:06.683" UserId="13225" ContentLicense="CC BY-SA 4.0" />
  <row Id="15256" PostId="9809" Score="0" Text="I think Guibas had some papers on this, I am not sure though. If I remember correctly you could violate energy conservation and still get a convergent scene (for example if there's a &quot;hole&quot; in your scene through which enough energy can escape). For the standard formulation it is enough that the scattering operator $T$ is a contraction: $\|T\|&lt;1$." CreationDate="2020-05-12T15:35:15.477" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15257" PostId="9809" Score="0" Text="See A Course of Modern Analysis by E. T. Whittaker, G. N. Watson for details on convergence of the Liouville-Neumann expansion." CreationDate="2020-05-12T15:43:01.930" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15258" PostId="9815" Score="0" Text="thank you, I really appreciated your explanation, your 2 answers are something that made me reflect, also on other types of projects. Furthermore, you made me understand how I had completely missed a fundamental point: you cannot understand what a tool is for if you don't know it and you don't know anything about its environment." CreationDate="2020-05-12T20:32:31.397" UserId="13346" ContentLicense="CC BY-SA 4.0" />
  <row Id="15259" PostId="9813" Score="0" Text="@lfgtm it is true, the first thing would be to study graphical programming more deeply, to understand at what level of abstraction it is better to act according to the problem. I'm sure that by studying the use cases they will come out on their own. PS: math will be the first thing I will study, thank you :)" CreationDate="2020-05-12T20:36:18.023" UserId="13346" ContentLicense="CC BY-SA 4.0" />
  <row Id="15260" PostId="9813" Score="0" Text="@Shahbaz thank you very much for the advice and congratulations for your career :)" CreationDate="2020-05-12T20:37:47.273" UserId="13346" ContentLicense="CC BY-SA 4.0" />
  <row Id="15262" PostId="9826" Score="0" Text="At least [this](https://youtu.be/Z6lYqgWAmrg) video seems to be available in AV1 codec in 2160p60 resolution and framerate." CreationDate="2020-05-13T07:53:39.427" UserId="13359" ContentLicense="CC BY-SA 4.0" />
  <row Id="15263" PostId="9826" Score="0" Text="In my experience Firefox has currently better performance playing AV1 content than Google Ćhrome has." CreationDate="2020-05-13T08:02:08.957" UserId="13359" ContentLicense="CC BY-SA 4.0" />
  <row Id="15264" PostId="9825" Score="0" Text="The decoder could start skipping decoding video frames to catch up. Up until the point it can't catch up and audio starts stuttering." CreationDate="2020-05-13T12:06:06.807" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15265" PostId="9830" Score="0" Text="I found that browser extension called enhanced-h264ify has an option to block 60fps video on YouTube, but probably there are separate video files of lower frame-rate." CreationDate="2020-05-13T14:46:00.020" UserId="13359" ContentLicense="CC BY-SA 4.0" />
  <row Id="15266" PostId="9825" Score="0" Text="When you upload a video to youtube they will automaticaly generate low resolution 30fps versions. When you play back the video on mobile the mobile device doesn't have to skip frames because it is reading a 30fps version already." CreationDate="2020-05-13T14:57:03.770" UserId="13159" ContentLicense="CC BY-SA 4.0" />
  <row Id="15267" PostId="9830" Score="0" Text="@jarno yes, a typical youtube video has at least 5-10 different stream options with different resolutions, bitrates and codecs. For example, I checked a random 4K video and it has: 256x144, 426x240, 640x360, 854x480, 1280x720, 1920x1080, 2560x1440, 3840x2160. Each of those resolutions is available in multiple versions, for a total of 22 choices. And that's not a 60fps video, those might have even more." CreationDate="2020-05-13T15:05:33.333" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15268" PostId="9830" Score="0" Text="I checked a 60fps one for fun and indeed, just for 4K there are 4 streams: two 60fps (avc/webp), one 30fps and one 60fps with HDR. Interestingly, there are 60fps HDR streams down to 256x144 resolution." CreationDate="2020-05-13T15:11:05.273" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15269" PostId="9823" Score="0" Text="Thanks a lot for the detailed answer. I was aware of concept of texture but was not sure why this paper and a couple more papers are using checkered pattern. It seemed weird to me but your explanation is right on!" CreationDate="2020-05-13T15:37:35.483" UserId="6281" ContentLicense="CC BY-SA 4.0" />
  <row Id="15270" PostId="9823" Score="0" Text="I assume they needed something generalizable. Like they couldn't have real clothings or hair as it wouldn't be much generalizable and those need their own mesh. Specially that emphasize of paper was on body models not hair or clothing. I now actually find the checkered pattern an excellent and smart choice." CreationDate="2020-05-13T15:39:08.180" UserId="6281" ContentLicense="CC BY-SA 4.0" />
  <row Id="15271" PostId="9831" Score="0" Text="Hi Peter thanks for your answer. May I ask, for 1), do you mean arnold is actually calculating weighted average (weighted sum here is actually more like an weighted average because it's lerp and lerp and lerp)? Thank you!" CreationDate="2020-05-13T16:18:29.780" UserId="13362" ContentLicense="CC BY-SA 4.0" />
  <row Id="15272" PostId="9831" Score="0" Text="If your interseted can read the OSL implementation here https://github.com/Autodesk/standard-surface/blob/master/reference/standard_surface.osl" CreationDate="2020-05-13T16:42:59.070" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15273" PostId="9831" Score="0" Text="The best way to think about it is that each layer is revealing the one below it as you reduce it's strength. For example if you have 3 layers of Specular -&gt; Metal -&gt; Diffuse, then you would multiply the diffuse albedo by (1 - specular) * (1 - metal) to get your fr term. This is a simplification but the post I linked shows exactly how each is calculated." CreationDate="2020-05-13T16:45:49.310" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15274" PostId="9831" Score="0" Text="Yes I got that. But still not confident about why PBRT is doing a sum for brdf..." CreationDate="2020-05-14T02:04:43.260" UserId="13362" ContentLicense="CC BY-SA 4.0" />
  <row Id="15275" PostId="9831" Score="0" Text="Basically the material or ui, which is the collection of bxdfs, will make sure they are energy conserving when you sum them together. If you just averaged all your bxdfs, they would get equal weight which would make things difficult if, for example, you wanted to have a strong diffuse component and only a subtle amount of specular." CreationDate="2020-05-14T10:18:53.107" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15276" PostId="9831" Score="0" Text="I think I've got your idea: instead of computing simple average, we should calculate the actual brdf more carefully (based on parameters for instance), while, it's still necessary to take care of energy conservation constraint." CreationDate="2020-05-14T10:37:09.650" UserId="13362" ContentLicense="CC BY-SA 4.0" />
  <row Id="15277" PostId="9788" Score="0" Text="when using a rotation matrix (4x4) where the 4th column and row is zero (except the [4,4] cell is 1) you will get a rotation around the center (0/0/0). Beware, that if you have a point at (1,1,1) and you rotate it, it can lie outside the [-1,1] area" CreationDate="2020-05-14T12:32:40.073" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15278" PostId="9839" Score="0" Text="Here's a good thread with some info regarding the tech https://reddit.com/r/GraphicsProgramming/comments/gj200c/how_unreal_engine_developers_achieved_so_great/" CreationDate="2020-05-14T12:40:46.337" UserId="10450" ContentLicense="CC BY-SA 4.0" />
  <row Id="15279" PostId="9832" Score="0" Text="RE *&quot;talking about having the meshes as nodes.&quot;* that seems very unlikely *unless* each mesh is trivial, say, only a few triangles.   Testing rays against triangles is *much* more expensive than against an AABB or sphere and, since most rays will typically miss all but a handful of triangles, in a given mesh, it would be quite inefficient." CreationDate="2020-05-14T14:52:42.977" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15280" PostId="9840" Score="0" Text="You could perhaps try searching for the term &quot;Photogrammetry&quot; ?  A quick search, for example, gives this approach https://www.youtube.com/watch?v=ye-C-OOFsX8  or perhaps https://www.youtube.com/watch?v=k4NTf0hMjtY  but there are no doubt others." CreationDate="2020-05-14T15:04:13.007" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15281" PostId="9844" Score="0" Text="wow!! I cannot thank you enough as I was banging my head for the past 2 days because of this issue, it happened to be what you said the upside-down I guess. when I changed the background color to black the patches became black too. I guess i need to study more on normals &amp; axis orientations I will update my code with more understanding in a few days and accept the answer if that's ok with you? anyways thanks a ton really :)" CreationDate="2020-05-14T22:51:45.560" UserId="13309" ContentLicense="CC BY-SA 4.0" />
  <row Id="15282" PostId="9844" Score="0" Text="Glad to hear that! Oh and now I see what you are doing wrong in the plane! I will add that to my answer." CreationDate="2020-05-14T22:59:57.053" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15283" PostId="9844" Score="0" Text="my y is -ve upwards, even then I changed the plane to `Vec3(0,1,0)` the black patch remains. I think I will rewrite the code with the correct axes. This is my first program in graphics programming so a bit confused with something that should be very obvious." CreationDate="2020-05-14T23:04:02.170" UserId="13309" ContentLicense="CC BY-SA 4.0" />
  <row Id="15284" PostId="9844" Score="0" Text="Nope, the reason is the epsilon in the `denom` check. Changing the normal is just for correctness and then the `denom` check needs to be inverted as well." CreationDate="2020-05-14T23:30:43.687" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15286" PostId="9846" Score="0" Text="Great, thanks. Did you mean 10000 photos or photos that are 10k in resolution?" CreationDate="2020-05-15T09:23:37.417" UserId="13376" ContentLicense="CC BY-SA 4.0" />
  <row Id="15287" PostId="9846" Score="0" Text="It was 10.000 photos of different resolutions, some were 4K some were 8K. Resolution is not that important, since you can always downsample and the 3d reconstruction frameworks often do that automatically. The number of photos is the crucial factor for computational complexity. There has to be enough overlap between them, but too much of it (like taking each video frame) is an overkill. Finding a sweet spot is important to balance the result quality vs. computation time." CreationDate="2020-05-15T10:12:54.357" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15288" PostId="9846" Score="0" Text="Btw I see you are from Orava @TimotejLeginus. I ♡ Tatry :) Will be glad to help you with any details!" CreationDate="2020-05-15T10:20:29.393" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15289" PostId="9846" Score="0" Text="Oh I noticed you're from Slovakia too. Thanks for the help. I am planning to make a 3D model out of Osobitá, or some peak in Roháče, to be used in a game (something similar to Mýrdalssandur, Iceland or Fushimi Inari, virtual walkthrough games). It is generally forbidden to visit Osobitá, it's restricted to only some time a year, so people can virtually visit it. How much photos would you assume that a peak like Osobitá would take? Check it out on Google Earth." CreationDate="2020-05-15T11:03:16.887" UserId="13376" ContentLicense="CC BY-SA 4.0" />
  <row Id="15290" PostId="9846" Score="0" Text="Yes, I know it. Please be aware that Osobita has a local nature reserve status within TANAP. Flying an UAV over there is a double violation of the park rules. That being said, the # of photos depends on the details you want. I would go for a combination of a reconstructed model enhanced with stock assets like rocks and generated foliage. You should be good with 5k-20k photos then. Before starting big, try it in small scale (e.g. a single object from ~50 photos, then something bigger with ~ 500, e.g. some boulder) so that you get a feeling for the workflow and expected results quality." CreationDate="2020-05-15T12:54:47.553" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15291" PostId="9846" Score="0" Text="Btw have you seen this beautiful game (just released) made by a small studio from Brno? https://www.youtube.com/watch?v=AzPOmcgup9o It includes detailed scenery from Czech woods, all hand modeled." CreationDate="2020-05-15T12:54:51.413" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15292" PostId="9846" Score="0" Text="Ou and this amazing new Unreal Engine demo! https://www.youtube.com/watch?v=qC5KtatMcUw" CreationDate="2020-05-15T13:01:04.060" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15293" PostId="9846" Score="0" Text="Thanks for the the help! The UE5 demo looks stunning, so does the Brno game! Yeah, I would like to start with some small object. I am aware of Osobitá's status and I want to stay within the law all the time. I'll try it with smaller objects, thanks." CreationDate="2020-05-15T13:09:04.793" UserId="13376" ContentLicense="CC BY-SA 4.0" />
  <row Id="15294" PostId="9846" Score="0" Text="Also it could also be a manned air vehicle though, if that matters. Is it more viable to do it via a drone or just being there and taking photos?" CreationDate="2020-05-15T13:14:48.230" UserId="13376" ContentLicense="CC BY-SA 4.0" />
  <row Id="15295" PostId="9846" Score="0" Text="Let us [continue this discussion in chat](https://chat.stackexchange.com/rooms/108038/discussion-between-isolin-and-timotej-leginus)." CreationDate="2020-05-15T13:24:37.203" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15296" PostId="9845" Score="1" Text="There's no reason you couldn't do analytical derivatives with simplex noise, but maybe no one has sat down and worked it out and published it yet." CreationDate="2020-05-15T17:11:35.317" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15297" PostId="9850" Score="0" Text="Could you please explain me what do you mean by second pass?" CreationDate="2020-05-15T22:17:14.980" UserId="13192" ContentLicense="CC BY-SA 4.0" />
  <row Id="15298" PostId="9841" Score="0" Text="Please delete this. I reposted on Math forum and cannot delete here as a new user." CreationDate="2020-05-16T10:56:33.353" UserId="13377" ContentLicense="CC BY-SA 4.0" />
  <row Id="15299" PostId="9848" Score="0" Text="Well it does not have to be triangulated. Most notoriously renderman didnt turn quads into triangles back in late 1990' and early 2000's. gaining a nice 2 times speed boost. But they do now since they are a raytracing which they didn't back then." CreationDate="2020-05-16T11:03:19.227" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15301" PostId="9850" Score="0" Text="You would first render the whole scene to a float texture using a simple fragment shader just to store the depth values, that would be the first pass. Then you would map the obtained texture to a full screen quad and bind it to the sampler of your main fragment shader that does all the heavy stuff (adding an early discard if the depth is not equal +- epsilon) and draw the whole scene once again, that would be the second pass with output set to screen." CreationDate="2020-05-16T13:19:31.657" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15302" PostId="9850" Score="0" Text="Maybe this can help (it is old OpenGL but still applies to current WebGL) https://stackoverflow.com/questions/23362076/opengl-how-to-access-depth-buffer-values-or-gl-fragcoord-z-vs-rendering-d." CreationDate="2020-05-16T13:19:33.950" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15303" PostId="9850" Score="0" Text="This thread discusses the depth test in fragment shader: https://www.gamedev.net/forums/topic/624260-depth-testing-in-the-fragment-shader/" CreationDate="2020-05-16T13:52:04.117" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15304" PostId="9854" Score="0" Text="&quot;*no idea how to put this data into a texture.*&quot; Um, why not? It's just bytes that represent values." CreationDate="2020-05-16T17:50:03.163" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15305" PostId="8471" Score="0" Text="To clarify, you never want to clamp values in the pathtracer - the only reason clamping is necessary is to fit the radiance value computed by the pathtracer into a range that will &quot;fit&quot; into an image. Linearly mapping radiance to final image values can also result in darks looking too dark and brights clipping, so you'll want to do some tonemapping to fix that" CreationDate="2020-05-16T18:06:45.393" UserId="12056" ContentLicense="CC BY-SA 4.0" />
  <row Id="15308" PostId="9838" Score="0" Text="One can think of many techniques to animate the transition, simple blending being just one of them. Your choice will depend on your artistic goal and taste. I think all answers to your question will be highly subjective." CreationDate="2020-05-16T19:52:08.550" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15309" PostId="9848" Score="0" Text="Can you provide documentation that they didn't use triangles?  AS far as I know you have to triangulate any non-coplanar quad.  Even if one point of a coplanar quad is moved perpendicular to the plane the surface has bent, creating 2 planes.  You must then calculate each plane, which are made from 3 of the quads 4 pionts (So if point B is moved on quad ABCD, this creates triangles ABC and CDA, with line AC being the hypotenuse). So, you end up with 1 noncoplanar quad calculation turning into 2 triangle calculations. Yet maybe Pixar had a different method than linear equations to the sides?" CreationDate="2020-05-16T19:57:09.333" UserId="13387" ContentLicense="CC BY-SA 4.0" />
  <row Id="15310" PostId="9848" Score="0" Text="Also, by having to triangulate, I don't mean triangulating your model mesh manually, but that Renderman does this in the background.  Even modern game engines do this in the background, hence the reason you can import a mesh with quads into Unreal Engine." CreationDate="2020-05-16T20:03:32.673" UserId="13387" ContentLicense="CC BY-SA 4.0" />
  <row Id="15311" PostId="9848" Score="0" Text="I think they dont calculate the intersection the way you think. They microdice so they only sample the vertex of the screen oriented microdiced surface. The underlying surface is a nurbs or catmul rom spline that is guaranteed to have a intersection in another way. For that primitive it does not matter if the face is flat or not since its curved anyway but it dont matter for the screen. Only reason its a quad is to get the derivates of the surface and screen space interpolation. Point being, i would be very careful when saying tgis is the way it has to work." CreationDate="2020-05-16T20:15:06.573" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15312" PostId="9848" Score="0" Text="It may be interesting to note that originally the computer graphics guys tried to do analytical solves. They didnt start with the premise of having simplest possible discrete shape. But were looking for higher order stuff. Its only later that we start to do this because we want dedicated silicon for it. Software renderers dont need to follow such rules. Neither do modern harware but we usually do so because we understand that workflow well today. If you can find old renderman manuals its clearly stated there" CreationDate="2020-05-16T20:23:33.940" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15313" PostId="9842" Score="0" Text="Well its useful to calculate volumes of paralell epipeds and by extension tetrahedra. So basically the volume of any polygon bound object. Which in turn is useful for making uniform random point clouds." CreationDate="2020-05-16T21:46:12.583" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15314" PostId="9823" Score="1" Text="@MonaJalal it showcases the fact that the uv map is present. But also makes surface flow more apoarent" CreationDate="2020-05-16T21:53:22.883" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15315" PostId="9842" Score="0" Text="Ive heard that argument for the scalar triple product, but here im asking about the vector triple product though. I dont think you can calculate the volune of parallelepipeds using the vector triple product" CreationDate="2020-05-17T01:17:03.647" UserId="5664" ContentLicense="CC BY-SA 4.0" />
  <row Id="15318" PostId="9853" Score="0" Text="This BRDF is normally defined as a mix of diffuse and an specular component with a lob. The specular component normally is defined using microfacets reflection model. Have a look at FresnelBlend section from PBRT in this link: http://www.pbr-book.org/3ed-2018/Reflection_Models/Microfacet_Models.html" CreationDate="2020-05-17T11:20:11.923" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15319" PostId="9853" Score="0" Text="@ali Thanks. Yes I've just found the pbrt to be very helpful in this regard, and by carefully reading chapter 14 I think I've got the answer to the above question: The above result is correct only in one particular case, when you have a matte + perfect mirror material. In case of a non-perfect mirror BOTH DiffuseBRDF and SpecularBRDF are included, and the two Pdfs are weighted, like so: (DiffuseBRDF + SpecularBRDF)*incomingLight / 0.5*DiffusePdf + 0.5*SpecularPdf" CreationDate="2020-05-17T13:46:56.057" UserId="11521" ContentLicense="CC BY-SA 4.0" />
  <row Id="15321" PostId="9848" Score="0" Text="A non planar quad is called a bilinear patch. You can render them directly just fine, including raytracing. And no, RenderMan did not turn quads into triangles. In fact, I'm fairly sure it did the opposite back then, turning triangles into quads with one half hidden." CreationDate="2020-05-17T17:25:02.953" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15322" PostId="9860" Score="0" Text="That's a fantastic answer. There is a lot of subtlety that I had ignored. Your thorough treatment of what's relative and what is global was especially helpful." CreationDate="2020-05-18T05:45:50.707" UserId="13396" ContentLicense="CC BY-SA 4.0" />
  <row Id="15323" PostId="9863" Score="0" Text="The compiler can't optimize them both into the same thing, as they don't do the same thing. The way the texture is set up on the application side also has to be changed to match." CreationDate="2020-05-18T21:04:25.197" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15324" PostId="9863" Score="0" Text="True, I forgot to mention that I'd also be changing the format of the texture being sampled from to accommodate 4 texture calls vs 16." CreationDate="2020-05-18T21:08:24.960" UserId="13404" ContentLicense="CC BY-SA 4.0" />
  <row Id="15325" PostId="9864" Score="0" Text="ok thank you for the clarity here - is it safe to assume then that if I had 4 texelFetch().r calls to the same location, this would be similar in speed to one texelFetch().rgba ?" CreationDate="2020-05-18T21:12:19.233" UserId="13404" ContentLicense="CC BY-SA 4.0" />
  <row Id="15326" PostId="9863" Score="1" Text="Speaking of better optimization, why not set these matrices as uniforms?" CreationDate="2020-05-18T21:35:31.770" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="15327" PostId="9832" Score="0" Text="I would go with a target of a handful of triangles, say 6-8 triangles per BVH node. There will be duplicate triangles though if you hit a leaf node and didn't hit any triangles and then move to a neighbouring node, the same triangle can exist in both nodes (The number of shared triangles typically increases the less triangles per node you opted for)" CreationDate="2020-05-19T06:06:45.163" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15331" PostId="9864" Score="0" Text="I’d expect the separate fetches to be slower, as @Isolin said—packing the values into one texture should mean you’re making the best possible use of the hardware." CreationDate="2020-05-20T00:25:29.713" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="15332" PostId="1691" Score="0" Text="That's a really good question. So what's the answer? Is there any reference that has comparison between rendered images using xyz and rgb?" CreationDate="2020-05-20T14:34:33.403" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15333" PostId="9867" Score="1" Text="True, the tiny distance between the cameras will strongly reduce 3D reconstruction precision. Once a feature point appears on the very same location in both pictures there is no way to do proper stereo reconstruction for it. I think the critical distance will be quite small." CreationDate="2020-05-20T17:30:45.233" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15334" PostId="9867" Score="0" Text="@Isolin Are you aware of an alternative, pre-TrueDepth, i.e. for models prior to iPhoneX?" CreationDate="2020-05-20T17:53:36.220" UserId="13410" ContentLicense="CC BY-SA 4.0" />
  <row Id="15335" PostId="9867" Score="1" Text="No, I have &quot;traditional&quot; photogrammetry experience, but honestly I was never interested in Apple products. If you assume your scene to be static, you can fallback to a progressive approach. At least for close objects the stereo could help a bit. But if stereo would be the ultimate solution to go, no one would ever think of integrating other technologies like structured light or LiDAR :)" CreationDate="2020-05-20T20:32:41.683" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15336" PostId="9867" Score="0" Text="@Isolin Well said! Yes, I suppose a Stereo solution would be limited to close objects." CreationDate="2020-05-20T21:58:44.030" UserId="13410" ContentLicense="CC BY-SA 4.0" />
  <row Id="15337" PostId="1691" Score="0" Text="would this paper help? https://radiance-online.org:447/radiance-workshop1/cd/Ward/PicturePerfect.pdf" CreationDate="2020-05-21T12:08:31.543" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15338" PostId="9872" Score="0" Text="Are you ok with throwing away outliers by mapping them outside the [0, 255] range? If so, you could normalize to place the mean at 128 and linearly map 2 or 3 standard deviations around it to [0, 255]." CreationDate="2020-05-21T21:59:48.023" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15339" PostId="9872" Score="0" Text="This is ocean currents data, so the occasional outliers are the interesting bits, but they are few and far between. Relative to these, the ocean appears calm, unless you can expand those more common data points to reveal the underlying currents." CreationDate="2020-05-22T03:58:24.093" UserId="13422" ContentLicense="CC BY-SA 4.0" />
  <row Id="15340" PostId="9870" Score="1" Text="The gamma correction covers how the real world measurement differes from the numerical values. So if you had say a color of 127, 127, 127 and measured its intensity. If you then measured 255,255,255. Then it wouldnt report double the intensity. There is no use measuring this inside the computer it does not know this. Its important of you want to calculate on color physicaly plausibly but not otherwise. Also if you do measure you will find that your monitor isnt sRGB since you havent profiled/calibrated it in less than a month." CreationDate="2020-05-22T05:34:24.283" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15341" PostId="9874" Score="0" Text="many thanks for the answer. Still I cannot understand it fully. I have updated the question could you plz take time and have a look." CreationDate="2020-05-22T14:32:24.230" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15342" PostId="9870" Score="1" Text="But that is again the reason we gamma correct. Human eye is not linearily sensitive. So gamma correction is a sort of compression to use the limited resolution of colors better." CreationDate="2020-05-22T15:42:16.663" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15343" PostId="9870" Score="0" Text="@joojaa, thanks for your comments. I appreciate if I have your thoughts on the update I made on the question plz." CreationDate="2020-05-22T16:49:55.473" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15344" PostId="9874" Score="0" Text="@ali I updated the answer" CreationDate="2020-05-22T19:56:43.987" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15345" PostId="9872" Score="0" Text="Try histogram equalization." CreationDate="2020-05-23T09:03:25.893" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15346" PostId="9874" Score="0" Text="that is a really good answer. Thanks. I also updated the question for couple of more questions . could you have a look plz." CreationDate="2020-05-23T11:19:57.620" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15347" PostId="9874" Score="0" Text="@ali Sure, updated again" CreationDate="2020-05-23T18:55:16.137" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15348" PostId="9874" Score="0" Text="So you said we see the right column (186) as half bright at white? I am bit confused: 186 -&gt; screen -&gt; 50% light -&gt; eye -&gt; ~70% ." CreationDate="2020-05-23T20:41:26.627" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15349" PostId="9874" Score="1" Text="186 input produces 50% intensity by the screen, and our eye percieves this as 70% as bright as white. Then the column 186 is not close to half white." CreationDate="2020-05-23T20:46:24.163" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15350" PostId="9874" Score="0" Text="Correct. The column 186 is _physically_ half white but _perceptually_ about 70% of white." CreationDate="2020-05-23T20:53:33.930" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15351" PostId="9874" Score="0" Text="So again that means  127 column is seen half bright as white by the eye, is that right? Back to sq one." CreationDate="2020-05-23T21:29:37.090" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15352" PostId="9874" Score="0" Text="127-&gt; screen -&gt; ~20% light -&gt; eye -&gt; ~50%" CreationDate="2020-05-23T21:33:40.203" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15353" PostId="9874" Score="0" Text="Yep. That's how it works. Blame our weird eyes. :P" CreationDate="2020-05-23T21:34:51.387" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15354" PostId="9874" Score="0" Text="But then when I first said this that “ open this file on a viewer you will see pixels (127)with brightness half as white“ , you said “think about this carefully “." CreationDate="2020-05-23T21:37:39.547" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15355" PostId="9874" Score="0" Text="So does it mean that we see the colors in for example MS color picker all linear?" CreationDate="2020-05-23T21:38:46.393" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15356" PostId="9874" Score="0" Text="Gamma encoded colors are approximately perceptually uniform, at least in brightness, if that's what you are asking. Sorry, I am not sure just what you are still confused about" CreationDate="2020-05-23T21:41:04.627" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15358" PostId="9876" Score="0" Text="`While both versions yield the same result` &lt;-- no, they really don't. Not with real float/double values which have limited precision. I can't say for certain that's the reason GLSL is specified that way however, which is why I'm not writing this as an answer." CreationDate="2020-05-24T18:06:43.460" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15359" PostId="9876" Score="0" Text="@Olivier okay... this might be a reason that prevents the compiler from performing this optimization. So, apart from the fact that it is most likely not making any difference regarding the overall frame rate, using parenthesis should be faster. But you delivered another reason why it might make sense to use parenthesis to enforce order since less operations usually also mean less floating-point errors." CreationDate="2020-05-24T18:24:22.377" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15360" PostId="9878" Score="0" Text="Thanks for your answer. Regarding your last point: That's true but I wasn't particularly interested in this specific case (vertex processing). It was just an example. For me, it was more interesting to know if I can simply write down a linear algebra equation and let the compiler figure out which execution order is the most performant (neglecting small floating-point variations) or if I have to enforce it. - Seems like I have to reactivate my brain before writing some equations in GLSL ;)" CreationDate="2020-05-25T07:21:34.203" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15361" PostId="9854" Score="0" Text="Maybe you should elaborate on what exactly you are struggling with (maybe some code examples). Assuming you know how to create the noise field and how to upload and use a texture in OpenGL, then it is hard to tell what your problem is since it is not really that difficult." CreationDate="2020-05-25T09:27:57.217" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15362" PostId="9882" Score="0" Text="I strongly disagree with your interpretation of information theory. It is true, that in the end you will need `#pixels * #channels * #bits_per_channel` but you don't want to guess that data, you want to compute them by evaluating the rendering equation. Note that you have many free variables like camera and light parameters, etc. You don't want to pre-bake an optimized scene for each possible camera position or light setup! For example that would mean to precompute all textures for all possible perspectives." CreationDate="2020-05-25T09:37:10.240" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15363" PostId="9882" Score="0" Text="Another wrong assumption of yours is perhaps that only the visible objects contribute to the final image. However, there may be shadow casters all around, layers of transparent objects that accumulate together, specular highlights and caustics from behind the camera reflecting towards object in front of the camera and possibly back again. In order to compute all the light bounces correctly, you will easily end up with scenes much more information content (orders of magnitude more) than the resulting image." CreationDate="2020-05-25T09:41:06.300" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15364" PostId="9882" Score="0" Text="The last bit of additional information you need to take into account is that if you use a spectral renderer, you will try to have more than just RGB data for your materials, but the rendering output will be in most cases just RGB." CreationDate="2020-05-25T09:50:22.390" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15365" PostId="9882" Score="0" Text="@Isolin Your first remark I would argue, yes we do. That's what we do with mipmaps. When we prepare any sort of map, especially impostors, we do pre-bake them for any sort of camera distance or angle. Any sort of LOD of meshes is exactly that, it's a pre-baked optimization for each possible camera position. Now your second remark is interesting. It made me ponder about the universe. We can say that the faint GI caused by galaxies contribute to the final image. Though I doubt that it's chaotic; meaning they can be reduced to few parameters. But caustics are. good catch" CreationDate="2020-05-25T10:44:27.143" UserId="1614" ContentLicense="CC BY-SA 4.0" />
  <row Id="15366" PostId="9883" Score="0" Text="Right above it you have $d\omega = \sin\theta \,d\theta\,d\phi$. Substitute and see what you get." CreationDate="2020-05-25T13:20:50.430" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15367" PostId="9883" Score="0" Text="I think the story should be like this: Premises: $d\omega=sin\theta\ d\theta\ d\phi$ and $p(\theta,\phi)d\theta\ d\phi=p(\omega)d\omega$; Conclusion: $p(\theta, \phi)=sin\theta\ p(\omega)$." CreationDate="2020-05-25T16:33:57.230" UserId="5944" ContentLicense="CC BY-SA 4.0" />
  <row Id="15368" PostId="9883" Score="0" Text="That's it really. And yes, the above is from probability and measure theory. You can define your pdf with respect to some measure and then find its expression with respect to a different measure." CreationDate="2020-05-25T17:04:10.830" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15369" PostId="9883" Score="0" Text="But I couldn't find a formal statement in math books. Could you give a reference? Thanks!" CreationDate="2020-05-25T17:15:17.360" UserId="5944" ContentLicense="CC BY-SA 4.0" />
  <row Id="15370" PostId="9875" Score="1" Text="Hi, welcome! Are you concerned about the lightness difference? Would it be possible to provide more converged images for comparison?" CreationDate="2020-05-25T17:26:56.313" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="15371" PostId="9881" Score="0" Text="Well, think about a vector quantity that is rotated. If your not reading the values in the exact direction of rotation its has smaller amplitude. But were you to analyze the  in polar coordinates it does not matter what the rotation is as you are readfing the magnitude and discarding teh angle. So the magnitude is now invariant of the data orientation... HSL is a polar transform. Perhaps is should convert this to an answer but its just my logical reasoning not very deep thinking." CreationDate="2020-05-25T17:35:43.653" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15372" PostId="9883" Score="0" Text="You need a statement about what? That $d\omega = \sin\theta \,d\theta\,d\phi$? That's by definition of solid angle. But if you want to get where the sine comes from, you could just compute the coordinate transformation from cartesian to spherical coordinates and then set $r=1$ ($r^2\sin\theta$ pops from the Jacobian of the transformation)." CreationDate="2020-05-25T20:19:13.520" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15373" PostId="9883" Score="0" Text="This one: $p(\theta,\phi)d\theta d\phi=p(\omega)d\omega$" CreationDate="2020-05-26T04:38:22.263" UserId="5944" ContentLicense="CC BY-SA 4.0" />
  <row Id="15374" PostId="9883" Score="0" Text="This is just writing the pdf with respect to different measures and equating it. If you've studied vector calculus it's similar to going from a surface integral to one in cartesian coordinates." CreationDate="2020-05-26T10:04:53.873" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15375" PostId="8807" Score="0" Text="Could you please add a small screenshot of the gaps? I can't imagine what you mean by _gaps from a typical deformer_." CreationDate="2020-05-26T10:51:47.777" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15376" PostId="9887" Score="0" Text="&quot;*too many parameters in the VBO each frame/vertex/fragment*&quot; If the parameter changes within a draw call, how exactly would you expect to have made such a thing use a different shader? You can't change shaders in the middle of a draw call, so a value used by the shader is the *only* way to have per-vertex/primitive/fragment parameters. So there's no choice in that case." CreationDate="2020-05-26T21:46:56.370" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15377" PostId="9875" Score="0" Text="Thanks for the reply. Yes I am concerned about the lightness and overall differences. Here are 2 images with 100k samples per pixel : https://imgur.com/a/n0lOSH7" CreationDate="2020-05-26T21:53:30.753" UserId="13436" ContentLicense="CC BY-SA 4.0" />
  <row Id="15378" PostId="9879" Score="1" Text="Thanks for coming back to answer your own question and to help people with the same problem in the future. Really appreciate this." CreationDate="2020-05-27T07:22:34.653" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15379" PostId="9889" Score="2" Text="I don't really understand what your problem is here. Yes, you want to draw a small coordinate system, but why is this a problem? What are you struggling with? Which API/Engine do you use? If you are somewhat familiar with a Graphics API like OpenGL or DirectX, drawing such a coordinate system is rather easy. So please tell us a little bit more about your problem." CreationDate="2020-05-27T07:52:45.647" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15380" PostId="9887" Score="0" Text="If I'm grouping shaders with a switch statement and a mode parameter, then 2 shaders of 4 parameters each become 1 shader of 9 parameters, if there is no reuse (4+4+1 for mode). I then have to pass those 9 down on each vertex and each fagment of course, at which point the question becomes how muh can I push this design." CreationDate="2020-05-27T09:31:38.697" UserId="13449" ContentLicense="CC BY-SA 4.0" />
  <row Id="15381" PostId="9887" Score="0" Text="with reuse I mean that you can re-apropriate parameters to have different meaning per mode. So, simple example: mode: 0 normal, param1 ignored, mode 1: hue shift, param1 is shift distance, mode 2: color burn, param 1 is burn intensity, and so on.." CreationDate="2020-05-27T09:35:41.277" UserId="13449" ContentLicense="CC BY-SA 4.0" />
  <row Id="15382" PostId="9889" Score="1" Text="Hi Herrgott, each raytracer needs some sort 3D transformations to place, rotate and scale all the objects in the scene correctly. Such transformations are mostly represented by matrices, since that is mathematically the most natural and easiest way. You should not try to avoid matrices (my impressions is you do). So briefly: 1) You may create the helper object as part of the raytraced scene, attached to the camera, 2) or you can render them as an overlay using rasterization. In my opinion the second approach is more suitable for your use-case." CreationDate="2020-05-27T16:30:08.707" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15383" PostId="367" Score="0" Text="I recently suggested a non recursive implementation of ray_color function of the book Raytracing in One Weekend, it might help to get you started: https://github.com/RayTracing/raytracing.github.io/issues/624" CreationDate="2020-05-28T04:59:30.253" UserId="13456" ContentLicense="CC BY-SA 4.0" />
  <row Id="15384" PostId="368" Score="0" Text="It is also possible to implement ray tracing algorithm using a `while` loop if the color contribution of each ray can be modeled linearly. This would eliminate the need for call stack, thus effectively eliminating the need for a hardcoded bounce count in shader code. Then you can set the bounce count from the host program" CreationDate="2020-05-28T05:07:04.800" UserId="13456" ContentLicense="CC BY-SA 4.0" />
  <row Id="15385" PostId="9891" Score="1" Text="To me, the first part of your answer reads a little bit like &quot;they do it because they do it&quot;. From the link you provided, I can see what you are intending to say (computationally more effective), but maybe you should clarify it a little bit more." CreationDate="2020-05-28T06:56:17.760" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15386" PostId="9888" Score="0" Text="I saw something similar before, and flipping the no and ni seemed to fix it for me then." CreationDate="2020-05-28T10:20:08.923" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15387" PostId="9888" Score="0" Text="Hi Marcus, did you try debugging your code by switching off all unrelated stuff? When you suspect that it is a refraction problem, please have a look how the result looks like when you omit the reflection part (i.e. disable the true if-branch)." CreationDate="2020-05-28T18:30:52.147" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15388" PostId="9888" Score="0" Text="Having this simple sphere scene, you can also include a debug trigger in the refractive branch that would catch 1) the middle point an 2) a corner point of the sphere. This is possible by something like (in pseudocode)  `if (dot(n, vec3(0, 0, 1)) &gt; 0.99) print(&quot;whatever&quot;)` for 1) and you just change to `vec3(0, 0, 1)` for a point at the corner. You can place a breakpoint on the `print` and step through the processed values for that points on the sphere. You should be able to compute by hand what should they look like, hopefully they will reveal the error." CreationDate="2020-05-28T18:38:20.960" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15389" PostId="9639" Score="0" Text="If you use Windows, you can go to Control Panel / Appearance and Personalization / Fonts and click &quot;Adjust ClearType text&quot; link on the left. It opens &quot;ClearType Text Tuner&quot; wizard, where you can uncheck &quot;Turn on ClearType&quot; checkbox. After that you should not see colored edges on enlarged screenshots from that machine. Removing colored edges from existing screenshots is more difficult, I use Adjustment / Black and White tool in paint.net on manually selected regions for that." CreationDate="2020-05-28T20:20:33.463" UserId="13462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15391" PostId="9897" Score="0" Text="So, if I take a vertex, and assign it a &quot;average&quot; normal which calculate by all the faces it connect, then use this to do the lighting calculation, the result will be more smooth, right? Does tools like blender use vertex normal to make a low poly looks more smooth?" CreationDate="2020-05-29T08:15:41.897" UserId="13466" ContentLicense="CC BY-SA 4.0" />
  <row Id="15392" PostId="9897" Score="0" Text="As your suggestion, can I understand that vertices in graphics (for example glsl) are data somehow related to the geometry points, so it can represent any thing, that why I always see colors put into a VBO?" CreationDate="2020-05-29T08:20:02.563" UserId="13466" ContentLicense="CC BY-SA 4.0" />
  <row Id="15393" PostId="9897" Score="0" Text="First question: I don't know for sure how blender does it since I haven't really worked with it, nor have I looked into the code, but I guess they do it that way. However, calculating the &quot;average normal&quot; is not as trivial as you might think. [See for example this question](https://computergraphics.stackexchange.com/questions/9702/calculate-normals-from-vertices/9705#9705)." CreationDate="2020-05-29T09:14:06.317" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15394" PostId="9897" Score="0" Text="Second question: To my understanding, a vertex is just a data point with arbitrary attributes. Not sure if this is  the correct technical definition. However, as you said, the data can be anything. Everything attribute you add to your vertex will be available in the vertex processing step of the graphics pipeline (assuming you coded it correspondingly). You can use and modify this data there and optionally pass it down the pipeline. For example, the color attribute you mentioned is a common tutorial to render a triangle with smooth color transition. Every vertex there has a different color..." CreationDate="2020-05-29T09:27:16.557" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15395" PostId="9897" Score="0" Text="... and the interpolation happens during rasterization. The fragment shader just passes the unmodified value from the rasterizer to the output." CreationDate="2020-05-29T09:28:26.993" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15396" PostId="9897" Score="0" Text="Thanks so much." CreationDate="2020-05-29T09:30:15.053" UserId="13466" ContentLicense="CC BY-SA 4.0" />
  <row Id="15397" PostId="9897" Score="1" Text="@ravenisadesk If you're interested in this stuff, it might be fun to check out the .OBJ file format for some details on how these things actually get stored. Basically, an .OBJ stores a series of points (as X,Y,Z coordinates), then a series of faces (as triangles between the previously defined points). From those, renderers can pre-calculate the normal vector for each face and, subsequently, each point." CreationDate="2020-05-29T12:02:39.797" UserId="13469" ContentLicense="CC BY-SA 4.0" />
  <row Id="15398" PostId="9897" Score="0" Text="@JoshEller, great suggestion." CreationDate="2020-05-29T14:12:16.493" UserId="13466" ContentLicense="CC BY-SA 4.0" />
  <row Id="15399" PostId="9883" Score="0" Text="Re &quot;I think the story should be like this:...&quot; Yes, the story is exactly that. Those three lines appear in exactly that order in the text." CreationDate="2020-05-30T00:52:15.163" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15402" PostId="9905" Score="0" Text="It depends on your scene, materials and the phenomena you are trying to model. Say you only want to model specular reflections, then 1 ray is enough. Say you want to model diffuse inter-reflections, then you should cast thousands of rays to converge in most cases. An interesting approach would be to cast as many rays as you need until reaching convergance. You can test the convergance per pixel, using the difference between previous and new color. If you are using CPU, this can be the best way. If on GPU, then thread divergance can be a problem..." CreationDate="2020-05-31T01:28:04.737" UserId="13237" ContentLicense="CC BY-SA 4.0" />
  <row Id="15403" PostId="9905" Score="0" Text="I am modeling diffuse inter-reflections. I will add that to the question. I tried detecting convergence by checking the amount of change due to the most recent ray, but it never went above a few hundred rays, even though some areas continued to visibly improve up past 100,000 rays. I'm not using a GPU currently." CreationDate="2020-05-31T05:05:48.907" UserId="13472" ContentLicense="CC BY-SA 4.0" />
  <row Id="15404" PostId="9907" Score="0" Text="doesn't the widget simply live in screen space? i.e after projection of all the other things we simply add the widget into screenspace?" CreationDate="2020-05-31T09:00:51.500" UserId="11386" ContentLicense="CC BY-SA 4.0" />
  <row Id="15405" PostId="9907" Score="0" Text="Think @AverageGatsby is right. After rendering the scene, you can just add one additional render pass to draw the UI without any 3d transformations." CreationDate="2020-05-31T09:31:01.760" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15406" PostId="9888" Score="0" Text="@Peter that does not seem to be the problem for me, I've tried to switch them but they seem correct" CreationDate="2020-05-31T11:04:15.080" UserId="13337" ContentLicense="CC BY-SA 4.0" />
  <row Id="15407" PostId="9888" Score="0" Text="@Isolin thanks I'll try to debug it with that mehod to see what is going on. Do you have any idea of what it could be looking at the above code?" CreationDate="2020-05-31T11:04:59.127" UserId="13337" ContentLicense="CC BY-SA 4.0" />
  <row Id="15409" PostId="9888" Score="0" Text="Unfortunately not yet. It's difficult to find it without being able to step through and see the actual values. The image shows a hard edge in the refraction direction. So I suspect some discontinuous function call perhaps with wrong parameters. For example `sin2ThetaI = max(...)` can be the troublemaker, or not. Just to check: are `n` and `wo` normalized when passed to this method?" CreationDate="2020-05-31T11:29:31.433" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15410" PostId="9905" Score="0" Text="Are you modelling múltiple bounces? In that case you want to use some caching strategy to reduce the calculations, like irradiance caching." CreationDate="2020-05-31T14:01:33.623" UserId="13237" ContentLicense="CC BY-SA 4.0" />
  <row Id="15411" PostId="9905" Score="1" Text="Another thing to analyze is using pixel coherence to improve the calculations. Nearby pixel data (like number of rays until convergance) can help in the calculation of the present pixel" CreationDate="2020-05-31T14:07:11.820" UserId="13237" ContentLicense="CC BY-SA 4.0" />
  <row Id="15412" PostId="9905" Score="1" Text="Be careful with your convergance algoithm. What threshold are you using? Try smaller values. A good thing to so is to use ray buckets to study convergance. For example, you cast 16 rays and test for convergance between current pixel color and the average of the 16 new colors." CreationDate="2020-05-31T14:12:41.660" UserId="13237" ContentLicense="CC BY-SA 4.0" />
  <row Id="15413" PostId="9905" Score="0" Text="For pixel coherence, only the pixel to the left is guaranteed to be available, since the pixel above is rendered by a different thread.&#xA;&#xA;I hadn't thought of doing the convergence test using buckets like you describe. I will try it and see how well it works, thanks." CreationDate="2020-05-31T16:46:12.440" UserId="13472" ContentLicense="CC BY-SA 4.0" />
  <row Id="15414" PostId="9905" Score="0" Text="I am modeling multiple bounces. I plan to implement something like Metropolis light transport or energy redistribution path tracing eventually though, so I'm not sure whether I should write irradiance caching at this point." CreationDate="2020-05-31T16:48:00.213" UserId="13472" ContentLicense="CC BY-SA 4.0" />
  <row Id="15415" PostId="9907" Score="0" Text="Those widgets are 3D, i.e they are placed in the world and live in world space. They however remain constant size in screen dimensions." CreationDate="2020-05-31T18:27:44.297" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15416" PostId="9907" Score="0" Text="I wasn't sure which parts you meant. Thought you are talking about the widgets on the side and referred to them as 3d widgets because they are drawn in the render window. ;)" CreationDate="2020-05-31T21:40:09.520" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15417" PostId="9908" Score="0" Text="I am confused.&#xA;&quot;Scale the object proportional to its depth (z in camera space) and it will retain the same size on screen regardless of its position in world space&quot;&#xA;&#xA;Assume `pos` is the world space position, `vpos` the view space position and `spos` the screen projected position.&#xA;&#xA;I tried  `gl_Position *= spos.z`; `vpos *= vpos.z`; and `gl_Position *= 1.f / gl_Position.z`&#xA;None are giving me a constant size image. Some just make everything disappear    :p" CreationDate="2020-06-01T03:07:05.247" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15418" PostId="9908" Score="0" Text="Scale the object in local space, by the camera z of its world position." CreationDate="2020-06-01T03:42:26.643" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15419" PostId="5633" Score="0" Text="&quot;Just beware of compiling your shaders in release for the intrinsics to work (due to nvidia bug/limitation)&quot; you saved my day, thank you." CreationDate="2020-06-01T09:06:49.503" UserId="13484" ContentLicense="CC BY-SA 4.0" />
  <row Id="15420" PostId="9894" Score="0" Text="Seems to be a general coding question - not 3D graphics.  Perhaps just stackoverflow?" CreationDate="2020-06-01T09:21:27.350" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15421" PostId="9867" Score="2" Text="Estimating depth from monocular images and videos is a quite hot topic. I id not keep an eye on it for a while but if you are looking for similar projects then just https://www.google.com/search?q=depth+from+monocular or https://www.google.com/search?q=depth+from+monocular+video. There are many with code and benchmarks available." CreationDate="2020-06-01T17:28:01.423" UserId="13486" ContentLicense="CC BY-SA 4.0" />
  <row Id="15423" PostId="9912" Score="0" Text="I am not really sure if this is the right place for your question since this site is about generating computer graphics and not about measurement techniques. I don't say that it doesn't fit here, I am just not sure if this community can provide you with an answer." CreationDate="2020-06-02T17:38:30.270" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15424" PostId="9912" Score="0" Text="Keyvan, I suggest to post your question in Radiance forum. Though it's not a specific software question, the Radiance community is likely to give you the answer." CreationDate="2020-06-03T04:18:31.763" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15425" PostId="9912" Score="0" Text="Here: https://discourse.radiance-online.org/" CreationDate="2020-06-03T04:18:43.790" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15426" PostId="9915" Score="0" Text="I would imagine that state changes in the GPU are at least an order of magnitude faster compared to the old OpenGL way, correct me if I'm wrong. I make this question in reference to how easily you can attach materials and shaders to your models in the modern versions of unity and unreal engine - almost like the cost to manage those is nil. Regardless, your reply is pointing to further study on vulkan on my part." CreationDate="2020-06-04T18:13:36.623" UserId="13449" ContentLicense="CC BY-SA 4.0" />
  <row Id="15427" PostId="9916" Score="1" Text="Is there a way to get context for this question without having to watch a lengthy video?" CreationDate="2020-06-05T16:36:54.620" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15428" PostId="9916" Score="0" Text="@NicolBolas The link should link to the place in the video where the quote is said. You could skip back by a few minutes, but I'm not sure there is much context, other than to make the point that power usage by a GPU is not always intuitive." CreationDate="2020-06-06T18:56:41.950" UserId="13497" ContentLicense="CC BY-SA 4.0" />
  <row Id="15429" PostId="9916" Score="0" Text="Actually, my question was more what is 'dense' and what is 'simple' in this context?" CreationDate="2020-06-06T19:20:39.197" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15430" PostId="9916" Score="0" Text="@NicolBolas Ah, right. I think he was talking about a few, large triangles compared to many small triangles. His example for simple geometry was a map screen, which would have a few large triangles with textures." CreationDate="2020-06-06T19:23:57.563" UserId="13497" ContentLicense="CC BY-SA 4.0" />
  <row Id="15431" PostId="9920" Score="0" Text="&quot;*thus the maximum ubo memory is not exceeded.*&quot; That's not a UBO; your `globalShaderData` variable isn't even a uniform." CreationDate="2020-06-08T04:03:53.760" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15432" PostId="9922" Score="0" Text="&quot;*with two shader storage buffers*&quot; OpenGL has vertex attribute arrays; why would you use SSBOs for something so trivial?" CreationDate="2020-06-08T17:48:42.650" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15433" PostId="9922" Score="1" Text="You cannot use vertex attribute arrays with mesh shaders. And I wanted to make sure the buffers are ok before starting with mesh shaders. And again: it‘s for learning." CreationDate="2020-06-08T18:03:44.747" UserId="13515" ContentLicense="CC BY-SA 4.0" />
  <row Id="15434" PostId="9923" Score="1" Text="It is a 2D array containing the color values: an image. You can present it on screen through various methods, a special case being drawing it as an OpenGL texture." CreationDate="2020-06-09T05:41:50.990" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15435" PostId="9920" Score="0" Text="That is true. It is the c++ side structure, I should have made it more clear." CreationDate="2020-06-09T10:21:14.803" UserId="10188" ContentLicense="CC BY-SA 4.0" />
  <row Id="15436" PostId="9925" Score="0" Text="I have little experience in graphics programming but seems a little hacky. It also seems easy so i guess its worth a try. I wonder what performance will it have. One critical component must be only updating the changed pixels instead of the whole canvas for each frame." CreationDate="2020-06-09T12:34:56.660" UserId="13517" ContentLicense="CC BY-SA 4.0" />
  <row Id="15437" PostId="9926" Score="0" Text="Here is a link to a related question: [click me](https://computergraphics.stackexchange.com/questions/9733/bresenham-circle-drawing-algorithm-compute-the-distance/9735#9735) - In my answer, I also linked a tutorial. Maybe that helps." CreationDate="2020-06-09T12:38:48.883" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15438" PostId="9925" Score="0" Text="The main performance problem in OpenGL is the texture update on the GPU side. But there exist some fast routes (search for &quot;OpenGL buffer streaming&quot; or &quot;OpenGL Approaching zero driver overhead&quot;). However, if you are just doing a regular paint program, this is not worth optimizing since you have &quot;plenty&quot; of time.  Also, I wouldn't call it hacky since it is a perfectly valid usage of the render pipeline. - Updating single pixels is also not that much of a problem because there are special OpenGL functions for that." CreationDate="2020-06-09T12:49:02.970" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15439" PostId="9926" Score="0" Text="Have you considered working with 2D bounds for each quadrant? Generating those for every possible case will involve some logic (more than I have time to hash out in a full answer) but only simple integer math." CreationDate="2020-06-09T13:59:05.780" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15440" PostId="9926" Score="0" Text="@Olivier Sorry, I don't understand, how would it help me to determine which pixels I must draw?" CreationDate="2020-06-09T14:12:49.003" UserId="13522" ContentLicense="CC BY-SA 4.0" />
  <row Id="15441" PostId="9926" Score="1" Text="By only drawing the pixels inside the 2D bounds. Try splitting your circle in 4 quadrants. Draw a random arc on the circle. Now draw a tight box around the part of the arc in each quadrant. You'll see that you can fully define any arc that way. And the coordinates of the boxes all correspond to your circle center, center +/- radius or start/end point. You just need the logic to pick the right numbers for each case." CreationDate="2020-06-09T14:37:15.323" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15442" PostId="9926" Score="0" Text="@Olivier Hmmm, ok, I think I start to understand it... I'm going to do some visual tests and see if I understood everything to code it. Thanks a lot!" CreationDate="2020-06-09T14:42:55.817" UserId="13522" ContentLicense="CC BY-SA 4.0" />
  <row Id="15443" PostId="9931" Score="0" Text="In that demo they mention, that it is part of their new &quot;Nanite&quot; technology for Mesh rendering, so I doubt you will find any resources just yet. UE4 is Open Source however, and in general they publish some of their techniques, so probably you will get some info next year or the year after." CreationDate="2020-06-10T12:50:33.010" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="15444" PostId="9931" Score="0" Text="Yes, but there must be some papers about it. I don't think that they built this &quot;Nanite&quot; engine without any prior research." CreationDate="2020-06-10T12:54:47.697" UserId="7028" ContentLicense="CC BY-SA 4.0" />
  <row Id="15445" PostId="9931" Score="2" Text="I didn't say they did. Brian Karis mentioned on twitter (https://twitter.com/BrianKaris/status/1260590413003362305), how long it took him/them from the first thought. However from what I read online, they don't share the technique just yet. There have been a lot of discussions on how they did it, but no answer. See for example this conversation between Rune Stubbe and Paul Malin: https://twitter.com/Stubbesaurus/status/1261648521431785475" CreationDate="2020-06-10T12:58:49.030" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="15449" PostId="9934" Score="0" Text="Sorry if my question wasn't clear, I need to draw an arc, not a circle, a segment of the circle. I already have implemented the circle function using the Bresenham's algorithm using integer math and no square root nor floating point operations. I have the start point and end point of the arc in the circle and the problem I face is how to determine if a pixel of the circle belongs to the arc segment to draw it or skip it. As I state in the question the Bresenham's algorithm uses octants (like your answser) what makes difficult to determine it." CreationDate="2020-06-10T20:08:33.127" UserId="13522" ContentLicense="CC BY-SA 4.0" />
  <row Id="15451" PostId="9934" Score="0" Text="I'm thinking that for any two points on the circumference of a circle, you can draw a bounding rectangle that encloses the minor arc. You can use this bounding rectangle to mask the arc segment you want. E.g., if you want the minor arc, only plot points inside the rectangle and if you want the major arc, only plot points outside the rectangle. The only issue with this technique is if the points are on a diameter. Then a corner of your bounding rectangle will touch the circle." CreationDate="2020-06-11T00:21:08.963" UserId="13524" ContentLicense="CC BY-SA 4.0" />
  <row Id="15452" PostId="9934" Score="0" Text="More or less that's what I'm testing as Oliver already gave me the idea of using bounding boxes, but instead of many boxes only one per quadrant." CreationDate="2020-06-11T00:24:06.547" UserId="13522" ContentLicense="CC BY-SA 4.0" />
  <row Id="15453" PostId="4197" Score="0" Text="For future readers this issue was continued at: https://computergraphics.stackexchange.com/questions/4269/setting-up-integer-texture-and-binding-as-image-pt-2" CreationDate="2020-06-11T08:29:09.300" UserId="13536" ContentLicense="CC BY-SA 4.0" />
  <row Id="15454" PostId="9935" Score="0" Text="I don't fully understand your question. You have a line segment and you are trying to use a fragment shader to display kind-of a clipped distance field on the mesh surface?" CreationDate="2020-06-11T10:29:40.110" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15455" PostId="9935" Score="0" Text="Thanks for your answer. I have a line segment and I compute the surface intersection closest point using the cylinder fragment shader. Using this closest segment point I compute a sphere distance field. Everything is done in world space." CreationDate="2020-06-11T11:02:44.443" UserId="2372" ContentLicense="CC BY-SA 4.0" />
  <row Id="15456" PostId="9935" Score="0" Text="Should there be a black circle displayed also when there is no true intersection but only strong proximity?" CreationDate="2020-06-11T11:04:39.113" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15457" PostId="9935" Score="0" Text="I'm not sure to understand, the main issue is the deformation of the computed length which should be uniform. I guess that there should be a black circle when there's no true intersection because it illustrates the distance but it should be uniform." CreationDate="2020-06-11T11:09:54.760" UserId="2372" ContentLicense="CC BY-SA 4.0" />
  <row Id="15458" PostId="9935" Score="0" Text="Got it, I will try to answer the question accordingly." CreationDate="2020-06-11T11:13:10.580" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15460" PostId="9935" Score="0" Text="Thanks, impatient to read your thoughts." CreationDate="2020-06-11T11:26:51.693" UserId="2372" ContentLicense="CC BY-SA 4.0" />
  <row Id="15461" PostId="9936" Score="1" Text="Notice, that the same question is also on the game development site: [link](https://gamedev.stackexchange.com/questions/183427/closest-sphere-on-segment). I wasn't sure how to handle this, so I found this [Meta question](https://meta.stackexchange.com/questions/172307/duplicate-questions-on-other-se-sites) - Long story short: Maybe you could provide your answer there too by copying or linking it (Don't want to farm reputation that you earned ;) )." CreationDate="2020-06-11T12:15:28.163" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15462" PostId="9936" Score="1" Text="Thank you :) Haha it automatically converted it to a comment stating that it is trivial :)" CreationDate="2020-06-11T12:25:05.517" UserId="13370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15463" PostId="9937" Score="2" Text="To get a better answer (or one at all), you should probably provide a little bit more detail about how exactly the matrix entries are calculated since most people here have no background in radiation therapy. Which techniques are used? Which formulas? What are the dependencies of the matrix entries?" CreationDate="2020-06-11T13:49:14.743" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15464" PostId="9934" Score="0" Text="I came up with another, perhaps better, idea this morning. You can use arc lengths. Math nerds use radians to measure angles because radians directly correspond to arc lengths. For this problem we can measure arc length in pixels to be set and pixels to skip. I may work this up as an answer today if I get a chance." CreationDate="2020-06-11T15:47:23.537" UserId="13524" ContentLicense="CC BY-SA 4.0" />
  <row Id="15466" PostId="9932" Score="1" Text="Yes, using a game engine to write a small application could be the simplest solution. For example Unity has APIs to create meshes from vertex/index buffers https://docs.unity3d.com/2020.1/Documentation/ScriptReference/Mesh.SetVertexBufferData.html and other APIs to render them." CreationDate="2020-06-12T15:44:56.113" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="15467" PostId="9915" Score="1" Text="@GnoSiS just because the software (API, game engine...) changed, does not mean your hardware (GPU, CPU... ) necessarily changed the same way. Asking processors to do less things, will always be faster than asking them to do more of the same things." CreationDate="2020-06-12T15:51:56.970" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="15468" PostId="9942" Score="0" Text="Are you after surface reconstruction? In that case to use PCA I suggest looking at Hoppe paper here" CreationDate="2020-06-13T00:27:04.683" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15469" PostId="9942" Score="1" Text="hhoppe.com/recon.pdf" CreationDate="2020-06-13T00:27:10.320" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15470" PostId="9939" Score="0" Text="Nice list. I experimented with CGAL last year, but  yes, distortions are a problem." CreationDate="2020-06-13T17:48:57.580" UserId="8314" ContentLicense="CC BY-SA 4.0" />
  <row Id="15471" PostId="9939" Score="0" Text="I've been using these quite a lot lately. Which algorithm does CGAL implement?" CreationDate="2020-06-13T17:54:45.290" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="15472" PostId="9939" Score="0" Text="https://doc.cgal.org/latest/Surface_mesh_parameterization/index.html" CreationDate="2020-06-13T18:03:38.180" UserId="8314" ContentLicense="CC BY-SA 4.0" />
  <row Id="15473" PostId="9939" Score="0" Text="Ah ok, some of the ones I mentioned ate there as well, good to know. Do you happen to know what sort of license they have? Is it GPL or MIT, or something more commercial." CreationDate="2020-06-13T19:00:46.503" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="15474" PostId="9939" Score="0" Text="It is really complicated: https://www.cgal.org/license.html" CreationDate="2020-06-14T08:24:27.580" UserId="8314" ContentLicense="CC BY-SA 4.0" />
  <row Id="15479" PostId="9946" Score="0" Text="In mathematics and physics it's common practice to denote the standard spatial coordinates in 3D with $x,y,z$. Similarly, if you study surfaces (especially in differential geometry) people usually use $u,v$, or $s,t$, or $\alpha, \beta$ for the surface parametrisation. In CG $u,v$ seems to have stuck around, but those are simply variables used to describe the coordinates in texture space in this instance. If you want there to be no ambiguity you should call them texture coordinates." CreationDate="2020-06-16T18:10:56.833" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15480" PostId="9946" Score="0" Text="Further reading: [UV mapping](https://en.wikipedia.org/wiki/UV_mapping). This concept is used in radar, too, to [map angular coordinates to a rectangular plane](https://www.mathworks.com/help/phased/ref/azel2uv.html#bs9sl0i-1)." CreationDate="2020-06-17T01:25:32.717" UserId="13563" ContentLicense="CC BY-SA 4.0" />
  <row Id="15481" PostId="9941" Score="1" Text="Fair points on most of my concerns, that more or less I already knew. On your last point though I really meant VBOs - as the shader parameters are passed down per vertex at first and then per fragment - how else can you have different shader/rendering per polygon/pixel?" CreationDate="2020-06-17T09:11:32.497" UserId="13449" ContentLicense="CC BY-SA 4.0" />
  <row Id="15482" PostId="9915" Score="1" Text="@wip true, but the design &quot;distance&quot; in years between vulkan and opengl is huge, so this implies a radically different aproach. I have no reference point on how faster is managing state changes in the GPU vs the CPU. I see people &quot;preaching&quot; vulkan alll around, but need to get the actual details right, before making decisions." CreationDate="2020-06-17T09:16:21.867" UserId="13449" ContentLicense="CC BY-SA 4.0" />
  <row Id="15483" PostId="9941" Score="0" Text="Very often Vertex Buffers contain the same kind of information for all objects in the scene (Position, Texture UV coordinates, Normal), and this content type is the same for all shaders in the scene. Most often what makes the materials look different, is the set of parameters passed by Uniform Buffer. Are you passing a lot of additional data in your VBO? It might be possible to optimize that." CreationDate="2020-06-17T22:07:34.600" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="15484" PostId="9915" Score="0" Text="Vulkan is a standardized API, how fast it is in practice depends on how well GPU vendors implemented it in their drivers (and how well it matches their hardware design). A few years ago it was not rare to have OpenGL faster than Vulkan on some mobile phones. Hopefully the situation has improved now (the hardware also changed since then)." CreationDate="2020-06-17T22:12:27.283" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="15485" PostId="9915" Score="0" Text="@GnoSiS: &quot;*so this implies a radically different aproach*&quot; It shouldn't. The API tells you (more or less) what is fast and slow. Slow things look slow, and fast things look fast. And no matter how &quot;radically different&quot; something might get, basic advice like &quot;don't repeat stuff you don't have to&quot; should be standard." CreationDate="2020-06-17T22:27:29.440" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15486" PostId="9949" Score="0" Text="I would suggest using an OpenGL texture and using glTexImage2D to update the contents. Then you can render it as a single full screen quad. That would be straight forward enough to implement and performance should be good. If you don't already have it covered I would recommend SDL to get up an running quickly, that will take care of window setup for you." CreationDate="2020-06-18T04:59:09.610" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15488" PostId="9948" Score="0" Text="I think this is the wrong community for your question since it is not really about computer graphics. I think [Game Development](https://gamedev.stackexchange.com/) might be better suited for your question." CreationDate="2020-06-18T10:52:16.767" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15489" PostId="9948" Score="0" Text="Thanks a lot for your reply! Maybe I should move it to GameDev StackExchange. Btw, does physical-based simulation belong to Computer Graphic Communnity? :O" CreationDate="2020-06-18T11:34:52.530" UserId="11814" ContentLicense="CC BY-SA 4.0" />
  <row Id="15490" PostId="9948" Score="0" Text="I don't think so. See [this link](https://computergraphics.stackexchange.com/help/on-topic). This site is mainly about ... well, computer graphics. So if your question would be about turning a simulation result or animated character into a visual representation, that would be a topic for this community. But purely simulation/animation related questions are discussed in other StackExchange network sites." CreationDate="2020-06-18T11:58:29.027" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15491" PostId="9948" Score="0" Text="Thanks a lot for your patient explanation! Though I still felt a little hard to understand the exactly difference between them, I still will move this question to another place. :D" CreationDate="2020-06-18T12:15:11.367" UserId="11814" ContentLicense="CC BY-SA 4.0" />
  <row Id="15492" PostId="9954" Score="1" Text="I don't really get what exactly you are asking. &quot;How does it work&quot; or &quot;where is the error in my code&quot;? Also, what results do you expect and which ones do you get? In an orthographic projection, all rays are parallel to the cameras' viewing direction. So you basically just need to find the position on the near-plane that corresponds to your cursor position and fire a ray into the cameras view direction." CreationDate="2020-06-19T10:18:53.290" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15493" PostId="9954" Score="0" Text="@wychmaster &quot;So you basically just need to find the position on the near-plane that corresponds to your cursor position&quot; That's what I tried to do in the second code snippet. The results I'm getting is that the calculated cursor position doesn't match the position of the cursor." CreationDate="2020-06-19T10:22:09.970" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15494" PostId="9941" Score="0" Text="No, parameters are different, even for the same shaders, but for the most part they It could be the same per object/component being drawn. Changing the Uniform buffer before each draw seens even more expensive, if each draw would just do a single mesh - that's like having 1000s of draws instead of just 2-5." CreationDate="2020-06-19T10:35:17.163" UserId="13449" ContentLicense="CC BY-SA 4.0" />
  <row Id="15495" PostId="9954" Score="0" Text="Have a look into my answer in [this question](https://gamedev.stackexchange.com/questions/183196/calculating-directional-shadow-map-using-camera-frustum/183199#183199) It is basically the same problem, just that you are looking for a point on the near plane (`z=-1`) and you use a different projection. As far as I can see, the vector `cScreen`  in your second code snipped needs a `w` component of `1` and not 0. Don't know if that already fixes the problem. You might also need to divide the result vector `cView`  by `w`" CreationDate="2020-06-19T11:42:45.850" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15496" PostId="9954" Score="0" Text="@wychmaster Appreciate it. Unfortunately changing w or dividing by w gives the same result. With my implementation x and y coordinates do get offset in the direction of the cursor but by much larger amount. So x and y maybe need to be divided by something. Dividing by w doesn't do anything though." CreationDate="2020-06-19T13:47:41.047" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15497" PostId="9956" Score="2" Text="&quot;*using them for deferred shading is an obvious, arguably common, use-case that I would expect to work well*&quot; Is it? I've never heard of it. There's little reason to use a CS for this when the CS and FS both have the same expressive power *and* the FS has access to blending operations, depth testing, and other useful functionality that can be used to improve overall rendering performance. Basically, the CS is never going to be *faster* than the FS, and the FS can do everything the CS can do, so why bother?" CreationDate="2020-06-19T14:48:05.300" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15498" PostId="9956" Score="0" Text="@NicolBolas As in the linked ref., the CS has _way_ more expressive power than the FS, and since it bypasses the entire graphics pipeline above and the ROPs below, it _can_ theoretically be faster. Indeed, in this very shader, the CS _is_ ≈10% faster if I replace these texture accesses with an expensive BRDF that touches no textures. As to why you'd prefer the CS: if that isn't enough, it's also _much_ cleaner to set up, and the CS's added flexibility allows you to subsequently _extend_ the FS's functionality after you've replicated it." CreationDate="2020-06-19T15:43:39.730" UserId="523" ContentLicense="CC BY-SA 4.0" />
  <row Id="15499" PostId="9956" Score="0" Text="&quot;*since it bypasses the entire graphics pipeline above and the ROPs below, it can theoretically be faster.*&quot; The cost of a graphics operation will be the cost of the slowest stage of that operation. Vertex processing will involve very few vertices, and ROP operations won't matter next to all of the FS processing. So the overall performance will be dominated by the slowest step: the fragment shader. Maybe if your FS is tiny you might get some performance benefit from a CS, but in general, no. Not for deferred rendering." CreationDate="2020-06-19T15:56:03.437" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15500" PostId="9956" Score="0" Text="&quot;*the CS has way more expressive power than the FS*&quot; Expressive power that just doesn't matter for the specific case of a deferred renderer. Shared data storage and invocation intercommunication is irrelevant, as deferred lighting doesn't need intercommunication with other invocations (post-effects like SSAO, blur effects, and the like are not specific to deferred rendering and thus don't count here). And control of the work group size is likewise immaterial. So whatever expressive power exists is kind of irrelevant." CreationDate="2020-06-19T15:59:07.133" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15501" PostId="9956" Score="0" Text="Lastly, remember that deferred rendering means ultimately blending with the currently accumulated lighting value in the framebuffer. That means doing a read/modify/write. An operation that needs to be *atomic*. That's not really possible with image load/store or SSBO operations (not on 4-element vector floating-point values). Whereas with ROPs, you get it for &quot;free&quot;." CreationDate="2020-06-19T16:01:49.943" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15502" PostId="9956" Score="4" Text="@NicolBolas You're shifting goalposts and being dismissive. And, while I actually do agree with many of the things you're saying, provided they were rephrased more carefully and as general heuristics, the question isn't about that. I don't care to get into it, and that's not what the comments section is for anyway . . ." CreationDate="2020-06-19T16:32:58.337" UserId="523" ContentLicense="CC BY-SA 4.0" />
  <row Id="15503" PostId="9958" Score="1" Text="Thank you so much!!!! This helps quite a bit." CreationDate="2020-06-19T22:07:08.833" UserId="13575" ContentLicense="CC BY-SA 4.0" />
  <row Id="15504" PostId="9957" Score="0" Text="It might be worth adding that we also call boundary representation models sold when they are enforced to be closed. They are still hollow though." CreationDate="2020-06-20T06:41:15.790" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15508" PostId="9958" Score="1" Text="The main problem with volume (voxel) data is, that with growing size of your object, you need more and more voxels (power of 3), especially when you want to maintain some precision. With BREP (boundary representation) you save memory space. So I guess for representing your archeological site you will choose BREP (some even choose a colored point cloud), while skull data of a mummy should be a voxel-based CT scan which you still can convert to a BREP (via iso-surface calculation) for a VR visualization." CreationDate="2020-06-21T16:51:45.467" UserId="8314" ContentLicense="CC BY-SA 4.0" />
  <row Id="15509" PostId="9963" Score="1" Text="Write out formally the formulae you use to construct the view and projection matrices." CreationDate="2020-06-21T19:22:34.050" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15510" PostId="9963" Score="0" Text="@lightxbulb I added the formulae." CreationDate="2020-06-21T20:17:20.407" UserId="13581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15511" PostId="9963" Score="2" Text="You're missing a minus in your view matrix. Just make the view matrix the inverse of the camera matrix. Also, your projection matrix flips the coordinate system so your camera starts looking in the opposite direction. Try fixing the missing sign first." CreationDate="2020-06-21T20:21:45.200" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15512" PostId="9963" Score="0" Text="@lightxbulb The minus sign in the camera matrix is missing, because *d* is defined to point in the opposite direction as usual; for example this [site](https://learnopengl.com/Getting-started/Camera) defines it the opposite way. The minus sign in the first and second component remain, because I also flipped the order of the crossproducts. I think that the matrix should work..." CreationDate="2020-06-21T20:50:29.927" UserId="13581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15513" PostId="9963" Score="2" Text="They did not make P_z positive when computing the view matrix. Rather than flip cross products, correctly set those up so no sign flips would need to leak into your matrices later on." CreationDate="2020-06-21T20:54:30.600" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15517" PostId="9961" Score="0" Text="You do not necessarily need raytracing to simulate wet surfaces. It is certainly the most realistic approach, but the important factor here is the reflectivity of the material. A water film on skin has a mirror-like reflection behavior, while the microstructure of the skin scatters light into all directions. So pure skin looks much duller than wet skin. In a Phong model you would just modify the specular term of wet skin to get a more reflective surface. In physically-based rendering approaches, you would decrease the microfacet roughness (not sure if this is the right term ;) )" CreationDate="2020-06-21T22:07:32.893" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15518" PostId="9961" Score="0" Text="There are also special techniques to simulate the &quot;partial transparency of skin&quot;. Just search for &quot;subsurface scattering&quot;. You might want to take a look into the rendering section of this [wikipedia articel](https://en.wikipedia.org/wiki/Subsurface_scattering) to get started." CreationDate="2020-06-21T22:11:32.610" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15519" PostId="9963" Score="1" Text="The projection matrix is correct. I checked it and got the same results with some minor floating point variations. @lightxbulb is right about the wrong sign." CreationDate="2020-06-21T22:25:19.653" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15520" PostId="9964" Score="0" Text="http://hhoppe.com/proj/geodesics/" CreationDate="2020-06-21T22:44:51.290" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15521" PostId="9963" Score="0" Text="@lightxbulb I see what you mean. Thank you, that fixed it." CreationDate="2020-06-21T22:52:30.993" UserId="13581" ContentLicense="CC BY-SA 4.0" />
  <row Id="15522" PostId="9963" Score="1" Text="Write an answer to it yourself and self-accept it so this doesn't have to stay open." CreationDate="2020-06-22T00:09:39.377" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15523" PostId="9961" Score="0" Text="I am not trying to render skin here but some organs like the brain/liver. SSS approximates material properties but what I need is surface scattering approximation." CreationDate="2020-06-22T05:11:10.263" UserId="10370" ContentLicense="CC BY-SA 4.0" />
  <row Id="15526" PostId="9969" Score="0" Text="This is what you should read: http://www.eugenedeon.com/project/importance-sampling-microfacet-based-bsdfs-using-the-distribution-of-visible-normals/" CreationDate="2020-06-22T19:52:41.337" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15527" PostId="9969" Score="0" Text="And this. https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html" CreationDate="2020-06-22T21:51:39.160" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="15528" PostId="9974" Score="1" Text="Welcome to CGSE. The question wasn't about how anti-aliasing works but which techniques are used/most suitable for font rendering." CreationDate="2020-06-23T20:27:45.763" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15529" PostId="9918" Score="0" Text="I only skimmed through the paper. Is the problem to express $S(v;z_1,z_2)$ as a &quot;smooth function&quot; $S(v,\tilde z)$ for some $\tilde z$? This is in general not possible as the product of two such smooth functions contains quadratic terms in $v$. If this is not the issue, can you expand on your question by stating the mathematical problem explicitly? Why do you think that the product of two ASGs is another ASG?" CreationDate="2020-06-23T21:01:56.480" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15530" PostId="9918" Score="0" Text="&quot;ASGs is another ASG&quot; Because the product of 2 Gaussian distributions in the euclidean case is another Gaussian distribution. I know that this does not guarantee that it will be the case for the spherical case too but that's why I had the hunch." CreationDate="2020-06-23T21:42:48.000" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15531" PostId="9918" Score="0" Text="As a mathematician I find the terminology used in the paper quite confusing. But as far as I can see the authors do not claim that the product of two ASGs is an ASG. In fact, $\max(v,0)\cdot\max(v,0) = v^2$ for all $v\ge 0$ and there is no way you could express this as &quot;$\max(v,\tilde z)$ times some exponential term&quot;." CreationDate="2020-06-23T22:43:26.773" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15532" PostId="9918" Score="0" Text="$v\cdot v = ||v||^2 = 1$ btw, remember these are vectors not scalars.&#xA;&#xA;More importantly, numerical simulations and a visualization of the result strongly suggest that the product of 2 ASG's is either an ASG or something very very close to an ASG. Consider the figures in this question:&#xA;https://math.stackexchange.com/questions/3704580/finding-a-new-lobe-in-the-product-of-2-anisotropic-gaussians" CreationDate="2020-06-23T23:04:54.603" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15533" PostId="9918" Score="0" Text="Without additional assumptions on $z_1,z_2$ and $v$ it's easy to construct a counterexample for the three dimensional case as well. Consider, for example, $z_1 = &lt;1,0,0&gt;$ and $z_2=&lt;0,1,0&gt;$." CreationDate="2020-06-24T11:34:24.750" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15534" PostId="9918" Score="0" Text="(In fact, even $z_1=z_2=&lt;1,0,0&gt;$ works as a counterexample)" CreationDate="2020-06-24T11:37:20.653" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15535" PostId="9975" Score="0" Text="I am not a DirectX user, but I guess the problem here is probably that you used a pointer to some allocated memory (`new`/`delete`) as `veBuffer`. Have a look at into [this question](https://stackoverflow.com/questions/36973045/c-basics-ranged-based-for-loop-and-passing-c-style-arrays-to-functions). So using a regular for loop (`for (int i=0; i&lt;SIZE, ++i)`) should already fix your problem." CreationDate="2020-06-24T18:47:15.980" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15536" PostId="9975" Score="0" Text="Alternatively, you can use a `std::vector` as `veBuffer`, resize it to the necessary buffer size and pass the data pointer to your DirectX function with `veBuffer.data()`." CreationDate="2020-06-24T18:52:14.523" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15541" PostId="9876" Score="0" Text="The $Tv$ version should be faster just off the number of ops required (3*16 mults and 3*12 adds vs 2*64 + 16 mults and 2*48+12 adds). Since the operator associativity is left to right in glsl you can use $vT_0^TT_1^TT_2^T$ if you do not want to explicitly write parentheses, which will yield exactly the same result (up to round off error) as $T_2T_1T_0v$. You can send in your matrices pre-transposed. As already mentioned sending a premultiplied single matrix is even better (in your case it reduces the number of ops ~3 times)." CreationDate="2020-06-25T10:06:49.343" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15542" PostId="9876" Score="0" Text="@lightxbulb I know about this possibility and I am honestly considering it. However, since I have a strong engineering background I am used to the column vector (left multiply) notation and I am currently trying to figure out what bothers me more: Using a &quot;weird&quot; notation or &quot;spamming&quot; parenthesis. Thx for your input." CreationDate="2020-06-25T10:36:59.087" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15544" PostId="9978" Score="0" Text="&quot;*the application's off-screen buffer the same as its 'screen buffer'*&quot; I think you misusued your own terminology here. You defined the &quot;off-screen buffer&quot; as being the front buffer, which is *not* what you render to. You render to the back buffer. So this would not be &quot;the same location in memory that fragments had their colors originally blended onto&quot;. So what exactly are you looking for here?" CreationDate="2020-06-25T16:52:47.183" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15549" PostId="9978" Score="0" Text="@NicolBolas Yes, it is my understanding, too, that the back buffer is rendered to (and not the front buffer). I found on [wikipedia](https://en.wikipedia.org/wiki/Multiple_buffering) that 'Software double buffering' copies the back buffer's content to the front buffer. However, it notes compositors can copy directly from the back buffer, thus considering **it** the 'off-screen buffer' instead of the front buffer. Does it simply copy from back buffer to front buffer in full-screen mode? If so, my main question is answered with 'no' for software double buffering. But, what about 'page flipping'?" CreationDate="2020-06-26T08:08:14.793" UserId="13601" ContentLicense="CC BY-SA 4.0" />
  <row Id="15550" PostId="9978" Score="0" Text="@NicolBolas In 'page flipping' the back buffer is rendered to, but this same location is **marked** as the front buffer when rendering is finished. Thus the front buffer is both the original location fragments were blended to and the location seen as 'off-screen buffer' by a compositor. But if the OpenGL application is in full-screen mode... could the compositor not also mark this original buffer rendered to as both the 'off-screen buffer' and the 'screen buffer'? If yes, then the answer to my original question is 'yes'." CreationDate="2020-06-26T08:22:10.530" UserId="13601" ContentLicense="CC BY-SA 4.0" />
  <row Id="15551" PostId="9981" Score="0" Text="Correct me if I am wrong, but that means that the correct interpretation is that the models live in world space to begin with and are instead transformed (&quot;deformed&quot;) with the `modelToWorld` matrix, instead of actually changing coordinate systems. As far as the normals go, I have no issue with that - they can be interpreted as covectors (up to a multiplicative factor) and thus do not require the metric tensor to lower the index." CreationDate="2020-06-27T13:14:49.070" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15552" PostId="9981" Score="0" Text="I don't know if it makes sense to have a sharp semantic distinction between &quot;transformations&quot; and &quot;changing coordinate basis&quot;. It very often makes sense to think of meshes, or individual triangles, etc as having local coordinates. We just avoid evaluating inner products in those coordinates." CreationDate="2020-06-27T15:56:23.063" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15553" PostId="9981" Score="0" Text="Coordinate system change implies that the metric tensor changes. If we go from a basis A to a basis B with a map F, then the metric tensor changes as: $g_{ij}' = \sum_{k,l}F^k_iF^l_jg_{kl}$, which affects how the inner product is computed in the new basis. Instead if we do not interpret this as acting on the basis, but rather only on the components of vectors (that is, the basis is preserved but points are translated, rotated, scaled), the inner product does not change (which seems to be the approach most often considered in CG)." CreationDate="2020-06-27T16:28:59.473" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15554" PostId="9982" Score="0" Text="I skimmed the paper, and it wasn't clear to me either. On a side note, this is something you might be interested in: From &quot;Fast Exact and Approximate Geodesics on Meshes&quot;: &quot;Unfortunately, as far as we know the MMP algorithm has not been implemented previously and thus has not made its way into practice.&quot;." CreationDate="2020-06-27T19:47:02.967" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15555" PostId="9982" Score="1" Text="Well that paper implemented it, and as far as I know there are 2 open source implementations of the algorithm, one seems to be based on the other:&#xA;Here's one:&#xA;https://github.com/mojocorp/geodesic" CreationDate="2020-06-27T20:37:05.360" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15556" PostId="9986" Score="0" Text="Read &quot;[Shirley, 1996] Monte Carlo Techniques for Direct Lighting Calculations&quot;." CreationDate="2020-06-29T16:50:43.860" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15557" PostId="9988" Score="0" Text="How do you change the location of the triangles then?" CreationDate="2020-06-29T18:12:53.493" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15558" PostId="9988" Score="0" Text="@lightxbulb , I use model matrix to change the location of the triangle. The algorithm is using the vertices of this triangle. When I change the location of triangle, the input vertices of the algorithm should also change according to new location of the triangle. I am asking for this change, I am not asking how to change the location of the triangle on the screen. Sorry if it was ambiguous." CreationDate="2020-06-29T18:18:33.243" UserId="13625" ContentLicense="CC BY-SA 4.0" />
  <row Id="15559" PostId="9988" Score="1" Text="To change the location of the triangle you multiply the vertices with the model matrix. That's it really - the new location is what you get from the matrix-vector product." CreationDate="2020-06-29T19:03:15.587" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15560" PostId="9988" Score="0" Text="@lightxbulb I don’t understand what you mean. How can I use a matrix as an vector in 3D? Can you be more specific if it is possible?" CreationDate="2020-06-29T21:22:49.563" UserId="13625" ContentLicense="CC BY-SA 4.0" />
  <row Id="15561" PostId="5665" Score="0" Text="Would a read only float buffer be faster than a texture?" CreationDate="2020-06-30T02:59:24.467" UserId="10332" ContentLicense="CC BY-SA 4.0" />
  <row Id="15562" PostId="9988" Score="1" Text="A matrix-vector product yields a vector. If a vertex has position $(x,y,z)$ and your model matrix is M, then make a 4d vector $v=(x,y,z,1)$ and then compute $v' = Mv$. The first 3 components of $v'$ are the coordinates of $v$ transformed with $M$. If you do that for the 3 vertices you can compute the new locations." CreationDate="2020-06-30T05:08:24.217" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15563" PostId="9989" Score="2" Text="It's possible that it is something specific to marching cubes. Looks like aliasing or ringing." CreationDate="2020-06-30T12:55:44.770" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15564" PostId="9989" Score="0" Text="I was thinking that too. So I ran Dual Marching Cubes and SurfaceNets on a voxelized sphere (also with Gaussian smoothing) but these rings kept appearing (although less apparent for SurfaceNets)" CreationDate="2020-06-30T14:01:12.020" UserId="13632" ContentLicense="CC BY-SA 4.0" />
  <row Id="15565" PostId="9989" Score="4" Text="Try ray marching this with a high resolution (zoom into the problematic region). If the rings disappear you know it's the discretization. If they do not disappear, then it's a feature if your surface." CreationDate="2020-06-30T14:39:57.760" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15566" PostId="9988" Score="1" Text="Thank you very much! That was exactly answer of my question. Can you please send it as an answer so I select it ? It might help people with similar question." CreationDate="2020-06-30T15:26:59.280" UserId="13625" ContentLicense="CC BY-SA 4.0" />
  <row Id="15567" PostId="9986" Score="0" Text="http://www.pbr-book.org/3ed-2018/Color_and_Radiometry/Working_with_Radiometric_Integrals.html" CreationDate="2020-07-01T10:50:20.997" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15568" PostId="9986" Score="0" Text="Also, I think you need to account for the probability of picking that triangle. I believe that the first equation you list is incorrect. You can't just add the direct and indirect samples together because they are from two different sampling schemes. You should look at multiple importance sampling which is a technique for combining sampling methods together. If you want help with implementing that don't hesitate to ask." CreationDate="2020-07-01T10:50:57.473" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15569" PostId="9986" Score="0" Text="@Peter i thought the probability is taken into account with $A$, as long as points are sampled uniformly from the surface. However i already got decent results adding together the samples. The link I mentioned explains that the samples can be added together when using the formula" CreationDate="2020-07-01T13:05:28.213" UserId="7035" ContentLicense="CC BY-SA 4.0" />
  <row Id="15570" PostId="9991" Score="0" Text="I'm not sure I understand the question. In Vulkan, you can render to whatever renderable image you like, but you cannot *display* whatever image you like. You can only present images acquired from the presentation engine, and you can only render to them when the presentation engine says that you can (ie: when you have acquired it). So it's not really clear what you're asking about. Also, g-buffer and HDR images are pretty much never presentable images, so you'd have to create them anyway." CreationDate="2020-07-01T15:59:47.170" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15571" PostId="9991" Score="0" Text="&quot;*since one of the benefits of multiple buffering is that the GPU can start rendering the next frame before the previous one is finished*&quot; That's only a benefit of *triple* (or more) buffering (and it's not even really the point of triple buffering). Standard double-buffering is all about not having to wait for an image to be finished being *displayed* before starting to render the next one." CreationDate="2020-07-01T16:01:16.010" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15572" PostId="9986" Score="0" Text="I think it would work as long as A is the surface area of the whole mesh (not the triangle you are sampling) and the probablity of selecting a triangle is proportional to its surface area. If you want to do more advanced triangle selection which takes into account occlusion or distance from the shading point then you would have to change this." CreationDate="2020-07-01T17:45:18.317" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15573" PostId="9986" Score="0" Text="As for why I don't think you can just add them up, even if they are in the same domain, your probablity p would actually be the probabiliy of sampling it in the indirect + the direct. The best way to check if this is right or not is to test direct, indirect and direct+indirect and make sure they all look exactly the same. If you want to read more about combining methods have a read of https://cgg.mff.cuni.cz/~jaroslav/teaching/2015-npgr010/slides/06%20-%20npgr010-2015%20-%20MIS.pdf . multiple importance sampling is easy to implement and will lower you variance even if your technique does work." CreationDate="2020-07-01T17:47:45.890" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15574" PostId="9991" Score="0" Text="Sorry for not being clear. My question is about whether there's a benefit to creating _multiple_ offscreen buffers to ping-pong between on consecutive frames, so frame n and frame n+1 render to different targets instead of the same ones. My reasoning was that for example frame 0 might still be reading from the G-buffer to calculate the lighting while frame 1 has already started rendering the G-buffer, so frame 1 has to wait for frame 0. I didn't find any resources on this however, so I feel like I'm missing something, I just don't know what it is." CreationDate="2020-07-01T19:42:47.143" UserId="13637" ContentLicense="CC BY-SA 4.0" />
  <row Id="15575" PostId="9955" Score="0" Text="Sorry for the late answer but I just could not find the time to try your suggestion." CreationDate="2020-07-02T00:07:50.250" UserId="13565" ContentLicense="CC BY-SA 4.0" />
  <row Id="15576" PostId="9955" Score="0" Text="I've never used SDL2 before, but as @PaulHK has also pointed out it's very easy to put together a prototype.&#xA;With streaming textures the results were quite good. I've tried both C++ and a C# wrapper and the difference is insignificant. I could to reach 1000fps at 640x400 by generating a simple pattern pixel by pixel. Creating a complex image is slower of course, but SDL and texture streaming won't be a bottleneck for sure.&#xA;Your code helped me a lot, thanks! (BTW, awesome project!)" CreationDate="2020-07-02T00:22:03.047" UserId="13565" ContentLicense="CC BY-SA 4.0" />
  <row Id="15577" PostId="9994" Score="0" Text="if i were to use the secound matrix what should the fovX and fovY be set to to match the standard fov settings" CreationDate="2020-07-02T16:54:34.943" UserId="13639" ContentLicense="CC BY-SA 4.0" />
  <row Id="15579" PostId="9994" Score="1" Text="Good question, I think 90 degrees for both values is pretty common. This also the value showed in one of the pictures in the second link I provided. However, take this as an orientation and not as a rule. Try some other values and take whatever you think looks best to you. Keep in mind, that different monitors/VR googles might need different settings." CreationDate="2020-07-02T21:07:54.487" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15580" PostId="9994" Score="1" Text="Nevermind i found i website for calculating it&#xA;https://themetalmuncher.github.io/fov-calc/" CreationDate="2020-07-02T21:18:27.127" UserId="13639" ContentLicense="CC BY-SA 4.0" />
  <row Id="15582" PostId="9995" Score="2" Text="Yes, that's what people do. Matrix multiplication is (theoretically) associative, so $((vA)B)C = v(ABC)$." CreationDate="2020-07-03T15:22:43.687" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15584" PostId="9995" Score="0" Text="Thanks for the information" CreationDate="2020-07-03T19:56:53.200" UserId="13639" ContentLicense="CC BY-SA 4.0" />
  <row Id="15586" PostId="9997" Score="1" Text="Doesn't 4.3 and onwards deal with this? It's probably also a good idea to consider: https://code.google.com/archive/p/geodesic/wikis/ExactGeodesic.wiki" CreationDate="2020-07-04T07:08:56.413" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15587" PostId="8329" Score="0" Text="@russ Yes, it's possible to compile HLSL shaders to GLSL using the [SPIRV-Cross](https://github.com/KhronosGroup/SPIRV-Cross) compiler." CreationDate="2020-07-04T13:47:25.500" UserId="7553" ContentLicense="CC BY-SA 4.0" />
  <row Id="15588" PostId="9997" Score="0" Text="Unfortunately section 4, section 4,3 included deals with their approximating method, however I need to use the exact geodesic, not the approximation. Those notes are indeed interesting, but they don't seem to address the problem I am currently facing, at minimum not explicitly. I have a hypothesis as to what the proper way to do this is, I am testing it right now, if successful I will answer my own question." CreationDate="2020-07-04T20:03:00.100" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15589" PostId="9995" Score="1" Text="But usually it is projectionmatrix * viewmatrix * positionvector" CreationDate="2020-07-05T18:44:52.050" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15590" PostId="7662" Score="0" Text="For anyone using this, i would like to point out that (x, y, z) are not absolute coordinates. I got the correct result by generating an orthonormal basis where y = surface normal. (This is mentioned in schutte's article though)" CreationDate="2020-07-06T15:24:44.110" UserId="7035" ContentLicense="CC BY-SA 4.0" />
  <row Id="15591" PostId="10002" Score="1" Text="Welcome Hafid! This looks like an application of Cavalieri's principle. You can calculate the (piecewise linear) function $b$ from the polygon's vertices after sorting them with respect to the $y$ coordinate. This should be the easiest way to evaluate the integral. (By the way, since this is a very mathematical question, it might be better suited for math stackexchange.)" CreationDate="2020-07-06T19:49:01.387" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15592" PostId="10002" Score="0" Text="@Chris the Cavalieri's principle! I am not aware of this. I think I have to do some research with google. Thank you very much." CreationDate="2020-07-06T19:53:13.090" UserId="13655" ContentLicense="CC BY-SA 4.0" />
  <row Id="15593" PostId="10002" Score="0" Text="What's the polynomial degree? You can triangulate the triangle and integrate within each triangle." CreationDate="2020-07-06T20:39:04.673" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15594" PostId="10002" Score="0" Text="@lightxbulb the polynomial is of degree 3." CreationDate="2020-07-06T20:52:00.083" UserId="13655" ContentLicense="CC BY-SA 4.0" />
  <row Id="15596" PostId="10002" Score="1" Text="See this: https://computergraphics.stackexchange.com/questions/9943/how-to-compute-the-following-integral-over-a-polygon/9944#9944  It's regarding degree 2 polynomials, but the procedure is similar minus the quadrature rule which won't work anymore." CreationDate="2020-07-06T21:34:13.650" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15598" PostId="10002" Score="0" Text="Interesting approach. If the polygon is not necessarily convex, this approach may be easier to implement than what I suggested, although it does not use the special structure of the polynomial $p$ in the question (which only depends on $y$)." CreationDate="2020-07-07T09:49:10.543" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15599" PostId="10002" Score="0" Text="Out of curiosity, when do you need to evaluate integrals like this in computergraphics?" CreationDate="2020-07-07T09:50:34.923" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15600" PostId="10002" Score="0" Text="@Chris I have to confess that I don't know if the integral could be related directly  to computergraphics. for me it is needed in structural engineering calculation. I am sorry if it is not very relevant to this site." CreationDate="2020-07-07T10:28:15.857" UserId="13655" ContentLicense="CC BY-SA 4.0" />
  <row Id="15601" PostId="10002" Score="0" Text="@Chris For antialiasing for example, or finite elements for solving PDEs on meshes." CreationDate="2020-07-07T16:28:26.767" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15602" PostId="10002" Score="1" Text="Yes the approach I linked is more general. If you want something specific to your problem then you have to split your integration into slabs divided by horizontal lines passing through the vertices. Let at some $y$ the left edge be between $l_1, l_2$ and the right between $r_1, r_2$. Any point on $l_1l_2$ is given by $p(\lambda) = (1-\lambda)l_1 + \lambda l_2$. You need $x$ expressed in terms of $y$ though, so $\lambda = \frac{y-l_{1,y}}{l_{2,y}- l_{1,y}}$, and you can substitute that in the expression for $x$ to get $x_l(y)$. Do the same for the right edge, then $b(y) = \|x_l(y) - x_r(y)\|$." CreationDate="2020-07-07T16:39:41.943" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15603" PostId="10002" Score="0" Text="@lightxbulb I think that is a good  strategy. Is it straightforward however to identify at each step the right and the left edges?" CreationDate="2020-07-07T17:21:26.580" UserId="13655" ContentLicense="CC BY-SA 4.0" />
  <row Id="15604" PostId="10002" Score="1" Text="Assuming that your polygon is convex, sort all of the vertices by $y$. Draw a line between the vertex with minimum $y$ and maximum $y$. All vertices to the left of this line will be considered on the left and all vertices to the right will be considered on the right. Start from the min $y$ vertex and walk along the left and right edges to get respectively the left and right edges. After you're done with 1 slab, move 1 vertex up (whichever's next in the ordering regardless if it is left or right)." CreationDate="2020-07-07T17:27:41.057" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15605" PostId="10007" Score="0" Text="This is obviously a homework/exercise question. So can you tell us what you have tried by yourself and what you are struggling with? I searched the internet for &quot;middle point algorithm parabola&quot; and already got some promising results.  Just copy-pasting the question is at least for me a bit to less effort on your side to be willing to help." CreationDate="2020-07-08T06:17:09.683" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15606" PostId="10007" Score="0" Text="I'll send you what i archieved today later cause i am busy with some other things, thank you for helping." CreationDate="2020-07-08T06:30:31.077" UserId="13664" ContentLicense="CC BY-SA 4.0" />
  <row Id="15607" PostId="10005" Score="2" Text="It's technically just the matrix used to prepare for the orthographic projection, the actual projection happens by dropping the $Z$ coord." CreationDate="2020-07-08T07:01:14.160" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15609" PostId="10007" Score="0" Text="https://www.youtube.com/watch?v=Y0Onvip-j9w ... By watching this video this is my try,  Ok in this problem the y increases y_{k+1} = y_k + 1 every time and x_{k+1} = x_k or x_{k+1} depending on the P_k value.  Here is code i tried with:  If P_{k} &gt;   then  (   p_{k+1} = p_k - 4*(a + b) + 2*x_k + 1;  and x in increased by one  x_{k+1} = x_k + 1;)  else (  p_{k+1} = p_k - 4*(a + b);) , It doesn't look very well i think.." CreationDate="2020-07-08T09:16:26.620" UserId="13664" ContentLicense="CC BY-SA 4.0" />
  <row Id="15610" PostId="10007" Score="0" Text="I added the photo i got in code" CreationDate="2020-07-08T09:17:28.940" UserId="13664" ContentLicense="CC BY-SA 4.0" />
  <row Id="15611" PostId="10009" Score="0" Text="See this for more details: https://en.m.wikipedia.org/wiki/Rotation_matrix#Rotation_matrix_from_axis_and_angle" CreationDate="2020-07-08T14:00:39.657" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15612" PostId="10009" Score="0" Text="@lightxbulb seems like the user was deleted for some reason." CreationDate="2020-07-08T14:31:44.483" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15613" PostId="10009" Score="1" Text="@wychmaster Yes, I noticed. Nevertheless, someone else may found this useful." CreationDate="2020-07-08T14:49:01.310" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15614" PostId="10006" Score="0" Text="Thanks! I would imagine this would cause some confusion for beginners in computer graphics. It would be nice if it was called something else." CreationDate="2020-07-08T17:29:45.127" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15615" PostId="10011" Score="1" Text="In my opinion flipping $Z$ is both confusing and unnecessary. It changes the handedness of the different spaces. It's just an arbitrary decision that was made for OpenGL afaik." CreationDate="2020-07-10T07:23:50.877" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15616" PostId="10012" Score="1" Text="Thanks! Just to clarify did you mean to write `In OpenGL screen-space (Normalized device coordinate), larger z value means FARTHER points.`? I mean if OpenGL uses z-axis that goes inwards the screen." CreationDate="2020-07-10T16:08:33.940" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15617" PostId="8692" Score="0" Text="Just a follow up but by neighbours do you means pixels directly next to each other? What if you want to sample say [x,y] and [x+5,y-5] or something further away? Would arrays then be a wiser choice?" CreationDate="2020-07-12T02:32:50.933" UserId="10332" ContentLicense="CC BY-SA 4.0" />
  <row Id="15618" PostId="10015" Score="0" Text="This seems to require a division/reciprocal for the interpolation of every element of every attribute. Isn't this more expensive at the end of the day, than doing it just for $z$ and then linearly interpolating (without division) the attributes without using any division/reciprocal?" CreationDate="2020-07-12T05:41:52.273" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15619" PostId="10015" Score="0" Text="Sure, it's more expensive. But without it, you don't get perspective-correct interpolation, and anything with a texture on it will look all kinds of wrong. It's a necessary expense to pay." CreationDate="2020-07-12T05:57:19.397" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15620" PostId="10015" Score="0" Text="I meant that performing the division per pixel attribute is more expensive. You can linearly interpolate $u$ if you know view-space $z$ to get perspective-correct interpolation. Only for view-space $z$ do you require a division." CreationDate="2020-07-12T06:38:41.187" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15622" PostId="10012" Score="2" Text="[From this link:](http://www.songho.ca/opengl/gl_projectionmatrix.html) &quot;Note that the eye coordinates are defined in the right-handed coordinate system, but NDC uses the left-handed coordinate system. That is, the camera at the origin is looking along -Z axis in eye space, but it is looking along +Z axis in NDC.&quot; --- So larger values mean farther away in NDC" CreationDate="2020-07-12T11:00:44.050" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15623" PostId="10012" Score="1" Text="So this is not correct: &quot;In OpenGL screen-space (Normalized device coordinate), larger z value means closer points.&quot;" CreationDate="2020-07-12T11:40:57.037" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15624" PostId="10015" Score="0" Text="But you can't linearly interpolate $u$ itself; you must interpolate $u/z$, then cancel out the $z$ per pixel. Or it is not perspective-correct. For example, textures will appear to shift and bend over a surface as you move the camera around, if the texture coordinates are not interpolated perspective-correctly. _All_ vertex attributes require perspective correction if the geometry is to look correct." CreationDate="2020-07-12T19:43:05.737" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15625" PostId="10015" Score="0" Text="To put it another way: each attribute needs to interpolate along its own rational function, like $((1-s)u_1 z_1 + s u_2 z_2) / ((1-s)z_1^{-1} + sz_2^{-1})$ (for $s$ a screen-space interpolant)." CreationDate="2020-07-12T19:46:53.673" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15626" PostId="10015" Score="1" Text="Hmm, maybe I see what you mean now. You could calculate $(1-s)z_1^{-1}/((1-s)z_1^{-1} + sz_2^{-1})$ once per pixel (and another one for the other screen-space coordinate), and use those scalars to interpolate each of the attributes. Yeah, that sounds like it works, so maybe that's what they actually do under the hood. I'm not sure how it's implemented exactly." CreationDate="2020-07-12T20:01:06.667" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15627" PostId="10015" Score="0" Text="Yeah, in my result I got that I only need to know perspective-correct $Z$ in view-space, then I can interpolate $u$ linearly in a perspective-correct fashion. I didn't write it out explicitly, but it was clear that it is a linear function. Thank you for confirming that it's supposedly correct." CreationDate="2020-07-12T20:10:25.237" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15628" PostId="10012" Score="2" Text="I see. I was wrong indeed. The misunderstanding came from my thought that the projection matrix maps [n, f] to [0, 1]. What actually happens is the matrix maps [-n, -f] to [0, 1]. I'll edit the answer accordingly. Thanks." CreationDate="2020-07-13T02:43:14.813" UserId="13305" ContentLicense="CC BY-SA 4.0" />
  <row Id="15630" PostId="10020" Score="0" Text="Thanks a lot! when layering both (color and depth) everything is working..." CreationDate="2020-07-13T14:21:23.980" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15631" PostId="8692" Score="0" Text="In general the farther away your samples are the less benefit you’ll get from the spatial encoding. Within 5px I would guess a texture would still be optimal; 50px, probably not." CreationDate="2020-07-13T19:57:43.757" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="15633" PostId="9998" Score="0" Text="Maybe a quadtree can make it faster? Grids are easier to implement and work great too." CreationDate="2020-07-14T06:04:56.760" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="15634" PostId="9948" Score="0" Text="How about physics stack exchange?" CreationDate="2020-07-14T06:16:55.690" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="15638" PostId="3589" Score="0" Text="&quot;the time you're spending with the API is time you're not spending learning graphics concepts.&quot; This is sooo true. I learned way more about graphics programming by implementing everything in cpu." CreationDate="2020-07-15T02:08:48.027" UserId="13456" ContentLicense="CC BY-SA 4.0" />
  <row Id="15644" PostId="10027" Score="0" Text="Have you talked with your supervisor? He ought to have some ideas." CreationDate="2020-07-15T20:48:27.957" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15645" PostId="10027" Score="0" Text="Unfortunately, your question does not really fit here since there is no clear answer to it and the answers you might get would be mostly opinion-based. Please have a look into [this page](https://computergraphics.stackexchange.com/help/on-topic) and [this page](https://computergraphics.stackexchange.com/help/dont-ask) too learn what you can ask here." CreationDate="2020-07-15T21:41:28.040" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15646" PostId="10033" Score="0" Text="Thank you for the feedback!" CreationDate="2020-07-16T14:43:00.760" UserId="7073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15647" PostId="10032" Score="0" Text="Welcome Przemek B! Could you elaborate on what exactly the desired result of your proposed algorithm is? More precisely, what do you want to achieve, i.e. what do you mean when you say you want to compare fabrics?" CreationDate="2020-07-16T18:29:49.797" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15648" PostId="10031" Score="0" Text="Oh wow accessing memory is the slowest? Did not know that. That seems likely then the trig functions are probably going to win out here. Thanks for the advice on profiling! I will look into it!" CreationDate="2020-07-16T20:39:06.533" UserId="10332" ContentLicense="CC BY-SA 4.0" />
  <row Id="15649" PostId="10031" Score="0" Text="@wduk GPU memory is many more times slower than a CPU memory (to access) due to higher latency. However, once the data starts streaming, the bandwidth is much larger than a CPU's mem bandwidth. I've updated the answer and added some screen captures demonstrating latency hiding!" CreationDate="2020-07-16T21:39:43.323" UserId="5353" ContentLicense="CC BY-SA 4.0" />
  <row Id="15650" PostId="10036" Score="0" Text="Very nice experimant! how have you changed the CS to tiles/groups rendering? What exactly does that mean? is it only the group size?" CreationDate="2020-07-17T12:32:24.017" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15651" PostId="10037" Score="1" Text="Pretty much any 'divergence' is going to cause a relative loss in peak shader efficiency: If A SIMD GPU has N parallel units but only M of them are doing 'useful' work, be that because of a 'mask bit' or branching, it's still only going to be achieving M/N efficiency." CreationDate="2020-07-17T13:42:18.270" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15652" PostId="10035" Score="0" Text="Hey thanks for a very detailed answer! Regarding the collision detection, does it have to be performed on the CPU? Do we cut the partially covered triangles, create a temporary mesh including the new vertices and pass it into vertex buffer?" CreationDate="2020-07-17T14:01:40.983" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15653" PostId="10037" Score="0" Text="This problem was discussed on stackoverflow link: https://stackoverflow.com/questions/37827216/do-conditional-statements-slow-down-shaders" CreationDate="2020-07-17T16:02:40.483" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15654" PostId="10037" Score="1" Text="one example would be: if(uniformVariable == 1)...  not branching, because every invocation is end up in the same condition.. but if(texture(tex,interpolatedCoordinate) == 1) this is branching, because some will go into the condition and other invocations not" CreationDate="2020-07-17T16:05:20.337" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15657" PostId="10033" Score="0" Text="Just as an update for future readers.  For my project we wanted to keep a pristine GL state whenever we did an operation.  So, it was surprising to us that when you bind a VAO, it binds the associated VBOs but when you unbind a VAO, it doesn't unbind the associated VBOs.  So for our project, our VAO class automatically unbinds the VBOs when the VAO was unbound." CreationDate="2020-07-17T19:18:57.557" UserId="7073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15658" PostId="10035" Score="0" Text="Also do you know if projective texture mapping is a viable technique?" CreationDate="2020-07-17T19:27:05.260" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15659" PostId="10036" Score="0" Text="@Thomas See the diagram on the right for a visualization; the strong red box represents individual thread groups." CreationDate="2020-07-17T19:52:31.637" UserId="523" ContentLicense="CC BY-SA 4.0" />
  <row Id="15660" PostId="10035" Score="0" Text="It should also be possible to do the collision detection in a compute shader on the GPU, but it depends on the complexity of your model if it is worth it. Regarding the second part of your question, that depends on how you are going to store the results in your model. In the first case, I mentioned you need to update your whole mesh with the new vertices. If you use a UV texture for your model, you can create a temporary mesh to render the triangles to the UV texture." CreationDate="2020-07-17T21:36:49.903" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15661" PostId="10035" Score="0" Text="&quot;Also do you know if projective texture mapping is a viable technique?&quot; --- Yes, why shouldn't it be? However, keep in mind that you should use the approach I described only if you want to include the projected texture directly into your model. If you just want to project a texture somewhere into your scene, there are easier, more effective ways to do that since you do not need to find a connection between model surface coordinates and texture coordinates. Just search for &quot;Projective texture mapping&quot; and you should find some material on how to implement it. It's similar to shadow mapping." CreationDate="2020-07-17T21:50:14.980" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15662" PostId="10032" Score="0" Text="I would like to assess if two materials have similar hue (or &quot;compatible&quot;) so i can mix them in a design and they don't look bad when composed." CreationDate="2020-07-17T22:16:05.587" UserId="13702" ContentLicense="CC BY-SA 4.0" />
  <row Id="15663" PostId="10035" Score="0" Text="Oh okay, I need the projected texture to be directly painted onto the model and its associated texture. So I guess projective texture mapping wouldn't work for me. When it comes to drawing onto models with brushes, I imagine the same technique (the original one you proposed) could work. However not sure if it would be too slow with continuous brush strokes. Something like this https://imgur.com/a/McpJy7b" CreationDate="2020-07-17T22:33:28.037" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15664" PostId="10040" Score="1" Text="I've had similar things cripple performance on CPUs so I would not bet on it not being a crippling performance issue. Especially as GPUs have so many more cores all fighting for the same memory location. With that said, you're right that the best thing to do is measure it." CreationDate="2020-07-18T00:13:41.083" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15665" PostId="10041" Score="0" Text="Welcome to CGSE. Please add the relevant code snippets of the linked source into your answer. If the link expires, your answer won't be helpful anymore." CreationDate="2020-07-18T12:21:01.443" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15666" PostId="10042" Score="0" Text="The cos term is actually a part of the rendering equation. The lambertian BRDF is just a constant $ R_{d}/{\pi}$. Hence it gets multiplied with the whole BRDF  (spec + diffuse)" CreationDate="2020-07-19T13:34:51.307" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15667" PostId="10042" Score="1" Text="Phong is a hack, so the specular brdf is actually $\frac{specular}{\cos\theta}$ which breaks energy conservation. There is a modified version of the brdf where it is energy conserving and the specular part is multiplied with $\cos\theta$." CreationDate="2020-07-19T13:50:51.577" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15668" PostId="10040" Score="1" Text="@Olivier: And yet, there are many algorithms and rendering patterns that do this all the time. Linked-list-based order independent transparency is ultimately based on everyone bumping the same atomic counter to get unique locations for their list nodes. That seems to work adequately performance-wise. Also, atomic counters can be specialized hardware; on AMD platforms, they're something that lives inside the rendering context, not a memory location. I'm not saying that it's cheap, but it shouldn't be looked on as an a priori show-stopper." CreationDate="2020-07-19T13:54:01.387" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15669" PostId="5799" Score="0" Text="I would like to point out that it is not true that there can never be formulae for roots of polynomial equations of degree greater than 4. Although I do agree that it is likely not a good idea to try to find the intersections of a line with a bicubic Bezier patch algebraically, the Abel-Ruffini theorem is at most a partial explanation for this." CreationDate="2020-07-19T14:30:14.380" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15670" PostId="10032" Score="0" Text="In this generality, I would expect this problem to be quite difficult to solve since you kind of want to &quot;invert&quot; the rendering equation. If you knew the scenes' geometries and lighting conditions sufficiently well, you could try to reverse engineer the material properties of the fabrics (without using a reference object), although this would likely need more work than you're willing to put into this problem. However, I also think that a software that solves step 4 approximately should exist somewhere. Maybe someone else here can provide you more useful feedback than me." CreationDate="2020-07-19T15:03:20.553" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15671" PostId="10045" Score="0" Text="As the dot product approaches zero,  1/dotproduct approaches infinity." CreationDate="2020-07-21T14:08:39.680" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15672" PostId="10045" Score="0" Text="Yes, I see that, but BRDF should not return values higher than 1 after all, that's what I ask about, why do we need this denominator" CreationDate="2020-07-21T14:13:13.937" UserId="10193" ContentLicense="CC BY-SA 4.0" />
  <row Id="15673" PostId="10046" Score="0" Text="Intriguing problem! If I am not mistaken, the situation is even worse than you expected in the sense that any path going through the blue point should be a little bit longer than the path you found with Dijkstra's algorithm." CreationDate="2020-07-21T14:15:07.133" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15674" PostId="10046" Score="0" Text="The analytic path should be shorter, but the approximation might be longer" CreationDate="2020-07-21T15:45:10.203" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15675" PostId="10046" Score="0" Text="Yes, this is what I meant to say. I think you found the (unique) shortest path between the two red points, among all paths that move along the triangles' edges." CreationDate="2020-07-21T15:54:11.707" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15676" PostId="10041" Score="0" Text="Interesting, but to be honest, it's only one step up from a triangle, and IIRC (but unfortunately being out of the office I can't access my copy of Farin's &quot;Curves and Surfaces for CAGD&quot; to check)  to get C1 (or G1?) continuity I believe you need a _quartic_ Bezier triangle, i.e. 15 control points" CreationDate="2020-07-21T16:59:01.067" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15677" PostId="10046" Score="0" Text="In this specific case there's just one, but in general there might be multiple, including infinite." CreationDate="2020-07-21T17:02:10.170" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15678" PostId="10047" Score="0" Text="I think you have it wrong, given the square subdivision you gave, a refinement of that will not converge towards the geodesic, because the distance between 2 points remains constant. I explained that on my original question : p" CreationDate="2020-07-21T17:03:52.147" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15679" PostId="10047" Score="0" Text="Your triangle scheme has given me and Idea, so I will test that out and get back to you" CreationDate="2020-07-21T17:04:30.093" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15680" PostId="10046" Score="0" Text="How can there be infinitely many shortest paths?" CreationDate="2020-07-21T18:30:44.573" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15681" PostId="10047" Score="0" Text="This is why I wrote *sets* of shortest paths. You always need to make sure you calculate all shortest paths and not just one random path. Then you indeed get convergence as I explained above." CreationDate="2020-07-21T18:40:31.440" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15682" PostId="10045" Score="0" Text="The BRDF can absolutely return values higher than 1 - it's a reflectance density, not a reflectance value. See also: [GGX BRDF is greater than 1?](https://gamedev.stackexchange.com/questions/62438/ggx-specular-brdf-is-way-over-1). To put it another way: a point light has an infinitely bright radiance at its single point, even if it has finite energy output overall; so the specular response (which is basically a blurred image of the light source) can be arbitrarily bright as well." CreationDate="2020-07-21T18:44:53.243" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15683" PostId="10049" Score="0" Text="Welcome to this community :) . Asking for off-site resources is off-topic here (and a lot of other SE sites). Consider reformulating your question in a way that it can be answered clearly and fact-based so that it is more [on topic](https://computergraphics.stackexchange.com/help/on-topic) here. Maybe focus on the &quot;how to do xyz&quot; aspects that are unclear to you." CreationDate="2020-07-21T19:34:14.157" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15684" PostId="10046" Score="0" Text="@Chris Consider the sphere, take the 2 poles, every single great circle connecting the 2 points is a geodesic path. Thus, there are infinitely many.&#xA;&#xA;Different shapes have different properties but most shapes have at least one pair of points that are connected by an infinite amount of geodesics." CreationDate="2020-07-21T21:23:43.647" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15685" PostId="10051" Score="0" Text="One thing is that when you map the buffer you're passing GL_WRITE_ONLY. But you want to retrieve the values in the buffer, so you probably want GL_READ_ONLY for that." CreationDate="2020-07-21T23:00:29.380" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15686" PostId="10051" Score="0" Text="Also, I think you want GL_BUFFER_UPDATE_BARRIER_BIT instead of GL_SHADER_STORAGE_BARRIER_BIT. The barrier bits refer to operations _after_ the barrier that need to wait for shader writes prior to the barrier. glMapBuffer falls under the BUFFER_UPDATE_BARRIER_BIT." CreationDate="2020-07-21T23:05:21.327" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15687" PostId="10051" Score="0" Text="@NathanReed The program still somehow returns null whenever I run it." CreationDate="2020-07-22T00:13:11.080" UserId="13732" ContentLicense="CC BY-SA 4.0" />
  <row Id="15688" PostId="10051" Score="0" Text="Next thing to try might be hooking up [debug output](https://www.khronos.org/opengl/wiki/Debug_Output) to see if there are any errors/warnings that might shed light on what's going wrong. See also [this question](https://computergraphics.stackexchange.com/questions/5945/how-to-do-error-handling-with-opengl)." CreationDate="2020-07-22T00:20:02.000" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15689" PostId="10051" Score="0" Text="@NathanReed after I switched out GL_DYNAMIC_COPY code for GL_READ_ONLY instead of returning null the whole thing just crashed." CreationDate="2020-07-22T00:24:55.253" UserId="13732" ContentLicense="CC BY-SA 4.0" />
  <row Id="15690" PostId="10047" Score="0" Text="I am going to have to ask another question here before I get back to you" CreationDate="2020-07-22T04:22:57.483" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15691" PostId="10047" Score="0" Text="This is not what I ended up doing but it;s probably better, my solution was just to start randomly splitting edges (I just did it this way because it;s easier in my current implementation) but this will probably work too." CreationDate="2020-07-22T04:46:46.473" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15692" PostId="10049" Score="0" Text="@wychmaster I appreciate that, but at the current time I am unable to formulate my question in such a way. I would require off-site resources in order to begin my learning process. Feel free to close the question if its inappropriate." CreationDate="2020-07-22T10:23:23.273" UserId="13730" ContentLicense="CC BY-SA 4.0" />
  <row Id="15693" PostId="10052" Score="0" Text="This sounds extremely promising. Thank you! I will definetly read that book, it sounds very interesting" CreationDate="2020-07-22T10:24:48.883" UserId="13730" ContentLicense="CC BY-SA 4.0" />
  <row Id="15694" PostId="10045" Score="0" Text="[link](https://twvideo01.ubm-us.net/o1/vault/gdc2017/Presentations/Hammon_Earl_PBR_Diffuse_Lighting.pdf) Here is a good presentation that derives the equation. And this paper is lengthy but filled with good info [link](http://jcgt.org/published/0003/02/03/paper.pdf)" CreationDate="2020-07-22T22:21:56.860" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15695" PostId="10051" Score="0" Text="Not sure what you mean there, you don't need to change the GL_DYNAMIC_COPY (and GL_READ_ONLY is not a valid value for that anyway). But you do need to replace GL_WRITE_ONLY with GL_READ_ONLY in the glMapBuffer call." CreationDate="2020-07-22T23:26:53.793" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15696" PostId="7814" Score="0" Text="When programmable shaders were first being introduced they were programmed using a language that was very &quot;assembly language&quot; like. It can still be found in books and papers of the time. Some of the game programming gems books have articles that show examples. The original &quot;orange book&quot; has some in it as well. It is fun to look at because you will see early versions of functions like fma. It also can fill in some of those &quot;why the heck is this done this way&quot; questions. AND, you will quickly see that different vendors had different &quot;languages&quot;." CreationDate="2020-07-23T00:19:52.230" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15697" PostId="7814" Score="0" Text="There is in fact still multiple layers, but programmers don't really have access to those layers. For example GLSL compiles to SPIR-V (as already mentioned) And those are really the only layers of the language you have access to as a graphics programmer. But inside the driver code you will find  mountains of vendor specific code...when this code actually gets inside the gpu it is further translated into micro code, micro code is where it all really happens, beyond that it is just a sea of numbers. Micro code is how vendors &quot;fix&quot; hardware bugs. By programming around them." CreationDate="2020-07-23T00:32:07.100" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15698" PostId="10049" Score="1" Text="Hi Daniel, yours seems like a perfectly acceptible question to me and I'd expect it'd be quite reasonable for answers to include &quot;off-site&quot; information. (Further, IMO, always expecting people to write&quot;on-site&quot; answers for questions that might require multiple pages, is just unrealistic)" CreationDate="2020-07-23T08:22:03.780" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="15699" PostId="10046" Score="0" Text="Modify your search to use the Euclidean distance between vertices (on the same face), it's possibly not as efficient but should give the correct results." CreationDate="2020-07-23T08:58:12.567" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15701" PostId="10049" Score="2" Text="@SimonF Don't get me wrong here. I don't see the question as totally off-topic, neither do I think that &quot;off-site&quot; resources shouldn't be allowed. I also think that it can be made &quot;valid&quot; by focussing more on the &quot;What are the known methods&quot; part. That's why I asked the OP for an edit. However, as it currently stands and as I read it, the question is too open for &quot;link-only&quot; answers and opinions, even though I think the accepted one is good. In the end, it's a community decision and I cast my (close) vote. I don't want to be dogmatic about the &quot;rules&quot;, just wanted to provide some guidance. :)" CreationDate="2020-07-23T10:43:24.793" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15703" PostId="10054" Score="0" Text="Yes in fact the two sets are well correlated. &quot;read from set one then resample the texture for set 2&quot; -- could you describe this process in more detail? This is what I was wondering about." CreationDate="2020-07-23T11:59:26.420" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15704" PostId="10054" Score="0" Text="I added an edit to the answer." CreationDate="2020-07-23T12:30:46.787" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15706" PostId="10054" Score="0" Text="Didn't think it was this easy, makes a lot of sense, thanks a lot! In the case where the uvs in the first set are outside the 0-1 range I guess I could just use `GL_CLAMP_TO_BORDER` with the desired background color to fill in the parts of the image that shouldn't have any texture." CreationDate="2020-07-23T14:29:19.330" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15707" PostId="10054" Score="1" Text="You could draw a fullscreen quad behind this with a blurred image to help with sampling errors to. This trick essentially turns opengl into a layered image editor!" CreationDate="2020-07-23T15:08:06.723" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15708" PostId="10046" Score="0" Text="@lightxbulb My faces are triangles, so it's technically already doing that." CreationDate="2020-07-23T16:50:35.593" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15709" PostId="6094" Score="0" Text="Shader toy has a simplex noise implementation that you can edit in real time [here](https://www.shadertoy.com/view/Msf3WH) This is a good place to compare your implementation and play with different values to see what results simplex noise will give. (and look at some other examples of simplex in action)" CreationDate="2020-07-23T17:15:38.490" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15710" PostId="10046" Score="0" Text="From my understanding if that was the case you would be getting the blue point. You already mentioned you use manhattan distance. I suggest using Euclidean distance." CreationDate="2020-07-23T18:44:22.537" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15711" PostId="10047" Score="0" Text="I am sure that there are many different solutions. A randomized subdivision scheme sounds very interesting too. If you were worried about using too much space and just wanted to find one particular geodesic, adaptive subdivision strategies would also be quite useful." CreationDate="2020-07-23T21:30:37.737" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15712" PostId="10046" Score="0" Text="It really is a problem directly related to the discretization. No matter how much you refine the triangles with the above subdivision strategy, you'll never get convergence to the true geodesic in the above example (using the euclidean distance). This is very unintuitive and makes the problem quite interesting imo" CreationDate="2020-07-23T21:59:25.837" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15713" PostId="10046" Score="0" Text="The explanation regarding the square grid is a bit misleading since it does not apply to the example given in the pictures." CreationDate="2020-07-23T22:04:31.873" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15714" PostId="10046" Score="0" Text="@lightxbulb The easiest way to understand the problem is perhaps, look at the last image and try to come up with a shorter path in the generated grid than the green path.&#xA;&#xA;You will notice that you can't find a shorter one. Then imagine you do one further subdivision step, you will notice that a newly detected path on that more refined grid is actually pretty close to the one on the last image, despite there being 3x more edges to pick from." CreationDate="2020-07-23T23:29:25.253" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15715" PostId="10046" Score="0" Text="I think that if the method was using Euclidean distance then you would get a &quot;staircase&quot; path, so even without a further refinement you will be a lot closer to the blue point. I'll elaborate. You say: &quot;The distance between any 2 points in the grid is the manhattan distance |x| + |y|&quot; - this doesn't need to be the case. You can measure the Euclidean distance between the two points. Then you would get the path that you want for your geodesics. To be sure, measure the distance in the continuous space, not on the graph." CreationDate="2020-07-24T06:24:15.117" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15716" PostId="10046" Score="0" Text="@Makogan Wouldn't the path on the refined grid be exactly the same as the one you have now, as long as you keep the red points at the same position? Maybe it would be helpful if you added more pictures" CreationDate="2020-07-24T10:25:17.593" UserDisplayName="user9485" ContentLicense="CC BY-SA 4.0" />
  <row Id="15717" PostId="10054" Score="0" Text="Sorry I don't have that much experience with OpenGL, why a blurred image to help with the sampling? Yeah that's what I was just thinking! Just use quads for each layer with alpha and different depths. We could also do different image processing operations like translation, rotation and scaling. I haven't seen GPU used for image processing and the like though. Are there limitations I'm not aware of?" CreationDate="2020-07-24T12:27:29.533" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15718" PostId="307" Score="0" Text="Make sure to reuse existing buffers for the same attribute indices." CreationDate="2020-07-26T08:40:14.047" UserId="9851" ContentLicense="CC BY-SA 4.0" />
  <row Id="15719" PostId="10032" Score="0" Text="Whats wrong with using grey cards? Why is it too limited? Its done in photography all the time. Anyway the problem is that a camera does not represent the spectum very well so if you want to be better you probably need a spectrometer" CreationDate="2020-07-26T18:40:09.070" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15720" PostId="10032" Score="0" Text="@Chris yeah its called gray balance. Most image editors can do this." CreationDate="2020-07-26T18:41:31.357" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15721" PostId="10059" Score="0" Text="I see, too bad. I was aware about the formats, however given that most of the time I use images the format is the same as the original one, I wanted to eliminate the chance of using a wrong format in the shader, and for the few times I'd need to use a different format, I'd use a view and compare the shader format with the view, ensuring I always have the good format set. However I wasn't aware that the access type could be different, so at least that's one less thing I have to worry about!" CreationDate="2020-07-26T22:15:39.520" UserId="8925" ContentLicense="CC BY-SA 4.0" />
  <row Id="15722" PostId="10059" Score="0" Text="Side question, does Vulkan also requires to specify the type of the image API side, or does it just assume that the view will have the correct format?" CreationDate="2020-07-26T22:19:16.123" UserId="8925" ContentLicense="CC BY-SA 4.0" />
  <row Id="15723" PostId="10059" Score="1" Text="@Aulaulz: The OpenGL image binding API predates the View Texture feature, so the image binding API needed a way to have the binding format be different from the texture's format. With view textures, there's really no point in it, since you could just create a new view with the desired format (which is why `glBindImageTextures` [note the &quot;s&quot;] does not allow you to specify the format). The two different specifiers use the same conversion rules. Since ImageViews are part of Vulkan's API from the beginning, there was no need for the redundant concept." CreationDate="2020-07-26T22:44:55.477" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15724" PostId="10059" Score="0" Text="Oh I wasn't aware of glBindImageTextures. I'll use that instead. All of that makes sense now, thanks!" CreationDate="2020-07-26T23:05:23.173" UserId="8925" ContentLicense="CC BY-SA 4.0" />
  <row Id="15726" PostId="10060" Score="0" Text="This is the expected behavior of a perspective transformation. Anything beyond the far plane is mapped to the range (1, 2f/(f-n)) and anything behind the camera gets mapped to (2f/(f-n), infinity). If you want test for &quot;behind the camera&quot;, you can check to see if the value is greater then 2f/f-n. Once an object has been transformed into clip space it is usually not examined further. Are you implementing a software pipeline?" CreationDate="2020-07-29T12:43:22.153" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15727" PostId="5687" Score="0" Text="A better answer 3 years later... halton sequence is a low discrepancy sequence. It converges faster than white noise but can have aliasing problems before it converges.  Blue noise converges at the same speed as white noise but has lower starting error. The error it leaves behind is better perceptually than white noise or LDS. Blue noise is for low sample counts basically." CreationDate="2020-07-29T17:29:40.177" UserId="56" ContentLicense="CC BY-SA 4.0" />
  <row Id="15728" PostId="10060" Score="0" Text="I decided to clip near plane in world space and for other planes in NDC. Works quite well. Thanks for reply, though!" CreationDate="2020-07-30T13:06:45.463" UserId="13751" ContentLicense="CC BY-SA 4.0" />
  <row Id="15729" PostId="10070" Score="0" Text="I'm not really convinced that picking the pixel on the camera is actually much of a problem. For instance, casting a ray to any of the pixels which are not occluded when doing the next event estimation is a valid path, so one might as well cast to all of them. Similarly, one can connect to the entirety of a mesh light and color accordingly. It just seems like the argument for eye-based tracing is based on speed, but I haven't been able to find any modern resources that actually compare intelligent implementations of each approach." CreationDate="2020-07-31T01:03:09.957" UserId="12073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15730" PostId="10070" Score="0" Text="The link about the pixar renderer is quite interesting also, so thanks for that." CreationDate="2020-07-31T01:04:00.420" UserId="12073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15731" PostId="10070" Score="0" Text="@jheindel: &quot;*It just seems like the argument for eye-based tracing is based on speed*&quot; Well... yes, it's about speed. Most light doesn't hit the viewpoint, so starting from a direction that you know will contribute to the image speeds things up." CreationDate="2020-07-31T01:29:15.760" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15732" PostId="10070" Score="0" Text="@NicolBolas I am asking where does the speed up primarily come from? Because the argument that it is due to rays not reaching the camera does not obviously result in slower tracing. In the one case we have to hope to hit a light source, which might be even smaller than the camera's image plane, so in both cases we can just complete the light path from every bounce. So the speed-up isn't obvious to me. Clearly the two methods converge to the same result, so you choose the faster one." CreationDate="2020-07-31T03:48:01.020" UserId="12073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15733" PostId="10070" Score="0" Text="@jheindel: If you aim at a light source, *you will hit it*. The only way you won't hit a light source is if there's something directly in the way. If you aim from a light source to a part of an object, whether that light ray matters depends on something you *cannot* know: whether that object is visible to the camera." CreationDate="2020-07-31T03:57:27.417" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15734" PostId="10071" Score="0" Text="Ah, I see what I was missing. It does feel like there are scenarios where it would make sense to begin from the light source for indirect lighting, but I can see how these might not be knowable in any general sense, so it's just better to begin from the camera. Thanks for clearing that up!" CreationDate="2020-07-31T06:39:05.853" UserId="12073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15736" PostId="10072" Score="1" Text="Welcome. I don't know if this is the right site to ask this question since it is quite an application-specific one. Also, I don't think it is really a Computer Graphics problem or something that fits into one of the categories defined in the [on-topic pages](https://computergraphics.stackexchange.com/help/on-topic). A better place to ask this question is the [Adobe Support Community](https://community.adobe.com/t5/acrobat/bd-p/acrobat?page=1&amp;sort=latest_replies&amp;filter=all)." CreationDate="2020-07-31T07:40:06.470" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15737" PostId="10072" Score="0" Text="You will probably get a faster and more accurate answer there." CreationDate="2020-07-31T07:40:25.703" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15738" PostId="10070" Score="0" Text="Much of rendering is about different techniques which works better in different scenarios. For example, reflections and specular materials are very easy to do with forward path tracing, but can be impossible with back path tracing. Likewise backwards path tracing is very good when a light source is hidden from the scene and requires a few bounces to find. Again, it just turns out that lights are rarely hidden and specular materials are common so forward path tracing turns out to be much faster in most situations." CreationDate="2020-07-31T11:39:47.973" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="15739" PostId="10073" Score="0" Text="Threshold the gradient magnitude, or use the zero crossings of the laplacian." CreationDate="2020-08-01T07:12:29.047" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15740" PostId="10073" Score="0" Text="Thresholding the gradient magnitude is essentially sobel. : p" CreationDate="2020-08-01T18:39:30.367" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15741" PostId="10073" Score="0" Text="Well, no: https://en.wikipedia.org/wiki/Sobel_operator" CreationDate="2020-08-01T19:35:34.963" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15742" PostId="10082" Score="0" Text="Thanks this is very helpful!" CreationDate="2020-08-02T14:40:35.223" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="15743" PostId="7804" Score="0" Text="Vulkan allows buffers to be created with multiple uses in mind. So (keep in mind I am not checking the spec for compartbility here) you could potentially use VK_BUFFER_USAGE_VERTEX_BUFFER_BIT | VK_BUFFER_USAGE_STORAGE_BUFFER_BIT when the buffer is created. Then you can use the same buffer for both uses, but it is also up to you to make sure you don't step on your own toes. I have done something similar with images and it worked. A lot depends on if you plan to write an equal number of output for each input, and if you want to save the original data." CreationDate="2020-08-02T22:59:57.357" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15744" PostId="10076" Score="0" Text="See this algorithm: https://en.m.wikipedia.org/wiki/Dykstra%27s_projection_algorithm" CreationDate="2020-08-03T09:08:06.663" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15745" PostId="365" Score="0" Text="@NathanReed Good answer. But &quot;converting UVs to texel coordinates&quot; What? A UV IS a texture coordinate, which is used to get the texel texture[u][v]." CreationDate="2020-08-04T21:49:24.110" UserId="5496" ContentLicense="CC BY-SA 4.0" />
  <row Id="15746" PostId="365" Score="0" Text="@Nikos Not quite. UVs are in the [0,1] range and have to be converted to find the set of actual texels needed, given the texture size, wrap mode, and filtering mode." CreationDate="2020-08-04T22:52:02.020" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15747" PostId="365" Score="0" Text="@NathanReed oh my. Thanks man!" CreationDate="2020-08-04T23:44:43.540" UserId="5496" ContentLicense="CC BY-SA 4.0" />
  <row Id="15748" PostId="8641" Score="0" Text="And the reason you pass this function pointer to the windowing system is so that the windowing system can use this &quot;OpenGL extension loader mechanism&quot; for loading functions..." CreationDate="2020-08-07T09:34:26.360" UserId="10450" ContentLicense="CC BY-SA 4.0" />
  <row Id="15749" PostId="10087" Score="0" Text="You make a screen oriented square out of 2 triangles bind it to your shader and ask the gpu to draw the screen. (This is also how shadertoy works) and after 10 or so steps is your users worrying about conflation artefacts." CreationDate="2020-08-07T13:45:03.767" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15750" PostId="10087" Score="0" Text="That's what I do right now but what if I want to draw multiple things in my square?" CreationDate="2020-08-07T13:48:14.870" UserId="13753" ContentLicense="CC BY-SA 4.0" />
  <row Id="15751" PostId="10087" Score="1" Text="You dont actually have to clear the screen between calls. But its easier just to emit more polygons. Also look up accumulation buffers" CreationDate="2020-08-07T13:49:57.180" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15752" PostId="10088" Score="1" Text="By not using linear interpolation. What you are seeing is the discontinuity in the first derivative. Try bilinear interpolation over quads, it will fix it. For polygons with more vertices read up on generalized barycentric coordinates." CreationDate="2020-08-07T17:01:23.460" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15753" PostId="10088" Score="0" Text="@lightxbulb thank you for responding - I have had a week of radio silence on stack overflow!" CreationDate="2020-08-07T17:03:47.310" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15754" PostId="9329" Score="0" Text="@ina You could try putting your unlit texture into the emissive parameter, and set the albedo/metallic/smoothness all to 0 (black). There may still be a tiny bit of Fresnel reflection on the edges, but smoothness 0 should minimize it and ensure that the lighting on the material is as close to 0 as possible." CreationDate="2020-08-07T18:28:54.780" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15755" PostId="10088" Score="0" Text="I did bilinear interpolation for UV coordinates on quads here: http://www.reedbeta.com/blog/quadrilateral-interpolation-part-2/ I just used those UVs to sample a texture, but they could also be used to interpolate color values, or any other attribute values." CreationDate="2020-08-07T20:46:49.543" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15756" PostId="10088" Score="0" Text="@NathanReed Thank you that looks like a very thorough response!" CreationDate="2020-08-07T20:50:40.590" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15757" PostId="10089" Score="0" Text="In 1) those a references, in glsl you'll just pass things as they are. For 2) you don't have polymorphism, so you can split your spheres, triangles, etc. in separate arrays and deal with those in separate parts of the code. Here's something you can check out: https://github.com/GraphicsProgramming/RVPT" CreationDate="2020-08-08T11:43:07.340" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15758" PostId="10093" Score="0" Text="You use the exact same math, except in 4 dimensions instead of 3." CreationDate="2020-08-09T16:25:13.713" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15759" PostId="10091" Score="0" Text="So the vertex attributes are interpolated, but is it also the case for every &quot;out&quot; variable that I pass to the pixel shader?" CreationDate="2020-08-09T19:23:07.393" UserId="13786" ContentLicense="CC BY-SA 4.0" />
  <row Id="15760" PostId="10091" Score="0" Text="@Kiosto: &quot;*So the vertex attributes are interpolated*&quot; Nowhere in my post did I mention &quot;vertex attributes&quot;. Attributes are the *inputs* to the vertex shader. I spoke specifically about the *outputs* of the vertex processing pipeline." CreationDate="2020-08-09T20:12:04.680" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15761" PostId="10095" Score="0" Text="&quot;*I understand that something like post processing or UI compositing could be implemented in a second render pass, as they need the result from the first 3d pass to present the final image.*&quot; Didn't you just answer your own question?" CreationDate="2020-08-09T20:20:32.780" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15762" PostId="10095" Score="0" Text="@NicolBolas I mentioned that in order to exclude such suggestions from potential answers - I am asking for any OTHER use cases" CreationDate="2020-08-09T20:21:25.017" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15763" PostId="10095" Score="1" Text="But why exclude them? It's like asking, &quot;if you ignore all of the reasons to want to go to the store, why would anyone want to go to the store?&quot;" CreationDate="2020-08-09T20:22:21.627" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15764" PostId="10095" Score="0" Text="@NicolBolas Do you mean to say that those used mentioned in that quote are the only common and reasonable use cases for multiple render passes?" CreationDate="2020-08-09T20:23:16.847" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15765" PostId="10095" Score="0" Text="@NicolBolas because I do not know if they are all the reasons for “going to the store”- which is the exact purpose of my question" CreationDate="2020-08-09T20:24:19.607" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15766" PostId="10095" Score="0" Text="@NicolBolas To be clear, you’re saying that those ARE all the reasons? And that that is your “answer”?" CreationDate="2020-08-09T20:25:25.143" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15767" PostId="10095" Score="0" Text="If you were asking for any use case, that might be a legitimate question for this site. But asking for some comprehensive &quot;every possible use case for X&quot; kind of thing isn't appropriate for Stack Exchange sites. No one person can give you every possible use for anything where the answer isn't obvious. &quot;*And that that is your “answer”?*&quot; If I wanted it to be an answer, I'd have posted it as an answer, not as a comment." CreationDate="2020-08-09T20:25:44.813" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15768" PostId="10095" Score="0" Text="@NicolBolas Any use case that I am unaware of would be welcome as an answer - I did not request for a comprehensive list at any point. Do you have such a use case or are you merely questioning my question?" CreationDate="2020-08-09T20:27:58.233" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15769" PostId="10095" Score="0" Text="Let us [continue this discussion in chat](https://chat.stackexchange.com/rooms/111600/discussion-between-nicol-bolas-and-idiotic-shrike)." CreationDate="2020-08-09T20:28:31.143" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15770" PostId="10095" Score="0" Text="@NicolBolas I cannot see a way to do so on the mobile app - apologies if I’m missing an obvious option to do so" CreationDate="2020-08-09T20:33:17.433" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15771" PostId="10095" Score="0" Text="Click the &quot;continue this discussion&quot; link and see where it goes. Or perhaps go to the website through your browser." CreationDate="2020-08-09T20:35:17.623" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15775" PostId="10097" Score="0" Text="Thanks, this helped a lot and yes i'm going through [learnopengl](https://learnopengl.com/)." CreationDate="2020-08-10T13:57:42.567" UserId="13813" ContentLicense="CC BY-SA 4.0" />
  <row Id="15776" PostId="10099" Score="0" Text="Thank you for taking the time to answer even though you disagreed with my question!" CreationDate="2020-08-10T14:06:47.653" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15777" PostId="9872" Score="0" Text="Histogram equalization makes a good image, but does't it distort the data values? This is a geospatial visualization of environmental data and the color is keyed to a colormap which is in turned keyed to numerical values." CreationDate="2020-08-10T17:11:12.550" UserId="13422" ContentLicense="CC BY-SA 4.0" />
  <row Id="15779" PostId="10072" Score="0" Text="@wychmaster thanks! i'll try it later" CreationDate="2020-08-11T02:50:49.323" UserId="13771" ContentLicense="CC BY-SA 4.0" />
  <row Id="15780" PostId="10100" Score="2" Text="It would be nice if you could cite some references to support those strong statements." CreationDate="2020-08-12T06:26:01.393" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15781" PostId="10100" Score="0" Text="Voxel octree can be modified without the complete rebuild, although it applies only when you have actual voxels, not the triangles packed into the voxel structure. SDF models can also be modified without a rebuild by fiddling with the field properties. It is just that triangle meshes are inherently resistant to modification - you can move, rotate and scale them, but that is all." CreationDate="2020-08-12T08:20:17.747" UserId="13814" ContentLicense="CC BY-SA 4.0" />
  <row Id="15782" PostId="10101" Score="0" Text="You are not representing more data with the same number of bits. You are representing different data which exploits properties of the human visual system. Gamma compression effectively results in non-uniform quantisation. Look up Weber's law." CreationDate="2020-08-12T12:57:33.017" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15783" PostId="10101" Score="0" Text="@lightxbulb What I meant by that was that it's used to encode a greater range of dark tones, I think I'm right in saying? It encodes data differently to how a raw RGB would work - as do all color spaces - but the screen cannot actually display that...? I think? That's essentially my whole question" CreationDate="2020-08-12T12:59:51.587" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15784" PostId="10101" Score="0" Text="It encodes data in the exact same way a &quot;raw RGB&quot; would. Assuming 8bits you still encode values in $[0,255]$, what has changed is what these values represent. In the most general case you can use those values to index into a LUT from which to retrieve the actual values. Gamma correction is more restrictive than the above, in the sense that these values use the gamma curve as a LUT. As far as monitors go - sure you can always take a 1bit monitor and have secondary quantisation effects due to that." CreationDate="2020-08-12T13:52:57.517" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15785" PostId="10101" Score="0" Text="CRTs used to follow a similar power law which meant that you were supposed to feed in gamma corrected input and it would have been reproduced as expected. I have no idea about LCDs, but I assume a similar thing must hold in order to exploit the human visual system's characteristics." CreationDate="2020-08-12T13:56:35.707" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15786" PostId="10101" Score="0" Text="@lightxbulb I refer to modern lcds. As far as I understand, they take in “raw” RGB and by raw I mean to say it’s not exactly encoded in any way it just represents intensity values in a linear fashion..?" CreationDate="2020-08-12T13:58:17.053" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15787" PostId="10101" Score="0" Text="They just take in a discrete range of values and produce a corresponding intensity. What this relationship is should be monitor dependent, but I believe that even for LCDs you have a power law, otherwise you'd get extreme banding with just 6/8bit monitors." CreationDate="2020-08-12T14:05:25.053" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15788" PostId="10102" Score="0" Text="I accept the need for standardised systems and consistency. I also respect that there are other industries. I was more angling for the literal display technology side of things... I’m interested in whether our LCD displays can be capable of representing colour spaces such as sRGB or Adobe RGB or CIE xyY or CIELAB etc. natively - no conversion to simple 8-but rgb intensities necessary." CreationDate="2020-08-12T16:22:39.467" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15789" PostId="10102" Score="0" Text="Thank you nonetheless though it was a great answer to perhaps a poorly worded question :)" CreationDate="2020-08-12T16:23:16.583" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15790" PostId="10102" Score="0" Text="@Idiotic Srike it depends on the monitor and its calibration (my work monitor can ahow whole sRGB, but obviously since its a digital device it can only show digital values) i believe the monitors in collor correction room can nearly show full Adobe RGB. I doubt anything we have deviced can display the entire CIE spectrum as thats the entire fidelty we humans can see as that would mean youd need specialized color luminants for the special wavelengths)" CreationDate="2020-08-12T16:27:43.900" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15791" PostId="10102" Score="0" Text="I had absolutely no idea! Thank you thas a perfect answer. They natively take the sRGB or Adobe RGB encoding and render that with no conversions?" CreationDate="2020-08-12T17:04:30.747" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15792" PostId="10102" Score="0" Text="@IdioticShrike ultimately there is some dedicated hardware in between so the graphics card makes comversions as does the monitor for the final stage that is analog. But &quot;color&quot; is a hard subject i mean generally speaking the structures and strength calculatiln courses were deemed really hatd by engineering students but color on a pire technical level is as hard or harder." CreationDate="2020-08-12T17:19:10.627" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15793" PostId="10102" Score="0" Text="ok well thank you v much" CreationDate="2020-08-12T17:20:23.563" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15794" PostId="10100" Score="2" Text="@lightxbulb I didnt add refs at the time I've answered due time constraints, but I will. I will also include refs about different types of BVHs.. such as SBVH, since they are currently used on the GPUs. Also, currently BVH traversal is almost as fast as the traversal of kDTrees, this was observed already in a paper from 2009... from Ingo Wald, I think. I will look for those references also." CreationDate="2020-08-13T11:14:57.877" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="15795" PostId="10106" Score="0" Text="Thank you for those references. I will update my answer, making reference to yours! The mention to TLAS/BLAS was also a great addition." CreationDate="2020-08-13T11:24:26.353" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="15796" PostId="10105" Score="0" Text="You've written out a finite difference Taylor expansion approximation that is valid on a regular grid. And while it gives some intuition, it doesn't answer the question imo." CreationDate="2020-08-13T12:09:36.573" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15797" PostId="10106" Score="0" Text="Thanks! That RT gems book is exactly what I need." CreationDate="2020-08-13T16:20:28.493" UserId="13814" ContentLicense="CC BY-SA 4.0" />
  <row Id="15798" PostId="10106" Score="1" Text="@NashGold If RT gems caught your attention, you may also want to read PBRT - http://www.pbr-book.org/3ed-2018/contents.html - it is much more focused on theoretical aspects of raytracing/path-tracing - sampling, theory of signals, camera models, color theory and so on." CreationDate="2020-08-13T18:32:37.553" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15799" PostId="10106" Score="0" Text="@DirectX_Programmer, not really. I'm doing a simple voxel engine for a pixelart game. PBR would be an overkill. But thanks anyway." CreationDate="2020-08-13T19:40:58.717" UserId="13814" ContentLicense="CC BY-SA 4.0" />
  <row Id="15800" PostId="10103" Score="0" Text="I wouldn't say that moving background layers directly is in any way &quot;faking&quot; unless you have specific reason to say so. I assume that parallax scrolling layers are just sprites/images, so by moving their transform, you're not modifying game logic and you're not changing colliders in any way.&#xA;&#xA;Most, if not all, Unity tutorials will deal with parallax scrolling by moving object transform in certain direction. Here is example from my freelance project - https://youtu.be/m28Ehta2CoA?t=62 - I am only modifying transform in background object to simulate driving car." CreationDate="2020-08-14T06:44:53.307" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15801" PostId="10105" Score="1" Text="I figured that an intuition as to how a sum of differences represents the second derivative was the required level here. The answers to [this recent MathOverflow question](https://mathoverflow.net/questions/368963) go far more in depth as to what a graph Laplacian is and what it represents." CreationDate="2020-08-14T08:15:07.247" UserId="7647" ContentLicense="CC BY-SA 4.0" />
  <row Id="15802" PostId="10105" Score="0" Text="This is a great answer. Thanks for linking it for completeness." CreationDate="2020-08-14T20:06:15.513" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15803" PostId="10111" Score="0" Text="Thank you for your response. You mentioned a complex dance of conversions... would my final LCD display accept the sRGB data from my GPU over the cable, and before display convert that into pure RGB intensity values, to program the individual pixels?" CreationDate="2020-08-15T10:55:17.610" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15804" PostId="10111" Score="0" Text="Just found out about EOTFs... seems like they are responsible for the final conversions into linear tristimuli?" CreationDate="2020-08-15T11:09:10.783" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15805" PostId="10111" Score="0" Text="The final format will be dependent on the display, usually it is specific to the model/brand. The video hardware on your computer will query the display for its acceptable formats, its response combined with your settings in windows will determine the actual format sent to the display. It will then convert that to a display specific format for final display. If for example you have an SD display that doesn't respond to queries for its capabilities then the format sent to the display will be sRGB (most likely). To get extended formats you need HD support in all the hardware/software involved.." CreationDate="2020-08-15T17:42:30.743" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15808" PostId="10119" Score="1" Text="Hi. Please consider editing more information into your question so that it can be understood without he links which might expire some day." CreationDate="2020-08-16T18:21:41.767" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15809" PostId="10114" Score="0" Text="Can you add a link to the image page on Wikipedia / Wikimedia Commons? Without a title or caption it's not clear what it's meant to depict." CreationDate="2020-08-17T12:58:06.607" UserId="30" ContentLicense="CC BY-SA 4.0" />
  <row Id="15810" PostId="10114" Score="0" Text="@waldyrious sure" CreationDate="2020-08-17T12:59:28.380" UserId="13805" ContentLicense="CC BY-SA 4.0" />
  <row Id="15811" PostId="10111" Score="0" Text="We set the value to sRGB if we dont know. This is not the same as it is sRGB. If you have ever calibrated a monitor youd know even two monitors of same make and model are not the same." CreationDate="2020-08-18T04:10:06.267" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15812" PostId="10121" Score="0" Text="$u$ is the temperature at each vertex - it's the solution of the heat diffusion." CreationDate="2020-08-18T06:51:09.630" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15813" PostId="10125" Score="0" Text="https://github.com/ssloy/tinyrenderer/wiki/Lesson-0-getting-started - I guess that you're looking for something like this i.e. creating your own rasterizer. I wasn't working much with microcontrollers, so I limited my code to simple shapes and touch input. But based on that, I assume, that there is already library which allows you to send data pixel by pixel (or line by line as you said)." CreationDate="2020-08-18T15:10:06.950" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15814" PostId="10125" Score="0" Text="@DirectX_Programmer: Triangle rasterizers, by their nature, *require* a framebuffer. This question is specifically about drawing stuff *without* a framebuffer." CreationDate="2020-08-18T15:13:01.750" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15815" PostId="10125" Score="0" Text="@NicolBolas You're probably right, I have never researched that subject. But how should we define framebuffer? If he's storing data in 64K application ROM and load only part of data to compute in RAM, store result, unload RAM, load next part of data and so on - can we call it framebuffer then? In a sense, it meet contraints that OP asked for. Or do I have a wrong idea?" CreationDate="2020-08-18T15:29:11.053" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15816" PostId="10125" Score="1" Text="@DirectX_Programmer: I think the OP makes it clear what they mean by &quot;framebuffer&quot;. Namely, the entire screen stored in memory as pixel data." CreationDate="2020-08-18T15:31:51.063" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15817" PostId="10129" Score="0" Text="&quot;Am I correct? Then why have Euclideon generated so much hatred and controversy back then?&quot; I think this is hard to answer. There are a lot of possible reasons why people like or dislike something. Maybe somebody can provide you a list of the main pro and con arguments if that is what you are looking for. However, I think this question is more or less opinion-based. This is why I am voting to close the question in its current form. Maybe you can reformulate it and ask for something that can be answered with facts." CreationDate="2020-08-18T22:14:27.180" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15818" PostId="10127" Score="0" Text="This hits the mark thanks, very interesting information." CreationDate="2020-08-18T22:55:47.423" UserId="13847" ContentLicense="CC BY-SA 4.0" />
  <row Id="15820" PostId="10129" Score="0" Text="Would a speculative post like &quot;How does Euclideon work&quot; be acceptable ? I think the topic itself is interesting enough." CreationDate="2020-08-19T05:13:10.187" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15821" PostId="10129" Score="0" Text="@wychmaster are there any facts on why the new technologies get accepted or not? It is understandable when people come out to protest against say self driving cars, but 3d rendering seems like a neutral subject and apolitical subject. So there should not be any overly emotional response. So Euclideon can be a nice case study on presenting your project or research, while avoiding controversy. I.e. what they did wrong." CreationDate="2020-08-19T06:46:15.470" UserId="13814" ContentLicense="CC BY-SA 4.0" />
  <row Id="15822" PostId="10129" Score="1" Text="@PaulHK What is acceptable and what not is decided by the votes of the community :) - On other SE sites questions like &quot;How does XY work/do AB&quot; are generally closed for being opinion-based because the only people who know for sure are the developers. However, you can easily modify those questions into &quot;How can I  achieve AB to get a similar effect as XY&quot;. This can be answered by anybody with the knowledge to do it. The answers you get might not do it exactly the same way as XY, but they should give the same results. So it's not about &quot;what&quot; you ask, but &quot;how&quot;." CreationDate="2020-08-19T07:43:09.897" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15823" PostId="10129" Score="0" Text="@NashGold The SE networks Q&amp;A format is meant to be fact-based and not opinion-based. However, deciding which question is opinion-based is opinion-based itself most of the time. To make a decision, SE gives every higher reputation user (500+ on this site) the privilege to vote for closure and see the total number of votes. 5 closure votes close the question. It is a democratic decision. So other users might see it differently and the question stays open. If I vote for closure, I always leave a comment to explain the reasons for the questioner and other reviewers." CreationDate="2020-08-19T08:14:39.710" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15824" PostId="10131" Score="0" Text="Are you sure that is what your seeing. Or rather that the line is just to thin that you would just miss the line entirely when rasterized. You should in general strive to avoid lines smaller than 1/4of a postscript point because thats close to a 300 dpi line. Which is allready hard thing for a 600 dpi printer to reproduce reliably as a uniform line any smaller than this is really going to tax your system. Anyway if you send it as vector then some drivers strive to make all vector lines some width. Byt if you send it as bitmap it can dissapear." CreationDate="2020-08-19T09:33:19.923" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15825" PostId="10131" Score="0" Text="Not sure you can draw any conclusions by pasting stuff in paint." CreationDate="2020-08-19T09:35:20.687" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15826" PostId="10131" Score="0" Text="@joojaa with lines I meant the invoice number, some letters and numbers, and with some invoices I was left with a blank background. &#xA;I used paint just to try and see if the image was loaded on the pdf then edited using pdf tools. It seems like there is a background and lines drawn on top of it to compensate for lost data. I was just wondering if it were a known enhancement that some scanners do, or if these lines were manually added." CreationDate="2020-08-19T11:47:57.750" UserId="13853" ContentLicense="CC BY-SA 4.0" />
  <row Id="15827" PostId="10131" Score="0" Text="Yeah scanners refuse to read items with omron rings. But cameras do not. Try photographing." CreationDate="2020-08-19T11:53:29.297" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15828" PostId="10111" Score="0" Text="The goal of monitor calibration is to match the colors we see on the monitor to the colors being sent to the monitor. Don't confuse a colorspace with the variations between individual pieces of hardware." CreationDate="2020-08-19T12:53:54.397" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15829" PostId="10131" Score="0" Text="That’s not exactly what I mean. I received scanned invoices from a client and I want to know if the client manually edited them or if the edit is just something scanners do to compensate for low quality." CreationDate="2020-08-19T14:34:51.807" UserId="13853" ContentLicense="CC BY-SA 4.0" />
  <row Id="15830" PostId="10131" Score="0" Text="Sounds like drawing errors of teh PDF viewer" CreationDate="2020-08-19T14:50:33.737" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15831" PostId="10131" Score="0" Text="I think if you should consult a professional expert on this matter if the outcome has any legal effects. Without having access to the &quot;data&quot; in question we can just guess and even if we had, I wouldn't draw any conclusions because of an answer from somebody on the internet where I don't know if he is really an expert on the subject." CreationDate="2020-08-20T08:06:26.473" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15832" PostId="10133" Score="1" Text="SNORM (-1 to 1) and UNORM (0 to 1) are by definition clamped (normalised). See also https://docs.microsoft.com/en-us/windows/win32/direct3d10/d3d10-graphics-programming-guide-resources-data-conversion" CreationDate="2020-08-20T10:28:34.280" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15833" PostId="10133" Score="0" Text="@PaulHK Thank you. I've already read about UNORM/SNORM formats and realized my mistake. Therefore, I tried to use it for storing normals. But as I said - it's academic project so I decided to use FP16 to avoid problems and get better image quality." CreationDate="2020-08-20T11:08:45.183" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15834" PostId="10134" Score="0" Text="Thanks a lot, that was exactly this kind of informations I was looking for ! After a bit of search and tweakings my buffers and my work group size, I got a 10x speed bump !" CreationDate="2020-08-20T12:20:39.653" UserId="13850" ContentLicense="CC BY-SA 4.0" />
  <row Id="15835" PostId="10131" Score="0" Text="Thank you for that advice. That’s what I was planning to do. But to seek an expert I need to present the case to the higher ups for approval, and I just wanted to check if this was a known behavior of scanners to consider and report on." CreationDate="2020-08-20T21:20:31.067" UserId="13853" ContentLicense="CC BY-SA 4.0" />
  <row Id="15836" PostId="10138" Score="0" Text="Have you tried contacting MSAR itself to get non-watermarked files? Because what you're want to do is illegal. It is called theft. In this case the theft of intellectual property. I advice people against helping here, since most of our IT jobs depend on the intellectual property protection. Unless OP can prove he has the MSAR approval." CreationDate="2020-08-22T12:34:15.530" UserId="13814" ContentLicense="CC BY-SA 4.0" />
  <row Id="15837" PostId="10139" Score="0" Text="Pardon my reading comprehension skills but you say it has 2 components but I only read one component in your answer, what's the other? : p" CreationDate="2020-08-22T14:42:08.020" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15839" PostId="10138" Score="0" Text="@NashGold - Thanks for the Msar ID explanantion, i never knew about this. I actually got this and a bunch of images as part of a freelance job with an expiry date (The project expires after few days), my work is to put these numbers in an excel sheet. Since the time is limited I thought maybe I can extract numbers and paste them directly in the sheet. I never knew what MSAR id is and i am not planning to forge the images or do something illegal about  it. I am just trying to make my work a bit easier so that it completes on time." CreationDate="2020-08-22T14:57:49.733" UserId="13861" ContentLicense="CC BY-SA 4.0" />
  <row Id="15840" PostId="10139" Score="0" Text="https://www.mathsisfun.com/algebra/trig-area-triangle-without-right-angle.html" CreationDate="2020-08-23T00:04:24.883" UserId="13838" ContentLicense="CC BY-SA 4.0" />
  <row Id="15841" PostId="10144" Score="2" Text="https://retrocomputing.stackexchange.com/ - people there might know more about it." CreationDate="2020-08-23T08:06:42.463" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15842" PostId="10144" Score="0" Text="Could it be Toon Shading or Cel Shading (https://en.wikipedia.org/wiki/Cel_shading)" CreationDate="2020-08-23T19:57:49.740" UserId="6496" ContentLicense="CC BY-SA 4.0" />
  <row Id="15843" PostId="10144" Score="0" Text="Look up painters algorithm" CreationDate="2020-08-24T05:23:26.160" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15844" PostId="10146" Score="0" Text="I'm also just learning raytracing at a moment, so it's only a suggestion to try - notice how in last code, your pdf is using dot to calculate its value. In previous cases you were using uniform pdf. I think that it might cause a faster convergence. Try to replace pdf in last bit of code with &quot;vec3 pdf = 1.0f/(2.0f Pi);&quot; or &quot;vec3 pdf = 1.0f/(1.0f Pi);&quot; and in first and second code snipet and compare results." CreationDate="2020-08-24T07:37:09.770" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15845" PostId="10146" Score="0" Text="@DirectX_Programmer,Thanks for answering!I have replaced pdf back to &quot;pdf = 1.0f/(1.0f*Pi)&quot;,that work,the image was back to the more realistic dark one,however it was still slightly different from the first one.I think the pdf item is to downweight the overweight part &quot;uvw.local(randCosDir())&quot;,but it seems that the pdf didn't do the job very well,it downweight too much!I've try another importance sampling method from(https://computergraphics.stackexchange.com/questions/4979/what-is-importance-sampling),but it was still,too bright,or maybe that was what lambertian actually looks like!" CreationDate="2020-08-24T10:45:45.460" UserId="13864" ContentLicense="CC BY-SA 4.0" />
  <row Id="15846" PostId="10146" Score="0" Text="@dsurekt I guess that best solution would be to create raytracing algorithm accumulating over frames (for uniformly sampled raytracing), converging to ground truth over time and compare your single frame results with that." CreationDate="2020-08-24T11:00:49.587" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15847" PostId="10146" Score="0" Text="@DirectX_Programmer,I've added the rendering with 10,000 samples per pixel(uniform hemisphere sampling)at the bottom of my question,It should be the correct lambertian look.I don't know why importance sampling will result in different image..." CreationDate="2020-08-24T11:27:21.737" UserId="13864" ContentLicense="CC BY-SA 4.0" />
  <row Id="15848" PostId="10147" Score="0" Text="I'm happy to hear that. Please accept your answer to let know that question no longer requires attention." CreationDate="2020-08-24T16:35:32.423" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15849" PostId="10118" Score="2" Text="&quot;*it actually does happen, especially when the z of the vertices are set to their w, for skyboxes for example.*&quot; That changes nothing about the math here. The whole point of (near) clipping is to ensure that all vertices have a positive value for `w`. If you get a value of `w` for a near-clipped vertex, then you did your clipping computation wrong." CreationDate="2020-08-24T16:39:56.533" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15850" PostId="10147" Score="1" Text="I'll accept it tomorrow(StackExchange says that I can only accept it tomorrow or after.)" CreationDate="2020-08-24T23:11:19.530" UserId="13864" ContentLicense="CC BY-SA 4.0" />
  <row Id="15851" PostId="7662" Score="0" Text="Should `wo` in surface space or world space?" CreationDate="2020-08-25T00:36:12.330" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15852" PostId="10027" Score="0" Text="You may read recent SIGGRAPH papers (2010-present) for ideas." CreationDate="2020-08-25T15:04:27.793" UserId="6470" ContentLicense="CC BY-SA 4.0" />
  <row Id="15853" PostId="10144" Score="0" Text="The terrain itself is looks like a &quot;height map grid&quot;, although I don't think there is a standard term for it." CreationDate="2020-08-26T08:21:29.460" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15854" PostId="10140" Score="0" Text="Thank you for your response, appreciate it. I got this solved by using Google API Vision" CreationDate="2020-08-26T20:54:30.847" UserId="13861" ContentLicense="CC BY-SA 4.0" />
  <row Id="15855" PostId="10142" Score="0" Text="Thank you for your response, appreciate it. I got this resolved by using Google's API vision" CreationDate="2020-08-26T20:55:04.163" UserId="13861" ContentLicense="CC BY-SA 4.0" />
  <row Id="15856" PostId="10152" Score="0" Text="If your problem is solved, mark the answer that solved your problem (probably your own) as accepted." CreationDate="2020-08-27T05:27:21.370" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15857" PostId="5802" Score="0" Text="You don't need to take a screenshot, you can export the result to pdf (or EMF) formats, and then open it in something like inkscape to export it to a PNG with the resolution of your choice." CreationDate="2020-08-28T18:29:40.827" UserId="11748" ContentLicense="CC BY-SA 4.0" />
  <row Id="15858" PostId="10124" Score="0" Text="did you find the link :p?" CreationDate="2020-08-29T00:39:51.580" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15860" PostId="10148" Score="0" Text="Make arrays with the black and blue edges, or with pointers to them, then there is no branching but it is likely slower. It's not clear what your goal is from your post. Optimization? You likely won't get this to be faster/simpler without additional details of what your algorithm does." CreationDate="2020-08-29T07:54:21.577" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15866" PostId="10160" Score="0" Text="Are you using bvh? If you are, can you try disabling it and see what you get?" CreationDate="2020-08-29T10:54:35.840" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15868" PostId="10160" Score="0" Text="@lightxbulb thanks for the reply! I was not using BVH for this example, but tried to disable it from the project anyways and it didn't make a difference in the rendered output..." CreationDate="2020-08-29T14:31:50.223" UserId="13559" ContentLicense="CC BY-SA 4.0" />
  <row Id="15869" PostId="10160" Score="0" Text="I looked through your code, and nothing struck me as wrong at first glance, that's why I suggested testing without a bvh. Currently my only guess is that it could be an issue due to self-intersection, since the lighting seems to fail at angles that are more shallow. I would suggest trying to change: `if(!scene.hit(r, 0.001, infinity, rec))` to `if(!scene.hit(r, 0.01, infinity, rec))` and see if that changes anything." CreationDate="2020-08-29T16:03:30.343" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15870" PostId="10148" Score="0" Text="Is your issue actually that you want to skip the twin of an edge that you've gone through? You don't even need to mark those. For example apply the operation only to the halfedge with the larger index from the twins:&#xA;`if (edge[i].twin_idx &lt; i) do_something();`, this still requires branching however. You cannot remove branching, unless you order your edges at construction so that the first half of the array is made of  halfedges on one side, and the second half is made of the other halfedges of the other side. Still, you need branching to order them like so." CreationDate="2020-08-29T16:09:53.160" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15871" PostId="10155" Score="0" Text="Do you mean output to the display and speakers of a system, or output to a file? If the former, probably SDL is a good bet as it will let you fill image and audio buffers in realtime and present/play them. For output to a file, if you're interested in using your own codecs inside standard containers like mp4/mkv, then adding codecs to ffmpeg might be useful." CreationDate="2020-08-29T18:43:24.037" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15872" PostId="10148" Score="0" Text="I guess You could run the above once to split the HE in the order in which they would get visited. &#xA;&#xA;The question is then, how could you enforce that property every time you add an edge. Which I guess you could do If you had 2 independent arrays for the HE... I wonder if that would introduce noticeably more performance drop due to cache misses." CreationDate="2020-08-29T21:16:25.140" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15873" PostId="10160" Score="0" Text="@lightxbulb hmm.. this still didn't change how the render looks. But thanks for the help and direction! I will keep troubleshooting and if you think of any other ideas, let me know :)" CreationDate="2020-08-29T21:42:31.157" UserId="13559" ContentLicense="CC BY-SA 4.0" />
  <row Id="15874" PostId="10148" Score="0" Text="I highly recommend that you profile your app before trying to optimize it. Did you get this branch as a hotspot or something?" CreationDate="2020-08-29T23:41:06.580" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15875" PostId="10148" Score="0" Text="Mostly this particular pattern is annoying to code every time and makes guaranteeing correctness ahrder I am trying to make the code easier to replicate without htting performance too much" CreationDate="2020-08-30T00:13:13.563" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15876" PostId="10148" Score="0" Text="You can make it a function: then you won't have to code it every time. I don't think you can make it much simpler." CreationDate="2020-08-30T09:45:36.610" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15877" PostId="10148" Score="0" Text="The problem is that the inner body is slightly different every time, so the iteration pattern is teh same but the logic isn;t" CreationDate="2020-08-30T17:33:38.140" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15878" PostId="10148" Score="0" Text="Try a higher level iteration function + a lambda. The lambda will be specialized, the higher level function would perform your custom iteration. But honestly, that's over-engineering, and you shouldn't really have an issue with a simple if statement imo." CreationDate="2020-08-30T17:38:41.773" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15879" PostId="10171" Score="0" Text="Thanks! Concise, clean, and professional, I totally understand these two terms!" CreationDate="2020-08-30T19:19:05.770" UserId="13896" ContentLicense="CC BY-SA 4.0" />
  <row Id="15880" PostId="10155" Score="0" Text="@NathanReed &quot;output to the display and speakers of a system&quot; That's what interests me, since you mention SDL, what about EGL and OpenAL, are they good alternatives? What I am interested in is having control over what will be shown on the screen and what will sound on the speakers" CreationDate="2020-08-30T20:42:23.643" UserId="13887" ContentLicense="CC BY-SA 4.0" />
  <row Id="15881" PostId="10169" Score="0" Text="Is there a similar way to compute the Laplacian operator for 3D mesh?" CreationDate="2020-08-31T01:16:24.613" UserId="13879" ContentLicense="CC BY-SA 4.0" />
  <row Id="15882" PostId="10169" Score="0" Text="Laplacians are tricky to even define, because it's a second-derivative operator, so with linear interpolation it would just be zero. Getting a useful answer out of it requires defining some higher-order interpolation scheme first. You can see some approaches to this in [Laplace–Beltrami: The Swiss Army Knife of Geometry Processing](http://ddg.cs.columbia.edu/SGP2014/LaplaceBeltrami.pdf)" CreationDate="2020-08-31T02:33:13.917" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15883" PostId="10169" Score="0" Text="Thank you. I will take a look at it." CreationDate="2020-08-31T05:46:48.340" UserId="13879" ContentLicense="CC BY-SA 4.0" />
  <row Id="15884" PostId="10160" Score="0" Text="I just had a quick browse through your code. In Lambertion::scatter there is a comment about attenuating light based on distance. For a raytracer this is not required as the distribution of rays themselves cause attenuation over distance. Typically albedo values get multiplied by the current paths attenuation (an exception would be importance sampling as that uses weighted samples) I didn't check the whole project source so I'm making a bit of a guess." CreationDate="2020-08-31T07:21:29.503" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15886" PostId="10124" Score="0" Text="Found the link and edited it into my answer. However, I forgot that it is in german so I don't know if it is of any help to you." CreationDate="2020-08-31T10:30:06.803" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15891" PostId="10156" Score="0" Text="What do you mean by sample points inside and outside the mesh? Just random points and determine if they are inside/outside? And by &quot;mesh&quot; are you referring to a closed manifold that has a well defined &quot;interior&quot;?" CreationDate="2020-08-31T16:08:25.747" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15892" PostId="10176" Score="1" Text="Getting darker from one frame to the next must mean that some input value is changing, or something about the scene is changing. The same math won't produce different results when evaluated with the same inputs. Whatever the problem is, I don't think it lies in the code you've posted here" CreationDate="2020-08-31T23:12:53.813" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15893" PostId="10176" Score="0" Text="It's progressive ray tracing. Simply saving the average color of previous frames and current frame, then use it for next frame. I guess the difference came from randomF and floating point issue. But FLT_MAX is a very large number, there shouldn't be overflow issue. @NathanReed" CreationDate="2020-09-01T00:59:22.510" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15896" PostId="9518" Score="0" Text="Right way to do that is setting the values directly on points (vertices) or using rotations." CreationDate="2020-01-28T23:29:40.697" UserId="11807" ContentLicense="CC BY-SA 4.0" />
  <row Id="15898" PostId="10176" Score="0" Text="I can't see code responsible for accumulating color. To correctly accumulate non-uniform sampling, you need to use appropriate pdf ( http://cwyman.org/code/dxrTutors/tutors/Tutor14/tutorial14.md.html ). I didn't check all of your D*F*G, but if there are no mistakes regarding missing terms or lacking of some PI, then my best guess is not using appropriate pdf. It might cause non-important frames to contribute too much, which is introducing darkening over time." CreationDate="2020-09-01T13:21:08.443" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15899" PostId="10160" Score="0" Text="@PaulHK BINGO!! It was that fix in the Lambertian class that fixed it! Thanks so much for your reply, I've been struggling with this bug for over a week " CreationDate="2020-09-01T13:36:12.797" UserId="13559" ContentLicense="CC BY-SA 4.0" />
  <row Id="15900" PostId="10160" Score="0" Text="Good to know :) - I'll post my reply as an answer so you can close the question" CreationDate="2020-09-01T15:21:22.020" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15902" PostId="10176" Score="0" Text="At a glance it looks like an gamma/linear conversion that shouldn't be there or maybe precision loss. How are you summing the output into the final image ?" CreationDate="2020-09-01T15:39:22.297" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15903" PostId="10180" Score="0" Text="I should say that they’re not *totally* arbitrary- they lie on the form corners of a square and it’s only the z coordinate that changes" CreationDate="2020-09-01T20:27:35.657" UserId="13696" ContentLicense="CC BY-SA 4.0" />
  <row Id="15904" PostId="10181" Score="0" Text="Interesting suggestion- thank you!" CreationDate="2020-09-01T20:28:11.913" UserId="13696" ContentLicense="CC BY-SA 4.0" />
  <row Id="15905" PostId="10183" Score="0" Text="Can you specify in which cases it works and in which cases it fails? What is the result if it fails? Those details might help to find the problem faster." CreationDate="2020-09-02T05:37:04.710" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15906" PostId="10183" Score="0" Text="@wychmaster i edited my question" CreationDate="2020-09-02T05:41:15.730" UserId="13909" ContentLicense="CC BY-SA 4.0" />
  <row Id="15907" PostId="10183" Score="0" Text="I currently don't have the time to double-check my claims and write a proper answer, but as far as I can remember, the algorithm is only supposed to work for the first octant starting at `x=0` and `y=+a` where `a` is an arbitrary number. You can get the values for all other octants by abusing the symmetry of the circle. Have a look into [this link](https://www.geeksforgeeks.org/bresenhams-circle-drawing-algorithm/)." CreationDate="2020-09-02T07:11:24.290" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15908" PostId="10184" Score="5" Text="I find it hard to understand what exactly you want to achieve. I guess you draw a straight line and then &quot;pull&quot; to make it a curve? If you add a sketch or two, it might help." CreationDate="2020-09-02T07:33:02.690" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="15909" PostId="10187" Score="0" Text="Thanks for the response @Joseph O'Rourke. Can you please provide some link to an article or whatever like this? It's a little hard for me to figure out exactly how this should be done." CreationDate="2020-09-02T12:40:51.230" UserId="13912" ContentLicense="CC BY-SA 4.0" />
  <row Id="15910" PostId="10187" Score="0" Text="@folibis: You might need to explore a bit of computational geometry. Maybe start here: [How to find 3D rectangle intersection with segment?](https://math.stackexchange.com/q/2741180/237)." CreationDate="2020-09-02T12:43:33.507" UserId="4748" ContentLicense="CC BY-SA 4.0" />
  <row Id="15911" PostId="10176" Score="0" Text="(previous_average * frame_count + current_color) / (frame_count + 1) @PaulHK" CreationDate="2020-09-02T14:07:42.583" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15912" PostId="10184" Score="0" Text="@Tare i edited my question" CreationDate="2020-09-02T14:42:57.653" UserId="13909" ContentLicense="CC BY-SA 4.0" />
  <row Id="15913" PostId="10176" Score="0" Text="I am a bit confused, when calculating G, should I use ray_driection or negative_ray_direction. If we didn't consider subsurface, the input ray direction will always have negative direction on normal direction.  @DirectX_Programmer" CreationDate="2020-09-02T14:55:52.213" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15914" PostId="10188" Score="0" Text="&quot;*I found this link here but I don't know how to use that.*&quot; What do you mean by that? The answer there seems to give a pretty clear equation. What's the difficulty in using it?" CreationDate="2020-09-02T16:54:51.410" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15915" PostId="10189" Score="0" Text="Thanks, I found out the main problem is Float16 render target. The maximum Float16 value is 65504. It will have overflow and precision problem. Another problem is `wo` and `wi` should both on the same hemi sphere on normal direction, sadly most documents I read didn't mention it. So one of them should be  negative ray direction." CreationDate="2020-09-02T20:44:58.300" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15916" PostId="10189" Score="0" Text="This tutor from cwyman looks good. While, I didn't see a complete repo for his code (-:" CreationDate="2020-09-02T20:47:22.440" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15917" PostId="10188" Score="0" Text="@Thomas is this related to your other question? If the quaternions here are all actually 3D vectors, there's a simpler way to solve this using cross products. If it needs to work for full 4D quaternions, it's slightly more involved." CreationDate="2020-09-02T23:42:29.480" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15918" PostId="10186" Score="0" Text="If the four corners can really be arbitrary (non rectangular) then I think you'll have to derive something akin to [inverse bilinear interpolation](https://iquilezles.org/www/articles/ibilinear/ibilinear.htm), but using slerps instead of lerps." CreationDate="2020-09-02T23:58:25.317" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15919" PostId="10184" Score="1" Text="From quickly reading up on the Bresenham algorithm, it has little to do with with your line you want to draw. You also don't mention the framework you're using, so this is still tricky to answer. However, the two things that come to mind when creating a curved line based on certain points are Splines and Bezier Curves. Your points would be the start and end point from your first line drawing, plus either the center or the point closest to the mouse click when pulling (and then to wherever you pull it). Based on that, you should be able to get a curve" CreationDate="2020-09-03T05:46:31.683" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="15920" PostId="10188" Score="0" Text="@NathanReed yes it is related to the other question. I think when solving this problem, I am aware of solving the other question too. It doesn't matter if q, q1 and q2 are quaternions or vectors. The only important thing is, that the result t is in both cases equal." CreationDate="2020-09-03T06:25:46.580" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15921" PostId="10188" Score="0" Text="@NicolBolas I can't see t as a result in the posted link. also Eigen seems to not have an operator to calculate the log of quaternions. To calculate the logarithm on quaternions is not really the problem, I'll find a way of doing that." CreationDate="2020-09-03T06:33:03.620" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15922" PostId="10176" Score="0" Text="What format is the texture these colours are stored in ? I'm suspecting a floating point truncation issue is to blame.." CreationDate="2020-09-03T07:02:10.030" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15923" PostId="10176" Score="0" Text="It's using RGBAFloat16, then changing to RGBAFloat32 could fix that problem. @PaulHK" CreationDate="2020-09-03T07:30:54.820" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15924" PostId="10189" Score="0" Text="http://cwyman.org/code/dxrTutors/dxr_tutors.md.html - at the end of page, there is link named &quot;code bundle&quot;. You can download .zip file with each tutorial. But keep in mind, that Chris Wyman (cwyman) is using Falcor framework, so C++ will be totally different if you're not using any framework. Also HLSL code will differ, because Falcor is providing helper methods which you're not going to have with bare code." CreationDate="2020-09-03T09:20:35.053" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15925" PostId="10188" Score="1" Text="@Thomas: &quot;*I can't see t as a result in the posted link.*&quot; It's the first sentence of the third paragraph. And even if he didn't spell it out, all you need to do is apply basic rules of algebra to the second equation to solve for `t`." CreationDate="2020-09-03T13:27:15.153" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="15926" PostId="10191" Score="0" Text="How do you measure that performance drop exactly? There are many possible explanations and knowing this would help narrow it down to the more likely ones." CreationDate="2020-09-03T15:09:38.160" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15927" PostId="10191" Score="0" Text="I am doing GPU ray tracing in shader. It will drop from 50+fps to 10-fps. @Olivier" CreationDate="2020-09-03T15:44:17.763" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15928" PostId="10184" Score="0" Text="@Tare bezier curves can i use to 3d as well?" CreationDate="2020-09-03T18:18:19.527" UserId="13909" ContentLicense="CC BY-SA 4.0" />
  <row Id="15931" PostId="10191" Score="0" Text="Does your screen go from mostly empty without the box to being filled by a cornell box? Or is the difference there no matter how you place the camera? Are you rendering only primary rays or some number of bounces?" CreationDate="2020-09-04T14:42:26.157" UserId="5717" ContentLicense="CC BY-SA 4.0" />
  <row Id="15932" PostId="10191" Score="0" Text="The mesh takes 50% screen, the box will take 80% maybe. Yeah, it has max 32 bouncing number. @Olivier" CreationDate="2020-09-04T15:55:27.293" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15933" PostId="10193" Score="0" Text="This question can not be adequately answered. First we do not know what the modeling famework your using to make a square, we do not know what you want the end result to be." CreationDate="2020-09-05T08:55:02.860" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="15934" PostId="10191" Score="0" Text="How big are the triangles of the box's walls? If you subdivide them 2 or 3 times, you'll notice an increase in performance but still not upto the original level. The walls will almost always lower the performance of the bvh as they are more a form of a container. You'll get faster performance by checking 12 triangles (assuming 2 per wall) separately instead of dumping them inside a bvh." CreationDate="2020-09-05T16:03:23.167" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15935" PostId="10191" Score="0" Text="The thing is, usually you expand the BVH nodes after constructing it. So if the triangles are big, the nodes will expand and overlap a lot more decreasing the efficiency a lot." CreationDate="2020-09-05T16:04:36.280" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15936" PostId="10193" Score="0" Text="As joojaa already mentioned, your question is too unspecific to give you a qualified answer. Please use the edit button to add more details. If I got you correctly, you want to automatically identify the borders of an arbitrary area and move it around, right? We still need to know in which form you work with your data (just pixel? parametric lines/curves? other?)" CreationDate="2020-09-05T16:13:17.250" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15937" PostId="10191" Score="0" Text="The box is not using any triangles, it's a collection of rect. @gallickgunner" CreationDate="2020-09-05T18:18:47.713" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15938" PostId="10191" Score="0" Text="@iaomw - sorry didn't understand. Do you mean you are doing ray-rectangle intersections in the leaf nodes? Even if that's the case, the point is if you expand the nodes after construction they'll overlap reducing the efficiency." CreationDate="2020-09-05T18:37:22.787" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15939" PostId="10193" Score="0" Text="@wychmaster i edited my question" CreationDate="2020-09-05T20:43:35.830" UserId="13909" ContentLicense="CC BY-SA 4.0" />
  <row Id="15940" PostId="10193" Score="0" Text="i think you can use a tranformation in any axis, x, y or z, in this specific case is Y," CreationDate="2020-09-05T20:47:19.633" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="15942" PostId="10184" Score="1" Text="@hubman i dont know sure, but bezier curves is used to 2d and 3d." CreationDate="2020-09-05T21:35:41.700" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="15943" PostId="10191" Score="0" Text="What do you mean by expand? @gallickgunner" CreationDate="2020-09-05T23:21:39.920" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15944" PostId="10191" Score="0" Text="@iaomw - BVH is object subdivision not space, so there will be nodes that overlap. So after you construct the bvh you don't only expand but I guess the proper word would be &quot;fit&quot; (expand/shrink) the node to its contents." CreationDate="2020-09-06T07:07:51.003" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15945" PostId="10191" Score="0" Text="I think I'm failing to express the key point here. Look at it this way. The key points to making efficient BVHs are, nodes should have minimal volumes, and pruning at the top level should prune the entire subtree of that child node. Now imagine a cornell box scene with a small teapot inside. If you try to dump the large walls inside the BVH these 2 points take a hit since dumping any wall to any node will make it expand it's width or height or both to that of the root Bounding node. So either you subdivide the walls or take them out of the bvh" CreationDate="2020-09-06T07:17:25.167" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15947" PostId="10193" Score="0" Text="Bresenham is just an algorithm to draw all pixels of a line defined by some parameters. So the approach would be to modify the parameters of the affected lines (for example increasing the y-values of the start and endpoint), clear your canvas, and redraw all lines (including the ones that didn't move). This is usually how graphic APIs like OpenGL and DirectX do it. You don't move the pixels in an existing image. You simply redraw it. So you need to store the parameters of all your drawn objects, update them if needed, and redraw everything if something changes." CreationDate="2020-09-06T11:37:01.680" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15948" PostId="10193" Score="0" Text="Do you need to be able to rotate the camera or objects? Or will you always look at those from a specific direction/finite set of directions?" CreationDate="2020-09-06T12:08:11.400" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="15949" PostId="10191" Score="0" Text="So, it better to divide big shape into small triangles. But how small it should be? @gallickgunner" CreationDate="2020-09-06T23:40:28.147" UserId="13874" ContentLicense="CC BY-SA 4.0" />
  <row Id="15950" PostId="10191" Score="0" Text="@iaomw - They can be big, but not as big as the root container itself. Take the scenario of the cornell box. The cornell box itself is the root node so it's walls kinda form up the walls of the root node itself. So when you try to insert any of the walls in a child node the node expands to the size of the root node in any or all of the 3 dimensions. Now assuming you are building a binary bvh, this calls for a poor hierarchy. If we had a bvh with 7 child nodes per level, then we could perhaps assign a subtree for each of the walls and the the 7th to cover up the space inside those walls." CreationDate="2020-09-07T06:59:59.510" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="15951" PostId="10184" Score="1" Text="In theory you could just use the Bezier Curve in any plane and then project it into your 3D world. If you want to use it for surfaces, there are Bezier Surfaces (see https://en.wikipedia.org/wiki/B%C3%A9zier_surface ). If you just want a curve going through all 3 dimensions, there are some sources for that as well (see https://mathcurve.com/courbes3d.gb/bezier3d/bezier3d.shtml ). So I guess any way you look at it you should be fine in 3D." CreationDate="2020-09-07T07:57:42.877" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="15952" PostId="10193" Score="0" Text="Please edit your question to clarify what exactly you want to achieve. Read through the comments to get some impressions of what additional information might be necessary to understand what you are asking. If you need help, feel free to contact me [in our chat](https://chat.stackexchange.com/rooms/26589/the-cornell-box) - We can reopen the question as soon as the missing details are added." CreationDate="2020-09-07T08:22:01.277" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15954" PostId="10156" Score="0" Text="Sorry for the late response. Yes, sample points uniformly or randomly outside and inside the mesh surface. By mesh I mean, surface(for eg. hand mesh, which is not watertight or not closed)." CreationDate="2020-09-09T10:14:33.293" UserId="13888" ContentLicense="CC BY-SA 4.0" />
  <row Id="15955" PostId="10158" Score="0" Text="Sorry for the late reply! Thank you @Hubble for your input. SDFs example helped me. But in my case meshes are not watertight or not closed. So, would you have any suggestions to convert them into watertight or closed meshes?" CreationDate="2020-09-09T10:17:47.090" UserId="13888" ContentLicense="CC BY-SA 4.0" />
  <row Id="15956" PostId="10163" Score="0" Text="Thank you, I followed a similar approach." CreationDate="2020-09-09T10:18:44.737" UserId="13888" ContentLicense="CC BY-SA 4.0" />
  <row Id="15957" PostId="10199" Score="0" Text="This does look like a case of precision loss, the reflections themselves seem wrong or maybe I'm misjudging where the reflective surface is (hard to tell). How large is the object in world space coordinates ?" CreationDate="2020-09-09T10:35:00.923" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15958" PostId="10201" Score="0" Text="inside each vertex is a 2D texture-coordinate. This allows the Graphics library to interpolate the texture-coordinates of all 3 vertices of each triangle during the rasterization step." CreationDate="2020-09-09T15:40:13.637" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15959" PostId="10201" Score="0" Text="some more information about the concept of vertex-buffer-objects (VBO) and texturing you can finde here: https://learnopengl.com/Getting-started/Hello-Triangle" CreationDate="2020-09-09T15:41:54.150" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15960" PostId="10199" Score="1" Text="Is this some particular engine? The problem may have to do with peculiarities of that engine's implementation of this effect, and you might have more success asking about it on a forum devoted to that engine." CreationDate="2020-09-09T17:17:45.837" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15961" PostId="10198" Score="1" Text="Sorry, this is really unclear - what do you mean &quot;upload std::vector or std::array&lt;...&gt; to GPU and use it with Texture2D[]&quot;? Can you add more details about what you're trying to achieve?" CreationDate="2020-09-09T17:20:46.550" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15962" PostId="10201" Score="0" Text="@Thomas what are all circles in the upper right corner?" CreationDate="2020-09-09T17:37:14.463" UserId="6281" ContentLicense="CC BY-SA 4.0" />
  <row Id="15963" PostId="10198" Score="1" Text="@NathanReed I've added more detailed explanation at the end of question." CreationDate="2020-09-09T18:36:22.227" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15964" PostId="10201" Score="2" Text="@MonaJalal Probably screws or holes in the model. Texture file you've sent looks perfectly normal. As Thomas said - by following tutorias from learnopengl, you can easily understand how this thing works. For DirectX (even though, in theory it works exactly the same, but you might want to learn this framework) - you can check http://www.rastertek.com/ ; Even though, his style is not the best, his explanation are nice. And there are a lot of tutorials for the start." CreationDate="2020-09-09T20:00:22.750" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15965" PostId="10205" Score="0" Text="I am just getting into the topic of computational geometry so excuse my ignorance. When you say looking for negative eigenvectors, I assume you mean negative eigenvectors of the hessian, which must mean the hessian is a matrix. Is that correct?" CreationDate="2020-09-09T20:32:57.383" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15966" PostId="10205" Score="0" Text="Yes (or in general, a linear operator). It contains all the second derivatives of the function along the mesh, just as the gradient contains the first derivatives. By finding the eigenvectors of the Hessian you get the principal curvature directions (of the graph of the function - not the curvatures of the mesh surface), and if one of those has negative curvature, that's the way out of the saddle point." CreationDate="2020-09-09T21:22:09.153" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15967" PostId="10203" Score="0" Text="I've wrestled a little bit with this issue myself but never came to a good conclusion. What would be the effect of reflecting the below hemisphere samples via the surface normal (e.g. flip the normal for that case) ?" CreationDate="2020-09-10T02:42:34.650" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15968" PostId="10200" Score="1" Text="You might wanna look into &quot;Optimization on a manifold&quot; which is literally the continuous analogous of what you're doing. Based on the manifold you work out &quot;gradient&quot;,&quot;retraction&quot; and &quot;hessian&quot;, the use of the latter allows you to implement second order scheme which should behave better in theory. Discretizing these will give you the analogous on a mesh." CreationDate="2020-09-10T03:33:59.503" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="15969" PostId="10204" Score="0" Text="It depends, if you want to represent a mesh you can look into &quot;open mesh&quot; implementation so you can find what you need to do to define face and vertices. If you want a general DCEL more than defining your self manually each point, half edge and face there's no much you can do. CGAL has a dcel representation of planar map, which as far I'm aware it's quite a general object you can represent with DCEL (so maybe they have specialized API for these object representation in terms of DCEL)" CreationDate="2020-09-10T03:37:49.430" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="15970" PostId="10201" Score="2" Text="@MonaJalal to figure out at what position in the mesh &quot;model&quot; these small circles are, you can color them lets say in magenta and see where in the model are magenta parts." CreationDate="2020-09-10T07:54:26.683" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="15971" PostId="10203" Score="0" Text="@PaulHK, that's a very good question. Reflecting your samples will change the actual shape of the PDF, not just scale as for cutting samples. You would have to adjust PDF evaluation (which should be doable), but it will degrade the importance sampling somehow for grazing angles. It's a matter of practical testing to find out which approach is better." CreationDate="2020-09-10T11:09:05.693" UserId="2479" ContentLicense="CC BY-SA 4.0" />
  <row Id="15972" PostId="10163" Score="0" Text="Please accept the answer so that others don't spend a bunch of time writing a more explicit response only to discover a &quot;thank you&quot;" CreationDate="2020-09-10T14:37:49.203" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="15973" PostId="10209" Score="0" Text="I found already the first two references you pointed out. But they're not deep learning based, I'm already aware of Whitted1980 and CT1967 but they have nothing to do with inverse rendering. Based on the thesis Inverse rendering it self has nothing to do with deep learning (I personally thought it was something *enabled* by Deep Learning, but I was wrong). I'm doing my research at the moment. The rendering network you pointed out is interesting, but it's dated 2019 and I have a feeling Inverse Rendering with Deep Learning started before that. But I can start with that I guess." CreationDate="2020-09-11T12:26:18.243" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="15980" PostId="10209" Score="0" Text="I am aware that they're not related. It was just meant to be an example of &quot;basic papers&quot; in other areas of computer graphics. I'm not sure if you understood me correctly. What I meant was to use references at the end of papers to find related works. In Yu/Smith work from 2019, I found in reference section this paper - http://papers.neurips.cc/paper/5851-deep-convolutional-inverse-graphics-network.pdf - Kulkarni et al 2015 - Deep Convolutional Inverse Graphics Network." CreationDate="2020-09-11T15:58:46.350" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15981" PostId="10209" Score="1" Text="I understood what you meant, essentially start from whatever paper I find and navigate backwards through the references until I find the first paper." CreationDate="2020-09-11T16:04:38.383" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="15985" PostId="10203" Score="0" Text="I suppose if the reflected ray starts from a small distance/epsilon above the surface the ray would re-collide with the same surface, albeit from a more scattered incoming angle." CreationDate="2020-09-11T17:41:45.143" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="15986" PostId="10213" Score="5" Text="You can find all the basics you need [on this site](https://learnopengl.com/Lighting/Basic-Lighting)." CreationDate="2020-09-11T21:20:08.697" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="15989" PostId="8757" Score="0" Text="http://wisionlab.cs.wisc.edu/project/quanta-burst-photography/ - maybe this paper will be helpful (&quot;Quanta Burst Photography&quot; by Ma et al. 2020)" CreationDate="2020-09-12T09:22:34.710" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="15990" PostId="2172" Score="0" Text="I would expect the histogram of real-world image to be a lot more noisy and chaotic that CG generated or CG-painted ( If image textures are not used ).&#xA;Of course, it's a pretty coarse criterion." CreationDate="2020-09-13T11:02:44.650" UserId="1810" ContentLicense="CC BY-SA 4.0" />
  <row Id="15991" PostId="10215" Score="1" Text="It's still a discrete timestep method, right, so it's not going to solve the whole thing in one step. It's not going between arbitrary points far apart on the mesh, but only between neighboring ones. The neighbor information is encoded in the Laplace operator. I presume you would still have to iterate this equation many times to get to an equilibrium solution for the whole mesh." CreationDate="2020-09-13T16:10:57.370" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15992" PostId="10215" Score="1" Text="$L_C$ here is not a diagonal matrix btw, it contains a nonzero entry for each neighboring pair of vertices $i, j$." CreationDate="2020-09-13T16:18:11.377" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15993" PostId="10217" Score="3" Text="Yes, this sort of thing is commonly done, it's called photogrammetry. If you google that you'll find a lot about it, and plenty of software. I haven't used any of it myself so I can't recommend a particular program, but most likely there's something out there that will serve your purpose." CreationDate="2020-09-13T17:41:05.283" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15994" PostId="10215" Score="0" Text="Ah i missunderstood, i thought the entire sum was put inside the diagonal. But thanks to your comment I see that you can also put each term of the sum in an entry of a matrix and get the same result." CreationDate="2020-09-13T18:55:08.910" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15995" PostId="10215" Score="1" Text="@NathanReed, The method actually only executes one step, because it's not interested in actually modelling the heat diffusion over time. It only uses it to model geodesics, as heat diffusion and geodesics are intimately related.&#xA;&#xA;Bot even if you were to simulate heat diffusion, my confusion remains.&#xA;How can you parallelize the solution in this fashion? Why can you parallelize the solution? Why don;t you need to look at the full conectivity? Why can you just focus on the local connectivy and that;s enough?&#xA;&#xA;That's what doesn;t click for me." CreationDate="2020-09-13T19:00:47.003" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="15996" PostId="10218" Score="1" Text="This is amazing information! Thank you both pmw1234 and Nathan Reed! I will take that information and try things out! Cheers!" CreationDate="2020-09-13T22:05:39.397" UserId="13959" ContentLicense="CC BY-SA 4.0" />
  <row Id="15997" PostId="10219" Score="1" Text="Honestly if you can script Blender to do what you need to do via Python, that might be the easiest way especially if you're already familiar with Blender. There's a lot of 3D graphics libs for Python but they all seem to focus on realtime, using OpenGL etc." CreationDate="2020-09-13T22:36:11.317" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="15998" PostId="10222" Score="0" Text="Thanks for your answer! I think Q1 is clear to me. But for Q2 and Q3, what I mean is: if we can estimate the path integral by generating a single path, why mainstream renderers(mitsuba and pbrt) uses the increasing path construction scheme and accumulate the contribution of different paths?" CreationDate="2020-09-14T01:24:16.343" UserId="13896" ContentLicense="CC BY-SA 4.0" />
  <row Id="15999" PostId="10222" Score="0" Text="If you use Russian roulette while constructing a path, that's a probability distribution over path lengths; you could as well choose the length randomly in advance, and construct the path to that length. Accumulating direct lighting samples along a path as it's constructed is a convenient/cheap kind of reuse, which I suppose results in correlated samples in path space (not totally independent of each other) but it's still unbiased and correct as long as you get the pdfs right." CreationDate="2020-09-14T03:13:54.537" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16000" PostId="10222" Score="0" Text="@NathanReed Thanks for the comment. I did some further research, reading and implementation. From the BDPT's paper and PBRT's BDPT implementation, I think I now understand that it is mathematically correct to use the path integral form to estimate the final integral. However, if I understand correctly, randomly generating the path to a specific length can only account for a specific set of scene parameters and for parameters that cannot the paths will have low contribution. That's probably why we construct incremental length so we can account for more scenarios." CreationDate="2020-09-15T02:22:18.253" UserId="13896" ContentLicense="CC BY-SA 4.0" />
  <row Id="16001" PostId="10184" Score="0" Text="I might add that for more control (with more implementation and math complexity off course) BSplines are great. Pomax has a primer on Bézier that ends with them so you might look into that." CreationDate="2020-09-15T04:52:19.673" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="16002" PostId="10226" Score="0" Text="take a look at this: https://www.ipb.uni-bonn.de/pdfs/Yang2010Plane.pdf" CreationDate="2020-09-15T09:48:53.533" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16003" PostId="10226" Score="0" Text="so a very common methodology is RANSAC...  it is as well easy to implement. I've a question... how fast should these planes be calculated? what are you planing to do? How good does the outcoming planes need to be (precise)?" CreationDate="2020-09-15T09:59:59.320" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16004" PostId="10219" Score="0" Text="All you really need for those images are a text editor to be honest. What is the output medium? Eg.a vector image is not really blenders forte. You probably wont want to pay for autocad just for this. Autocad is really a very overpriced legacy system for thise who can not move with the times. In fact its a extortion scheme." CreationDate="2020-09-15T12:29:38.763" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16005" PostId="10223" Score="0" Text="can you also add an image of the normals when you make this small rotation? I think this could help understanding the problem" CreationDate="2020-09-15T15:19:12.287" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16006" PostId="10223" Score="0" Text="Thank you, I added one!" CreationDate="2020-09-15T15:28:27.997" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16007" PostId="10084" Score="0" Text="Radiosity (and any type of rendering formulation) needs to represent diffuse reflectance as a color, whether RGB or spectral, not as a single value for the whole spectrum. That wouldn't be correct; you wouldn't get colored bounce light e.g. in the Cornell Box." CreationDate="2020-09-15T16:18:05.097" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16008" PostId="10226" Score="0" Text="Any segmentation algorithm sounds like it would do the job. You can run some kind of edge preserving denoising (e.g. Perona-Malik), and then form the segmentation on the result." CreationDate="2020-09-15T17:14:09.547" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16009" PostId="10228" Score="0" Text="Thank you for your reply Nathan!&#xA;&#xA;I agree I was off with the nearplane (Note: Justing nearplane = 1).&#xA;&#xA;I think the depth value was stored with correct sign? &quot;vDepth = -(vPos.z - NearPlane) / ( FarPlane - NearPlane);&quot;. You maybe missed the first minus sign there? (anyways did try exactly what you said).&#xA;&#xA;&#xA;Anyways, I been trying what you said now, I get same result :(!&#xA;&#xA;I think only part of what you wrote that I did not get was &quot;and move the light to a negative Z value&quot;&#xA;&#xA;What do you mean by that? It is a stationary light, I did put it in a place where I want it?" CreationDate="2020-09-15T17:47:32.727" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16010" PostId="10228" Score="0" Text="I been studying the method posted in the blog &quot;MJP's Position from Depth 3 post.&quot;.&#xA;I try to do like him, reconstruct the viewspace position from the stored viewspace depthvalue. I cant see what the difference is. But he do not calculate any light in viewspace, dunno if that is something I am failing on somehow. Or maybe I am just crazy, certently starting to feel crazy...." CreationDate="2020-09-15T17:50:36.637" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16011" PostId="10228" Score="0" Text="The depth value was stored with the correct sign, but then reconstructed with the wrong sign, is what I'm saying. The original sign is not getting restored by the code you posted." CreationDate="2020-09-15T17:52:44.543" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16012" PostId="10228" Score="0" Text="Re: &quot;Move the light to a negative Z-value&quot; never mind that, I missed that the light position is in world space not view space. In any case, after transforming by the ViewMatrix it should end up with a negative Z value since it's in front of the camera." CreationDate="2020-09-15T17:55:27.587" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16013" PostId="10228" Score="0" Text="Okok. Hmm does not help so far =/. Light still rotating aound" CreationDate="2020-09-15T17:59:05.800" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16014" PostId="10228" Score="0" Text="BTW for debugging, try visualizing the XYZ positions as RGB, by setting your shaders to output `fract(vPos)` to see the view position from the first pass, or `fract(position)` to see the reconstructed position in the second pass. The results from those two images should match." CreationDate="2020-09-15T17:59:09.867" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16015" PostId="10228" Score="0" Text="Much love! You dont know hos much i been trying. Will do in a moment! And post screen" CreationDate="2020-09-15T18:16:53.767" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16016" PostId="10228" Score="0" Text="Not sure how to look at that. But I guess it looks wrong? Not sure.&#xA;&#xA;I added screenshots above" CreationDate="2020-09-15T18:37:52.487" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16017" PostId="10228" Score="0" Text="OK. So you've got your fract(position) from the second pass, now what you need to do is write out fract(vPos) from the first pass and compare that. The first pass vPos is the ground truth. The second pass position needs to be made to match that." CreationDate="2020-09-15T19:42:27.103" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16018" PostId="10228" Score="0" Text="I added GIF's with that, but dont know how to go forward from here, I am running out of ideas" CreationDate="2020-09-15T20:21:48.917" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16019" PostId="10226" Score="0" Text="Thanks guys. I’m trying to get as close to real-time as possible. Needs to be fairly accurate and the corner points May need to be guessed for one or two points as the items will be handheld so partly occluded." CreationDate="2020-09-16T12:20:05.907" UserId="13968" ContentLicense="CC BY-SA 4.0" />
  <row Id="16020" PostId="10219" Score="0" Text="@joojaa please elaborate on what you mean by &quot;you just need a text editor?&quot; to create images. the output medium would be jpg/png whatever." CreationDate="2020-09-16T14:10:35.320" UserId="13961" ContentLicense="CC BY-SA 4.0" />
  <row Id="16021" PostId="10219" Score="0" Text="You can write eps, svg, dxf, iges, step in any text editor. Since these are vector formats all you need to do is supply a list of coordinates, both eps (better) and svg (with some copying) are even smart enough to turn your 2D coordinates into isometric without you or your starting application know anything about this. Once you know how to write the image by hand (which is easy by the way) all you need is a imaging application since eps and svg are quite commonly used image format quite many applications open them as a image. And as a side effect you now know how to program it." CreationDate="2020-09-16T14:54:50.033" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16022" PostId="10219" Score="0" Text="Anyway i was more interested in the input output medium not format, like webpage, print, app etc. As to where the image goes next. Also would be nice to know what your data comes from?" CreationDate="2020-09-16T14:55:28.707" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16023" PostId="10219" Score="0" Text="its an input of sizes through a form on a website/app to the server which then calculates the cost and generates a mock image, writing to disk and including in the pdf generated, attached to email and sent back, its a little icing on the cake for a quote. Do you have any examples of any of those formats that would be smart enough to transfer my 2d coordinates into 3d. I have used SVG in text format in the browser before and have no way of knows how they could &quot;make it 3d&quot; apart from dxf, which would need something to create a scene" CreationDate="2020-09-16T18:13:32.537" UserId="13961" ContentLicense="CC BY-SA 4.0" />
  <row Id="16024" PostId="10219" Score="0" Text="All 3D is, is a trick. I will quickly make a demo as soon as i have a keybard and arent busy. If you want a pdf then vector drawings are definitely better than raster graphics. Isometric is easy." CreationDate="2020-09-16T19:06:20.003" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16025" PostId="10236" Score="0" Text="This is not a problem of fp precission, it comes from using a linear approximation, as the aproximated gradient won;t be equal to the true gradient, regardless of fp precision." CreationDate="2020-09-17T01:37:39.067" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="16026" PostId="10223" Score="0" Text="in the first shader (vertex) what is the normalMatrix? is it the transpose(inverse(modelViewMatrix))? If not, maybe this is your problem" CreationDate="2020-09-17T14:39:37.600" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16027" PostId="10230" Score="0" Text="sometimes these very tiny things cost the most time to find... But nice, that you found it." CreationDate="2020-09-17T15:08:08.677" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16028" PostId="10230" Score="1" Text="They really do!&#xA;But I learned _alot_ from having this problem. I was forced to read up on alot of stuff on the way" CreationDate="2020-09-17T15:10:51.000" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16029" PostId="10234" Score="0" Text="Oh this is a great answer, thanks! After reading your answer I realised something else. If you think of the 3 columns as the basis vectors for an affine transform, then by changing that `Syz`, it pulls the Z axis upward. That makes a lot of intuitive sense to me." CreationDate="2020-09-17T15:52:56.633" UserId="13824" ContentLicense="CC BY-SA 4.0" />
  <row Id="16030" PostId="10234" Score="0" Text="Glad I could help! You are welcome!" CreationDate="2020-09-17T18:20:13.557" UserId="13963" ContentLicense="CC BY-SA 4.0" />
  <row Id="16031" PostId="10237" Score="2" Text="Can you give us more context for where you've seen these phrases? &quot;Blend shape&quot; is a standard term but I haven't heard &quot;pose blend shape&quot; or &quot;shape blend shape&quot; before." CreationDate="2020-09-18T00:12:53.557" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16032" PostId="10229" Score="0" Text="You could generate rays without using a near/far plane so you avoid introducing precision loss" CreationDate="2020-09-18T06:46:45.557" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="16033" PostId="10229" Score="0" Text="@PaulHK I'm using inverse viewProj matrix to transform ray from ndc to world space. What is other way of generating ray direction?" CreationDate="2020-09-18T07:20:04.153" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16034" PostId="10229" Score="2" Text="You could interpolate between the 4 far corner positions of your view frustum in screen space (keep the distance near 1, we only care about direction and not length) and generate per-pixel ray directions from that. The ray starts at the eye position." CreationDate="2020-09-18T07:37:23.923" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="16035" PostId="10229" Score="2" Text="Your current solution of modifying near/far to be close in magnitude is also valid, ray generation shouldn't need to concern itself with far/near planes" CreationDate="2020-09-18T07:41:02.643" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="16036" PostId="10240" Score="0" Text="to avaid the problem of two faces having the same texture-coordinates, you can re-uv map the geometry while loading the mesh and store both (the original and the new calculated uv-coordinates) into the vertex buffer object (VBO). While rendering the model, use the original uv-coordinates to render the original color and use the new uv-coordinates to add the changes." CreationDate="2020-09-18T08:30:15.993" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16037" PostId="10240" Score="0" Text="2nd hint: the texture resolution can be higher than the original texture, because the uv-coordinates are between 0 and 1 in both dimensions" CreationDate="2020-09-18T08:32:28.823" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16038" PostId="10229" Score="0" Text="Yeah, it worked for me and for now I don't need to change it and I've seen that invViewProj matrix is used in many code samples. I don't know how it looks in industry standard AAA engines, but I'm grateful for an insight of other method of casting rays." CreationDate="2020-09-18T11:57:15.010" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16039" PostId="10235" Score="0" Text="Just a guess off the top of my head, but I wonder if it would help to slerp the gradients instead of lerp?" CreationDate="2020-09-18T16:30:57.153" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16040" PostId="10235" Score="0" Text="That may work well on spheres, but this should work on arbitrary meshes.&#xA;In fact this entire thing is trying to generalize lerping to arbitrary meshes : p" CreationDate="2020-09-18T16:41:54.103" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="16041" PostId="10235" Score="0" Text="I think that slerping might help for arbitrary meshes as well." CreationDate="2020-09-18T16:48:04.110" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16042" PostId="10235" Score="0" Text="How? I thought slerping isn;t really affine. By that I mean that if I offset my mesh theresults I would get would be different." CreationDate="2020-09-18T16:52:40.887" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="16043" PostId="10235" Score="0" Text="I'm imagining that when you interpolate a gradient vector along a triangle edge you do slerp(gradient1, gradient2, t) instead of lerp(gradient1, gradient2, t). It's just a different interpolation function that's sensitive to the angle between the gradients. It doesn't depend on absolute position or orientation of the gradients or the triangle." CreationDate="2020-09-18T17:05:41.133" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16044" PostId="10235" Score="0" Text="Imma try it just to see if it makes a difference" CreationDate="2020-09-18T20:15:20.270" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="16046" PostId="10237" Score="1" Text="You can find both terms here: https://www.mpi-inf.mpg.de/fileadmin/inf/d2/GM/2017/gm-2018-0801-bodymodels_2.pdf" CreationDate="2020-09-19T03:21:26.983" UserId="6281" ContentLicense="CC BY-SA 4.0" />
  <row Id="16047" PostId="10241" Score="0" Text="Thanks! It's nice to know that this texture projection approach is viable for texture painting!" CreationDate="2020-09-19T15:09:11.783" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="16048" PostId="10240" Score="0" Text="Thanks! Could you clarify what you mean by &quot;uv coordinates you've hit&quot;? It's way too vague. Are you talking about uv coordinates I get after generating brush projection frustum?" CreationDate="2020-09-19T15:17:01.030" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="16049" PostId="10244" Score="0" Text="(1/2) Thank you for information about BRDF, I'll fix uniform sampling tomorrow and check results. Regarding image convergance - it is noisy after 5000 samples (please take a look at first post) and it doesn't look different from 100 samples. I am using R8B8G8A8_UNORM, now that I've realized it, it definately pointless to use alpha channel here, it's just default format that I'm using in my function and haven't realized it. I think that payload.color code is correct. BounceColor is color returned after next ray, so it's just recurvise call." CreationDate="2020-09-19T19:34:21.337" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16050" PostId="10244" Score="0" Text="(2/2)Therefore, first it (primary ray) should contain all the information of the path, unless I don't understand it correctly. I was comparing my code to MJP's and it seemed that this part of code works the same - https://github.com/TheRealMJP/DXRPathTracer/blob/master/DXRPathTracer/RayTrace.hlsl ; Here is my exposure - https://github.com/komilll/RTCP/blob/master/RTCP/Shaders/PS_Postprocess.hlsl ; Can you provide more details about applying correct tone curve and gamma correction to an image, with some examples? Great post, thank you Nathan. I'll get back to you tomorrow with tested results." CreationDate="2020-09-19T19:38:13.037" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16051" PostId="10244" Score="1" Text="OK, maybe I misunderstood how the payload stuff works. For tone mapping, it's a pretty huge topic but a good place to start is here: [ACES filmic tone curve](https://knarkowicz.wordpress.com/2016/01/06/aces-filmic-tone-mapping-curve/). You would do exposure first, then apply the tone curve, and finally convert the values from linear to sRGB space, see the function $\gamma(u)$ defined here: [sRGB](https://en.wikipedia.org/wiki/SRGB#Specification_of_the_transformation). 8-bit colors are definitely too small to do sample accumulation in—bump that up to 16 bits at a minimum, 32 bits is better." CreationDate="2020-09-19T23:20:50.770" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16052" PostId="10246" Score="0" Text="do you want to generate the traslation ?" CreationDate="2020-09-21T05:07:41.673" UserId="9256" ContentLicense="CC BY-SA 4.0" />
  <row Id="16053" PostId="10244" Score="1" Text="I've tested your solution and changing texture format fixed converging image. Also, tone mapping and sRGB fixed oversaturation. I've added appropriate edit to main post to show how images generated now, looks like. Thank you!" CreationDate="2020-09-21T10:43:13.213" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16054" PostId="10222" Score="0" Text="@NathanReed I have a followup question regarding this topic.  Are each connection strategy in BDPT without the MIS weighing, for example: (t=1, s=2) and (t=2, s=1) and etc, an unbiased estimate of the final integral?" CreationDate="2020-09-21T13:42:42.840" UserId="13896" ContentLicense="CC BY-SA 4.0" />
  <row Id="16055" PostId="10084" Score="0" Text="@NathanReed this is not necessary. What you are saying is correct if you work on the color world. But in case that you do not care to represent the radiance/irradiance as color but rather than as energy color does not matter." CreationDate="2020-09-22T08:00:20.727" UserId="13790" ContentLicense="CC BY-SA 4.0" />
  <row Id="16056" PostId="10117" Score="2" Text="from my research you answer is almost correct the suggested way to do it is first to linearize your image, address the gamma correction and then get the luma." CreationDate="2020-09-22T08:07:53.180" UserId="13790" ContentLicense="CC BY-SA 4.0" />
  <row Id="16057" PostId="10248" Score="1" Text="You might have a better time asking about this on [Photography.SE](photo.stackexchange.com), I'd guess they will know more about camera sensor geometry. This is a bit out of our bailiwick here at CGSE." CreationDate="2020-09-22T17:51:46.280" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16058" PostId="10249" Score="1" Text="Have you tried calculating an average normal for the polygon, and projecting the whole thing onto that?" CreationDate="2020-09-22T18:22:47.090" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16059" PostId="10248" Score="0" Text="Jup just dit, thanks fo the hin." CreationDate="2020-09-22T18:44:59.627" UserId="14000" ContentLicense="CC BY-SA 4.0" />
  <row Id="16060" PostId="10219" Score="0" Text="@Mark ok sorry its taken a while my motherboard blew a capacitor." CreationDate="2020-09-23T06:19:52.303" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16062" PostId="10249" Score="0" Text="How can you clip off a triangle, with a straight edge, when such an edge is extremely unlikely to lie in the &quot;surface&quot; (however you decide to define it) of the non-planar polygon?  For example, if you take the simple case of a non-planar quad, the 'obvious' surface to define is a bilinear one and the diagonals are not straight lines - they are quadratics (e.g. https://blog.demofox.org/2018/04/23/taking-a-stroll-between-the-pixels/ ) .   Furthermore, you could bend the surface so much that it self-intersects!" CreationDate="2020-09-23T08:35:06.080" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="16063" PostId="10249" Score="0" Text="@NathanReed Is it mathematically rigorous approach?" CreationDate="2020-09-24T04:52:13.563" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="16064" PostId="10256" Score="0" Text="This does not really apply to a tetradedral outcropping but it does apply most quatrilateral turned to triangles meshes where the repetition of triangulation is uniform" CreationDate="2020-09-24T06:13:24.623" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16065" PostId="10253" Score="0" Text="holy SMOKES! my mind is blown. I must admit I thought I would not hear back! I have been grinding away at blender/freecad using the python interface, but this is IT!" CreationDate="2020-09-24T08:41:15.463" UserId="13961" ContentLicense="CC BY-SA 4.0" />
  <row Id="16066" PostId="10249" Score="0" Text="@LennyWhite no, but then an arbitrary non-planar polygon isn't even mathematically rigorously defined as to what the actual surface is." CreationDate="2020-09-24T16:49:01.223" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16067" PostId="10116" Score="1" Text="Does that mean i can bind an arbitrary  ID3D11Buffer as a texture?" CreationDate="2020-09-25T15:49:00.250" UserId="11739" ContentLicense="CC BY-SA 4.0" />
  <row Id="16068" PostId="10258" Score="0" Text="So basically sort the array of half-edges so that pairs are adjacent to each other? What do you do for unpaired half-edges (on the boundary of the mesh)?" CreationDate="2020-09-25T20:17:05.887" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16069" PostId="10258" Score="1" Text="Half edges at the boundary should have pairs. however those pairs should have null faces.&#xA;&#xA;At minimum in my implementations I have found that not putting half edges at the boundary breaks many many algorithms. Plus if you put half edges at the boundary you can get the closed curve of the boundary through a linear iteration, which is nice." CreationDate="2020-09-25T21:54:57.620" UserId="7462" ContentLicense="CC BY-SA 4.0" />
  <row Id="16070" PostId="4274" Score="0" Text="In case it provides any help, according to https://www.3dgep.com/understanding-the-view-matrix/#Look_At_Camera some signs are changed, which would not make sense according to me, as it does not make sense with the rotation matrix multiplication results. You can check that in the FPS camera section" CreationDate="2020-09-26T17:51:08.510" UserId="13348" ContentLicense="CC BY-SA 4.0" />
  <row Id="16071" PostId="4274" Score="0" Text="Oh In the case if the example provided it says that &quot;The basic theory of this camera model is that we want to build a camera matrix that first rotates pitch angle about the X axis, then rotates yaw angle about the Y axis, then translates to some position in the world&quot; that means that the order of the rotation is inverted. So if I do RpT * RyT, the signs are correct. Sorry if that was confusing" CreationDate="2020-09-26T18:00:00.157" UserId="13348" ContentLicense="CC BY-SA 4.0" />
  <row Id="16072" PostId="10116" Score="0" Text="@Raildex I can see that ID3D11Buffer is child of ID3D11Resource. I was using resources it only for textures in DX11. However, keep in mind that you're not binding stuff in texture buffers always as textures. StructuredBuffer is accessed as an array so it's not used as texture. You have to keep in mind that tbuffer doesn't mean that it's a texture. It would be best read resources above to better understand that." CreationDate="2020-09-26T19:27:12.930" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16073" PostId="10253" Score="0" Text="could you put some contact details in your bio so I can contact you to help further this along, I would be happy to pay you." CreationDate="2020-09-27T09:16:05.717" UserId="13961" ContentLicense="CC BY-SA 4.0" />
  <row Id="16074" PostId="10265" Score="1" Text="I think your question is too general and needs to be more specific. What have you tried so far and what are the problems you are facing? Also, try to focus on a single problem per question. In case this is a homework question (like it seems to me) I would start my research by reading the sources I got during class." CreationDate="2020-09-28T07:26:35.480" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16075" PostId="10267" Score="0" Text="Hi, for sake of this exercise I'd rather not use the shader too much. I read the link you mentioned but I still can't figure why I can sort out my problem with the VAO. Can you maybe give me an insight on why this would work and how would I need to modify my code to achieve the result?" CreationDate="2020-09-28T15:45:02.897" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="16076" PostId="10270" Score="0" Text="how about vertical Gabor filters? do they yield same result?" CreationDate="2020-09-28T20:28:19.070" UserId="14025" ContentLicense="CC BY-SA 4.0" />
  <row Id="16077" PostId="10270" Score="1" Text="@ivan866 Gabor filters make sense when you want spatial locality, like you want to examine the frequency content in the neighborhood of some particular point in the image. That doesn't sound like what you're doing? Moreover you said you wanted a histogram, so you would need a separate Gabor filter for each frequency bin in the histogram. It could get slow. FFT is much more efficient." CreationDate="2020-09-28T21:33:04.630" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16078" PostId="10271" Score="1" Text="Handle animations of what? None of what you're talking about (&quot;some bones, and the skinning weights of the mesh vertices with respect to the bones&quot;) are animations. They're just what you need to do skinning; they don't provide actual animations. Also, there's nothing &quot;simple&quot; when it comes to skinning. Lastly, you don't need to &quot;tackle&quot; glTF, because there are dozens of libraries out there who did the hard part for you already." CreationDate="2020-09-29T04:54:22.937" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16079" PostId="10232" Score="0" Text="Your answer would easier to understand if you a) used far fewer conjunctions in each sentence and b) Put in some paragraphs." CreationDate="2020-09-29T08:02:32.197" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="16080" PostId="10266" Score="0" Text="I don't know how your end result should look like, but to visualize volume data you could implement the &quot;marching cubes&quot; algorithm. Beware, the result is not minecraft like... an alternative is: first store the information about all filled voxel. When this step is done, you can iterate through all voxel and create the cube mesh. where for each face of the cube you need to check if the corresponding voxel is empty." CreationDate="2020-09-29T09:41:04.787" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16081" PostId="10267" Score="0" Text="The VAO stores the whole state of the object you want to render. Which Buffers does it use, how is the data in those buffers structured (`glVertexAttribPointer`), etc. Without the VAO, `glDrawElements` doesn't know what to render. Maybe I'll find the time on the weekend to give you a minimal working example, but I can't promise that. In the meantime, try to understand from the tutorial how VOAs work. You can't use modern OpenGL without them and as soon as you understood them, you might already know how to solve your problem." CreationDate="2020-09-29T11:17:37.033" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16082" PostId="10266" Score="0" Text="I have looked at marching cubes algorithm previously. And it seems it generates different combinations of triangles from vertices you create at cube edges https://www.cs.upc.edu/~virtual/SGI/docs/1.%20Theory/Unit%2010.%20Volume%20models.%20Marching%20Cubes/Marching%20Cubes.pdf. But are you saying there's an alternative to create voxelized(minecraft like) mesh instead? I'm sorry I can't follow the steps you mentioned, it's a bit too vague." CreationDate="2020-09-29T11:47:23.360" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="16083" PostId="10266" Score="0" Text="what exactly is the problem? Is the problem to delete the &quot;inner&quot; faces of two neighboring cubes? or to have each vertex once, so two neighboring cubes share one vertex?" CreationDate="2020-09-29T12:03:02.180" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16084" PostId="10247" Score="1" Text="Try rounding off your random number to either 0 or 1 (greater than 0.5 = 1, else 0) and printing a black and white image based on that. I think your random numbers repeat sequence in certain blocks which are those squares/rectangles you see. The black and white image will depict better the randomness" CreationDate="2020-09-30T04:01:11.003" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16085" PostId="10278" Score="0" Text="thanks a lot! that saved me a lot of time... The result looks interesting, but this is absolutly not what I need... So I'll look for another technique. Thanks!" CreationDate="2020-10-01T16:07:17.453" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16086" PostId="10278" Score="0" Text="One more question: Can you add a second noise with a higher frequency above this one? I hope it is not so much effort for you" CreationDate="2020-10-01T16:09:24.903" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16087" PostId="10278" Score="0" Text="and of cause with a lower amplitute" CreationDate="2020-10-01T16:10:13.843" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16088" PostId="10278" Score="0" Text="I did try it out with more octaves, but it turns into even more of a mess." CreationDate="2020-10-01T17:47:58.210" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="16089" PostId="10278" Score="0" Text="okay, thanks a lot" CreationDate="2020-10-01T18:29:12.117" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16090" PostId="10280" Score="0" Text="What is pdf for the cosine distribution? ; I managed to get that black spots by using ggxProb mentioned in here - http://cwyman.org/code/dxrTutors/tutors/Tutor14/tutorial14.md.html ; Image on the right is explained in details in blog post and I managed to get it working and I understand most of theory. I would say that I only don't understand why we have to Jacobians and how do we find them." CreationDate="2020-10-02T08:45:56.673" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16091" PostId="10280" Score="1" Text="The pdf for the cosine distribution is $\cos(\theta)/\pi$." CreationDate="2020-10-02T17:10:40.633" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16092" PostId="10283" Score="0" Text="Could you please also look into this since you're experienced with Python too? https://stackoverflow.com/questions/64163042/splitting-hue-and-saturation-from-an-image" CreationDate="2020-10-02T19:18:44.457" UserId="14040" ContentLicense="CC BY-SA 4.0" />
  <row Id="16093" PostId="10278" Score="0" Text="Who doesn't love VEX?" CreationDate="2020-10-02T21:28:46.397" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="16094" PostId="10276" Score="0" Text="Thanks for the detailed answer with diagram! It clears a few things up. But now I'm confused about bmax. They compute it with v0, v1, and those vectors could have any length, right? I don't understand how bmax can be a valid cos(angle) in that case." CreationDate="2020-10-02T21:38:43.540" UserId="13367" ContentLicense="CC BY-SA 4.0" />
  <row Id="16095" PostId="10276" Score="0" Text="Your right, it looks like I am completely wrong about bmax! Seems like bmax is cos(θmin) if the actually minimum falls out of the range between v0 and v1. So it is the cos of the angle between Axis and v0/v1 (whichever is shorter)? Then it is selecting bmax if v0 dot v1 is &lt; cosφ (which is what I thought bmax was). You also seem to be correct about them needing to be normalized. Potentially a mistake in the paper?" CreationDate="2020-10-03T16:27:17.613" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="16096" PostId="10287" Score="3" Text="Do you mean you want to draw lines along the edges of the triangles, like in this image, or do you want to also texture and shade the triangles, etc? Anyway, triangle rasterization is kind of the whole reason GPUs were created, so, load up OpenGL (or other API of choice) and have at it! " CreationDate="2020-10-04T17:02:55.237" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16097" PostId="10280" Score="0" Text="So throughput for cosine distribution will be: $$\frac{NoL * ggx}{pdf} = NoL * ggx ~\frac{\pi}{cos(\theta)} = NoL * ggx ~\frac{\pi}{NoL} = ggx ~ * ~ \pi$$ ? I'm not planning to use it due to more efficient methods, but I'm interested if my understanding is correct." CreationDate="2020-10-05T08:38:08.850" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16098" PostId="10288" Score="0" Text="Have you tried it?  COLLADA (.dae) does have a node hierarchy, so I'd guess it should work.  I know that COLLADA2GLTF will preserve a hierarchy IF it exists in the .dae file." CreationDate="2020-10-05T18:46:18.943" UserId="1908" ContentLicense="CC BY-SA 4.0" />
  <row Id="16099" PostId="10280" Score="1" Text="@DirectX_Programmer Yep that looks right. The fact that it cancels out the $N \cdot L$ is exactly why the cosine distribution is used for diffuse light." CreationDate="2020-10-05T23:56:32.873" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16100" PostId="10291" Score="0" Text="If creating library is your goal, that's ok. If you just need UI for testing - check ImGui project. ; Regarding your question - it depends how complicated your library is going to be. I've achieved similar results to ImGui with using very simple shaders with single color. To create text, I've used Rastertek font engine - http://www.rastertek.com/dx11tut12.html - it was working reasonably well, at least for my purposes. I don't understand part about mixing 2D and 3D graphics. What do you mean by that?" CreationDate="2020-10-06T08:18:38.533" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16101" PostId="10290" Score="0" Text="I've heard about using something called a Scanline algorithm. Would that work too?" CreationDate="2020-10-06T11:22:59.157" UserId="14044" ContentLicense="CC BY-SA 4.0" />
  <row Id="16102" PostId="10292" Score="1" Text="From glancing at the code, it seems as if you are mixing tangent-space and world-space in the same equation. At least that is suggested by the variable names, e.g. incomingRayDirWS, normalTS." CreationDate="2020-10-06T12:43:28.220" UserId="13367" ContentLicense="CC BY-SA 4.0" />
  <row Id="16103" PostId="10292" Score="0" Text="@B_old You're right, I've mispelled TS/WS. It looks slightly better now, but it's still not fully correct." CreationDate="2020-10-06T13:22:37.200" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16104" PostId="10288" Score="0" Text="This is most probably true. Thank you!" CreationDate="2020-10-06T13:33:09.703" UserId="13375" ContentLicense="CC BY-SA 4.0" />
  <row Id="16105" PostId="10285" Score="1" Text="thank you for this comprehensive answer, this cleared a bunch of confusion for me!" CreationDate="2020-10-06T13:47:29.293" UserId="7913" ContentLicense="CC BY-SA 4.0" />
  <row Id="16106" PostId="10291" Score="1" Text="+1 for [dear imgui](https://github.com/ocornut/imgui) if you're looking for a ready made UI library you can plug into an OpenGL renderer, that allows creating UI elements in code by calling functions, and everything is dynamic. Or if you want to develop your own, you could still look at that for design inspiration." CreationDate="2020-10-06T18:06:18.950" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16108" PostId="10290" Score="0" Text="This is a scanline algorithm.&#xA;&#xA;Broadly speaking, there are two ways to rasterise geometry. You either start with the raster grid and find out what geometry it corresponds to, or you start with the geometry and find out what parts of the raster grid it corresponds to. We typically call anything in the first category &quot;ray tracing&quot; or &quot;ray casting&quot;, and we typically call anything in the second category &quot;scanline&quot;." CreationDate="2020-10-06T22:50:33.107" UserId="159" ContentLicense="CC BY-SA 4.0" />
  <row Id="16109" PostId="10287" Score="0" Text="I am curious for what application was the above required?" CreationDate="2020-10-07T19:27:02.127" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16110" PostId="9146" Score="0" Text="Do you want to have a camera which can be controlled like an airplane? What I mean is: It can roll pitch yaw? That is quite simple: your camera has a projection and a view-matrix. In this case we need to change the view-matrix. By calculating viewMatrix = viewMatrix * rotationMatrix where rotationMatrix can be X-Rotation (pitch), Y-Rotation (yaw) and Z-Rotation (roll). With this solution you only need the lookat method to initialize the camera. At the rest of the time you only need to multiply a rotation onto it." CreationDate="2020-10-08T09:36:13.223" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16111" PostId="10296" Score="0" Text="Thanks a lot! GL_MAX_TESS_GEN_LEVEL is what I was looking for." CreationDate="2020-10-08T15:05:20.833" UserId="13273" ContentLicense="CC BY-SA 4.0" />
  <row Id="16112" PostId="10290" Score="0" Text="@Pseudonym I disagree with that - the algorithm you've described here is _not_ a scanline rasterizer. A scanline rasterizer operates by iterating over the rows of pixels and tracking where the triangle edges intersect each row." CreationDate="2020-10-08T18:30:37.020" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16113" PostId="10290" Score="0" Text="@NathanReed Fair point. Perhaps the method as a whole is not really &quot;scanline&quot;, but Pineda's method is certainly a scanline method, since it operates on rows rather than polygons or pixels." CreationDate="2020-10-09T00:01:44.483" UserId="159" ContentLicense="CC BY-SA 4.0" />
  <row Id="16114" PostId="10276" Score="0" Text="I asked the author of the paper, and he was kind enough to clarify that v0, v1 should be normalized here." CreationDate="2020-10-09T09:09:41.897" UserId="13367" ContentLicense="CC BY-SA 4.0" />
  <row Id="16115" PostId="10297" Score="0" Text="Use a small field of view lens and move further away. It's perspective, unless you are talking about some kind of extra lens distortion too. See telephoto on wikipedia." CreationDate="2020-10-09T11:07:53.113" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16117" PostId="10276" Score="0" Text="Fantastic! Glad you were able to followup. Most people probably don't implement equangular sampling so I guess it must have slipped through." CreationDate="2020-10-09T15:20:33.897" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="16119" PostId="10297" Score="0" Text="@lightxbulb thanks for the suggestion, but I’m looking for a transformation in software" CreationDate="2020-10-10T05:55:36.120" UserId="14062" ContentLicense="CC BY-SA 4.0" />
  <row Id="16121" PostId="10302" Score="0" Text="Yes indirect illumination can bring extra light. By cutting off the bounces at some length you are cutting off part of the energy." CreationDate="2020-10-12T19:55:51.520" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16122" PostId="10302" Score="0" Text="@lightxbulb You're right and I'm aware of that. However, shouldn't some corners, creases, holes etc. still be dark as presented in ground truth reference (last image)? I feel like my image is failing to present any contrast between objects because everything becomes too bright. In advance - I am using exposure slider, aces filmic tone mapping and convertion from linear to sRGB space." CreationDate="2020-10-12T20:28:16.783" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16123" PostId="10301" Score="0" Text="I found one here so far (at the bottom) that essentially lets me take a random user profile GUID and represent them as a cloud (or animated cloud) https://lodev.org/cgtutor/randomnoise.html" CreationDate="2020-10-13T05:26:44.090" UserId="14074" ContentLicense="CC BY-SA 4.0" />
  <row Id="16124" PostId="10304" Score="0" Text="Try slerp between BA and BC. https://en.m.wikipedia.org/wiki/Slerp" CreationDate="2020-10-13T08:32:49.130" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16125" PostId="10302" Score="1" Text="Considering you're using exposure slider, tone mapping, etc, and that similarly the &quot;reference&quot; may use a 100 different things, it's impossible to tell anything. It could be that your code or the code used to generate the reference has a GI bug, or both. Or it could be that the two don't agree on some parameters/tonemapping." CreationDate="2020-10-13T08:35:42.117" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16126" PostId="10302" Score="0" Text="@lightxbulb I read MJP's code multiple times but you're right that our postprocesses are a bit different and it may be causing difference in final image. You're right that there is some bug probably but I cannot find it. Maybe taking a look at postprocess will be helpful as you've said. I don't know if reference is wrong. I'm not experienced in production quality raytracing so I'd appreciate your opinion. My personal guess that my code is wrong is based on a fact that there are no darkenings in holes/corners which should naturally happen for RTAO and even more for RTGI/Path Tracing." CreationDate="2020-10-13T08:48:42.143" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16127" PostId="10302" Score="1" Text="Actually with more bounces, corners/holes may become brighter due to indirect illumination. The other way to keep it darker is to: decrease the light intensity or decrease the materials reflectivity. In either case the image will look darker overall however." CreationDate="2020-10-13T09:51:24.557" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16128" PostId="10302" Score="0" Text="You could also make only the corners &amp; holes darker by modifying only their material. So if you use a texture, just darken the material near corners/holes. To keep it realistic you can make it due to mold/wear and tear." CreationDate="2020-10-13T10:32:30.783" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16129" PostId="10302" Score="0" Text="Changing MJP's postprocess to ones that I've created makes no noticable difference. Roughness and metalness of materials is basically same. BRDF is also the same. Thanks for your advice on modifing texture/material but what I'm trying to achieve ground truth reference. I'm not planning on making any artistic driven adjustments." CreationDate="2020-10-13T13:26:13.067" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16130" PostId="10302" Score="1" Text="Disclaimer: I do not plan to wade through MJP or your code to search for a bug that may not be there. After all very minor differences (that are not necessarily bugs) can result in the differences you mention. What you could do however, is to pick a very simple scene: cornell, some spheres, or w/e. Then remove all postprocess and check that the results are not too different between the two renderers. The usual suspects for bugs would be: random number generation, ray scattering and its associated pdf, the estimator that you are using. I could not find where you acc attenuation either." CreationDate="2020-10-13T15:49:01.960" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16131" PostId="10302" Score="0" Text="I understand that. Thank you for your help. I've started changing my code a little and through experimentation I'm starting to get better results. I'll be modifying throughput settings a little, reducing specular path length and adding russian roulette. Thank you for your help. I'll make sure to post my solution and reason why previous results weren't satisfying as soon as I finish my work." CreationDate="2020-10-13T18:06:06.560" UserId="10129" ContentLicense="CC BY-SA 4.0" />
  <row Id="16134" PostId="10311" Score="0" Text="Hey thanks a lot! I just have a question, how do I pass the weights as normalized shorts/bytes?" CreationDate="2020-10-14T18:19:29.237" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="16135" PostId="10312" Score="0" Text="Here you go: http://lolengine.net/blog/2014/02/24/quaternion-from-two-vectors-final" CreationDate="2020-10-15T08:13:46.693" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16137" PostId="10316" Score="0" Text="&quot;*in OpenGL ES 2.0 &amp; Metal*&quot; These APIs represent two *very* different classes of hardware. Also, are you sure you want to use a fragment shader to do this? Wouldn't it make more sense to use a compute shader (in Metal)?" CreationDate="2020-10-16T02:41:12.103" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16138" PostId="10312" Score="0" Text="@lightxbulb: Thanks for pointing me to the link. I tried the code but it doesn't give correct rotation for some reason. For example if v = (0,1,0) and u= (5,0,0), quaternion value function gives is : &#xA;'Quaternion : ', 0.7071067811865475, 0.0, 0.0, 0.7071067811865475&#xA;'Euler : ', 90.0, 0.0, 0.0 (in degrees)&#xA;which is not correct." CreationDate="2020-10-16T04:24:40.657" UserId="10250" ContentLicense="CC BY-SA 4.0" />
  <row Id="16139" PostId="10312" Score="0" Text="Or you're reading it wrong and it is 0,0,90. Which would be correct." CreationDate="2020-10-16T07:49:30.060" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16140" PostId="10316" Score="0" Text="Yes, you're right, I guess I should remove the Metal part, it's a bit broad." CreationDate="2020-10-16T09:34:43.677" UserId="13742" ContentLicense="CC BY-SA 4.0" />
  <row Id="16141" PostId="10318" Score="0" Text="Answer is ok except maybe for the sRGB part although i agree its ok for the scope if explanation (lies to children and all that, also situation is getting better finally). Most monitors and devices are not sRGB. They are assumed to be, we just dont know because bobody has actually recently measured them so we cant know for sure. Whenever i have measured they have never been sRGB although thats what they claim. They claim because its easier than actually being sRGB." CreationDate="2020-10-16T18:14:54.290" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16142" PostId="10317" Score="0" Text="This quadratic Bezier curve will not exactly reproduce a circular arc, but only approximate it. To truly represent a circular arch you should use a rational quadratic Bezier curve." CreationDate="2020-10-16T23:11:27.140" UserId="7724" ContentLicense="CC BY-SA 4.0" />
  <row Id="16143" PostId="10317" Score="0" Text="@Reynolds Thank you for the reply. How would I do that with three points and an equation in Python?" CreationDate="2020-10-16T23:14:18.147" UserId="14079" ContentLicense="CC BY-SA 4.0" />
  <row Id="16144" PostId="10317" Score="0" Text="@Reynolds, I found this equation&#xA;&#xA;```CurvePoint = (A*W1*(1-t)^2 + B*W2*2t(1-t) + C*W3*t^2) / (W1*(1-t)^2 + W2*2t(1-t) + W3*t^2)```&#xA;&#xA;My Python interpretation is:&#xA;```p = (a * w1 * (1-t) ** 2 + b * w2 * 2*t (1-t) + C * w3 * t ** 2 ) / (w1 * (1-t) ** 2 + w2 * 2*t (1-t) + w3 * t**2)```&#xA;Would 2t be 2*t? Do I have the rest correct? Also, what would the weights (w values) be for a circular arc? Thank you." CreationDate="2020-10-16T23:26:34.293" UserId="14079" ContentLicense="CC BY-SA 4.0" />
  <row Id="16146" PostId="10318" Score="0" Text="@bram0101, thank you so much for your answer.  Is it correct to say that I can use a tone mapper and choose a strategy where camera stops in the mid tones get more bit representation than other areas of the curve?  I was reading something about the Reinhard tone mapper which mentions this, hopefully I did not misinterpret. I think things got tangled in my head because a transfer function could also have this job as well." CreationDate="2020-10-18T02:01:23.340" UserId="7571" ContentLicense="CC BY-SA 4.0" />
  <row Id="16147" PostId="10318" Score="0" Text="@RafaelSabino Are you talking about &quot;the mid tones getting more bit representation&quot; in terms of a 'look' as in &quot;can I compress my shadows and highlights while expanding my mid tones?&quot;. If so, the answer is a simple yes. You can choose a tone mapper that does this or modify your tone mapper, with for example a contrast operator, to do this. However, the phrase &quot;having more bits represent the mid tones rather than the shadows or highlights&quot; generally implies you want it undone when the image gets displayed. Then you're talking about a transfer function, not a tone mapper." CreationDate="2020-10-18T12:53:00.023" UserId="4908" ContentLicense="CC BY-SA 4.0" />
  <row Id="16148" PostId="10319" Score="0" Text="Have you read [this](https://pomax.github.io/bezierinfo/)" CreationDate="2020-10-19T12:22:11.357" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16149" PostId="10300" Score="0" Text="Here's one paper that may interest you: [Efficient Rendering of Linear Brush Strokes by Apoorva Joshi](http://jcgt.org/published/0007/01/01/)." CreationDate="2020-10-19T16:54:13.793" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16150" PostId="10300" Score="0" Text="@NathanReed Thank you. This looks interesting for sure. I planed to NOT use GPU since i feared the transfer to/from GPU becoming the bottleneck, but this looks like it might not be an issue at all. Also i found some good references  in that paper.  &#xA;Maybe move your comment to an answer and i'll accept it." CreationDate="2020-10-19T18:51:58.763" UserId="1865" ContentLicense="CC BY-SA 4.0" />
  <row Id="16151" PostId="10324" Score="0" Text="Shouldn't the second to last line read: _Likewise trying to read from a null **root descriptor** can crash._, because if I understand correctly _descriptors directly in the root signature_ refers to `D3D12_ROOT_DESCRIPTOR` (as opposed to `D3D12_ROOT_DESCRIPTOR_TABLE`)?" CreationDate="2020-10-19T20:08:41.627" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="16152" PostId="10324" Score="1" Text="No, I meant that in addition to null root descriptors (root SRVs etc), also another thing that causes crashes is null/unbound descriptor tables." CreationDate="2020-10-19T20:17:21.590" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16153" PostId="10324" Score="0" Text="Wasn't that already assumed by the first paragraph: if the shader looks at a CBV/SRV/UAV the associated root descriptor or root descriptor table needs to be bound, and only in case of the latter it may point to a null descriptor with a matching format (_and the other contiguous descriptors in that same table can be unbound / invalid / uninitialized if not being looked at in the shader_). I was a bit confused with a _null descriptor table_ as `SetGraphicsRootDescriptorTable` expects the base offset descriptor." CreationDate="2020-10-19T20:30:51.783" UserId="2287" ContentLicense="CC BY-SA 4.0" />
  <row Id="16154" PostId="10324" Score="0" Text="AFAIK nothing prevents you from setting a null descriptor to SetGraphicsRootDescriptorTable as well. I'm just pointing out that if a shader attempts to retrieve a descriptor out of a null descriptor table then bad things may happen, there isn't a guranteed fallback for that case in the way that there is for e.g. loading from a buffer described by a non-root null descriptor. Also note the difference between a _null_ descriptor and one that's _invalid_ (but non-null) by virtue of pointing to an invalid memory address / a resource that's been deleted or unmapped, etc." CreationDate="2020-10-19T21:06:28.713" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16156" PostId="10319" Score="0" Text="Thanks for the comment. I haven't read the document, but I saw it in my research. I'm in a bit of a time crunch so more specificity to my issue would be greatly appreciated (in code would be even better). I don't understand mathematic symbols very well. Have you read it? If so, where does it answer how to set 3-point circular Bezier weights?" CreationDate="2020-10-20T00:07:26.260" UserId="14079" ContentLicense="CC BY-SA 4.0" />
  <row Id="16161" PostId="10327" Score="0" Text="you've forgot to mention that i also need to account for the length of the edges in cases when the triangle is not regular" CreationDate="2020-10-20T20:10:42.080" UserId="14025" ContentLicense="CC BY-SA 4.0" />
  <row Id="16162" PostId="10327" Score="1" Text="Not sure what you mean. If you put the linear gradient vertices at the right spots everything should be taken care of" CreationDate="2020-10-20T20:15:40.747" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16164" PostId="10325" Score="0" Text="Thank you! Please excuse my ignorance, but what do I use in my code from this equation: `w1 = sin(45) = 1/sqrt(2)`? Having the two equal signs confuses me: is it saying that the `sin(45) == 1/sqrt(2)`?" CreationDate="2020-10-21T01:28:54.477" UserId="14079" ContentLicense="CC BY-SA 4.0" />
  <row Id="16165" PostId="10325" Score="0" Text="Yes, I was just pointing out that the sine of 45 degrees is one over the square root of two. You'd probably just write `w1 = 1 / sqrt(2)`." CreationDate="2020-10-21T07:17:13.797" UserId="7647" ContentLicense="CC BY-SA 4.0" />
  <row Id="16167" PostId="10325" Score="0" Text="Thanks for the reply. Where is the angle variable in that equation? Wouldn't `w1 = sin(x)` be better for a program? Maybe I'm not understanding correctly." CreationDate="2020-10-21T18:23:26.540" UserId="14079" ContentLicense="CC BY-SA 4.0" />
  <row Id="16168" PostId="10328" Score="0" Text="Your vertex buffer layout looks wrong. For one thing, your data is a pair of vec2s, not a vec4 and a vec2, so `position` in your vertex shader should be declared as a `vec2` and the `gl_Position` line should be set to `vec4(position, 0.0, 1.0)`; for another, the layout you’re declaring appears to be specifying only a single vec4. It looks like that should be two subsequent calls to `layout.push&lt;float&gt;(2);`." CreationDate="2020-10-21T18:36:42.113" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="16169" PostId="10267" Score="0" Text="I've asked a related question : https://computergraphics.stackexchange.com/questions/10332/understanding-vao-and-vbo. Feel free to jump in." CreationDate="2020-10-22T16:45:21.057" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="16170" PostId="10335" Score="0" Text="Thanks! I see the purpose of using a bounding sphere in this case. But if we map the scene onto the three-dimensional sphere. Wouldn't it be like wrapping the paper with the image of our scene onto a globe? Maybe my issue is that I don't completely understand what is meant by mapping an image to sphere in this case and how it is done." CreationDate="2020-10-23T01:00:59.250" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="16171" PostId="10336" Score="0" Text="what do you mean with *A VAO is an object that stores vertex bindings*, what do you mean with *store vertex bindings*?" CreationDate="2020-10-23T17:27:38.310" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="16172" PostId="10333" Score="0" Text="I'm reading through your second link and slowly it's becoming a bit clearer. However the bit &quot;* ... allows OpenGL operations that access certain state to find that state within the bound object. So if you call a function that modifies certain state that is from a bound object, that function will modify the state within the object.&quot;* isn't totally clear." CreationDate="2020-10-23T17:30:58.690" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="16173" PostId="10333" Score="0" Text="First of all why is the &quot;binding&quot; relevant (namely why is having the object state the same as the context important?) and second if I bind the object to have the same state as the context I guess this is important but why would I want to modify the state only within the object? I guess this actually comes down to what the meaning of these states are." CreationDate="2020-10-23T17:31:47.243" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="16174" PostId="10333" Score="0" Text="@user8469759: Binding the object to the context means that state that references those locations will get their data from the object. And they're not &quot;states&quot;; they're *state*. As I analogized, they're like member variables for objects." CreationDate="2020-10-23T18:14:29.517" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16175" PostId="10333" Score="0" Text="What do you do you mean with &quot;They're not states, they're state&quot;, probably something stupid I know..." CreationDate="2020-10-23T21:29:38.580" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="16176" PostId="10338" Score="0" Text="Thanks, so how can I ensure that a tet has the right face orientation? I am using the above to check the handedness of a solid (a tet), should I check all faces? So checking for each face $j$, ie, $(-1)^{j}[v_{0},\ldots ,v_{j-1},v_{j+1},\ldots ,v_{n}]$ some property? Is there a closed formula?" CreationDate="2020-10-24T13:20:47.407" UserId="15121" ContentLicense="CC BY-SA 4.0" />
  <row Id="16177" PostId="10338" Score="1" Text="I would need to know more about the program and the context of this requirement. I would expect that given a tet in the correct orientation that it would automatically generate the correct faces. The requirement might refer to some other faces, e.g. the faces of a volume which is being subdivided and filled with tetrahedra, etc." CreationDate="2020-10-24T22:12:04.350" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16178" PostId="10331" Score="0" Text="Thanks @bram0101 !" CreationDate="2020-10-24T23:44:35.457" UserId="7571" ContentLicense="CC BY-SA 4.0" />
  <row Id="16180" PostId="10331" Score="0" Text="I think spectral radiance (from Radiometry) might be the better term to refer to the values used in the EOTF for L. Luminance (or relative luminance if it's normalized 1 or 100) from photometry is the one that I was incorrectly thinking of. Correct me if I'm wrong just so that I don't throw someone off out there :) thanks." CreationDate="2020-10-25T00:01:20.957" UserId="7571" ContentLicense="CC BY-SA 4.0" />
  <row Id="16181" PostId="10338" Score="0" Text="Thanks for the answer. I've included my code for a tet (maybe I am doing something wrong and I don't see it), the software I am interfacing with [is this](https://www.comsol.de/model/comsol-multiphysics-mesh-import-and-export-guide-72351), here is **[the link to the documentation](https://www.comsol.de/model/download/711411/COMSOL_MeshImportExportGuide.pdf)**, where tets are on page 38. I thought my code would be sufficient, but writing a mesh file with vertices and tets will raise an error on faces. Of course a single tet works." CreationDate="2020-10-25T07:47:02.667" UserId="15121" ContentLicense="CC BY-SA 4.0" />
  <row Id="16182" PostId="10342" Score="0" Text="I've never done it, and I've not thought though the details of it, but as a basic first guess, I would presume that they'd just apply to the UVs the inverse transform of whatever transform they're applying to the model-space vertex positions." CreationDate="2020-10-25T19:08:07.577" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16183" PostId="10342" Score="0" Text="@NicolBolas Thanks, I guess I could try it out I see if just using the inverse would achieve the effect! However since we work with 2x2 matrices in 2d space, I guess we should use the inverse of transformation along a given plane in object space. Maybe the plane built from the normal of a given face we're transforming." CreationDate="2020-10-25T21:20:17.343" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="16184" PostId="10338" Score="0" Text="Hmm. Looking at your tet handedness equation again, I think you have a sign error - the vectors should be like $v_i - v_0$ rather than $v_0 - v_i$. In other words, all the vectors should be based at $v_0$ and pointing toward the other vertices. That's consistent with the tet diagram on page 38, I think." CreationDate="2020-10-25T21:29:51.960" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16185" PostId="10338" Score="0" Text="Nathan, thank you very much for pointing that out. I've corrected the code but it seems that faces are still a problem. There's something I am missing here, maybe. If you have any idea that would be great. Thanks!" CreationDate="2020-10-26T09:53:27.770" UserId="15121" ContentLicense="CC BY-SA 4.0" />
  <row Id="16186" PostId="10346" Score="0" Text="&quot;*In the first picture the Primitive assembly is executed for example after the Geometry shader*&quot; It talks about something they call &quot;shape assembly&quot;, not &quot;primitive assembly&quot;." CreationDate="2020-10-26T16:30:47.867" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16187" PostId="10346" Score="0" Text="Even if it is a &quot;shape assembly&quot; the order is swapped, I don't really understand. Also In the tutorial it seems to me that both &quot;shape assembly&quot; and &quot;primitive assembly&quot; are used as synonyms (see my update)." CreationDate="2020-10-26T16:43:28.177" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="16188" PostId="10338" Score="1" Text="Sorry, I don't think I can help further with the information available. The error message seems to suggest that the set of tets might be duplicated, or contain overlapping tets reusing the same faces somehow." CreationDate="2020-10-26T17:08:04.107" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16189" PostId="10345" Score="0" Text="`Why is the difference?` - they're just using a different definition with a scaling factor, it shouldn't really matter." CreationDate="2020-10-26T17:10:42.447" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16190" PostId="10347" Score="0" Text="Can you tell me where I can take such diagram? I've been going back and forth between Specification (https://www.khronos.org/registry/OpenGL/specs/gl/glspec46.core.pdf) and the Khronos web page and I can't find a correspondence. I'm starting to assume that tutorial &quot;Learn OpenGL&quot; is good to get some feeling of OpenGL but to be honest the theory explained doesn't satisfy me at all." CreationDate="2020-10-26T17:24:34.783" UserId="228" ContentLicense="CC BY-SA 4.0" />
  <row Id="16191" PostId="10347" Score="0" Text="@user8469759: &quot;*I can't find a correspondence*&quot; I don't know what you mean by that. The wiki is just a more readable description of the specification. The spec for example says &quot;&quot;If there is an active program for the geometry stage, the executable version of the program’s geometry shader is used to process primitives resulting from the primitive assembly stage.*&quot; That means primitive assembly needs to happen before the GS." CreationDate="2020-10-26T17:55:12.507" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16192" PostId="2320" Score="0" Text="I know this is an old question but have you found a solution?" CreationDate="2020-10-26T22:08:19.797" UserId="15137" ContentLicense="CC BY-SA 4.0" />
  <row Id="16193" PostId="10345" Score="0" Text="@lightxbulb Thanks for your reply! I thought so at first, but the triangle filter r(x) = max(0, 1 - |x|) that they give as an example in 7.1.2, doesn't seem to unscale it (while the box function given later does). Maybe it's simply too trivial that they overlooked? Still what good does it do to have the T factor and then 1/T it?" CreationDate="2020-10-27T01:42:09.120" UserId="15133" ContentLicense="CC BY-SA 4.0" />
  <row Id="16194" PostId="10345" Score="0" Text="It doesn't change anything if you have $T$ and then unscale it. It could be that they wanted a specific relationship to the Fourier transform. Similar to how one would choose different scaling factors for the DFT, e.g. to make it unitary." CreationDate="2020-10-27T07:17:58.377" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16195" PostId="2320" Score="0" Text="@HenryLeBerre my apologies Henry but unfortunately I cannot help you. It's been years since I touched that project and I don't work in the field anymore so I don't even remember if eventually I solved it or not.. good luck!" CreationDate="2020-10-27T09:46:52.683" UserId="3069" ContentLicense="CC BY-SA 4.0" />
  <row Id="16196" PostId="2320" Score="0" Text="@Tarta it’s not a problem at all! Thanks" CreationDate="2020-10-27T09:54:18.423" UserId="15137" ContentLicense="CC BY-SA 4.0" />
  <row Id="16197" PostId="10348" Score="0" Text="&quot;*the volume is completely behind the near clipping plane*&quot; If the volume is behind the near clipping plane, then it's not visible. Maybe you meant &quot;in front of&quot;." CreationDate="2020-10-27T18:16:21.683" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16198" PostId="10348" Score="0" Text="@NicolBolas Well, it depends on the definitions. To me it seems reasonable to define &quot;in front of&quot; to mean &quot;closer to the camera&quot;, which is what I did mean. I'll update the question to prevent misunderstanding." CreationDate="2020-10-27T18:28:09.580" UserId="9044" ContentLicense="CC BY-SA 4.0" />
  <row Id="16199" PostId="10345" Score="0" Text="@lightxbulb Thanks. I'm convinced." CreationDate="2020-10-28T11:34:12.117" UserId="15133" ContentLicense="CC BY-SA 4.0" />
  <row Id="16200" PostId="10270" Score="0" Text="@ivan866 could you give an insight into Wavelets? And how they relate to FTs and Gabor filters?" CreationDate="2020-10-28T23:01:18.693" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="16202" PostId="10351" Score="0" Text="It's called morphological erosion. Most graphic apps know how to do this out of the box." CreationDate="2020-10-30T14:12:43.797" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16209" PostId="10360" Score="0" Text="What does &quot;mask&quot; mean in this context? That looks more like a multiplication than any kind of &quot;masking&quot;." CreationDate="2020-11-01T22:10:56.317" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16210" PostId="10360" Score="0" Text="By mask I meant an image filled with (45, 78, 251, 102) RGBA pixels." CreationDate="2020-11-01T23:16:47.607" UserId="15159" ContentLicense="CC BY-SA 4.0" />
  <row Id="16211" PostId="10360" Score="0" Text="OK, so you have an image with that color. But what do you *do* with it? The term &quot;mask&quot; is usually meant to apply to an image that obscures part of another image. Are you talking about blending with the image? And if so, is it a weighted blend based on the alpha? My question is what *exactly* is the mathematical function you're talking about here. Because &quot;mask&quot; isn't a function that I'm aware of." CreationDate="2020-11-02T00:33:44.803" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16212" PostId="10360" Score="0" Text="I'm just looking for the RGBA color (this is what I meant by mask) I need to apply on top of my first image in order to obtain the second image." CreationDate="2020-11-02T01:53:55.747" UserId="15159" ContentLicense="CC BY-SA 4.0" />
  <row Id="16213" PostId="10360" Score="0" Text="And I'm looking for the math function you used when applying that color to the source image to get the result. The word &quot;mask&quot; is telling me precisely *nothing*. Is this a multiplication operation? What is the mathematical equation being performed per-pixel to get the result? Please put that in your question." CreationDate="2020-11-02T01:57:32.063" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16214" PostId="10361" Score="0" Text="&quot;*2D application users seem to be using transparency as a proxy for coverage more often than not.*&quot; What do you mean by &quot;coverage&quot; in this context? Are you talking about multisample coverage?" CreationDate="2020-11-02T06:07:37.687" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16215" PostId="10361" Score="0" Text="Ia am talking about the fact that people by default dont want to alpha blend but instead they want to antialias edges. Alphablending mimics transparency which is one case but in practice people expect it to blend as if two objects were laid out next to eachother instead of layer at time. Which leaves cracks showing like for example in vector graphics. But same is expected in raster graphics. More like 50% +50% being 100% than 75 alpha blending gives." CreationDate="2020-11-02T06:26:50.293" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16216" PostId="10361" Score="0" Text="&quot;*in practice people expect it to blend as if two objects were laid out next to eachother instead of layer at time*&quot; What &quot;people&quot; expect this? Where have you seen this expectation? Also, a coverage mask is *mathematically identical* to alpha blending (assuming the alpha value exact matches a specific number of samples, and assuming the bottom one set all of the sample values), once you perform the antialiasing resolve. So I don't know what you're talking about." CreationDate="2020-11-02T06:30:41.477" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16217" PostId="10361" Score="0" Text="@NicolBolas No its not mathematically identical. It just seems naively to be. There are thousands uppon thousands of humans manually tweaking renders and composites because of this. In practice people dont expect that the statistical average is the real value but the complementary maximum. This would solve a lot of problems." CreationDate="2020-11-02T06:42:19.227" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16218" PostId="10360" Score="0" Text="I edited my post:" CreationDate="2020-11-02T07:07:15.313" UserId="15159" ContentLicense="CC BY-SA 4.0" />
  <row Id="16219" PostId="10361" Score="1" Text="&quot;*No its not mathematically identical. It just seems naively to be.*&quot; If you have 4 samples that are black and 12 samples that are white, multisample resolve will give you a 75% grey. If you have a black pixel, and you draw a white pixel with a 75% alpha and alpha-blend them, you get a 75% grey. These are mathematically equivalent operations. If they are not, please explain in your question how they aren't identical, with specific examples of where they are different." CreationDate="2020-11-02T14:18:27.580" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16220" PostId="10360" Score="0" Text="What does &quot;printing&quot; mean in this context? You don't &quot;print&quot; images to the screen. Please be specific as to the exact operation you're performing." CreationDate="2020-11-02T14:19:33.587" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16221" PostId="10361" Score="0" Text="@NicolBolas yes, i am doing that as soon as i have a computer in front of me. Anyway your right in your descriptions scope.  But not if i have more than foreground and background layers." CreationDate="2020-11-02T14:56:12.827" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16222" PostId="10357" Score="0" Text="Compare what, specifically? What quantities do you want to measure?" CreationDate="2020-11-02T17:44:54.817" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16223" PostId="10361" Score="0" Text="@NicolBolas Coverage in this sense: http://jcgt.org/published/0004/02/03/" CreationDate="2020-11-02T17:47:27.573" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16224" PostId="10360" Score="0" Text="I edited my post again." CreationDate="2020-11-02T18:56:16.107" UserId="15159" ContentLicense="CC BY-SA 4.0" />
  <row Id="16225" PostId="10364" Score="0" Text="after adding the translation after projection , i get a straight line from center to the top left corner of screen" CreationDate="2020-11-03T10:40:57.663" UserId="13309" ContentLicense="CC BY-SA 4.0" />
  <row Id="16226" PostId="10364" Score="0" Text="Can you post your modified code so I can check it for you ?" CreationDate="2020-11-03T10:48:51.277" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="16227" PostId="10364" Score="0" Text="Did you remove the translation before projection ? (so circle has 0,0 passed for its X,Y coord)" CreationDate="2020-11-03T10:56:49.590" UserId="3073" ContentLicense="CC BY-SA 4.0" />
  <row Id="16229" PostId="10360" Score="0" Text="This sounds a little like  https://computergraphics.stackexchange.com/questions/9759/given-a-background-image-b-and-a-composie-image-of-a-over-b-can-we-somehow-reco/9795#9795" CreationDate="2020-11-03T11:03:39.567" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="16230" PostId="10357" Score="0" Text="For example, comparing shape similarities" CreationDate="2020-11-03T16:31:43.777" UserId="13888" ContentLicense="CC BY-SA 4.0" />
  <row Id="16231" PostId="10357" Score="0" Text="OK, but how do you want to measure shape similarity? What's the specific metric you want to use?" CreationDate="2020-11-03T16:47:24.803" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16232" PostId="10369" Score="1" Text="If you use a time integration scheme for your simulation that is not unconditionally stable a different time delta on both systems can explain the weird behavior at the end - the system might get unstable and the solution blows up. So check the time delta you use. Also, check if both systems use the same Graphics card in case you are running it on a system (Laptop) where the CPU has a graphics processor too." CreationDate="2020-11-04T10:33:07.490" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16233" PostId="10369" Score="1" Text="Agree with @wychmaster. Try to use `glfwSwapInterval(0);` to free fps limitation, and sleep a few milliseconds as [this topic hints](https://stackoverflow.com/questions/4184468/sleep-for-milliseconds)." CreationDate="2020-11-05T08:35:27.413" UserId="15180" ContentLicense="CC BY-SA 4.0" />
  <row Id="16234" PostId="10369" Score="0" Text="I set `glfwSwapInterval(0);` and put my own frame limitation on the main loop. The speed of the simulation is now correct, but it still blows up near the end." CreationDate="2020-11-05T12:51:42.253" UserId="11598" ContentLicense="CC BY-SA 4.0" />
  <row Id="16235" PostId="10338" Score="0" Text="Thanks anyway, Nathan, I appreciate it." CreationDate="2020-11-05T15:20:05.110" UserId="15121" ContentLicense="CC BY-SA 4.0" />
  <row Id="16236" PostId="10372" Score="2" Text="There's no relationship between those. The ambient term is a heuristic anyways that's there to substitute for the lack of indirect illumination - not that it does a good job at that." CreationDate="2020-11-05T17:58:54.117" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16237" PostId="10357" Score="0" Text="Actually, that is my question :)" CreationDate="2020-11-05T18:21:57.263" UserId="13888" ContentLicense="CC BY-SA 4.0" />
  <row Id="16238" PostId="5483" Score="0" Text="If you use glsl function textureGather (or DX11 GatherRed) at the same coordinates, it will return the values for 4 contiguous texels. You can then compare these values to the full-resolution depth buffer depth value, and find which is closest." CreationDate="2020-11-06T03:11:19.503" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="16239" PostId="5483" Score="0" Text="For a quarter resolution buffer, I see 2 possible approaches: &#xA;A) Do it in two steps: first upscale your quarter-resolution buffer into a half-resolution buffer, then upscale the half-resolution buffer into a full-resolution buffer. With this approach you can use the sme formula as above.&#xA;B) Do it in one step: in this case you need to sample 4x4 texels in the quarter-resolution buffer, and compare their values with the target full-resolution depth buffer depth" CreationDate="2020-11-06T03:17:33.367" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="16240" PostId="5483" Score="0" Text="This presentation seems to have a few slides presenting &quot;bilateral upsample&quot; that looks related: https://www.gdcvault.com/play/1022982/Mixed-Resolution-Rendering-in-Skylanders" CreationDate="2020-11-06T03:18:37.487" UserId="110" ContentLicense="CC BY-SA 4.0" />
  <row Id="16241" PostId="10357" Score="0" Text="This is totally up to you to define which metrics are important to you and how you rate them. In its current form, I would vote to close the question since it will probably lead to opinion-based answers. Note that you can edit your question if you think you can improve it. Also, have a look at our [help pages](https://computergraphics.stackexchange.com/help) for some guidance." CreationDate="2020-11-06T10:20:14.700" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16242" PostId="10372" Score="0" Text="@lightxbulb so for example on a sunny day outdoors, can the skylight (ambient) be larger than sunlight (diffuse)? I feel like there should be some kind of relationship between diffuse and ambient reflection when there is only one light source in a scene." CreationDate="2020-11-06T13:06:52.047" UserId="15183" ContentLicense="CC BY-SA 4.0" />
  <row Id="16243" PostId="10372" Score="1" Text="There is actually no such thing as the ambient term. It's an outdated model and is just an approximation. When you get into pathtracing this ambient term gets replaced by indirect illumination which is again based on either diffuse or specular terms or any other term depending on the BRDF of the surface." CreationDate="2020-11-06T17:54:27.350" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16244" PostId="10372" Score="0" Text="@gallickgunnerThank you for the clear explanation. Do you know any reflection models that are most recent? Also, if there is only one light source in a scene, shouldn't **indirect illumination** (e.g. shadows) always be less than **(total illumination - indirect illumination)**? (assuming total illumination = direct + indirect)" CreationDate="2020-11-06T20:50:13.740" UserId="15183" ContentLicense="CC BY-SA 4.0" />
  <row Id="16245" PostId="10374" Score="0" Text="You can use an acceleration structure to avoid raymarching like crazy. As far as rasterization goes, you could use marching cubes. For the normals you can use the numerical derivative at the point of the intersection." CreationDate="2020-11-07T00:29:25.757" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16246" PostId="10374" Score="0" Text="@lightxbulb numerical derivative of what with respect to what? I don't know how I would get the gradient of the surface if I only get an intersection coordinate (using raymarching). Also, isn't marching cubes only applicable to grid structures?" CreationDate="2020-11-07T00:36:42.207" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16247" PostId="10372" Score="1" Text="Microfacet models are used commonly. More complex materials require a layered approach. As for the other question, the answer is not neccessarily. Think of a scenario where an object is lit by sun directly and it's again lit by placing a mirror or a magnifying glass at a specific angle so all the light converges or reflects in the same direction and arrives at our said object. This way light coming indirectly either by refraction through the glass or reflection illuminates the object more and creates a blinding whiteness. Reason why u see caustics." CreationDate="2020-11-07T06:12:50.770" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16249" PostId="10374" Score="0" Text="The derivative of the density with respect to the global coordinates. Check out Inigo Quilez's site, or some shadertoy with numerical derivatives. For marching cubes you can make a grid that contains your particles, and estimate the density at each grid point." CreationDate="2020-11-07T10:29:28.153" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16250" PostId="10369" Score="1" Text="Small hint: Next time you answer a comment use `@` in combination with a user name. This will automatically inform the user that you have answered. Otherwise, he will only notice it if he visits the question again. Alternatively, use the edit button to update your question with the new information. It will bubble up on the main page and we will also know that there is new information available. ;)" CreationDate="2020-11-07T10:34:36.447" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16251" PostId="10369" Score="1" Text="Regarding your question: It is hard to tell what causes the &quot;blow-up&quot; by simply staring at the code. I would suggest to feed some debugging tools with your code ---&gt; [cppcheck](http://cppcheck.sourceforge.net/), [clang-tidy and some clang analyzers](https://clang.llvm.org/docs/index.html), [valgrind](https://valgrind.org/) and whatever else you find. Also, use multiple compilers (GCC, Clang, MSVC) and turn on as many warnings as you can. Maybe there is some undefined behaviour hidden in your code like an uninitialized variable. If true, this should be found quickly with the suggested actions." CreationDate="2020-11-07T10:49:15.113" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16252" PostId="21" Score="0" Text="May I ask, is there a single Stencil &amp; Depth Buffer for the entire GPU? Or a different one is assigned per object we `Draw`?" CreationDate="2020-11-07T14:21:25.063" UserId="5496" ContentLicense="CC BY-SA 4.0" />
  <row Id="16253" PostId="10372" Score="0" Text="@gallickgunner Ok I see what you are saying. The caustic was a good example (although, I think for the most part it should be **intensity = sun + indirect** and **sun &gt;= indirect**) but not always. And thank you for mentioning Microfacet models. I was searching a lot." CreationDate="2020-11-07T19:11:09.420" UserId="15183" ContentLicense="CC BY-SA 4.0" />
  <row Id="16254" PostId="10376" Score="0" Text="Thanks for the great rundown! I still have some trouble seeing how I could easily turn a bunch of random SPH particles into a grid. However, that does seem like the way to go, because a grid allows me to implement marching cubes for the meshification. (The reason I don't do a grid based fluid sim in the first place is because particles allow for a great freedom of simulation domain)" CreationDate="2020-11-07T20:26:20.870" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16255" PostId="10376" Score="0" Text="@AnnoyinC You define a density field based on the particles—meaning put some falloff function with a defined radius around each particle, and add up all the functions for all the particles. That gives you an analytic function you can evaluate at any point in space (using an acceleration structure to skip particles too far away to contribute). Then you plop down a grid and evaluate the function and its gradient at each grid point." CreationDate="2020-11-07T20:48:49.017" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16256" PostId="10376" Score="0" Text="So density(x,y,z) = &quot;for each particle{add particle.density*fallof(x,y,z, particle.position)}&quot;. This doesn't sound very realtime? Do you know any codebases or papers which implement what you speak of?" CreationDate="2020-11-07T22:55:30.430" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16257" PostId="10376" Score="0" Text="Yep. It's certainly possible to make this kind of thing work in realtime, but it's not going to be a trivial job. I'm sure there are SPH implementations out there that you could look at how they do their rendering, or libraries that can automate some of this, but it's not really my area so I don't know of any good examples." CreationDate="2020-11-08T00:31:41.900" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16258" PostId="10377" Score="0" Text="You just need better interpolation. Try bilinear, biquadratic, bicubic." CreationDate="2020-11-08T10:03:52.477" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16259" PostId="10379" Score="0" Text="wow, I fully understood, Thanks!!" CreationDate="2020-11-08T15:07:07.903" UserId="15196" ContentLicense="CC BY-SA 4.0" />
  <row Id="16260" PostId="10377" Score="0" Text="@lightxbulb I guess interpolation only applies when upscaling." CreationDate="2020-11-08T15:18:07.853" UserId="15191" ContentLicense="CC BY-SA 4.0" />
  <row Id="16261" PostId="10377" Score="1" Text="It applies always when you're sampling the signal at locations where the data was not defined. Even when you're downscaling it.It's just that you additionally apply some convolution filter on top when downscaling." CreationDate="2020-11-08T16:22:18.747" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16263" PostId="10379" Score="0" Text="Doesn't compilers tend to compute both branches of the ternary and just discard when needed? From what I have read smart enough compilers should do the same (when not very performance intensive) with branches." CreationDate="2020-11-09T03:24:04.903" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="16264" PostId="10379" Score="0" Text="@FelipeGutierrez: That has nothing to do with the issue here. That's a matter of optimization. The OP's issue is that the `if` version specifically does something different from the `?:` version. Because they're *supposed to* do different things; the statements aren't equivalent." CreationDate="2020-11-09T03:37:02.403" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16265" PostId="10372" Score="1" Text="Phongs reflection model is a pure guess it makes nontrue assumptions. But its simple, by design, so thats why it gets used." CreationDate="2020-11-09T07:19:37.933" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16266" PostId="10374" Score="0" Text="You might try searching google for &quot;Particle system isosurface extraction&quot; this brings up several good papers on the subject, the marching cubes algorithm may also be a practical solution..I've seen particle systems composed of metaballs efficiently animated using marching cubes to extract the mesh but haven't (yet) written such a system myself." CreationDate="2020-11-09T12:04:32.837" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="16267" PostId="10379" Score="0" Text="Yep I am aware, just adding to the knowledge base I guess. That's why I didn't add it as an answer." CreationDate="2020-11-10T03:07:48.263" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="16268" PostId="10368" Score="0" Text="May I ask if these methods also applies to secondary rays?" CreationDate="2020-11-10T08:33:40.933" UserId="15172" ContentLicense="CC BY-SA 4.0" />
  <row Id="16269" PostId="10368" Score="0" Text="@Spade000 yes, why shouldn't they? But you have to treat them like a new ray and retest all objects." CreationDate="2020-11-10T09:39:55.353" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16270" PostId="10371" Score="0" Text="Thanks a lot for such an elaborate, Nathan!" CreationDate="2020-11-10T09:46:21.253" UserId="9044" ContentLicense="CC BY-SA 4.0" />
  <row Id="16271" PostId="10376" Score="0" Text="Before I accept your answer, would you mind explaining &quot;For getting the surface normal: assuming you've defined a density field for your particles, the surface normal is just the gradient vector of the density field. This can be calculated by finite differences between nearby points in the field, **or analytically by adding up gradient contributions from each particle.**&quot; With some concrete algorithms or papers?" CreationDate="2020-11-10T18:27:51.880" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16272" PostId="10376" Score="0" Text="@AnnoyinC I added some more explanation of the density field and the gradient." CreationDate="2020-11-10T19:10:29.783" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16273" PostId="10376" Score="0" Text="Thanks! I assume &quot;vec{r}&quot; means a position vector, and I assume epsilon is just like a &quot;h&quot; in integrals. What is capital R? And lastly, in the &quot;sum each i: normalize(r-r_i)1/R_i df/du&quot;, I feel like I do not understand the math behind this yet. Is there a textbook that handles this? df/du means that you derive the entire sum over space relative to fallof f(u)? I have some trouble imagining what it culminates in. It would help me out greatly if you could point me to a class that handles this." CreationDate="2020-11-10T19:19:30.677" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16274" PostId="10376" Score="1" Text="@AnnoyinC Yes $\vec r$ is position in space. $\epsilon$ is just some reasonably small number. $R_i$ is particle radius. $df/du$ is just the derivative of $f(u)$. If you're having trouble with these concepts, you might need to do some studying about vector calculus / multivariable calculus." CreationDate="2020-11-10T19:21:59.703" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16275" PostId="10376" Score="0" Text="Thank you, I appreciate you taking the time to help me." CreationDate="2020-11-10T19:23:04.233" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16276" PostId="10382" Score="0" Text="Did you make sure that the refraction ray is spawned somewhat inside of the sphere so that it does not re-intersect with the same sphere? Are you using epsilon values for floating point comparisons?" CreationDate="2020-11-10T20:04:31.853" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16277" PostId="195" Score="0" Text="dead link (http://cs.au.dk/~toshiya/mcp.pdf)" CreationDate="2020-11-10T20:41:43.377" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16278" PostId="10383" Score="0" Text="&quot;Since I simply split all the triangles, the number of vertices in the model increased more than 6 times&quot;, Why did you split the triangles? And a vertex is a single point, you can't split a point. I'm not sure I understand your question." CreationDate="2020-11-10T20:58:17.820" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16279" PostId="10383" Score="0" Text="@AnnoyinC I guess he simply means that as long as he only uses coordinates he just needs 8 vertices and can render all his triangles from them using an index buffer (or how it is called in dx). If you add uv-coordinates you will get multiple vertices with the same spatial coordinate but different uv-coordinates. However, as far as I know you can't do anything in this case to avoid the duplications that doesn't make things overly complicated. --- still wondering about the factor 6. It should be factor 3 since every coordinate is shared by 3 faces." CreationDate="2020-11-10T22:13:54.217" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16280" PostId="10372" Score="0" Text="@joojaa Thank you, that wasn't mentioned anywhere I looked. Do you know where I can find the most recent models? Is there a type of papers published working on this?" CreationDate="2020-11-11T18:27:42.443" UserId="15183" ContentLicense="CC BY-SA 4.0" />
  <row Id="16281" PostId="10382" Score="0" Text="This was actually originally one of the problems I was having with the refraction. I was adding `worldSpaceSurfaceNormal * 1e-3f` to the worldSpaceIntersectionPoint when updating the ray origin to avoid re-intersecting the same triangle again.  This worked well for the diffuse and metallic materials but failed completely for the glass.  I just realised after your comment about it being spawned inside of the sphere that I should instead subtract this epsilon value and now it is working great!" CreationDate="2020-11-11T21:07:36.037" UserId="14003" ContentLicense="CC BY-SA 4.0" />
  <row Id="16282" PostId="10388" Score="0" Text="Thank you. Is there a convention as to which channel corresponds to which axis? e.g., R=X, G=Y, B=Z etc" CreationDate="2020-11-11T21:11:07.030" UserId="15206" ContentLicense="CC BY-SA 4.0" />
  <row Id="16283" PostId="10388" Score="0" Text="Yes always R=X, G=Y, B=Z. There are different conventions about which ways the axes point, though, such as Y up versus Z up." CreationDate="2020-11-11T21:29:55.070" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16285" PostId="10382" Score="0" Text="Yup! Small errors like that will get you, as well as off-by-ones. One of the first things I do is flip fractions, change signs, etc when something doesn't work. Not very analytical but very pragmatic." CreationDate="2020-11-12T00:03:12.070" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16286" PostId="10391" Score="0" Text="&quot;*RGB triplets*&quot; You really don't want to do that. It'd be best to use four channels, even if the alpha channel is 1. Your upload times to the GPU will improve." CreationDate="2020-11-12T01:57:00.160" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16288" PostId="10392" Score="0" Text="That's a really good point. All the pixels compute the sample function for generating a couple of float values (even though the normal is used as reference for computing the sampling direction). However, I don't know how to generate random values that are different for each pixel in a shader." CreationDate="2020-11-12T19:35:27.647" UserId="6172" ContentLicense="CC BY-SA 4.0" />
  <row Id="16289" PostId="10392" Score="1" Text="@user1754322 The basic idea is to include the pixel coordinates (gl_FragCoord.xy) as seed values in the hash function used to generate the random values. Also common to include a frame index so that it gets re-randomized each frame, this allows temporal accumulation of samples as well. There was a [nice paper on GPU hash functions](http://jcgt.org/published/0009/03/02/) recently. For something quick and dirty I have [an old blog post](http://www.reedbeta.com/blog/quick-and-easy-gpu-random-numbers-in-d3d11/) as well (you'll have to translate from HLSL to GLSL)." CreationDate="2020-11-13T00:24:27.573" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16290" PostId="1771" Score="0" Text="Great answer! Would you mind clarifying [in the answer] how widespread the use of these terms is, - are they used in the industry, do they have the same meaning in most books? I think that would help a lot (me at least)." CreationDate="2020-11-13T14:58:28.410" UserId="8434" ContentLicense="CC BY-SA 4.0" />
  <row Id="16291" PostId="10391" Score="0" Text="You're right, I don't want to do that, but I don't have a choice since that's how the data comes to me from the ffmpeg library. The problem turned out to be a helluva lot more complicated than I realized - ffmpeg is giving me YUV streams in 8:4:4 grouping instead of RGB. However, other formats I deal with will still present the problem posed in the question, so I'll leave it up." CreationDate="2020-11-13T18:06:51.353" UserId="9779" ContentLicense="CC BY-SA 4.0" />
  <row Id="16292" PostId="10391" Score="0" Text="Why would you expect that RGBA would be faster than RGB? It's 33% more data to have to transfer to the graphics hardware every frame (the video in question is 1920x1080 @ 30fps, so adding an alpha channel increases the data transfer demand by 60 million bytes per second." CreationDate="2020-11-13T18:08:42.703" UserId="9779" ContentLicense="CC BY-SA 4.0" />
  <row Id="16293" PostId="10391" Score="0" Text="&quot;*Why would you expect that RGBA would be faster than RGB?*&quot; Because GPUs generally cannot do misaligned reads. So they don't *have* RGB formats; they're a fiction created by the API. If you upload from RGB data, the driver will internally have to copy each pixel of data to a 4-channel format, then DMA that into GPU memory. That's slower than you simply preparing the data correctly ahead of time." CreationDate="2020-11-13T19:39:52.767" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16295" PostId="10394" Score="0" Text="I would not expect that to work reliably if different threads are going to be touching the same pixels; that’s why an atomic add is a distinct operation, because it provides that guarantee. It should be fine (albeit probably slow) if you can ensure that each pixel is only touched by one thread in a given dispatch." CreationDate="2020-11-13T20:56:32.453" UserId="506" ContentLicense="CC BY-SA 4.0" />
  <row Id="16296" PostId="10394" Score="0" Text="Yes, setting local sizes as 1 is an option because its still quite fast. It gets the performance drop abouth 6 times though." CreationDate="2020-11-13T21:07:21.913" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="16297" PostId="10386" Score="0" Text="One thing that slightly confuses me here is your comments for when the ray is entering and exiting the object... My understanding is that when the dot product between the ray's direction and the surface normal is positive then the normal is pointing with the ray and the ray is therefore inside the sphere and will be exiting the sphere after the intersection." CreationDate="2020-11-14T12:00:56.107" UserId="14003" ContentLicense="CC BY-SA 4.0" />
  <row Id="16298" PostId="10382" Score="0" Text="Thanks for the tip." CreationDate="2020-11-14T12:01:54.193" UserId="14003" ContentLicense="CC BY-SA 4.0" />
  <row Id="16303" PostId="10395" Score="0" Text="In my shader x are destination texels and y are different directions of samples. I changed local_size_y to 1 so threads don't overlap for each texel and each workgroup works on single y row, but the behaviour is still incorrect." CreationDate="2020-11-15T09:26:45.750" UserId="4958" ContentLicense="CC BY-SA 4.0" />
  <row Id="16304" PostId="10386" Score="0" Text="Oh, I might have flipped those comments on accident. A lot of 3d engines invert the normals, as does mine. But in theory you're right." CreationDate="2020-11-15T13:30:58.383" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16305" PostId="10402" Score="1" Text="Good that you found your mistake :) Please mark your answer as the accepted one so that everybody knows the problem is resolved." CreationDate="2020-11-15T22:33:53.040" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16306" PostId="10402" Score="0" Text="Thanks ! Will do asap. Must wait until tomorrow because privileges low :)" CreationDate="2020-11-15T22:35:01.167" UserId="11224" ContentLicense="CC BY-SA 4.0" />
  <row Id="16308" PostId="10403" Score="0" Text="Normals dont tell how far that plane is. But im sure same ai has a depth estimate too." CreationDate="2020-11-16T05:48:17.233" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16309" PostId="10403" Score="0" Text="yes i have depth maps. so how can i do that if i have the depth information?" CreationDate="2020-11-16T05:59:34.917" UserId="15206" ContentLicense="CC BY-SA 4.0" />
  <row Id="16310" PostId="10403" Score="0" Text="Depth + knowledge of camera FOV is 3D data" CreationDate="2020-11-16T06:14:05.347" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16311" PostId="10404" Score="0" Text="Have you read this?&#xA;https://www.khronos.org/opengl/wiki/Rendering_Pipeline_Overview" CreationDate="2020-11-16T09:59:03.720" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16312" PostId="10406" Score="0" Text="*&quot;and you need to then take the square root,&quot;*&#xA;Why do you need to take the sqrt? If you are comparing the perpendicular distance from centre to ray against the radius, surely one just compares against the square of both values?" CreationDate="2020-11-16T13:26:51.703" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="16313" PostId="10403" Score="0" Text="so in effect the surface normal has no use in this application?" CreationDate="2020-11-16T14:45:31.320" UserId="15206" ContentLicense="CC BY-SA 4.0" />
  <row Id="16314" PostId="10403" Score="0" Text="Well, it can make triangulation easier. Its also useful for a system to validate the data." CreationDate="2020-11-16T15:32:49.373" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16315" PostId="10409" Score="0" Text="Hello, thank you for your exhaustive answer this will very much help me forward!&#xA;Just a short part on the quantization, this means the fragments are not quantized on lets say a pixel cube with 1024x1024x1024 but still use the full precision of loat?" CreationDate="2020-11-16T15:38:39.473" UserId="8757" ContentLicense="CC BY-SA 4.0" />
  <row Id="16316" PostId="3619" Score="0" Text="I have [discussed](https://mcejp.github.io/2020/11/06/bresenham.html) this issue quite in-depth on my blog. The particular rule that I follow is the same as in [Direct3D 10+](https://docs.microsoft.com/en-us/windows/win32/direct3d11/d3d10-graphics-programming-guide-rasterizer-stage-rules)." CreationDate="2020-11-16T14:08:35.710" UserId="15231" ContentLicense="CC BY-SA 4.0" />
  <row Id="16317" PostId="10406" Score="0" Text="@SimonF because of the math to work out an *intersection* between a line and a sphere, you need to solve a quadratic equation which involves a root. We're not just checking distance, we're generating intersection points. There is also another method but even that requires taking a root. I recommend reading this: https://www.scratchapixel.com/lessons/3d-basic-rendering/minimal-ray-tracer-rendering-simple-shapes/ray-sphere-intersection&#xA;&#xA;You'll see there's even more math than I mentioned. Here is the math for a ray-AABB intersection: https://tavianator.com/2011/ray_box.html" CreationDate="2020-11-16T20:02:00.837" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16318" PostId="10409" Score="0" Text="Its better to think of floats as either can be fully represented or can't be fully represented. Any multiple of 2 can be fully represented by a floating point number exactly. Issues with quantization occur when 2 numbers that can not be fully represented are the multiplied together and the error which was minor previously begins to multiply. When generating fragments, the exact positions can always be calculated since texels are computed on boundaries that can be exactly represented inside the values stored on the GPU." CreationDate="2020-11-17T00:27:52.123" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="16319" PostId="10409" Score="0" Text="I recommend writing a small test program that adds a very small amount to a floating point number in a loop allowing it to accumulate, say 0.0001f it will slowly accumulate until the number becomes to large to fully represent. Along the way it will begin to skip certain numbers, before stopping to accumulate all together. There is a really great website that explains really well but I can seem to find a link to it...sorry." CreationDate="2020-11-17T00:38:08.820" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="16321" PostId="10403" Score="0" Text="You still gonna need the normal after you find a point on the plane through the depth map, I think. How else you gonna find the black point then?" CreationDate="2020-11-17T01:12:30.807" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16322" PostId="10413" Score="1" Text="In order to rotate around any arbitrary point, you first move the object to the origin, rotate it, then back to the point where you wanna move it around." CreationDate="2020-11-17T01:15:04.407" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16323" PostId="10403" Score="0" Text="@gallickgunner I actually don't understand why normal + origin (where normal sticks out of) is not enough. I am trying to build a plane (a rectangular region) which I can use for homography. I can visually select four points manually, but I want to automate this process via normals and if necessary, depth maps. Do you have any conceptual advice I can try to work on?" CreationDate="2020-11-17T01:37:59.410" UserId="15206" ContentLicense="CC BY-SA 4.0" />
  <row Id="16324" PostId="10403" Score="0" Text="Well I'm not familiar with homography. But if it's only about picking four points on a plane you need a normal and a point on the plane. You can get a point through the depth map (search for recalculating world space position from depth map values) and you already have the normal. Now all you need to do is find a vector parallel to the plane. Just cross the normal with any vector that's not parallel to the normal and you'll get that. Now add this vector with any length to the point vector (one gotten through depth map) and you have different points on the plane." CreationDate="2020-11-17T02:04:46.483" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16325" PostId="10412" Score="2" Text="consider using one of the online databases to look up that type of info such as this one with give a nice graph showing sizes and distribution: https://vulkan.gpuinfo.org/displaydevicelimit.php?name=pointSizeRange[1]&amp;platform=windows" CreationDate="2020-11-17T02:13:37.313" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="16326" PostId="10413" Score="0" Text="I tried with: translate to middle screen -&gt; rotate -&gt; inverse translate to middle screen -&gt; translate to position but the rotation does not stay in the center when the camera moves" CreationDate="2020-11-17T08:38:03.467" UserId="15233" ContentLicense="CC BY-SA 4.0" />
  <row Id="16327" PostId="10413" Score="0" Text="What exactly is `Position` ? And what exactly do you mean by middle screen? Rotation is always around origin i.e `(0,0,0)`. So you need to first translate to origin then rotate then translate to any arbitrary position you want. Need more clear code and possibly some pictures to understand what problem you are facing." CreationDate="2020-11-17T12:18:54.143" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16328" PostId="10412" Score="0" Text="@pmw1234 I wasn't aware that such a site existed. Thank you!" CreationDate="2020-11-17T16:18:54.720" UserId="15232" ContentLicense="CC BY-SA 4.0" />
  <row Id="16330" PostId="10415" Score="0" Text="What do you mean &quot;more like a heart&quot;? Can you give a drawing or an image or something of how you want it to look?" CreationDate="2020-11-17T18:23:33.037" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16331" PostId="10415" Score="0" Text="Google: Heart ascii graphics." CreationDate="2020-11-17T20:39:13.817" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16332" PostId="10415" Score="0" Text="Related: https://youtu.be/aNR4n0i2ZlM" CreationDate="2020-11-18T00:48:55.497" UserId="9749" ContentLicense="CC BY-SA 4.0" />
  <row Id="16333" PostId="10419" Score="0" Text="That was perfect. Thank you very much. couple of questions: could you elaborate how you did the &quot;simple &quot;levels&quot; color edit&quot;? Did you first made the ground transparent and then adjust the color level?" CreationDate="2020-11-18T11:30:49.960" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="16334" PostId="10419" Score="0" Text="@ali No, I did not do any masking or editing of individual areas. The ground is white because I adjusted the white level so the light gray became white; there's no transparency. In the second image with the restored buildings, I used transparency, which I got by deleting the ground and shadows with a &quot;magic wand&quot; selection tool. For an automated process, you should render it with a transparent background (or a color key, if your raytracer can't output transparency). Remember, that's a *separate render* from the version with shadows. Don't try to make the shadows transparent; that won't help." CreationDate="2020-11-18T14:57:20.973" UserId="445" ContentLicense="CC BY-SA 4.0" />
  <row Id="16335" PostId="10411" Score="0" Text="Make your lights weaker." CreationDate="2020-11-18T18:41:43.280" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16336" PostId="8456" Score="0" Text="See also [Reprojecting Reflections](http://bitsquid.blogspot.com/2017/06/reprojecting-reflections_22.html) on the bitsquid blog." CreationDate="2020-11-18T23:33:34.023" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16337" PostId="10411" Score="0" Text="The shadow and non shadow areas change in same proportion together, when changing the lights intensity." CreationDate="2020-11-19T07:32:29.257" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="16338" PostId="10411" Score="0" Text="As is to be expected. If you want to change only the ratio then you don't have many options except tonemapping." CreationDate="2020-11-19T08:47:42.873" UserId="10096" ContentLicense="CC BY-SA 4.0" />
  <row Id="16340" PostId="10406" Score="0" Text="Perhaps I wasn't clear enough. Most of the scene testing is *rejecting* potential hits against geometry. You don't need the sqrt for that part. The OP was asking why use the AABB of the sphere for a reject test, rather than the sphere itself, and the reject test for a sphere, to my knowledge, doesn't need a sqrt.  (For example, see &quot;An Introduction to Ray Tracing, 1989, Ed: Andrew Glassner)." CreationDate="2020-11-19T09:48:24.563" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="16341" PostId="10416" Score="0" Text="Can you provide a little bit more information about what you did? I think that your current answer is not of much use to somebody who has the same or a related question." CreationDate="2020-11-19T12:33:57.007" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16344" PostId="5104" Score="0" Text="Related effect: [Diffusion Clock](https://www.technoblogy.com/show?3AAM) - &quot;Every minute the display disappears randomly, a dot at a time, to a blank display, and then the new time appears a dot at a time, making it look as if the time diffuses between the two displays.&quot;" CreationDate="2020-11-20T08:53:17.250" UserId="6628" ContentLicense="CC BY-SA 4.0" />
  <row Id="16348" PostId="10422" Score="0" Text="Schouldn't you use `vbo` instead of `vao` in the second `glDeleteBuffers` call?" CreationDate="2020-11-22T11:17:39.390" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16349" PostId="10422" Score="0" Text="@wychmaster Yeah sorry it should be `vbo`" CreationDate="2020-11-22T14:25:32.103" UserId="11058" ContentLicense="CC BY-SA 4.0" />
  <row Id="16350" PostId="10423" Score="1" Text="This is probably not anything to do with the texture, and almost certainly nothing to do with double vs float. Look closely at the edge of the sphere: in the first image it is sharp, and in the second blurry, blending into the background color. This suggests to me that it's the ray distribution on the image plane that's the issue. Is the second image distributing rays over a larger area per pixel, using a wider AA filter, etc?" CreationDate="2020-11-22T17:34:33.440" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16351" PostId="10422" Score="0" Text="@LennyWhite: &quot;*but it doesn't seem to be correct.*&quot; ... why not?" CreationDate="2020-11-23T14:51:36.253" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16352" PostId="10417" Score="1" Text="Do you need to divide by the probablity of taking the sample? In the uniform case it would be divide by 1/2pi?" CreationDate="2020-11-23T16:41:58.600" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="16353" PostId="10417" Score="0" Text="@Peter Thanks for your comment. Why do I have to divide by 1/pi? I know that with importance sampling you need to divide by the PDF (probability distribution function), but I thought with uniform sampling you don't have to?" CreationDate="2020-11-23T18:57:53.317" UserId="6172" ContentLicense="CC BY-SA 4.0" />
  <row Id="16354" PostId="8811" Score="0" Text="What happens when you reduce the render resolution to something very low, like 32 x 32 pixels? What do your shaders look like? How often do you bind textures?" CreationDate="2020-11-24T06:06:24.150" UserId="15262" ContentLicense="CC BY-SA 4.0" />
  <row Id="16355" PostId="8648" Score="0" Text="I think it might be because it is straightforward. In deferred, you &quot;defer&quot; the actual rendering of the full scene (with lights etc.) but in forward rendering, everything is straight&quot;forward&quot;. You just render the actual geometry in a straightforward way, first the vertex shader, then the fragment shader and that's it." CreationDate="2020-11-24T06:13:44.897" UserId="15262" ContentLicense="CC BY-SA 4.0" />
  <row Id="16356" PostId="9324" Score="0" Text="What is the format of your position buffer texture? Also, do you have any reason not to use the depth buffer to reconstruct the position? You would need RGBA32F for your position buffer to not cause artifacts while calculating the lighting. That uses up a lot of bandwidth." CreationDate="2020-11-24T06:20:46.590" UserId="15262" ContentLicense="CC BY-SA 4.0" />
  <row Id="16357" PostId="10417" Score="1" Text="So far your code you have taken random samples and divided by the number of samples. If you think about what this is doing, you are getting an approximation of the average light from each ray hitting the point. However you don't want the ligh of just one ray you want the total light from all directions. Luckily we know that in discrete maths TOTAL = AVERAGE * COUNT, it is also true here but we are collecting over the area of the hemisphere, 2*PI. So to get the total light in you need to multiply your approxmate average by 2*PI. Same as dividing by 1 / (2*PI)." CreationDate="2020-11-24T15:13:34.180" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="16358" PostId="10417" Score="1" Text="Another way to think of it: you are right that in importance sampling you need to divide by the PDF, and uniform sampling is just a special case of importance sampling where every sample is equally important. You would still need to divide by the PDF." CreationDate="2020-11-24T15:17:17.190" UserId="13321" ContentLicense="CC BY-SA 4.0" />
  <row Id="16359" PostId="6193" Score="0" Text="Notch deleted his old tweets, so we'll probably never know..." CreationDate="2020-11-24T21:16:57.003" UserId="8125" ContentLicense="CC BY-SA 4.0" />
  <row Id="16360" PostId="10417" Score="1" Text="Did you look at?: While accumulating specular, the last statement  computes the average from the samples, but it &quot;looks&quot; like some of the samples are effectively zero from the line max(0.0001, dot(N,L)), so the average has to high of a weight...and suggests that many of the random samples are not useful. (since the dot product is near or below zero)." CreationDate="2020-11-25T13:28:13.433" UserId="7952" ContentLicense="CC BY-SA 4.0" />
  <row Id="16361" PostId="10417" Score="0" Text="@pmw1234 Thanks for your comment. The samples are generated inside the hemisphere in the direction of N. So that &quot;max&quot; doesn't have any effect (the dot product won't have negative values). I forgot to remove it when I copy-pasted from my importance sampling implementation." CreationDate="2020-11-25T13:54:57.983" UserId="6172" ContentLicense="CC BY-SA 4.0" />
  <row Id="16362" PostId="10432" Score="0" Text="Thank you for your answer. Unfortunately I think that the geometry modification is actually the crux of the problem. But I will take a look!" CreationDate="2020-11-26T05:07:58.143" UserId="15275" ContentLicense="CC BY-SA 4.0" />
  <row Id="16363" PostId="10432" Score="0" Text="Maybe you could add to your question some more details about what kind of geometry modifications you want to do? People might be able to recommend something." CreationDate="2020-11-26T19:52:43.620" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16364" PostId="6193" Score="0" Text="Deve Strawn has compiled [a pretty good summary](https://www.devonstrawn.com/articles/Notchs-rendering-experiments.html) though." CreationDate="2020-11-26T22:23:25.520" UserId="8125" ContentLicense="CC BY-SA 4.0" />
  <row Id="16365" PostId="10433" Score="0" Text="Minor nitpick stackexhange is not a forum. Its stictly a Q and A site. Anyway i think op whants to know how to unproject." CreationDate="2020-11-27T06:42:12.057" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16366" PostId="10436" Score="0" Text="I looked at the video but it's not clear to me why, when using the &quot;mark surface&quot; tool, the selection encompasses the flanges and doesn't go any further. It doesn't seem like it's simply curvature or angle based since why would it grab the flanges in the first place then? I doubt if this can be answered without being more specific about what the desired behavior is." CreationDate="2020-11-27T06:48:13.290" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16367" PostId="10432" Score="0" Text="I have updated my question accordingly." CreationDate="2020-11-27T07:31:06.487" UserId="15275" ContentLicense="CC BY-SA 4.0" />
  <row Id="16368" PostId="10436" Score="0" Text="Nathan, I completely agree with you, I don't have access to magics so the only thing I can do is to speculate... I guess the whole key revolves around the concept of curvature but I don't know . That's why I asked here in the first place in case somebody familiar with a similar algo." CreationDate="2020-11-27T09:18:29.547" UserDisplayName="user4801" ContentLicense="CC BY-SA 4.0" />
  <row Id="16369" PostId="10431" Score="2" Text="This sounds like an animation issue. That is, the light would affect the geometry you would use *next* frame, not on the current frame. And you'd need to store the changes to the geometry from frame to frame, lest the removal of light cause the geometry to repair itself." CreationDate="2020-11-27T14:55:12.000" UserId="2654" ContentLicense="CC BY-SA 4.0" />
  <row Id="16370" PostId="10431" Score="0" Text="@NicolBolas Yeah that sounds right. Right now everything is done exactly but very slow, so basically if this whole problem could be mostly solved &quot;efficiently&quot; using some well-known graphics library we would save a lot of trouble, so that's what I was hoping for." CreationDate="2020-11-27T20:23:23.467" UserId="15275" ContentLicense="CC BY-SA 4.0" />
  <row Id="16371" PostId="10432" Score="0" Text="Hi @ChristopherA.Wong. You are right to prefer not to reinvent the wheel. A library like Mistuba that can not only render an image, but also as a by-product tell how much light some materials receive, in a physically accurate fashion, looks like a very strong asset for you. It can probably produce accurately the precise information your specific application needs to know how to change the geometry (e.g., melting wax). Writing your code like: make a mesh -&gt; get an image + illumination info -&gt; your_plugin_that changes_geometry -&gt; new mesh, loop, looks like a sane and strong approach." CreationDate="2020-11-28T19:27:31.770" UserId="8776" ContentLicense="CC BY-SA 4.0" />
  <row Id="16372" PostId="10439" Score="2" Text="Because we don't have the compute power to simulate an entire city worth of molecules and their physical interactions real time. Until then we will have to make do with approximations, that while good, are still approximations." CreationDate="2020-11-29T14:03:26.103" UserId="10615" ContentLicense="CC BY-SA 4.0" />
  <row Id="16373" PostId="10439" Score="2" Text="Just to nitpick a bit, a whole lot of what you see around you is 3D simulation that works. There is a lot more CGI thats entirely invisible for people on TV and movies in scenes that you dont expect. Yes sometimes they fail in the harder shots but still you dont notice. Did you know that most stuff you see in a Ikea catalog is a render? Well the answer is we arent missing anything as such just ant do it fast enough for realtime renders." CreationDate="2020-11-29T15:35:00.437" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16374" PostId="10439" Score="0" Text="Fine-scale geometry, lighting, and texture details are a big part of it. For instance, in the thumbnail frame of the Forza video, what jumps out to me as unrealistic is the comparatively low-res textures and geometry of the building and sidewalk at left. You can see that they are flat images stretched across boxes. The car is a lot more realistic but you can still see a lack of material detail around the bumper, tailpipes, and tires." CreationDate="2020-11-29T23:41:20.557" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16375" PostId="10433" Score="0" Text="The link has information on camera calibration, unprojection, and using solvepnp for the system of equations. I am sorry for not providing a proper answer." CreationDate="2020-11-30T16:18:23.437" UserId="4768" ContentLicense="CC BY-SA 4.0" />
  <row Id="16376" PostId="10445" Score="1" Text="Judging by your question, you don't know all too much about shaders. If you want to get into shader programming, I suggest using a few tutorials, as they explain the basics nicely.&#xA;&#xA;That being said, you probably want your object to be greenn but shaded / lighted. So rather than changing your shader, you need to change the input to your shader. In your PhongPixel method, there is the `return` line at the end, which contains `AmbientColor`, `DiffuseColor` and `SpecularColor`. These are inputs to the shader, so wherever they are set, you need to change the inputs to your green color." CreationDate="2020-12-01T06:30:40.063" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="16377" PostId="10445" Score="1" Text="Also note that in the very beginning of your code it says `// Parameters that should be set from the program` and then lists a bunch of variables to do this for. In case I misunderstood and you want everything to turn green, you need to multiply the current result of your `return` line with something like `float4(1.0, 0.0, 0.0,1 .0)`." CreationDate="2020-12-01T06:33:29.727" UserId="7008" ContentLicense="CC BY-SA 4.0" />
  <row Id="16378" PostId="10442" Score="0" Text="Can you describe the problem a little bit more? I am not sure if I am understanding you correctly. Do you want to take an arbitrary 3d mesh and approximate it with voxels?" CreationDate="2020-12-01T08:56:25.297" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16379" PostId="10446" Score="0" Text="if you can compare check the result of `pvw * p1` in DirectX with the same values. If its same as your `wvp * p1` then your implementation is opposite. If it's not the same, then your implementation might be fine and the problem might be somewhere else I guess." CreationDate="2020-12-01T10:53:08.143" UserId="6046" ContentLicense="CC BY-SA 4.0" />
  <row Id="16380" PostId="10446" Score="1" Text="In matrix mathematics transposing the matrix reverses the calculation order. So $A * B = ( B^T * A^T)^T$" CreationDate="2020-12-01T12:55:47.760" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16381" PostId="10446" Score="0" Text="I saw a mention of that in another thread, but I don't believe I'm transposing the matrices anywhere. Does DirectX or OpenGl implicitly transpose these matrices somewhere?" CreationDate="2020-12-02T03:22:56.837" UserId="15310" ContentLicense="CC BY-SA 4.0" />
  <row Id="16382" PostId="10442" Score="0" Text="Yes. I want to calculate minimal voxel size, as to not lose any details in the mesh" CreationDate="2020-12-02T09:23:50.780" UserId="15305" ContentLicense="CC BY-SA 4.0" />
  <row Id="16383" PostId="8270" Score="1" Text="Correcting M−1UB.MCR I've tested the matrices using renderings, the top right and bottom left values of the matrix are wrong, they should be negative." CreationDate="2020-12-02T03:03:32.287" UserId="15316" ContentLicense="CC BY-SA 4.0" />
  <row Id="16384" PostId="10100" Score="0" Text="@ChristianPagot You say that by using space subdivision we can stop traversal as soon as a surface is hit. Is that really true though? If you have a very big triangle that spans more than one volume/node, a ray may intercept that triangle while there may actually be a occluding triangle in front, but in another volume, which wouldn't be caught if we stopped on the first intersection." CreationDate="2020-12-02T12:03:09.927" UserId="15318" ContentLicense="CC BY-SA 4.0" />
  <row Id="16385" PostId="10423" Score="1" Text="@NathanReed Thanks for  the response. Indeed it turned out to be the camera aperture: on the GPU it was 0.1f whereas on the CPU 0.0f. If you like to provide your input as an &quot;answer&quot;, I will gladly mark it so." CreationDate="2020-12-02T17:31:53.933" UserId="6496" ContentLicense="CC BY-SA 4.0" />
  <row Id="16386" PostId="10423" Score="0" Text="From Ray Tracing in One Weekend:For our virtual camera, we can have a perfect sensor and never need&#xA;more light, so we only have an aperture when we want defocus blur." CreationDate="2020-12-02T18:45:56.870" UserId="6496" ContentLicense="CC BY-SA 4.0" />
  <row Id="16387" PostId="10457" Score="0" Text="Thank you very much! But I still wonder what exactly do the values following the `vt` command mean? For example, in the updated question, the line `vt 0.000000 1.000000 0.000000` refers to which point in the image? Are the coordinates of the pixels in a image normalized to be in [0,1]? I think I've also seen `vt` coordinates to be outside [0,1], what do those mean?" CreationDate="2020-12-03T06:07:51.143" UserId="15320" ContentLicense="CC BY-SA 4.0" />
  <row Id="16388" PostId="10457" Score="1" Text="@trisct normalized yes. Values outside just repeat the texture useful for tiling etc." CreationDate="2020-12-03T06:23:51.130" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16389" PostId="8270" Score="0" Text="My apologies.. I must have mistyped copying the values when I used Maple to do the calculations. The error is more than just signs!  Will fix ASAP" CreationDate="2020-12-03T08:54:31.490" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="16391" PostId="10100" Score="0" Text="@andre_ss6 In space subdivision schemes there is no overlap between subspaces, and we can usually traverse them in a prescribed order (e.g. front to back).In this case, according to your example, the triangle that is in front, in another volume (subspace) would be tested first. An explanation regarding kD-Tree traversal can be found here http://www.pbr-book.org/3ed-2018/Primitives_and_Intersection_Acceleration/Kd-Tree_Accelerator.html and here https://graphics.stanford.edu/papers/gpu_kdtree/kdtree.pdf" CreationDate="2020-12-03T13:44:46.083" UserId="5681" ContentLicense="CC BY-SA 4.0" />
  <row Id="16392" PostId="10458" Score="0" Text="*&quot;Draw all your objects &quot;looking from the top&quot; and populate your *stencil* buffer&quot;*&#xA;Did you mean &quot;Z buffer&quot;?" CreationDate="2020-12-03T14:57:03.757" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="16393" PostId="10458" Score="1" Text="@SimonF I actually meant the stencil buffer. After rendering all the objects, the stencil buffer should contain basically a `true` or `false` value for each pixel. However, you still need to transform the data to the camera perspective which most likely requires creating a texture from it or populating the z-buffer. In the latter case, it is probably much easier to use the standard shadow mapping technique. In fact, I think shadow mapping is generally the best approach for this purpose. It's just that the word silhouette always makes me automatically think about stencil testing." CreationDate="2020-12-03T16:31:41.713" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16394" PostId="10458" Score="0" Text="After some time of thinking, I think this whole stencil buffer thing is unnecessary and makes everything overly complicated. I'll update my answer as soon as I find the time." CreationDate="2020-12-03T17:35:11.663" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16395" PostId="10458" Score="0" Text="I'm reasonably familiar with shadow volume approaches (from development of PowerVR series 1 and 2 (Dreamcast) which had optimised support for volumes) but what you were describing sounded more like a shadow buffer approach, ie. rendering from the light's view." CreationDate="2020-12-04T08:41:31.740" UserId="209" ContentLicense="CC BY-SA 4.0" />
  <row Id="16396" PostId="10458" Score="1" Text="@SimonF I already updated my answer and you are right, what I described was a specialized shadow mapping approach and I never meant it to be something else. However, in my original approach, I unnecessarily wanted to use the stencil buffer to cut out the silhouettes. But it has nothing to do with the stencil buffer based shadow volumes technique. I think this is where the confusion came from." CreationDate="2020-12-04T08:53:48.097" UserId="13095" ContentLicense="CC BY-SA 4.0" />
  <row Id="16397" PostId="10446" Score="0" Text="You dont have to transpose a matrix to get the effect. All you have to do is build the matrix differently with different assumptions. A column major build order results in a matrix that is seeminly transposed version of a row major matrix. So all that needs to happen is that your not aware of the convention used. Buth are mathematically equivalent and as correct. I mean  you could also have a convention where the translate part is first, or second and not last like is customary. But no one does that. But its entirely calid to use different building rule" CreationDate="2020-12-04T16:34:22.727" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16398" PostId="10446" Score="0" Text="I mean its entirely valid to think from world to object than it is from object to world. Mathematicians prefer inside out because it follows the logic of function composition but you dont have to its natural for humans to think of other objects from outside in." CreationDate="2020-12-04T16:39:41.977" UserId="38" ContentLicense="CC BY-SA 4.0" />
  <row Id="16399" PostId="10459" Score="0" Text="If you mean the band near the horizon that is intended. The ray of light coming from the horizon travels a longer distance and so more of its blue light is scattered and the red light is left." CreationDate="2020-12-04T18:15:04.970" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="16400" PostId="10459" Score="0" Text="Have a look at here https://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/simulating-sky/simulating-colors-of-the-sky" CreationDate="2020-12-04T18:15:16.137" UserId="6041" ContentLicense="CC BY-SA 4.0" />
  <row Id="16401" PostId="10459" Score="0" Text="From the screenshots it looks like the artifact may be located at an altitude equal to the camera. There may be something in the math that is depending on the sign of of the ray direction (whether it's traveling up or down relative to the planet) or something that's numerically unstable there. I would try to isolate where the artifact is coming from by commenting out different parts of the code / different lighting components, and check whether the artifact is still produced. That should help you narrow down what part of the math is creating it." CreationDate="2020-12-04T19:15:43.980" UserId="48" ContentLicense="CC BY-SA 4.0" />
  <row Id="16402" PostId="10459" Score="0" Text="@NathanReed that is actually a very good observation, thanks! I will start debugging the shader. If you get anymore insights please let me know. Thanks again :P" CreationDate="2020-12-04T22:28:26.153" UserId="15325" ContentLicense="CC BY-SA 4.0" />
  <row Id="16403" PostId="10452" Score="0" Text="You have computed the NDC size, to finish your computation you have to now compute its screen space size. Searching for &quot;compute screen space size of object glsl&quot; get lots of hits and good examples" CreationDate="2020-12-06T01:07:07.923" UserId="7952" ContentLicense="CC BY-SA 4.0" />
</comments>